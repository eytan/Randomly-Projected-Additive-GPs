{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import rp_experiments\n",
    "import training_routines\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "import json\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py:184: RuntimeWarning: Mean of empty slice.\n",
      "  ma[i] = losses[i-patience+1:i+1].mean()\n",
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ian/gpytorch/gpytorch/models/exact_gp.py:190: UserWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  \"The input matches the stored training data. Did you forget to call model.train()?\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:02:49.089445, fold=0, rep=0, eta=0d 0h 4m 6s \n",
      "{'fold': 0, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.539741039276123, 'train_time': 12.968127058004029, 'prior_train_nmll': 1.5086475610733032, 'train_nll': 21.77501678466797, 'test_nll': 3.524848222732544, 'train_mse': 0.42348822951316833, 'state_dict_file': 'model_state_dict_5481701310660915877.pkl'}\n",
      "2019-03-16 15:03:00.186879, fold=0, rep=1, eta=0d 0h 3m 36s \n",
      "{'fold': 0, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.19903410971164703, 'train_time': 11.097243868978694, 'prior_train_nmll': 1.504563570022583, 'train_nll': 20.27582550048828, 'test_nll': 2.9719150066375732, 'train_mse': 0.3344864249229431, 'state_dict_file': 'model_state_dict_-2282922694711208705.pkl'}\n",
      "2019-03-16 15:03:09.763946, fold=1, rep=0, eta=0d 0h 3m 10s \n",
      "{'fold': 1, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.5020298361778259, 'train_time': 9.572137424955145, 'prior_train_nmll': 1.4897948503494263, 'train_nll': 11.776873588562012, 'test_nll': 3.2702648639678955, 'train_mse': 0.09221820533275604, 'state_dict_file': 'model_state_dict_7033948814869623064.pkl'}\n",
      "2019-03-16 15:03:21.731063, fold=1, rep=1, eta=0d 0h 3m 2s \n",
      "{'fold': 1, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.34153851866722107, 'train_time': 11.966880692983977, 'prior_train_nmll': 1.470295786857605, 'train_nll': 12.658514022827148, 'test_nll': 2.866973876953125, 'train_mse': 0.11608357727527618, 'state_dict_file': 'model_state_dict_1226232652619179556.pkl'}\n",
      "2019-03-16 15:03:31.400950, fold=2, rep=0, eta=0d 0h 2m 45s \n",
      "{'fold': 2, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.875917911529541, 'train_time': 9.665900059044361, 'prior_train_nmll': 1.4607353210449219, 'train_nll': 18.231178283691406, 'test_nll': 4.275941848754883, 'train_mse': 0.2515183687210083, 'state_dict_file': 'model_state_dict_4183148556585905819.pkl'}\n",
      "2019-03-16 15:03:40.961736, fold=2, rep=1, eta=0d 0h 2m 31s \n",
      "{'fold': 2, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.7476749420166016, 'train_time': 9.560563714941964, 'prior_train_nmll': 1.4689013957977295, 'train_nll': 19.264476776123047, 'test_nll': 3.934051275253296, 'train_mse': 0.2995811104774475, 'state_dict_file': 'model_state_dict_5635698873257492025.pkl'}\n",
      "2019-03-16 15:03:53.292640, fold=3, rep=0, eta=0d 0h 2m 23s \n",
      "{'fold': 3, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 1.9047565460205078, 'train_time': 12.325028069899417, 'prior_train_nmll': 1.411368727684021, 'train_nll': 6.062746047973633, 'test_nll': 5.6391520500183105, 'train_mse': 0.03723924234509468, 'state_dict_file': 'model_state_dict_-2090655822804753852.pkl'}\n",
      "2019-03-16 15:04:07.558766, fold=3, rep=1, eta=0d 0h 2m 17s \n",
      "{'fold': 3, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 1.9637326002120972, 'train_time': 14.265895630931482, 'prior_train_nmll': 1.3612067699432373, 'train_nll': 14.118461608886719, 'test_nll': 6.173206329345703, 'train_mse': 0.14822739362716675, 'state_dict_file': 'model_state_dict_-139825982313006691.pkl'}\n",
      "2019-03-16 15:04:12.112347, fold=4, rep=0, eta=0d 0h 1m 57s \n",
      "{'fold': 4, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 3.1678683757781982, 'train_time': 4.548558574984781, 'prior_train_nmll': 1.1562132835388184, 'train_nll': 0.20898056030273438, 'test_nll': 29.2530574798584, 'train_mse': 0.02061283215880394, 'state_dict_file': 'model_state_dict_-9022249589547738386.pkl'}\n",
      "2019-03-16 15:04:16.686932, fold=4, rep=1, eta=0d 0h 1m 40s \n",
      "{'fold': 4, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 3.7492246627807617, 'train_time': 4.574275991995819, 'prior_train_nmll': 1.0731585025787354, 'train_nll': -1.7145805358886719, 'test_nll': 50.182594299316406, 'train_mse': 0.017721785232424736, 'state_dict_file': 'model_state_dict_-1101573603144495157.pkl'}\n",
      "2019-03-16 15:04:24.021874, fold=5, rep=0, eta=0d 0h 1m 28s \n",
      "{'fold': 5, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.220115527510643, 'train_time': 7.330599689972587, 'prior_train_nmll': 1.520897626876831, 'train_nll': 18.10640525817871, 'test_nll': 2.7005624771118164, 'train_mse': 0.23199263215065002, 'state_dict_file': 'model_state_dict_-7117110953417941374.pkl'}\n",
      "2019-03-16 15:04:32.465532, fold=5, rep=1, eta=0d 0h 1m 17s \n",
      "{'fold': 5, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.21240663528442383, 'train_time': 8.443434390006587, 'prior_train_nmll': 1.4965742826461792, 'train_nll': 18.584548950195312, 'test_nll': 2.634784698486328, 'train_mse': 0.266084760427475, 'state_dict_file': 'model_state_dict_6151956646786945680.pkl'}\n",
      "2019-03-16 15:04:39.849303, fold=6, rep=0, eta=0d 0h 1m 6s \n",
      "{'fold': 6, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 1.1073025465011597, 'train_time': 7.378614210989326, 'prior_train_nmll': 1.4791359901428223, 'train_nll': 14.548095703125, 'test_nll': 4.660927772521973, 'train_mse': 0.1484179049730301, 'state_dict_file': 'model_state_dict_-2583685294879070532.pkl'}\n",
      "2019-03-16 15:04:47.017456, fold=6, rep=1, eta=0d 0h 0m 56s \n",
      "{'fold': 6, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.7891602516174316, 'train_time': 7.167929287999868, 'prior_train_nmll': 1.474351167678833, 'train_nll': 20.349529266357422, 'test_nll': 3.9449942111968994, 'train_mse': 0.35370272397994995, 'state_dict_file': 'model_state_dict_-4830500014261765795.pkl'}\n",
      "2019-03-16 15:05:01.094708, fold=7, rep=0, eta=0d 0h 0m 48s \n",
      "{'fold': 7, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.22581282258033752, 'train_time': 14.071898718946613, 'prior_train_nmll': 1.432175636291504, 'train_nll': 17.55820655822754, 'test_nll': 1.8668408393859863, 'train_mse': 0.21040408313274384, 'state_dict_file': 'model_state_dict_-5094700764478834962.pkl'}\n",
      "2019-03-16 15:05:12.826049, fold=7, rep=1, eta=0d 0h 0m 39s \n",
      "{'fold': 7, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.2091527283191681, 'train_time': 11.731153319007717, 'prior_train_nmll': 1.4728076457977295, 'train_nll': 18.197843551635742, 'test_nll': 1.8587431907653809, 'train_mse': 0.21883925795555115, 'state_dict_file': 'model_state_dict_-9199351250203808717.pkl'}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:08:25.327567, fold=0, rep=0, eta=0d 0h 5m 40s \n",
      "{'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.4536058902740479, 'train_time': 17.900123110972345, 'prior_train_nmll': 1.0528591871261597, 'train_nll': -1.2246627807617188, 'test_nll': 31.79458236694336, 'train_mse': 0.0164335910230875, 'state_dict_file': 'model_state_dict_2735516277037853461.pkl'}\n",
      "2019-03-16 15:08:45.384507, fold=0, rep=1, eta=0d 0h 5m 41s \n",
      "{'fold': 0, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.2620031833648682, 'train_time': 20.056727370945737, 'prior_train_nmll': 1.0191502571105957, 'train_nll': -6.0626678466796875, 'test_nll': 29.829910278320312, 'train_mse': 0.013760607689619064, 'state_dict_file': 'model_state_dict_3885118029543572562.pkl'}\n",
      "2019-03-16 15:08:57.628880, fold=1, rep=0, eta=0d 0h 4m 44s \n",
      "{'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1141897439956665, 'train_time': 12.241445621009916, 'prior_train_nmll': 1.0472252368927002, 'train_nll': 19.362468719482422, 'test_nll': 20.233734130859375, 'train_mse': 0.03740076348185539, 'state_dict_file': 'model_state_dict_-665659854449504531.pkl'}\n",
      "2019-03-16 15:09:14.573136, fold=1, rep=1, eta=0d 0h 4m 28s \n",
      "{'fold': 1, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.4251911640167236, 'train_time': 16.944036686909385, 'prior_train_nmll': 1.0802438259124756, 'train_nll': 21.08556365966797, 'test_nll': 20.468101501464844, 'train_mse': 0.0377255342900753, 'state_dict_file': 'model_state_dict_4170868110569443453.pkl'}\n",
      "2019-03-16 15:09:29.448768, fold=2, rep=0, eta=0d 0h 4m 6s \n",
      "{'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.6070430278778076, 'train_time': 14.872497632051818, 'prior_train_nmll': 0.9888104796409607, 'train_nll': 17.6124267578125, 'test_nll': 30.117900848388672, 'train_mse': 0.03714125603437424, 'state_dict_file': 'model_state_dict_2075754474969790559.pkl'}\n",
      "2019-03-16 15:09:45.353428, fold=2, rep=1, eta=0d 0h 3m 48s \n",
      "{'fold': 2, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.696716547012329, 'train_time': 15.904438728000969, 'prior_train_nmll': 0.9993088841438293, 'train_nll': 17.137527465820312, 'test_nll': 32.198097229003906, 'train_mse': 0.03623097762465477, 'state_dict_file': 'model_state_dict_7701855412881109887.pkl'}\n",
      "2019-03-16 15:09:57.250530, fold=3, rep=0, eta=0d 0h 3m 23s \n",
      "{'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2124135494232178, 'train_time': 11.893827205058187, 'prior_train_nmll': 1.0946540832519531, 'train_nll': 20.04468536376953, 'test_nll': 22.83242416381836, 'train_mse': 0.036635056138038635, 'state_dict_file': 'model_state_dict_-2822867626615900893.pkl'}\n",
      "2019-03-16 15:10:12.712355, fold=3, rep=1, eta=0d 0h 3m 7s \n",
      "{'fold': 3, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.998009204864502, 'train_time': 15.461602723109536, 'prior_train_nmll': 1.0535153150558472, 'train_nll': 21.60326385498047, 'test_nll': 20.097370147705078, 'train_mse': 0.03785321116447449, 'state_dict_file': 'model_state_dict_3154871282069635497.pkl'}\n",
      "2019-03-16 15:10:26.031205, fold=4, rep=0, eta=0d 0h 2m 49s \n",
      "{'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.8086762428283691, 'train_time': 13.315338790067472, 'prior_train_nmll': 1.0744892358779907, 'train_nll': 20.72333526611328, 'test_nll': 14.278376579284668, 'train_mse': 0.03716084733605385, 'state_dict_file': 'model_state_dict_-181657369454269136.pkl'}\n",
      "2019-03-16 15:10:38.032065, fold=4, rep=1, eta=0d 0h 2m 30s \n",
      "{'fold': 4, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.6427204012870789, 'train_time': 12.000615125056356, 'prior_train_nmll': 1.0594358444213867, 'train_nll': 20.9952392578125, 'test_nll': 13.741243362426758, 'train_mse': 0.03762785717844963, 'state_dict_file': 'model_state_dict_2527346664171458536.pkl'}\n",
      "2019-03-16 15:10:50.120068, fold=5, rep=0, eta=0d 0h 2m 13s \n",
      "{'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2778680324554443, 'train_time': 12.08456159895286, 'prior_train_nmll': 1.0818887948989868, 'train_nll': 13.418785095214844, 'test_nll': 23.254024505615234, 'train_mse': 0.027772871777415276, 'state_dict_file': 'model_state_dict_2366321473454518303.pkl'}\n",
      "2019-03-16 15:11:03.243500, fold=5, rep=1, eta=0d 0h 1m 57s \n",
      "{'fold': 5, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.1053333282470703, 'train_time': 13.123203765950166, 'prior_train_nmll': 0.9840680360794067, 'train_nll': 7.906623840332031, 'test_nll': 24.36137580871582, 'train_mse': 0.025125376880168915, 'state_dict_file': 'model_state_dict_3317951095216557727.pkl'}\n",
      "2019-03-16 15:11:21.152761, fold=6, rep=0, eta=0d 0h 1m 44s \n",
      "{'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3475751876831055, 'train_time': 17.90623406402301, 'prior_train_nmll': 1.0019818544387817, 'train_nll': -3.2494049072265625, 'test_nll': 28.381027221679688, 'train_mse': 0.015490429475903511, 'state_dict_file': 'model_state_dict_6706300528422674956.pkl'}\n",
      "2019-03-16 15:11:35.539381, fold=6, rep=1, eta=0d 0h 1m 29s \n",
      "{'fold': 6, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.474230408668518, 'train_time': 14.386161755071953, 'prior_train_nmll': 1.0318595170974731, 'train_nll': -2.0582962036132812, 'test_nll': 30.265037536621094, 'train_mse': 0.01628510281443596, 'state_dict_file': 'model_state_dict_-5471822538068318969.pkl'}\n",
      "2019-03-16 15:11:51.257028, fold=7, rep=0, eta=0d 0h 1m 14s \n",
      "{'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.742012083530426, 'train_time': 15.714212808990851, 'prior_train_nmll': 1.1136126518249512, 'train_nll': 23.209884643554688, 'test_nll': 14.170076370239258, 'train_mse': 0.0387934111058712, 'state_dict_file': 'model_state_dict_-2572019818479950011.pkl'}\n",
      "2019-03-16 15:12:09.699274, fold=7, rep=1, eta=0d 0h 1m 0s \n",
      "{'fold': 7, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.5658707022666931, 'train_time': 18.441998352995142, 'prior_train_nmll': 1.0989738702774048, 'train_nll': 21.99810028076172, 'test_nll': 12.246196746826172, 'train_mse': 0.03614334762096405, 'state_dict_file': 'model_state_dict_2937591179076678606.pkl'}\n",
      "2019-03-16 15:12:25.519940, fold=8, rep=0, eta=0d 0h 0m 45s \n",
      "{'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 2.1262354850769043, 'train_time': 15.817443760926835, 'prior_train_nmll': 1.0536372661590576, 'train_nll': 20.265350341796875, 'test_nll': 32.458744049072266, 'train_mse': 0.038448575884103775, 'state_dict_file': 'model_state_dict_2119607228814835471.pkl'}\n",
      "2019-03-16 15:12:42.927706, fold=8, rep=1, eta=0d 0h 0m 30s \n",
      "{'fold': 8, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.4746137857437134, 'train_time': 17.40755514195189, 'prior_train_nmll': 1.01187002658844, 'train_nll': 19.261600494384766, 'test_nll': 27.058696746826172, 'train_mse': 0.03761899098753929, 'state_dict_file': 'model_state_dict_-3258460892715794207.pkl'}\n",
      "2019-03-16 15:13:00.158603, fold=9, rep=0, eta=0d 0h 0m 15s \n",
      "{'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.5440942645072937, 'train_time': 17.22728738700971, 'prior_train_nmll': 1.090135097503662, 'train_nll': 21.13345718383789, 'test_nll': 11.32058334350586, 'train_mse': 0.038186196237802505, 'state_dict_file': 'model_state_dict_2633690366260352703.pkl'}\n",
      "2019-03-16 15:13:12.951591, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.7205531001091003, 'train_time': 12.792775274021551, 'prior_train_nmll': 1.0993486642837524, 'train_nll': 24.524723052978516, 'test_nll': 14.556057929992676, 'train_mse': 0.038929082453250885, 'state_dict_file': 'model_state_dict_6325980663460822712.pkl'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/ipykernel_launcher.py:15: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:13:24.418215, fold=0, rep=0, eta=0d 0h 3m 37s \n",
      "{'fold': 0, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.06646811217069626, 'train_time': 11.443595984950662, 'prior_train_nmll': -0.03912442550063133, 'train_nll': -421.0924072265625, 'test_nll': 1.8512744903564453, 'train_mse': 7.906831160653383e-05, 'state_dict_file': 'model_state_dict_7111611273834999375.pkl'}\n",
      "2019-03-16 15:13:33.106216, fold=0, rep=1, eta=0d 0h 3m 1s \n",
      "{'fold': 0, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.01964564435184002, 'train_time': 8.687782558030449, 'prior_train_nmll': 0.0731954351067543, 'train_nll': -489.7598876953125, 'test_nll': -6.138891220092773, 'train_mse': 9.490145748713985e-05, 'state_dict_file': 'model_state_dict_6645346290150733212.pkl'}\n",
      "2019-03-16 15:13:45.037882, fold=1, rep=0, eta=0d 0h 3m 1s \n",
      "{'fold': 1, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.02842322736978531, 'train_time': 11.928161144955084, 'prior_train_nmll': -0.12236659228801727, 'train_nll': -491.32342529296875, 'test_nll': -8.587963104248047, 'train_mse': 9.857933036983013e-05, 'state_dict_file': 'model_state_dict_2896289302706431466.pkl'}\n",
      "2019-03-16 15:13:55.351874, fold=1, rep=1, eta=0d 0h 2m 49s \n",
      "{'fold': 1, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.04248085245490074, 'train_time': 10.313765114056878, 'prior_train_nmll': -0.23038139939308167, 'train_nll': 32659.892578125, 'test_nll': -4.005521774291992, 'train_mse': 0.0001137209328589961, 'state_dict_file': 'model_state_dict_2962023097274472735.pkl'}\n",
      "2019-03-16 15:14:05.327756, fold=2, rep=0, eta=0d 0h 2m 37s \n",
      "{'fold': 2, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.007843798026442528, 'train_time': 9.972585230018012, 'prior_train_nmll': -0.09697945415973663, 'train_nll': -479.21746826171875, 'test_nll': -13.310068130493164, 'train_mse': 0.0001251647772733122, 'state_dict_file': 'model_state_dict_7036536336670674910.pkl'}\n",
      "2019-03-16 15:14:15.763127, fold=2, rep=1, eta=0d 0h 2m 26s \n",
      "{'fold': 2, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.006345918867737055, 'train_time': 10.435079991002567, 'prior_train_nmll': -0.011826950125396252, 'train_nll': -523.5498657226562, 'test_nll': -11.795454025268555, 'train_mse': 0.00010576523345662281, 'state_dict_file': 'model_state_dict_-3336394290217259031.pkl'}\n",
      "2019-03-16 15:14:26.828311, fold=3, rep=0, eta=0d 0h 2m 17s \n",
      "{'fold': 3, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.011784224770963192, 'train_time': 11.061341812950559, 'prior_train_nmll': -0.11200305819511414, 'train_nll': -481.75994873046875, 'test_nll': -11.94619369506836, 'train_mse': 7.629208994330838e-05, 'state_dict_file': 'model_state_dict_8173414427730786542.pkl'}\n",
      "2019-03-16 15:14:35.682577, fold=3, rep=1, eta=0d 0h 2m 4s \n",
      "{'fold': 3, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.011318080127239227, 'train_time': 8.854043095023371, 'prior_train_nmll': -0.08734975010156631, 'train_nll': -424.96746826171875, 'test_nll': -10.084842681884766, 'train_mse': 8.505758160026744e-05, 'state_dict_file': 'model_state_dict_2248549458607619936.pkl'}\n",
      "2019-03-16 15:14:45.367028, fold=4, rep=0, eta=0d 0h 1m 52s \n",
      "{'fold': 4, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.004708151798695326, 'train_time': 9.680957228061743, 'prior_train_nmll': -0.14103274047374725, 'train_nll': -563.3156127929688, 'test_nll': -13.024282455444336, 'train_mse': 8.465926657663658e-05, 'state_dict_file': 'model_state_dict_-8500811981912954067.pkl'}\n",
      "2019-03-16 15:14:59.409340, fold=4, rep=1, eta=0d 0h 1m 46s \n",
      "{'fold': 4, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.004253069870173931, 'train_time': 14.04208273801487, 'prior_train_nmll': -0.006590200588107109, 'train_nll': -412.6546630859375, 'test_nll': -11.984121322631836, 'train_mse': 8.314786828123033e-05, 'state_dict_file': 'model_state_dict_-379746806537483363.pkl'}\n",
      "2019-03-16 15:15:12.524035, fold=5, rep=0, eta=0d 0h 1m 37s \n",
      "{'fold': 5, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.012241813354194164, 'train_time': 13.111360101029277, 'prior_train_nmll': -0.0015564586501568556, 'train_nll': -570.0518188476562, 'test_nll': -10.324579238891602, 'train_mse': 0.00010650255717337132, 'state_dict_file': 'model_state_dict_-3970493034372201366.pkl'}\n",
      "2019-03-16 15:15:23.906926, fold=5, rep=1, eta=0d 0h 1m 27s \n",
      "{'fold': 5, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.021960170939564705, 'train_time': 11.382673728978261, 'prior_train_nmll': -0.09339595586061478, 'train_nll': -482.41107177734375, 'test_nll': -12.937227249145508, 'train_mse': 0.00012702947424259037, 'state_dict_file': 'model_state_dict_3964469567244374108.pkl'}\n",
      "2019-03-16 15:15:35.612329, fold=6, rep=0, eta=0d 0h 1m 16s \n",
      "{'fold': 6, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.017526449635624886, 'train_time': 11.701983125065453, 'prior_train_nmll': -0.006961884908378124, 'train_nll': -470.88592529296875, 'test_nll': -7.723653793334961, 'train_mse': 7.10674503352493e-05, 'state_dict_file': 'model_state_dict_9171712479154756156.pkl'}\n",
      "2019-03-16 15:15:46.855738, fold=6, rep=1, eta=0d 0h 1m 5s \n",
      "{'fold': 6, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.012603861279785633, 'train_time': 11.243186621111818, 'prior_train_nmll': -0.19596131145954132, 'train_nll': -472.4881591796875, 'test_nll': -12.101091384887695, 'train_mse': 0.00011338565673213452, 'state_dict_file': 'model_state_dict_-7420673884544297876.pkl'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py:63: UserWarning: NaNs encountered in preconditioner computation. Attempting to continue without preconditioning.\n",
      "  \"NaNs encountered in preconditioner computation. Attempting to continue without preconditioning.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:15:58.139799, fold=7, rep=0, eta=0d 0h 0m 55s \n",
      "{'fold': 7, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.09353262931108475, 'train_time': 11.280552237993106, 'prior_train_nmll': -0.2574930489063263, 'train_nll': -469.515869140625, 'test_nll': 0.3037242889404297, 'train_mse': 0.00011069536412833259, 'state_dict_file': 'model_state_dict_1959115837024645230.pkl'}\n",
      "2019-03-16 15:16:09.391290, fold=7, rep=1, eta=0d 0h 0m 44s \n",
      "{'fold': 7, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.12539328634738922, 'train_time': 11.251142958062701, 'prior_train_nmll': -0.07086212933063507, 'train_nll': -468.983154296875, 'test_nll': 6.032941818237305, 'train_mse': 0.0001313518878305331, 'state_dict_file': 'model_state_dict_-6016895112307526631.pkl'}\n",
      "2019-03-16 15:16:18.926324, fold=8, rep=0, eta=0d 0h 0m 32s \n",
      "{'fold': 8, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.008294757455587387, 'train_time': 9.529598235036246, 'prior_train_nmll': -0.055480439215898514, 'train_nll': 370637.78125, 'test_nll': -14.655086517333984, 'train_mse': 0.00016185393906198442, 'state_dict_file': 'model_state_dict_-8517373448206723107.pkl'}\n",
      "2019-03-16 15:16:26.950638, fold=8, rep=1, eta=0d 0h 0m 21s \n",
      "{'fold': 8, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.03327471390366554, 'train_time': 8.024073182954453, 'prior_train_nmll': 0.06939384341239929, 'train_nll': nan, 'test_nll': -11.709680557250977, 'train_mse': 8.456816431134939e-05, 'state_dict_file': 'model_state_dict_-8241110142538878723.pkl'}\n",
      "2019-03-16 15:16:36.003087, fold=9, rep=0, eta=0d 0h 0m 10s \n",
      "{'fold': 9, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.0020079880487173796, 'train_time': 9.049229486961849, 'prior_train_nmll': 0.009401745162904263, 'train_nll': -581.665771484375, 'test_nll': -5.650827884674072, 'train_mse': 0.0001571656612213701, 'state_dict_file': 'model_state_dict_6764511418767222114.pkl'}\n",
      "2019-03-16 15:16:44.690026, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.014204630628228188, 'train_time': 8.68666641798336, 'prior_train_nmll': -0.045478880405426025, 'train_nll': -540.1359252929688, 'test_nll': -5.44662618637085, 'train_mse': 0.00011137364344904199, 'state_dict_file': 'model_state_dict_6573282218701052243.pkl'}\n",
      "2019-03-16 15:17:09.438774, fold=0, rep=0, eta=0d 0h 7m 49s \n",
      "{'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.07674781233072281, 'train_time': 24.726253519998863, 'prior_train_nmll': 0.19591230154037476, 'train_nll': -129.85110473632812, 'test_nll': 1.353438377380371, 'train_mse': 0.0034556190948933363, 'state_dict_file': 'model_state_dict_-3044418744805417849.pkl'}\n",
      "2019-03-16 15:17:32.288913, fold=0, rep=1, eta=0d 0h 7m 8s \n",
      "{'fold': 0, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.07888510823249817, 'train_time': 22.849917484098114, 'prior_train_nmll': 0.16557574272155762, 'train_nll': -130.827880859375, 'test_nll': 2.23372745513916, 'train_mse': 0.003686628071591258, 'state_dict_file': 'model_state_dict_-7246927397375080009.pkl'}\n",
      "2019-03-16 15:18:07.349614, fold=1, rep=0, eta=0d 0h 7m 48s \n",
      "{'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4003998041152954, 'train_time': 35.05731719301548, 'prior_train_nmll': 0.11715678125619888, 'train_nll': -166.95724487304688, 'test_nll': 30.27404022216797, 'train_mse': 0.0018291500164195895, 'state_dict_file': 'model_state_dict_-4900383991779304560.pkl'}\n",
      "2019-03-16 15:18:28.144916, fold=1, rep=1, eta=0d 0h 6m 53s \n",
      "{'fold': 1, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.4053459167480469, 'train_time': 20.79502452700399, 'prior_train_nmll': 0.17925803363323212, 'train_nll': -157.89389038085938, 'test_nll': 34.064598083496094, 'train_mse': 0.0021136971190571785, 'state_dict_file': 'model_state_dict_-4566165215277838971.pkl'}\n",
      "2019-03-16 15:18:51.964656, fold=2, rep=0, eta=0d 0h 6m 21s \n",
      "{'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.16383397579193115, 'train_time': 23.81669995491393, 'prior_train_nmll': 0.24114713072776794, 'train_nll': -193.07769775390625, 'test_nll': 24.958847045898438, 'train_mse': 0.0009770491160452366, 'state_dict_file': 'model_state_dict_5555835767946574541.pkl'}\n",
      "2019-03-16 15:19:24.238948, fold=2, rep=1, eta=0d 0h 6m 12s \n",
      "{'fold': 2, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.11299850791692734, 'train_time': 32.27407090191264, 'prior_train_nmll': 0.16501373052597046, 'train_nll': -211.78485107421875, 'test_nll': 26.318532943725586, 'train_mse': 0.0008840680820867419, 'state_dict_file': 'model_state_dict_-8231785232139634040.pkl'}\n",
      "2019-03-16 15:19:50.472747, fold=3, rep=0, eta=0d 0h 5m 44s \n",
      "{'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1785510778427124, 'train_time': 26.230446630972438, 'prior_train_nmll': 0.16744382679462433, 'train_nll': -135.30718994140625, 'test_nll': 24.519710540771484, 'train_mse': 0.0031950934790074825, 'state_dict_file': 'model_state_dict_5487979886011040892.pkl'}\n",
      "2019-03-16 15:20:12.921165, fold=3, rep=1, eta=0d 0h 5m 12s \n",
      "{'fold': 3, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.1449941247701645, 'train_time': 22.44816445792094, 'prior_train_nmll': 0.2268034964799881, 'train_nll': -129.33514404296875, 'test_nll': 11.483116149902344, 'train_mse': 0.003347812918946147, 'state_dict_file': 'model_state_dict_4606012838443498414.pkl'}\n",
      "2019-03-16 15:20:47.166934, fold=4, rep=0, eta=0d 0h 4m 56s \n",
      "{'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.17970404028892517, 'train_time': 34.24231459503062, 'prior_train_nmll': 0.11189276725053787, 'train_nll': -157.7156982421875, 'test_nll': 32.35161590576172, 'train_mse': 0.0023395561147481203, 'state_dict_file': 'model_state_dict_1560201305293163102.pkl'}\n",
      "2019-03-16 15:21:14.393776, fold=4, rep=1, eta=0d 0h 4m 29s \n",
      "{'fold': 4, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.1303805708885193, 'train_time': 27.226597090018913, 'prior_train_nmll': 0.113428995013237, 'train_nll': -145.55422973632812, 'test_nll': 21.765653610229492, 'train_mse': 0.002835444640368223, 'state_dict_file': 'model_state_dict_3633931001364488134.pkl'}\n",
      "2019-03-16 15:21:44.617233, fold=5, rep=0, eta=0d 0h 4m 5s \n",
      "{'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.14509451389312744, 'train_time': 30.219327123020776, 'prior_train_nmll': 0.13375018537044525, 'train_nll': -142.75009155273438, 'test_nll': 14.188505172729492, 'train_mse': 0.0030396936926990747, 'state_dict_file': 'model_state_dict_-1894321863873855773.pkl'}\n",
      "2019-03-16 15:22:16.013757, fold=5, rep=1, eta=0d 0h 3m 40s \n",
      "{'fold': 5, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.30218517780303955, 'train_time': 31.396306459093466, 'prior_train_nmll': 0.10466479510068893, 'train_nll': -141.84588623046875, 'test_nll': 24.5684814453125, 'train_mse': 0.003105483716353774, 'state_dict_file': 'model_state_dict_-7489452874987659268.pkl'}\n",
      "2019-03-16 15:22:57.391514, fold=6, rep=0, eta=0d 0h 3m 20s \n",
      "{'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.11550398170948029, 'train_time': 41.374249481013976, 'prior_train_nmll': 0.12582990527153015, 'train_nll': -141.46014404296875, 'test_nll': 13.344560623168945, 'train_mse': 0.0032666611950844526, 'state_dict_file': 'model_state_dict_5544691810273984125.pkl'}\n",
      "2019-03-16 15:23:32.787762, fold=6, rep=1, eta=0d 0h 2m 54s \n",
      "{'fold': 6, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.14590716361999512, 'train_time': 35.39594251406379, 'prior_train_nmll': 0.12447454780340195, 'train_nll': -138.85888671875, 'test_nll': 12.775487899780273, 'train_mse': 0.003237449796870351, 'state_dict_file': 'model_state_dict_606438358643688825.pkl'}\n",
      "2019-03-16 15:23:53.984976, fold=7, rep=0, eta=0d 0h 2m 23s \n",
      "{'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1631755232810974, 'train_time': 21.193433068925515, 'prior_train_nmll': 0.18006743490695953, 'train_nll': -131.16952514648438, 'test_nll': 9.335939407348633, 'train_mse': 0.0036007973831146955, 'state_dict_file': 'model_state_dict_-7840802811885394083.pkl'}\n",
      "2019-03-16 15:24:19.681972, fold=7, rep=1, eta=0d 0h 1m 53s \n",
      "{'fold': 7, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.10911069065332413, 'train_time': 25.69675993709825, 'prior_train_nmll': 0.20793035626411438, 'train_nll': -134.42816162109375, 'test_nll': 11.983678817749023, 'train_mse': 0.003438358660787344, 'state_dict_file': 'model_state_dict_3780314593244641487.pkl'}\n",
      "2019-03-16 15:24:39.828874, fold=8, rep=0, eta=0d 0h 1m 23s \n",
      "{'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4815264046192169, 'train_time': 20.142901577986777, 'prior_train_nmll': 0.18033990263938904, 'train_nll': -121.69113159179688, 'test_nll': 25.305492401123047, 'train_mse': 0.0040757195092737675, 'state_dict_file': 'model_state_dict_-6780372710784218377.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:25:15.026148, fold=8, rep=1, eta=0d 0h 0m 56s \n",
      "{'fold': 8, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.33556902408599854, 'train_time': 35.19703944702633, 'prior_train_nmll': 0.08283122628927231, 'train_nll': -141.48379516601562, 'test_nll': 17.182613372802734, 'train_mse': 0.00313000101596117, 'state_dict_file': 'model_state_dict_-2630996341086971605.pkl'}\n",
      "2019-03-16 15:25:40.595463, fold=9, rep=0, eta=0d 0h 0m 28s \n",
      "{'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.07682517170906067, 'train_time': 25.56570063403342, 'prior_train_nmll': 0.1806652992963791, 'train_nll': -158.50466918945312, 'test_nll': 4.4984025955200195, 'train_mse': 0.0022535312455147505, 'state_dict_file': 'model_state_dict_-4608979545482689361.pkl'}\n",
      "2019-03-16 15:26:13.486946, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.09440872073173523, 'train_time': 32.89124977705069, 'prior_train_nmll': 0.11620782315731049, 'train_nll': -158.94354248046875, 'test_nll': 9.416964530944824, 'train_mse': 0.002330068964511156, 'state_dict_file': 'model_state_dict_-3457616739981792964.pkl'}\n",
      "2019-03-16 15:26:26.953439, fold=0, rep=0, eta=0d 0h 4m 15s \n",
      "{'fold': 0, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.11789390444755554, 'train_time': 13.444154226919636, 'prior_train_nmll': 0.14323706924915314, 'train_nll': -290.60565185546875, 'test_nll': 22.789630889892578, 'train_mse': 0.004618191625922918, 'state_dict_file': 'model_state_dict_-8727332891634146916.pkl'}\n",
      "2019-03-16 15:26:43.724868, fold=0, rep=1, eta=0d 0h 4m 31s \n",
      "{'fold': 0, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.1306280791759491, 'train_time': 16.771220532944426, 'prior_train_nmll': 0.1757945865392685, 'train_nll': -100.11489868164062, 'test_nll': 17.3427734375, 'train_mse': 0.010622943751513958, 'state_dict_file': 'model_state_dict_6014131882630994109.pkl'}\n",
      "2019-03-16 15:26:53.510583, fold=1, rep=0, eta=0d 0h 3m 46s \n",
      "{'fold': 1, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.24796609580516815, 'train_time': 9.782406347920187, 'prior_train_nmll': 0.22181101143360138, 'train_nll': -96.38888549804688, 'test_nll': 31.81417465209961, 'train_mse': 0.011045273393392563, 'state_dict_file': 'model_state_dict_1522775367259040963.pkl'}\n",
      "2019-03-16 15:27:10.542450, fold=1, rep=1, eta=0d 0h 3m 48s \n",
      "{'fold': 1, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.1476411670446396, 'train_time': 17.031622952083126, 'prior_train_nmll': 0.20377884805202484, 'train_nll': -114.70819091796875, 'test_nll': 9.404324531555176, 'train_mse': 0.008820217102766037, 'state_dict_file': 'model_state_dict_-6229500361426842652.pkl'}\n",
      "2019-03-16 15:27:23.494098, fold=2, rep=0, eta=0d 0h 3m 29s \n",
      "{'fold': 2, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.2857395112514496, 'train_time': 12.94862608902622, 'prior_train_nmll': 0.18810579180717468, 'train_nll': -108.75752258300781, 'test_nll': 39.49673843383789, 'train_mse': 0.008304191753268242, 'state_dict_file': 'model_state_dict_8107480207291141115.pkl'}\n",
      "2019-03-16 15:27:33.461188, fold=2, rep=1, eta=0d 0h 3m 6s \n",
      "{'fold': 2, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.08399657905101776, 'train_time': 9.96687445603311, 'prior_train_nmll': 0.2695463001728058, 'train_nll': -60.97834777832031, 'test_nll': 3.2541351318359375, 'train_mse': 0.01828564703464508, 'state_dict_file': 'model_state_dict_2933072305021292093.pkl'}\n",
      "2019-03-16 15:27:58.113265, fold=3, rep=0, eta=0d 0h 3m 14s \n",
      "{'fold': 3, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.19587373733520508, 'train_time': 24.648804898024537, 'prior_train_nmll': 0.17387056350708008, 'train_nll': -318.72418212890625, 'test_nll': 52.624473571777344, 'train_mse': 0.004042312502861023, 'state_dict_file': 'model_state_dict_4291660968179508575.pkl'}\n",
      "2019-03-16 15:28:07.190013, fold=3, rep=1, eta=0d 0h 2m 50s \n",
      "{'fold': 3, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.1494976431131363, 'train_time': 9.0765150670195, 'prior_train_nmll': 0.25385376811027527, 'train_nll': -523.0758056640625, 'test_nll': 32.83576965332031, 'train_mse': 0.007315021939575672, 'state_dict_file': 'model_state_dict_-2398383766201197142.pkl'}\n",
      "2019-03-16 15:28:17.641698, fold=4, rep=0, eta=0d 0h 2m 31s \n",
      "{'fold': 4, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.05680370703339577, 'train_time': 10.448293326073326, 'prior_train_nmll': 0.24771606922149658, 'train_nll': -70.77737426757812, 'test_nll': -0.8474740982055664, 'train_mse': 0.015203275717794895, 'state_dict_file': 'model_state_dict_3732024596101611030.pkl'}\n",
      "2019-03-16 15:28:32.048033, fold=4, rep=1, eta=0d 0h 2m 18s \n",
      "{'fold': 4, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.13110104203224182, 'train_time': 14.405673600034788, 'prior_train_nmll': 0.26379942893981934, 'train_nll': 1113.898193359375, 'test_nll': 24.79666519165039, 'train_mse': 0.0037915415596216917, 'state_dict_file': 'model_state_dict_-1284372468888936085.pkl'}\n",
      "2019-03-16 15:28:47.687372, fold=5, rep=0, eta=0d 0h 2m 6s \n",
      "{'fold': 5, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.2785254418849945, 'train_time': 15.636283530038781, 'prior_train_nmll': 0.23775146901607513, 'train_nll': -139.86546325683594, 'test_nll': 48.36687088012695, 'train_mse': 0.006499435752630234, 'state_dict_file': 'model_state_dict_1918470254736981342.pkl'}\n",
      "2019-03-16 15:28:58.383570, fold=5, rep=1, eta=0d 0h 1m 49s \n",
      "{'fold': 5, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.2864825427532196, 'train_time': 10.695959764998406, 'prior_train_nmll': 0.21699956059455872, 'train_nll': 996581.8125, 'test_nll': 40.61200714111328, 'train_mse': 0.006445042788982391, 'state_dict_file': 'model_state_dict_-1480370495132933584.pkl'}\n",
      "2019-03-16 15:29:13.098388, fold=6, rep=0, eta=0d 0h 1m 36s \n",
      "{'fold': 6, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.062278952449560165, 'train_time': 14.711358343949541, 'prior_train_nmll': 0.16927313804626465, 'train_nll': -3938.583251953125, 'test_nll': 1.033543586730957, 'train_mse': 0.0065732914954423904, 'state_dict_file': 'model_state_dict_1765368482900883118.pkl'}\n",
      "2019-03-16 15:29:24.193555, fold=6, rep=1, eta=0d 0h 1m 21s \n",
      "{'fold': 6, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.08597519993782043, 'train_time': 11.09494924498722, 'prior_train_nmll': 0.2990030348300934, 'train_nll': -539.351806640625, 'test_nll': 9.879986763000488, 'train_mse': 0.004440425429493189, 'state_dict_file': 'model_state_dict_9210931607272688912.pkl'}\n",
      "2019-03-16 15:29:37.843016, fold=7, rep=0, eta=0d 0h 1m 8s \n",
      "{'fold': 7, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.15413036942481995, 'train_time': 13.64595982094761, 'prior_train_nmll': 0.2262987196445465, 'train_nll': -418.572265625, 'test_nll': 25.575946807861328, 'train_mse': 0.0050254506058990955, 'state_dict_file': 'model_state_dict_2708068879008630306.pkl'}\n",
      "2019-03-16 15:29:48.471692, fold=7, rep=1, eta=0d 0h 0m 53s \n",
      "{'fold': 7, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.09217363595962524, 'train_time': 10.62841473997105, 'prior_train_nmll': 0.20718933641910553, 'train_nll': 145.8806610107422, 'test_nll': 4.4706010818481445, 'train_mse': 0.007891656830906868, 'state_dict_file': 'model_state_dict_-8350551446057374363.pkl'}\n",
      "2019-03-16 15:30:03.178878, fold=8, rep=0, eta=0d 0h 0m 40s \n",
      "{'fold': 8, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.07127220928668976, 'train_time': 14.703750757034868, 'prior_train_nmll': 0.195535346865654, 'train_nll': -20724.99609375, 'test_nll': 2.8907432556152344, 'train_mse': 0.009371604770421982, 'state_dict_file': 'model_state_dict_8120506026716141257.pkl'}\n",
      "2019-03-16 15:30:15.685027, fold=8, rep=1, eta=0d 0h 0m 26s \n",
      "{'fold': 8, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.11627344787120819, 'train_time': 12.505869994056411, 'prior_train_nmll': 0.16292497515678406, 'train_nll': -88.00514221191406, 'test_nll': 12.033360481262207, 'train_mse': 0.011944709345698357, 'state_dict_file': 'model_state_dict_7679068975029686840.pkl'}\n",
      "2019-03-16 15:30:27.772184, fold=9, rep=0, eta=0d 0h 0m 13s \n",
      "{'fold': 9, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.1052679494023323, 'train_time': 12.083937440998852, 'prior_train_nmll': 0.2569026052951813, 'train_nll': -252.12237548828125, 'test_nll': 9.21279525756836, 'train_mse': 0.005787996109575033, 'state_dict_file': 'model_state_dict_-7225468838248611543.pkl'}\n",
      "2019-03-16 15:30:43.891026, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.15256686508655548, 'train_time': 16.11824518290814, 'prior_train_nmll': 0.1602586805820465, 'train_nll': -360.14764404296875, 'test_nll': 50.37119674682617, 'train_mse': 0.0021913691889494658, 'state_dict_file': 'model_state_dict_-4504456524554414056.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:31:09.603158, fold=0, rep=0, eta=0d 0h 8m 8s \n",
      "{'fold': 0, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.7551144361495972, 'train_time': 25.688570062979124, 'prior_train_nmll': 0.7126064300537109, 'train_nll': -584.4906616210938, 'test_nll': 87.87945556640625, 'train_mse': 0.00010283885785611346, 'state_dict_file': 'model_state_dict_4455568237930429054.pkl'}\n",
      "2019-03-16 15:31:38.978203, fold=0, rep=1, eta=0d 0h 8m 15s \n",
      "{'fold': 0, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.7222751379013062, 'train_time': 29.37481420894619, 'prior_train_nmll': 0.7358039021492004, 'train_nll': -616.7403564453125, 'test_nll': 90.58358764648438, 'train_mse': 4.541098314803094e-05, 'state_dict_file': 'model_state_dict_-2290775982462551909.pkl'}\n",
      "2019-03-16 15:32:12.662365, fold=1, rep=0, eta=0d 0h 8m 22s \n",
      "{'fold': 1, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.0703811645507812, 'train_time': 33.68113467597868, 'prior_train_nmll': 0.6798458099365234, 'train_nll': -753.834228515625, 'test_nll': 128.09048461914062, 'train_mse': 0.00010342026507714763, 'state_dict_file': 'model_state_dict_-8329655536038404598.pkl'}\n",
      "2019-03-16 15:32:48.920869, fold=1, rep=1, eta=0d 0h 8m 20s \n",
      "{'fold': 1, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.9082948565483093, 'train_time': 36.25825509999413, 'prior_train_nmll': 0.7593631148338318, 'train_nll': -759.1567993164062, 'test_nll': 110.1688232421875, 'train_mse': 8.553153747925535e-05, 'state_dict_file': 'model_state_dict_7134577646007320776.pkl'}\n",
      "2019-03-16 15:33:12.969639, fold=2, rep=0, eta=0d 0h 7m 27s \n",
      "{'fold': 2, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.296911358833313, 'train_time': 24.04568806407042, 'prior_train_nmll': 0.7708112597465515, 'train_nll': -541.64404296875, 'test_nll': 152.62075805664062, 'train_mse': 7.741135050309822e-05, 'state_dict_file': 'model_state_dict_6268441109897381762.pkl'}\n",
      "2019-03-16 15:33:42.709866, fold=2, rep=1, eta=0d 0h 6m 57s \n",
      "{'fold': 2, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.0893834829330444, 'train_time': 29.739985967054963, 'prior_train_nmll': 0.7007604241371155, 'train_nll': -614.6353149414062, 'test_nll': 125.25387573242188, 'train_mse': 3.9850881876191124e-05, 'state_dict_file': 'model_state_dict_2133475985478517786.pkl'}\n",
      "2019-03-16 15:34:14.996745, fold=3, rep=0, eta=0d 0h 6m 32s \n",
      "{'fold': 3, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.9844121932983398, 'train_time': 32.28371838491876, 'prior_train_nmll': 0.661144495010376, 'train_nll': -675.4699096679688, 'test_nll': 130.60711669921875, 'train_mse': 5.40866021765396e-05, 'state_dict_file': 'model_state_dict_-1535912958207282431.pkl'}\n",
      "2019-03-16 15:34:37.128128, fold=3, rep=1, eta=0d 0h 5m 49s \n",
      "{'fold': 3, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.0481107234954834, 'train_time': 22.13075705105439, 'prior_train_nmll': 0.7110598087310791, 'train_nll': -476.008544921875, 'test_nll': 95.57892608642578, 'train_mse': 7.021709461696446e-05, 'state_dict_file': 'model_state_dict_5525023073383592872.pkl'}\n",
      "2019-03-16 15:35:00.355372, fold=4, rep=0, eta=0d 0h 5m 13s \n",
      "{'fold': 4, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.8499759435653687, 'train_time': 23.224011969054118, 'prior_train_nmll': 0.743695855140686, 'train_nll': -693.3444213867188, 'test_nll': 102.793701171875, 'train_mse': 7.521772931795567e-05, 'state_dict_file': 'model_state_dict_9092348534082272243.pkl'}\n",
      "2019-03-16 15:35:23.794862, fold=4, rep=1, eta=0d 0h 4m 39s \n",
      "{'fold': 4, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.6926666498184204, 'train_time': 23.439255125937052, 'prior_train_nmll': 0.7405940294265747, 'train_nll': -421.31793212890625, 'test_nll': 78.00595092773438, 'train_mse': 6.025735638104379e-05, 'state_dict_file': 'model_state_dict_-418907118720706372.pkl'}\n",
      "2019-03-16 15:35:47.761277, fold=5, rep=0, eta=0d 0h 4m 8s \n",
      "{'fold': 5, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.2594090700149536, 'train_time': 23.96323306998238, 'prior_train_nmll': 0.6386882662773132, 'train_nll': -457.87017822265625, 'test_nll': 151.74432373046875, 'train_mse': 7.130642188712955e-05, 'state_dict_file': 'model_state_dict_5613444250265913366.pkl'}\n",
      "2019-03-16 15:36:10.806678, fold=5, rep=1, eta=0d 0h 3m 37s \n",
      "{'fold': 5, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.1219291687011719, 'train_time': 23.044891946949065, 'prior_train_nmll': 0.7209460735321045, 'train_nll': -370.87640380859375, 'test_nll': 123.20303344726562, 'train_mse': 9.062772005563602e-05, 'state_dict_file': 'model_state_dict_329489515810668737.pkl'}\n",
      "2019-03-16 15:36:30.439326, fold=6, rep=0, eta=0d 0h 3m 6s \n",
      "{'fold': 6, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.45979148149490356, 'train_time': 19.6294462189544, 'prior_train_nmll': 0.6615974307060242, 'train_nll': -400.86822509765625, 'test_nll': 65.02179718017578, 'train_mse': 8.061746484600008e-05, 'state_dict_file': 'model_state_dict_3984880507060306263.pkl'}\n",
      "2019-03-16 15:37:01.829238, fold=6, rep=1, eta=0d 0h 2m 41s \n",
      "{'fold': 6, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.5199680328369141, 'train_time': 31.389673092984594, 'prior_train_nmll': 0.7164275050163269, 'train_nll': -541.0924682617188, 'test_nll': 76.4832992553711, 'train_mse': 7.29259118088521e-05, 'state_dict_file': 'model_state_dict_6662698990038638309.pkl'}\n",
      "2019-03-16 15:37:23.838071, fold=7, rep=0, eta=0d 0h 2m 13s \n",
      "{'fold': 7, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.2855424880981445, 'train_time': 22.005659568938427, 'prior_train_nmll': 0.689614474773407, 'train_nll': -495.934326171875, 'test_nll': 130.0301055908203, 'train_mse': 6.724111881339923e-05, 'state_dict_file': 'model_state_dict_2287707868977831991.pkl'}\n",
      "2019-03-16 15:37:52.113653, fold=7, rep=1, eta=0d 0h 1m 47s \n",
      "{'fold': 7, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.225441813468933, 'train_time': 28.275324402027763, 'prior_train_nmll': 0.7535302042961121, 'train_nll': -567.2587280273438, 'test_nll': 123.44178771972656, 'train_mse': 7.114093023119494e-05, 'state_dict_file': 'model_state_dict_6312376881399729860.pkl'}\n",
      "2019-03-16 15:38:24.282835, fold=8, rep=0, eta=0d 0h 1m 21s \n",
      "{'fold': 8, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.3090081214904785, 'train_time': 32.16594181000255, 'prior_train_nmll': 0.7601162195205688, 'train_nll': 75.15928649902344, 'test_nll': 136.9474334716797, 'train_mse': 5.205488923820667e-05, 'state_dict_file': 'model_state_dict_4131333146659273476.pkl'}\n",
      "2019-03-16 15:38:53.777174, fold=8, rep=1, eta=0d 0h 0m 54s \n",
      "{'fold': 8, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.2111293077468872, 'train_time': 29.494089846033603, 'prior_train_nmll': 0.6112164258956909, 'train_nll': -653.0634155273438, 'test_nll': 154.0265350341797, 'train_mse': 0.00010690264025470242, 'state_dict_file': 'model_state_dict_-4949772816813952542.pkl'}\n",
      "2019-03-16 15:39:16.508323, fold=9, rep=0, eta=0d 0h 0m 26s \n",
      "{'fold': 9, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.8679459691047668, 'train_time': 22.72783257998526, 'prior_train_nmll': 0.7547544836997986, 'train_nll': -379.67401123046875, 'test_nll': 58.10493469238281, 'train_mse': 9.890246292343363e-05, 'state_dict_file': 'model_state_dict_-3641998751299635607.pkl'}\n",
      "2019-03-16 15:39:39.387315, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.0743296146392822, 'train_time': 22.878749686060473, 'prior_train_nmll': 0.673466145992279, 'train_nll': -477.92852783203125, 'test_nll': 70.71385955810547, 'train_mse': 0.00010433770512463525, 'state_dict_file': 'model_state_dict_-8041504445266061421.pkl'}\n",
      "2019-03-16 15:40:05.023270, fold=0, rep=0, eta=0d 0h 8m 6s \n",
      "{'fold': 0, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.08127428591251373, 'train_time': 25.612846617004834, 'prior_train_nmll': 0.35563674569129944, 'train_nll': -33.83099365234375, 'test_nll': 2.6324291229248047, 'train_mse': 0.02815677598118782, 'state_dict_file': 'model_state_dict_-6930679057670981396.pkl'}\n",
      "2019-03-16 15:40:25.178664, fold=0, rep=1, eta=0d 0h 6m 51s \n",
      "{'fold': 0, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.10106579214334488, 'train_time': 20.155142807983793, 'prior_train_nmll': 0.38160860538482666, 'train_nll': -17.073989868164062, 'test_nll': 7.172553062438965, 'train_mse': 0.03605324774980545, 'state_dict_file': 'model_state_dict_8449349134329220440.pkl'}\n",
      "2019-03-16 15:40:41.137324, fold=1, rep=0, eta=0d 0h 5m 49s \n",
      "{'fold': 1, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.28255635499954224, 'train_time': 15.95566549594514, 'prior_train_nmll': 0.36304351687431335, 'train_nll': -23.20050048828125, 'test_nll': 34.02984619140625, 'train_mse': 0.03501855954527855, 'state_dict_file': 'model_state_dict_-947563007401739610.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:40:59.920679, fold=1, rep=1, eta=0d 0h 5m 22s \n",
      "{'fold': 1, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.2459728866815567, 'train_time': 18.782648036023602, 'prior_train_nmll': 0.30793383717536926, 'train_nll': -28.801055908203125, 'test_nll': 30.053295135498047, 'train_mse': 0.03230217844247818, 'state_dict_file': 'model_state_dict_6455621186431643394.pkl'}\n",
      "2019-03-16 15:41:14.523170, fold=2, rep=0, eta=0d 0h 4m 45s \n",
      "{'fold': 2, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.10418874770402908, 'train_time': 14.599433812079951, 'prior_train_nmll': 0.4755271375179291, 'train_nll': 8.602142333984375, 'test_nll': 5.556902885437012, 'train_mse': 0.04854276403784752, 'state_dict_file': 'model_state_dict_1044435379693084194.pkl'}\n",
      "2019-03-16 15:41:31.312268, fold=2, rep=1, eta=0d 0h 4m 21s \n",
      "{'fold': 2, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.11320599168539047, 'train_time': 16.788610424962826, 'prior_train_nmll': 0.42787688970565796, 'train_nll': -1.7401580810546875, 'test_nll': 7.139056205749512, 'train_mse': 0.0429733544588089, 'state_dict_file': 'model_state_dict_2526581307547344453.pkl'}\n",
      "2019-03-16 15:41:54.967998, fold=3, rep=0, eta=0d 0h 4m 11s \n",
      "{'fold': 3, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.17325210571289062, 'train_time': 23.65261150605511, 'prior_train_nmll': 0.3818185329437256, 'train_nll': -17.94451904296875, 'test_nll': 18.879047393798828, 'train_mse': 0.03541218116879463, 'state_dict_file': 'model_state_dict_-5315879208805682692.pkl'}\n",
      "2019-03-16 15:42:14.214522, fold=3, rep=1, eta=0d 0h 3m 52s \n",
      "{'fold': 3, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.2272540032863617, 'train_time': 19.24586016696412, 'prior_train_nmll': 0.39108413457870483, 'train_nll': -16.427841186523438, 'test_nll': 21.044679641723633, 'train_mse': 0.036710064858198166, 'state_dict_file': 'model_state_dict_2974833196508292017.pkl'}\n",
      "2019-03-16 15:42:43.822162, fold=4, rep=0, eta=0d 0h 3m 45s \n",
      "{'fold': 4, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.22048985958099365, 'train_time': 29.60454150394071, 'prior_train_nmll': 0.3862069249153137, 'train_nll': -14.611297607421875, 'test_nll': 12.05073356628418, 'train_mse': 0.03728117421269417, 'state_dict_file': 'model_state_dict_2356759668293110222.pkl'}\n",
      "2019-03-16 15:43:02.075494, fold=4, rep=1, eta=0d 0h 3m 22s \n",
      "{'fold': 4, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.55137038230896, 'train_time': 18.253093188046478, 'prior_train_nmll': 0.3730930685997009, 'train_nll': -24.974517822265625, 'test_nll': 22.390653610229492, 'train_mse': 0.03366518393158913, 'state_dict_file': 'model_state_dict_-1716492403852068825.pkl'}\n",
      "2019-03-16 15:43:28.242907, fold=5, rep=0, eta=0d 0h 3m 7s \n",
      "{'fold': 5, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.24381060898303986, 'train_time': 26.164194534067065, 'prior_train_nmll': 0.3453653156757355, 'train_nll': -49.05877685546875, 'test_nll': 22.264394760131836, 'train_mse': 0.022689232602715492, 'state_dict_file': 'model_state_dict_-9186549040399170472.pkl'}\n",
      "2019-03-16 15:43:57.214814, fold=5, rep=1, eta=0d 0h 2m 51s \n",
      "{'fold': 5, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.21749621629714966, 'train_time': 28.97165673004929, 'prior_train_nmll': 0.32471969723701477, 'train_nll': -57.205718994140625, 'test_nll': 25.164287567138672, 'train_mse': 0.02121068723499775, 'state_dict_file': 'model_state_dict_-7356762167492194514.pkl'}\n",
      "2019-03-16 15:44:20.326833, fold=6, rep=0, eta=0d 0h 2m 31s \n",
      "{'fold': 6, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.13331755995750427, 'train_time': 23.108822223963216, 'prior_train_nmll': 0.4071260392665863, 'train_nll': 0.588775634765625, 'test_nll': 8.533586502075195, 'train_mse': 0.046675462275743484, 'state_dict_file': 'model_state_dict_2780785250886904835.pkl'}\n",
      "2019-03-16 15:44:42.727446, fold=6, rep=1, eta=0d 0h 2m 9s \n",
      "{'fold': 6, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.11602414399385452, 'train_time': 22.400358195998706, 'prior_train_nmll': 0.38113853335380554, 'train_nll': -22.499252319335938, 'test_nll': 7.242170333862305, 'train_mse': 0.033895984292030334, 'state_dict_file': 'model_state_dict_6523900300592310619.pkl'}\n",
      "2019-03-16 15:45:00.801877, fold=7, rep=0, eta=0d 0h 1m 47s \n",
      "{'fold': 7, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.19987328350543976, 'train_time': 18.071133263991214, 'prior_train_nmll': 0.35128816962242126, 'train_nll': -38.22235107421875, 'test_nll': 20.64440155029297, 'train_mse': 0.028038332238793373, 'state_dict_file': 'model_state_dict_5004434242482060644.pkl'}\n",
      "2019-03-16 15:45:31.162089, fold=7, rep=1, eta=0d 0h 1m 27s \n",
      "{'fold': 7, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.22823719680309296, 'train_time': 30.359961192007177, 'prior_train_nmll': 0.3618234694004059, 'train_nll': -48.76409912109375, 'test_nll': 30.789064407348633, 'train_mse': 0.023778270930051804, 'state_dict_file': 'model_state_dict_-811092246662088479.pkl'}\n",
      "2019-03-16 15:45:57.843823, fold=8, rep=0, eta=0d 0h 1m 6s \n",
      "{'fold': 8, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.23404784500598907, 'train_time': 26.67803603899665, 'prior_train_nmll': 0.3535696864128113, 'train_nll': -29.924896240234375, 'test_nll': 19.88419532775879, 'train_mse': 0.030131202191114426, 'state_dict_file': 'model_state_dict_1856414756970374709.pkl'}\n",
      "2019-03-16 15:46:18.758632, fold=8, rep=1, eta=0d 0h 0m 44s \n",
      "{'fold': 8, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.26756370067596436, 'train_time': 20.9145144529175, 'prior_train_nmll': 0.3140451908111572, 'train_nll': -40.318603515625, 'test_nll': 21.97804832458496, 'train_mse': 0.028135107830166817, 'state_dict_file': 'model_state_dict_7401324554808928670.pkl'}\n",
      "2019-03-16 15:46:46.457645, fold=9, rep=0, eta=0d 0h 0m 22s \n",
      "{'fold': 9, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.3772881329059601, 'train_time': 27.694721976993605, 'prior_train_nmll': 0.3374955654144287, 'train_nll': -57.67124938964844, 'test_nll': 19.932231903076172, 'train_mse': 0.021161295473575592, 'state_dict_file': 'model_state_dict_649589048319757783.pkl'}\n",
      "2019-03-16 15:47:08.552540, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.2966914772987366, 'train_time': 22.0946414610371, 'prior_train_nmll': 0.3290421962738037, 'train_nll': -50.461029052734375, 'test_nll': 13.534858703613281, 'train_mse': 0.023778455331921577, 'state_dict_file': 'model_state_dict_-6060459507490378253.pkl'}\n",
      "2019-03-16 15:47:29.671270, fold=0, rep=0, eta=0d 0h 6m 40s \n",
      "{'fold': 0, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.0025246809236705303, 'train_time': 21.09469260601327, 'prior_train_nmll': -0.37944793701171875, 'train_nll': -1699.3065185546875, 'test_nll': -118.31575012207031, 'train_mse': 0.0024744144175201654, 'state_dict_file': 'model_state_dict_8729572830191086759.pkl'}\n",
      "2019-03-16 15:47:52.269095, fold=0, rep=1, eta=0d 0h 6m 33s \n",
      "{'fold': 0, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.00686614541336894, 'train_time': 22.597600847948343, 'prior_train_nmll': -0.3854496479034424, 'train_nll': -1117.2186279296875, 'test_nll': 244.74966430664062, 'train_mse': 0.003900039941072464, 'state_dict_file': 'model_state_dict_-6871848619414597464.pkl'}\n",
      "2019-03-16 15:48:16.413161, fold=1, rep=0, eta=0d 0h 6m 24s \n",
      "{'fold': 1, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.013560356572270393, 'train_time': 24.140862058033235, 'prior_train_nmll': -0.2952876091003418, 'train_nll': -1331.014892578125, 'test_nll': 777.46142578125, 'train_mse': 0.007851757109165192, 'state_dict_file': 'model_state_dict_-2309291410258005013.pkl'}\n",
      "2019-03-16 15:48:37.428238, fold=1, rep=1, eta=0d 0h 5m 55s \n",
      "{'fold': 1, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.0047750528901815414, 'train_time': 21.014448091969825, 'prior_train_nmll': -0.5949133634567261, 'train_nll': -1161.0594482421875, 'test_nll': 2984.35302734375, 'train_mse': 0.0008823430398479104, 'state_dict_file': 'model_state_dict_2401513264924647113.pkl'}\n",
      "2019-03-16 15:48:59.968177, fold=2, rep=0, eta=0d 0h 5m 34s \n",
      "{'fold': 2, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.020220234990119934, 'train_time': 22.536625702981837, 'prior_train_nmll': -0.4348994791507721, 'train_nll': -1527.7901611328125, 'test_nll': 284.76995849609375, 'train_mse': 0.011009889654815197, 'state_dict_file': 'model_state_dict_-1159408398957603098.pkl'}\n",
      "2019-03-16 15:49:19.501564, fold=2, rep=1, eta=0d 0h 5m 5s \n",
      "{'fold': 2, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.005873601883649826, 'train_time': 19.532778071938083, 'prior_train_nmll': -0.3222455382347107, 'train_nll': -1386.0850830078125, 'test_nll': -8.677989959716797, 'train_mse': 0.004106548149138689, 'state_dict_file': 'model_state_dict_-2502827247635719037.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:49:38.049125, fold=3, rep=0, eta=0d 0h 4m 37s \n",
      "{'fold': 3, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.004673239775002003, 'train_time': 18.544197222101502, 'prior_train_nmll': -0.30420801043510437, 'train_nll': -1626.6341552734375, 'test_nll': 114.39070129394531, 'train_mse': 0.004086801316589117, 'state_dict_file': 'model_state_dict_-4789496634876534802.pkl'}\n",
      "2019-03-16 15:50:01.060718, fold=3, rep=1, eta=0d 0h 4m 18s \n",
      "{'fold': 3, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.0018997184233739972, 'train_time': 23.011358499061316, 'prior_train_nmll': -0.6581302285194397, 'train_nll': -1517.4185791015625, 'test_nll': 4564.28173828125, 'train_mse': 0.00031270389445126057, 'state_dict_file': 'model_state_dict_-3783618945754078518.pkl'}\n",
      "2019-03-16 15:50:24.329071, fold=4, rep=0, eta=0d 0h 3m 59s \n",
      "{'fold': 4, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.002319477265700698, 'train_time': 23.264144022017717, 'prior_train_nmll': -0.5380138158798218, 'train_nll': -1668.9764404296875, 'test_nll': 3232001.5, 'train_mse': 0.0013052435824647546, 'state_dict_file': 'model_state_dict_-7381712259507408426.pkl'}\n",
      "2019-03-16 15:50:47.313808, fold=4, rep=1, eta=0d 0h 3m 38s \n",
      "{'fold': 4, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.006280004512518644, 'train_time': 22.984464750043117, 'prior_train_nmll': -0.4975552260875702, 'train_nll': -1064.3726806640625, 'test_nll': 40.417118072509766, 'train_mse': 0.0031258396338671446, 'state_dict_file': 'model_state_dict_-2355946802573022983.pkl'}\n",
      "2019-03-16 15:51:09.370245, fold=5, rep=0, eta=0d 0h 3m 17s \n",
      "{'fold': 5, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.03851183503866196, 'train_time': 22.05329912097659, 'prior_train_nmll': -0.3982175588607788, 'train_nll': -1461.4932861328125, 'test_nll': 4224.47802734375, 'train_mse': 0.004547898657619953, 'state_dict_file': 'model_state_dict_-9196356401681594751.pkl'}\n",
      "2019-03-16 15:51:29.894920, fold=5, rep=1, eta=0d 0h 2m 54s \n",
      "{'fold': 5, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.013632317073643208, 'train_time': 20.524419059976935, 'prior_train_nmll': -0.34118416905403137, 'train_nll': -1565.505615234375, 'test_nll': 47917540.0, 'train_mse': 0.0009887071792036295, 'state_dict_file': 'model_state_dict_1182561391787853215.pkl'}\n",
      "2019-03-16 15:51:54.244032, fold=6, rep=0, eta=0d 0h 2m 33s \n",
      "{'fold': 6, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.07573188096284866, 'train_time': 24.346004558959976, 'prior_train_nmll': -0.6763780117034912, 'train_nll': 12813.607421875, 'test_nll': 7279.24462890625, 'train_mse': 0.011045643128454685, 'state_dict_file': 'model_state_dict_5409659151953035221.pkl'}\n",
      "2019-03-16 15:52:18.420298, fold=6, rep=1, eta=0d 0h 2m 12s \n",
      "{'fold': 6, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.061922743916511536, 'train_time': 24.176055460004136, 'prior_train_nmll': -0.6234166622161865, 'train_nll': -1726.7867431640625, 'test_nll': 6328223.5, 'train_mse': 0.0015806620940566063, 'state_dict_file': 'model_state_dict_5222800440787279302.pkl'}\n",
      "2019-03-16 15:52:45.451396, fold=7, rep=0, eta=0d 0h 1m 52s \n",
      "{'fold': 7, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.014007069170475006, 'train_time': 27.02758898306638, 'prior_train_nmll': -0.5160716772079468, 'train_nll': -1599.997314453125, 'test_nll': -88.72280883789062, 'train_mse': 0.004638273734599352, 'state_dict_file': 'model_state_dict_-644872492409844050.pkl'}\n",
      "2019-03-16 15:53:08.341370, fold=7, rep=1, eta=0d 0h 1m 29s \n",
      "{'fold': 7, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.011149139143526554, 'train_time': 22.88967419404071, 'prior_train_nmll': -0.3751545548439026, 'train_nll': -1534.0589599609375, 'test_nll': 254.30941772460938, 'train_mse': 0.0029394966550171375, 'state_dict_file': 'model_state_dict_-8432637169181050082.pkl'}\n",
      "2019-03-16 15:53:29.796366, fold=8, rep=0, eta=0d 0h 1m 7s \n",
      "{'fold': 8, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.01538991741836071, 'train_time': 21.451500077033415, 'prior_train_nmll': -0.48354193568229675, 'train_nll': -1398.4549560546875, 'test_nll': 1178619.125, 'train_mse': 0.006960059516131878, 'state_dict_file': 'model_state_dict_-7294170671551055859.pkl'}\n",
      "2019-03-16 15:53:53.146198, fold=8, rep=1, eta=0d 0h 0m 44s \n",
      "{'fold': 8, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.0027808004524558783, 'train_time': 23.349596804007888, 'prior_train_nmll': -0.48327508568763733, 'train_nll': -1654.2154541015625, 'test_nll': 8115197.5, 'train_mse': 0.0009083718759939075, 'state_dict_file': 'model_state_dict_-1087263307368667676.pkl'}\n",
      "2019-03-16 15:54:17.825119, fold=9, rep=0, eta=0d 0h 0m 22s \n",
      "{'fold': 9, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.009536021389067173, 'train_time': 24.67561650101561, 'prior_train_nmll': -0.41331905126571655, 'train_nll': -1763.4765625, 'test_nll': 427704.9375, 'train_mse': 0.00972032267600298, 'state_dict_file': 'model_state_dict_-3419506697941858556.pkl'}\n",
      "2019-03-16 15:54:42.454032, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.0037599769420921803, 'train_time': 24.628609100007452, 'prior_train_nmll': -0.2860187590122223, 'train_nll': -1090.2294921875, 'test_nll': 157141312.0, 'train_mse': 0.0027943497989326715, 'state_dict_file': 'model_state_dict_5001816524376734273.pkl'}\n",
      "2019-03-16 15:55:09.458866, fold=0, rep=0, eta=0d 0h 8m 32s \n",
      "{'fold': 0, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.11392232030630112, 'train_time': 26.98102852399461, 'prior_train_nmll': 0.17406989634037018, 'train_nll': -151.4163818359375, 'test_nll': 16.773189544677734, 'train_mse': 0.017773212864995003, 'state_dict_file': 'model_state_dict_1771873671086591898.pkl'}\n",
      "2019-03-16 15:55:32.309157, fold=0, rep=1, eta=0d 0h 7m 28s \n",
      "{'fold': 0, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.12444359064102173, 'train_time': 22.850042975973338, 'prior_train_nmll': 0.19932827353477478, 'train_nll': -113.99179077148438, 'test_nll': 21.193317413330078, 'train_mse': 0.02428743802011013, 'state_dict_file': 'model_state_dict_-6672282064641935122.pkl'}\n",
      "2019-03-16 15:56:13.256872, fold=1, rep=0, eta=0d 0h 8m 34s \n",
      "{'fold': 1, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.17407388985157013, 'train_time': 40.944431648007594, 'prior_train_nmll': 0.12478956580162048, 'train_nll': -226.604736328125, 'test_nll': 54.18091583251953, 'train_mse': 0.013462276197969913, 'state_dict_file': 'model_state_dict_-3595850344533003598.pkl'}\n",
      "2019-03-16 15:56:50.751683, fold=1, rep=1, eta=0d 0h 8m 33s \n",
      "{'fold': 1, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.1949479877948761, 'train_time': 37.49451517208945, 'prior_train_nmll': 0.115674689412117, 'train_nll': -233.65447998046875, 'test_nll': 61.980018615722656, 'train_mse': 0.010434665717184544, 'state_dict_file': 'model_state_dict_-3408056285208681650.pkl'}\n",
      "2019-03-16 15:57:17.898697, fold=2, rep=0, eta=0d 0h 7m 46s \n",
      "{'fold': 2, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.1586117148399353, 'train_time': 27.14347456896212, 'prior_train_nmll': 0.14078684151172638, 'train_nll': -465.1041259765625, 'test_nll': 45.70750045776367, 'train_mse': 0.018014531582593918, 'state_dict_file': 'model_state_dict_-8173652860056193171.pkl'}\n",
      "2019-03-16 15:57:50.494101, fold=2, rep=1, eta=0d 0h 7m 18s \n",
      "{'fold': 2, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.14228394627571106, 'train_time': 32.59518493700307, 'prior_train_nmll': 0.15751047432422638, 'train_nll': -150.25201416015625, 'test_nll': 34.45951461791992, 'train_mse': 0.018629448488354683, 'state_dict_file': 'model_state_dict_126823933648032168.pkl'}\n",
      "2019-03-16 15:58:24.874586, fold=3, rep=0, eta=0d 0h 6m 53s \n",
      "{'fold': 3, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.14089304208755493, 'train_time': 34.376993611920625, 'prior_train_nmll': 0.1737264096736908, 'train_nll': -159.68963623046875, 'test_nll': 33.060726165771484, 'train_mse': 0.017319737002253532, 'state_dict_file': 'model_state_dict_-2925961855644428048.pkl'}\n",
      "2019-03-16 15:58:54.914958, fold=3, rep=1, eta=0d 0h 6m 18s \n",
      "{'fold': 3, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.1925637274980545, 'train_time': 30.040118752978742, 'prior_train_nmll': 0.11731527000665665, 'train_nll': 268722.28125, 'test_nll': 62.176456451416016, 'train_mse': 0.00925293006002903, 'state_dict_file': 'model_state_dict_7472931209694835466.pkl'}\n",
      "2019-03-16 15:59:22.444258, fold=4, rep=0, eta=0d 0h 5m 42s \n",
      "{'fold': 4, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.12714651226997375, 'train_time': 27.52577621396631, 'prior_train_nmll': 0.19026900827884674, 'train_nll': -102.75039672851562, 'test_nll': 20.442825317382812, 'train_mse': 0.02480829320847988, 'state_dict_file': 'model_state_dict_-145622379304072577.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:59:51.382985, fold=4, rep=1, eta=0d 0h 5m 8s \n",
      "{'fold': 4, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.1724410355091095, 'train_time': 28.938469129963778, 'prior_train_nmll': 0.1446312814950943, 'train_nll': -169.029052734375, 'test_nll': 50.61069107055664, 'train_mse': 0.016567975282669067, 'state_dict_file': 'model_state_dict_-6988182838259350375.pkl'}\n",
      "2019-03-16 16:00:26.157582, fold=5, rep=0, eta=0d 0h 4m 41s \n",
      "{'fold': 5, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.08835114538669586, 'train_time': 34.771137755014934, 'prior_train_nmll': 0.16706109046936035, 'train_nll': -118.30990600585938, 'test_nll': 10.056137084960938, 'train_mse': 0.02344626747071743, 'state_dict_file': 'model_state_dict_5908412381593831531.pkl'}\n",
      "2019-03-16 16:00:59.531087, fold=5, rep=1, eta=0d 0h 4m 11s \n",
      "{'fold': 5, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.08967592567205429, 'train_time': 33.37325592292473, 'prior_train_nmll': 0.17613078653812408, 'train_nll': 1619.868408203125, 'test_nll': 11.369644165039062, 'train_mse': 0.01663113757967949, 'state_dict_file': 'model_state_dict_-4644375233706760931.pkl'}\n",
      "2019-03-16 16:01:28.839007, fold=6, rep=0, eta=0d 0h 3m 38s \n",
      "{'fold': 6, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.10656015574932098, 'train_time': 29.304404860944487, 'prior_train_nmll': 0.17072910070419312, 'train_nll': -138.586669921875, 'test_nll': 15.162673950195312, 'train_mse': 0.01914430409669876, 'state_dict_file': 'model_state_dict_-6696308288339615077.pkl'}\n",
      "2019-03-16 16:02:01.357256, fold=6, rep=1, eta=0d 0h 3m 8s \n",
      "{'fold': 6, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.10437855869531631, 'train_time': 32.51801067695487, 'prior_train_nmll': 0.15665321052074432, 'train_nll': -187.0146484375, 'test_nll': 15.242721557617188, 'train_mse': 0.015091383829712868, 'state_dict_file': 'model_state_dict_1422993420796903884.pkl'}\n",
      "2019-03-16 16:02:30.389151, fold=7, rep=0, eta=0d 0h 2m 35s \n",
      "{'fold': 7, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.2600601017475128, 'train_time': 29.028322221012786, 'prior_train_nmll': 0.13333353400230408, 'train_nll': -132.44918823242188, 'test_nll': 66.43347930908203, 'train_mse': 0.020972033962607384, 'state_dict_file': 'model_state_dict_-3504028218200932388.pkl'}\n",
      "2019-03-16 16:03:12.633008, fold=7, rep=1, eta=0d 0h 2m 7s \n",
      "{'fold': 7, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.32972365617752075, 'train_time': 42.2436333730584, 'prior_train_nmll': 0.10313092917203903, 'train_nll': -177.61993408203125, 'test_nll': 101.80606079101562, 'train_mse': 0.016370074823498726, 'state_dict_file': 'model_state_dict_4652636463916008420.pkl'}\n",
      "2019-03-16 16:04:03.485771, fold=8, rep=0, eta=0d 0h 1m 39s \n",
      "{'fold': 8, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.12107175588607788, 'train_time': 50.84922797197942, 'prior_train_nmll': 0.14314207434654236, 'train_nll': -13.0506591796875, 'test_nll': 23.206005096435547, 'train_mse': 0.020234571769833565, 'state_dict_file': 'model_state_dict_-4091703430369849128.pkl'}\n",
      "2019-03-16 16:04:30.375567, fold=8, rep=1, eta=0d 0h 1m 5s \n",
      "{'fold': 8, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.1977182924747467, 'train_time': 26.889534714980982, 'prior_train_nmll': 0.13880683481693268, 'train_nll': -155.739990234375, 'test_nll': 56.956138610839844, 'train_mse': 0.018002184107899666, 'state_dict_file': 'model_state_dict_-2032982154520199096.pkl'}\n",
      "2019-03-16 16:05:28.720095, fold=9, rep=0, eta=0d 0h 0m 34s \n",
      "{'fold': 9, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.1227860152721405, 'train_time': 58.34089491004124, 'prior_train_nmll': 0.1409958451986313, 'train_nll': -21.707061767578125, 'test_nll': 26.88729476928711, 'train_mse': 0.011370742693543434, 'state_dict_file': 'model_state_dict_-200739261493315471.pkl'}\n",
      "2019-03-16 16:06:13.167945, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.16673994064331055, 'train_time': 44.44759537500795, 'prior_train_nmll': 0.11096570640802383, 'train_nll': -199.34649658203125, 'test_nll': 34.366783142089844, 'train_mse': 0.016130022704601288, 'state_dict_file': 'model_state_dict_-6047274387754345528.pkl'}\n",
      "2019-03-16 16:06:57.244473, fold=0, rep=0, eta=0d 0h 13m 56s \n",
      "{'fold': 0, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.1406373530626297, 'train_time': 44.05053236708045, 'prior_train_nmll': 0.17434991896152496, 'train_nll': -77.96237182617188, 'test_nll': 35.399532318115234, 'train_mse': 0.003950240556150675, 'state_dict_file': 'model_state_dict_-6350674967322665025.pkl'}\n",
      "2019-03-16 16:07:38.306188, fold=0, rep=1, eta=0d 0h 12m 46s \n",
      "{'fold': 0, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.15979863703250885, 'train_time': 41.061474314075895, 'prior_train_nmll': 0.23833543062210083, 'train_nll': -370.2430114746094, 'test_nll': 39.10065841674805, 'train_mse': 0.00573227321729064, 'state_dict_file': 'model_state_dict_3317749488168573405.pkl'}\n",
      "2019-03-16 16:08:18.737783, fold=1, rep=0, eta=0d 0h 11m 51s \n",
      "{'fold': 1, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.12758983671665192, 'train_time': 40.42829534201883, 'prior_train_nmll': 0.24341146647930145, 'train_nll': -439.8300476074219, 'test_nll': 76.78819274902344, 'train_mse': 0.005233184900134802, 'state_dict_file': 'model_state_dict_-5439291334689352492.pkl'}\n",
      "2019-03-16 16:09:00.033919, fold=1, rep=1, eta=0d 0h 11m 7s \n",
      "{'fold': 1, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.10516941547393799, 'train_time': 41.29586578998715, 'prior_train_nmll': 0.28663235902786255, 'train_nll': 5855.3173828125, 'test_nll': 39.061161041259766, 'train_mse': 0.005239289253950119, 'state_dict_file': 'model_state_dict_-6999366384737615163.pkl'}\n",
      "2019-03-16 16:09:35.526960, fold=2, rep=0, eta=0d 0h 10m 7s \n",
      "{'fold': 2, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.2802201509475708, 'train_time': 35.48999300401192, 'prior_train_nmll': 0.23457874357700348, 'train_nll': -467.6391906738281, 'test_nll': 89.7015380859375, 'train_mse': 0.006542632821947336, 'state_dict_file': 'model_state_dict_2018421074638962969.pkl'}\n",
      "2019-03-16 16:10:23.845605, fold=2, rep=1, eta=0d 0h 9m 44s \n",
      "{'fold': 2, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.20331647992134094, 'train_time': 48.31840003200341, 'prior_train_nmll': 0.2476080060005188, 'train_nll': -457.7607727050781, 'test_nll': 98.37507629394531, 'train_mse': 0.004693666007369757, 'state_dict_file': 'model_state_dict_-5780949218782133261.pkl'}\n",
      "2019-03-16 16:11:05.270859, fold=3, rep=0, eta=0d 0h 9m 2s \n",
      "{'fold': 3, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.26717475056648254, 'train_time': 41.42182444105856, 'prior_train_nmll': 0.14882448315620422, 'train_nll': -707.547119140625, 'test_nll': 121.28274536132812, 'train_mse': 0.004450026899576187, 'state_dict_file': 'model_state_dict_8275287097312356955.pkl'}\n",
      "2019-03-16 16:11:46.550701, fold=3, rep=1, eta=0d 0h 8m 20s \n",
      "{'fold': 3, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.20587673783302307, 'train_time': 41.27957691100892, 'prior_train_nmll': 0.14440728724002838, 'train_nll': 5396.279296875, 'test_nll': 92.80891418457031, 'train_mse': 0.005215902347117662, 'state_dict_file': 'model_state_dict_5472255642912601257.pkl'}\n",
      "2019-03-16 16:12:35.549028, fold=4, rep=0, eta=0d 0h 7m 47s \n",
      "{'fold': 4, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.2523859143257141, 'train_time': 48.994772939011455, 'prior_train_nmll': 0.2139149308204651, 'train_nll': -428.3529357910156, 'test_nll': 68.63706970214844, 'train_mse': 0.00477692810818553, 'state_dict_file': 'model_state_dict_185522896563692389.pkl'}\n",
      "2019-03-16 16:13:18.298993, fold=4, rep=1, eta=0d 0h 7m 5s \n",
      "{'fold': 4, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.22401225566864014, 'train_time': 42.749720401945524, 'prior_train_nmll': 0.21433116495609283, 'train_nll': -364248.125, 'test_nll': 73.46791076660156, 'train_mse': 0.004925157409161329, 'state_dict_file': 'model_state_dict_-7731946865093898761.pkl'}\n",
      "2019-03-16 16:14:04.195867, fold=5, rep=0, eta=0d 0h 6m 25s \n",
      "{'fold': 5, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.17780335247516632, 'train_time': 45.89340811804868, 'prior_train_nmll': 0.2195064127445221, 'train_nll': -984.79052734375, 'test_nll': 54.52536392211914, 'train_mse': 0.004033313132822514, 'state_dict_file': 'model_state_dict_839082211661046915.pkl'}\n",
      "2019-03-16 16:14:47.794601, fold=5, rep=1, eta=0d 0h 5m 43s \n",
      "{'fold': 5, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.1185406893491745, 'train_time': 43.59846407198347, 'prior_train_nmll': 0.18531741201877594, 'train_nll': -636.4403076171875, 'test_nll': 34.44002914428711, 'train_mse': 0.004202378913760185, 'state_dict_file': 'model_state_dict_2657733512494709757.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 16:15:32.478941, fold=6, rep=0, eta=0d 0h 5m 1s \n",
      "{'fold': 6, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.1643943041563034, 'train_time': 44.68115670001134, 'prior_train_nmll': 0.22544990479946136, 'train_nll': -633.69921875, 'test_nll': 67.48878479003906, 'train_mse': 0.004811123479157686, 'state_dict_file': 'model_state_dict_-2682496804543518167.pkl'}\n",
      "2019-03-16 16:16:17.036576, fold=6, rep=1, eta=0d 0h 4m 18s \n",
      "{'fold': 6, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.13831764459609985, 'train_time': 44.55739258194808, 'prior_train_nmll': 0.2290034145116806, 'train_nll': -744.3709716796875, 'test_nll': 56.61843490600586, 'train_mse': 0.005006663501262665, 'state_dict_file': 'model_state_dict_-4768241488200235452.pkl'}\n",
      "2019-03-16 16:17:11.963603, fold=7, rep=0, eta=0d 0h 3m 39s \n",
      "{'fold': 7, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.2620334327220917, 'train_time': 54.92352542595472, 'prior_train_nmll': 0.20642919838428497, 'train_nll': -657.6129150390625, 'test_nll': 87.19322204589844, 'train_mse': 0.003367049153894186, 'state_dict_file': 'model_state_dict_-2638286904156489157.pkl'}\n",
      "2019-03-16 16:17:51.082026, fold=7, rep=1, eta=0d 0h 2m 54s \n",
      "{'fold': 7, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.1321890950202942, 'train_time': 39.11812567396555, 'prior_train_nmll': 0.2095034271478653, 'train_nll': -226.32913208007812, 'test_nll': 31.233394622802734, 'train_mse': 0.0076355598866939545, 'state_dict_file': 'model_state_dict_6953030707443113655.pkl'}\n",
      "2019-03-16 16:18:29.103638, fold=8, rep=0, eta=0d 0h 2m 9s \n",
      "{'fold': 8, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.19893696904182434, 'train_time': 38.01844265602995, 'prior_train_nmll': 0.26126956939697266, 'train_nll': -181.50588989257812, 'test_nll': 64.07026672363281, 'train_mse': 0.006530562415719032, 'state_dict_file': 'model_state_dict_4109449124248256255.pkl'}\n",
      "2019-03-16 16:19:07.496575, fold=8, rep=1, eta=0d 0h 1m 26s \n",
      "{'fold': 8, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.10744662582874298, 'train_time': 38.39265062205959, 'prior_train_nmll': 0.2084430307149887, 'train_nll': -1633.9302978515625, 'test_nll': 20.856952667236328, 'train_mse': 0.005716734565794468, 'state_dict_file': 'model_state_dict_7954601859813550344.pkl'}\n",
      "2019-03-16 16:19:45.501368, fold=9, rep=0, eta=0d 0h 0m 42s \n",
      "{'fold': 9, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.12690497934818268, 'train_time': 38.00157074199524, 'prior_train_nmll': 0.19915923476219177, 'train_nll': -368.8572998046875, 'test_nll': 39.25920104980469, 'train_mse': 0.006978449411690235, 'state_dict_file': 'model_state_dict_-7304403770821877721.pkl'}\n",
      "2019-03-16 16:20:22.121032, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.23419718444347382, 'train_time': 36.61940222000703, 'prior_train_nmll': 0.2054329514503479, 'train_nll': 3978.850341796875, 'test_nll': 58.634727478027344, 'train_mse': 0.006324142683297396, 'state_dict_file': 'model_state_dict_-6695017290534985104.pkl'}\n",
      "2019-03-16 16:22:19.453963, fold=0, rep=0, eta=0d 0h 37m 8s \n",
      "{'fold': 0, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.4141228199005127, 'train_time': 117.3050604229793, 'prior_train_nmll': 1.0127620697021484, 'train_nll': -93.68911743164062, 'test_nll': 239.6928253173828, 'train_mse': 0.012759950943291187, 'state_dict_file': 'model_state_dict_7867299116898803997.pkl'}\n",
      "2019-03-16 16:24:36.193065, fold=0, rep=1, eta=0d 0h 38m 6s \n",
      "{'fold': 0, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.4790899753570557, 'train_time': 136.73887651099358, 'prior_train_nmll': 0.9092207551002502, 'train_nll': -129.64938354492188, 'test_nll': 269.77801513671875, 'train_mse': 0.01140739768743515, 'state_dict_file': 'model_state_dict_849772071826303898.pkl'}\n",
      "2019-03-16 16:26:15.424556, fold=1, rep=0, eta=0d 0h 33m 21s \n",
      "{'fold': 1, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.9098286628723145, 'train_time': 99.22802049305756, 'prior_train_nmll': 0.974236011505127, 'train_nll': -106.50137329101562, 'test_nll': 277.44818115234375, 'train_mse': 0.011666769161820412, 'state_dict_file': 'model_state_dict_-3652224077926875203.pkl'}\n",
      "2019-03-16 16:28:00.661837, fold=1, rep=1, eta=0d 0h 30m 34s \n",
      "{'fold': 1, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.8528128862380981, 'train_time': 105.23703644692432, 'prior_train_nmll': 0.9673808813095093, 'train_nll': -103.91629028320312, 'test_nll': 344.6268310546875, 'train_mse': 0.01374739222228527, 'state_dict_file': 'model_state_dict_-7416870123927320616.pkl'}\n",
      "2019-03-16 16:30:12.118618, fold=2, rep=0, eta=0d 0h 29m 29s \n",
      "{'fold': 2, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.4378724098205566, 'train_time': 131.45336289703846, 'prior_train_nmll': 0.9811270833015442, 'train_nll': -103.87088012695312, 'test_nll': 210.83641052246094, 'train_mse': 0.011904753744602203, 'state_dict_file': 'model_state_dict_2023856577887823546.pkl'}\n",
      "2019-03-16 16:32:06.020670, fold=2, rep=1, eta=0d 0h 27m 22s \n",
      "{'fold': 2, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.3545629978179932, 'train_time': 113.90175089996774, 'prior_train_nmll': 0.9240682721138, 'train_nll': -108.66171264648438, 'test_nll': 231.56414794921875, 'train_mse': 0.012009416706860065, 'state_dict_file': 'model_state_dict_-8838288344178290494.pkl'}\n",
      "2019-03-16 16:33:56.401350, fold=3, rep=0, eta=0d 0h 25m 12s \n",
      "{'fold': 3, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 2.1583526134490967, 'train_time': 110.3775150309084, 'prior_train_nmll': 0.9403313994407654, 'train_nll': -104.27761840820312, 'test_nll': 334.639404296875, 'train_mse': 0.012353683821856976, 'state_dict_file': 'model_state_dict_7187732174151903888.pkl'}\n",
      "2019-03-16 16:35:52.815216, fold=3, rep=1, eta=0d 0h 23m 16s \n",
      "{'fold': 3, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 2.339597702026367, 'train_time': 116.41352381499019, 'prior_train_nmll': 0.9181825518608093, 'train_nll': -124.66122436523438, 'test_nll': 381.7451171875, 'train_mse': 0.01151000801473856, 'state_dict_file': 'model_state_dict_-4341514196387778211.pkl'}\n",
      "2019-03-16 16:37:35.453868, fold=4, rep=0, eta=0d 0h 21m 2s \n",
      "{'fold': 4, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.9176911115646362, 'train_time': 102.63470020890236, 'prior_train_nmll': 0.9637883901596069, 'train_nll': -105.12429809570312, 'test_nll': 335.18707275390625, 'train_mse': 0.01232060045003891, 'state_dict_file': 'model_state_dict_-5368432392140436347.pkl'}\n",
      "2019-03-16 16:39:01.736115, fold=4, rep=1, eta=0d 0h 18m 39s \n",
      "{'fold': 4, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.6786859035491943, 'train_time': 86.28199304197915, 'prior_train_nmll': 0.9108984470367432, 'train_nll': -116.53866577148438, 'test_nll': 300.4266052246094, 'train_mse': 0.012900236994028091, 'state_dict_file': 'model_state_dict_-1391349725188029534.pkl'}\n",
      "2019-03-16 16:41:02.237271, fold=5, rep=0, eta=0d 0h 16m 54s \n",
      "{'fold': 5, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.648552417755127, 'train_time': 120.4976633999031, 'prior_train_nmll': 0.9218929409980774, 'train_nll': -155.74649047851562, 'test_nll': 319.45263671875, 'train_mse': 0.00931717362254858, 'state_dict_file': 'model_state_dict_-7821658022649644733.pkl'}\n",
      "2019-03-16 16:42:16.556483, fold=5, rep=1, eta=0d 0h 14m 36s \n",
      "{'fold': 5, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.9868830442428589, 'train_time': 74.31895980599802, 'prior_train_nmll': 0.9927984476089478, 'train_nll': -107.40567016601562, 'test_nll': 278.04296875, 'train_mse': 0.011636706069111824, 'state_dict_file': 'model_state_dict_-3433343680978836901.pkl'}\n",
      "2019-03-16 16:44:35.846153, fold=6, rep=0, eta=0d 0h 13m 2s \n",
      "{'fold': 6, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.3390412330627441, 'train_time': 139.28613443707582, 'prior_train_nmll': 0.9353432655334473, 'train_nll': -166.24447631835938, 'test_nll': 245.0000457763672, 'train_mse': 0.008159130811691284, 'state_dict_file': 'model_state_dict_4150057987464401164.pkl'}\n",
      "2019-03-16 16:46:35.450353, fold=6, rep=1, eta=0d 0h 11m 14s \n",
      "{'fold': 6, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.3813440799713135, 'train_time': 119.6038895950187, 'prior_train_nmll': 0.9010919332504272, 'train_nll': -171.14913940429688, 'test_nll': 246.0154571533203, 'train_mse': 0.008366131223738194, 'state_dict_file': 'model_state_dict_5658920921027790168.pkl'}\n",
      "2019-03-16 16:48:33.012740, fold=7, rep=0, eta=0d 0h 9m 23s \n",
      "{'fold': 7, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.0896916389465332, 'train_time': 117.55876539205201, 'prior_train_nmll': 0.942841649055481, 'train_nll': -73.270751953125, 'test_nll': 172.48997497558594, 'train_mse': 0.01539577730000019, 'state_dict_file': 'model_state_dict_-3560199524026181322.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 16:50:26.296606, fold=7, rep=1, eta=0d 0h 7m 31s \n",
      "{'fold': 7, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.1789768934249878, 'train_time': 113.28359137196094, 'prior_train_nmll': 0.9860050082206726, 'train_nll': -28.3173828125, 'test_nll': 158.3406524658203, 'train_mse': 0.01896815001964569, 'state_dict_file': 'model_state_dict_-8394055197699233599.pkl'}\n",
      "2019-03-16 16:53:02.743707, fold=8, rep=0, eta=0d 0h 5m 45s \n",
      "{'fold': 8, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 2.4667649269104004, 'train_time': 156.4438734789146, 'prior_train_nmll': 0.902582585811615, 'train_nll': -82.21484375, 'test_nll': 380.3978271484375, 'train_mse': 0.01582585833966732, 'state_dict_file': 'model_state_dict_8695866091731253082.pkl'}\n",
      "2019-03-16 16:55:03.235284, fold=8, rep=1, eta=0d 0h 3m 51s \n",
      "{'fold': 8, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 2.0212414264678955, 'train_time': 120.49088609591126, 'prior_train_nmll': 0.9633032083511353, 'train_nll': -85.27529907226562, 'test_nll': 294.12237548828125, 'train_mse': 0.01482625026255846, 'state_dict_file': 'model_state_dict_2387182587207907637.pkl'}\n",
      "2019-03-16 16:56:54.745930, fold=9, rep=0, eta=0d 0h 1m 55s \n",
      "{'fold': 9, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.5000429153442383, 'train_time': 111.50693470996339, 'prior_train_nmll': 0.9752531051635742, 'train_nll': -58.58087158203125, 'test_nll': 206.08541870117188, 'train_mse': 0.016685254871845245, 'state_dict_file': 'model_state_dict_-5345454817978472647.pkl'}\n",
      "2019-03-16 16:59:25.303316, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.3956069946289062, 'train_time': 150.55709788005333, 'prior_train_nmll': 0.9477730393409729, 'train_nll': -92.95098876953125, 'test_nll': 195.60394287109375, 'train_mse': 0.014403334818780422, 'state_dict_file': 'model_state_dict_-334664140252720834.pkl'}\n",
      "2019-03-16 16:59:56.339008, fold=0, rep=0, eta=0d 0h 9m 49s \n",
      "{'fold': 0, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.2097003012895584, 'train_time': 31.0064683459932, 'prior_train_nmll': 0.3865593373775482, 'train_nll': -232.60006713867188, 'test_nll': 52.76676940917969, 'train_mse': 0.01298761647194624, 'state_dict_file': 'model_state_dict_1992043468515185699.pkl'}\n",
      "2019-03-16 17:00:27.823577, fold=0, rep=1, eta=0d 0h 9m 22s \n",
      "{'fold': 0, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.16951967775821686, 'train_time': 31.48430189699866, 'prior_train_nmll': 0.39962732791900635, 'train_nll': -256.0716247558594, 'test_nll': 52.52117919921875, 'train_mse': 0.011384747922420502, 'state_dict_file': 'model_state_dict_7785103842510130407.pkl'}\n",
      "2019-03-16 17:01:01.301149, fold=1, rep=0, eta=0d 0h 9m 3s \n",
      "{'fold': 1, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.2572002708911896, 'train_time': 33.474456968018785, 'prior_train_nmll': 0.3368043899536133, 'train_nll': -349.6499328613281, 'test_nll': 77.28097534179688, 'train_mse': 0.009748613461852074, 'state_dict_file': 'model_state_dict_1082283794725101641.pkl'}\n",
      "2019-03-16 17:01:30.099986, fold=1, rep=1, eta=0d 0h 8m 19s \n",
      "{'fold': 1, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.25418365001678467, 'train_time': 28.798552133957855, 'prior_train_nmll': 0.4049648940563202, 'train_nll': 50.739837646484375, 'test_nll': 69.13827514648438, 'train_mse': 0.01338245440274477, 'state_dict_file': 'model_state_dict_-5164977340315114184.pkl'}\n",
      "2019-03-16 17:02:04.887474, fold=2, rep=0, eta=0d 0h 7m 58s \n",
      "{'fold': 2, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.24536973237991333, 'train_time': 34.78437959996518, 'prior_train_nmll': 0.42198076844215393, 'train_nll': -275.3241882324219, 'test_nll': 59.35009002685547, 'train_mse': 0.009337215684354305, 'state_dict_file': 'model_state_dict_-2068852555333946321.pkl'}\n",
      "2019-03-16 17:02:31.931811, fold=2, rep=1, eta=0d 0h 7m 15s \n",
      "{'fold': 2, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.19435346126556396, 'train_time': 27.043871904024854, 'prior_train_nmll': 0.4718557894229889, 'train_nll': -195.02743530273438, 'test_nll': 53.679771423339844, 'train_mse': 0.017600731924176216, 'state_dict_file': 'model_state_dict_2106598072742245387.pkl'}\n",
      "2019-03-16 17:03:27.504272, fold=3, rep=0, eta=0d 0h 7m 29s \n",
      "{'fold': 3, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.23812657594680786, 'train_time': 55.56932054599747, 'prior_train_nmll': 0.357564777135849, 'train_nll': -346.8020935058594, 'test_nll': 85.86631774902344, 'train_mse': 0.007063053082674742, 'state_dict_file': 'model_state_dict_-4705749008631694063.pkl'}\n",
      "2019-03-16 17:04:02.271825, fold=3, rep=1, eta=0d 0h 6m 55s \n",
      "{'fold': 3, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.1369646191596985, 'train_time': 34.76732385600917, 'prior_train_nmll': 0.35061898827552795, 'train_nll': -169.75454711914062, 'test_nll': 38.01734924316406, 'train_mse': 0.009997034445405006, 'state_dict_file': 'model_state_dict_-8396658373502201495.pkl'}\n",
      "2019-03-16 17:04:35.360243, fold=4, rep=0, eta=0d 0h 6m 18s \n",
      "{'fold': 4, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.19692103564739227, 'train_time': 33.08485161699355, 'prior_train_nmll': 0.41985759139060974, 'train_nll': -262.5577697753906, 'test_nll': 55.498748779296875, 'train_mse': 0.011594617739319801, 'state_dict_file': 'model_state_dict_-1888436802293793367.pkl'}\n",
      "2019-03-16 17:05:04.464942, fold=4, rep=1, eta=0d 0h 5m 39s \n",
      "{'fold': 4, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.2200087308883667, 'train_time': 29.10442486696411, 'prior_train_nmll': 0.4066065847873688, 'train_nll': -242.92404174804688, 'test_nll': 63.17870330810547, 'train_mse': 0.013664613477885723, 'state_dict_file': 'model_state_dict_6597383531310102284.pkl'}\n",
      "2019-03-16 17:05:35.000107, fold=5, rep=0, eta=0d 0h 5m 2s \n",
      "{'fold': 5, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.19061371684074402, 'train_time': 30.531998469028622, 'prior_train_nmll': 0.45805710554122925, 'train_nll': -174.49105834960938, 'test_nll': 41.26607894897461, 'train_mse': 0.01587134413421154, 'state_dict_file': 'model_state_dict_3494528373955860348.pkl'}\n",
      "2019-03-16 17:06:03.988610, fold=5, rep=1, eta=0d 0h 4m 25s \n",
      "{'fold': 5, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.15333937108516693, 'train_time': 28.98780204099603, 'prior_train_nmll': 0.40378206968307495, 'train_nll': -211.84231567382812, 'test_nll': 43.38145446777344, 'train_mse': 0.013789392076432705, 'state_dict_file': 'model_state_dict_-8062593438545614505.pkl'}\n",
      "2019-03-16 17:07:01.298472, fold=6, rep=0, eta=0d 0h 4m 5s \n",
      "{'fold': 6, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.400647908449173, 'train_time': 57.30670556204859, 'prior_train_nmll': 0.3293085992336273, 'train_nll': -134.72561645507812, 'test_nll': 88.1484375, 'train_mse': 0.007100233342498541, 'state_dict_file': 'model_state_dict_4517752403205142671.pkl'}\n",
      "2019-03-16 17:07:49.149114, fold=6, rep=1, eta=0d 0h 3m 35s \n",
      "{'fold': 6, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.5399431586265564, 'train_time': 47.85033102205489, 'prior_train_nmll': 0.37238726019859314, 'train_nll': -308.2693786621094, 'test_nll': 114.95220947265625, 'train_mse': 0.009153407998383045, 'state_dict_file': 'model_state_dict_1980152420034258115.pkl'}\n",
      "2019-03-16 17:08:16.684792, fold=7, rep=0, eta=0d 0h 2m 57s \n",
      "{'fold': 7, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.26220765709877014, 'train_time': 27.532396293012425, 'prior_train_nmll': 0.4367961287498474, 'train_nll': -205.05587768554688, 'test_nll': 44.370361328125, 'train_mse': 0.015364776365458965, 'state_dict_file': 'model_state_dict_-5232915558805330759.pkl'}\n",
      "2019-03-16 17:08:46.076080, fold=7, rep=1, eta=0d 0h 2m 20s \n",
      "{'fold': 7, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.22348977625370026, 'train_time': 29.391035001026466, 'prior_train_nmll': 0.37271150946617126, 'train_nll': -226.32827758789062, 'test_nll': 35.87563705444336, 'train_mse': 0.014695785008370876, 'state_dict_file': 'model_state_dict_2676517399163477771.pkl'}\n",
      "2019-03-16 17:09:14.451389, fold=8, rep=0, eta=0d 0h 1m 43s \n",
      "{'fold': 8, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.22930443286895752, 'train_time': 28.371696457033977, 'prior_train_nmll': 0.3998384475708008, 'train_nll': -175.42282104492188, 'test_nll': 50.31402587890625, 'train_mse': 0.01730787754058838, 'state_dict_file': 'model_state_dict_-2005494686572110605.pkl'}\n",
      "2019-03-16 17:09:47.867007, fold=8, rep=1, eta=0d 0h 1m 9s \n",
      "{'fold': 8, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.22791634500026703, 'train_time': 33.41495796502568, 'prior_train_nmll': 0.49088120460510254, 'train_nll': -249.21932983398438, 'test_nll': 50.218353271484375, 'train_mse': 0.012249787338078022, 'state_dict_file': 'model_state_dict_-3802677012907239239.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 17:10:14.875619, fold=9, rep=0, eta=0d 0h 0m 34s \n",
      "{'fold': 9, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.1901572197675705, 'train_time': 27.005351635045372, 'prior_train_nmll': 0.37345749139785767, 'train_nll': -202.61114501953125, 'test_nll': 56.70737838745117, 'train_mse': 0.016008496284484863, 'state_dict_file': 'model_state_dict_6204907276785055143.pkl'}\n",
      "2019-03-16 17:10:44.955468, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.1900961995124817, 'train_time': 30.07951354107354, 'prior_train_nmll': 0.41003742814064026, 'train_nll': -254.7215576171875, 'test_nll': 64.25230407714844, 'train_mse': 0.012227339670062065, 'state_dict_file': 'model_state_dict_1873593972695175368.pkl'}\n",
      "2019-03-16 17:11:48.495904, fold=0, rep=0, eta=0d 0h 20m 6s \n",
      "{'fold': 0, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.7408064007759094, 'train_time': 63.510896897059865, 'prior_train_nmll': 0.7592323422431946, 'train_nll': 1468.589599609375, 'test_nll': 148.31167602539062, 'train_mse': 0.00874989666044712, 'state_dict_file': 'model_state_dict_2285637333798083509.pkl'}\n",
      "2019-03-16 17:12:59.485794, fold=0, rep=1, eta=0d 0h 20m 10s \n",
      "{'fold': 0, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.7387932538986206, 'train_time': 70.9896059599705, 'prior_train_nmll': 0.7762901186943054, 'train_nll': 1246.440185546875, 'test_nll': 210.5229034423828, 'train_mse': 0.008629160933196545, 'state_dict_file': 'model_state_dict_-3425945511495139781.pkl'}\n",
      "2019-03-16 17:14:06.421019, fold=1, rep=0, eta=0d 0h 19m 1s \n",
      "{'fold': 1, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.5970796942710876, 'train_time': 66.93203893501777, 'prior_train_nmll': 0.7251006364822388, 'train_nll': -1704.333251953125, 'test_nll': 144.39329528808594, 'train_mse': 0.008740744553506374, 'state_dict_file': 'model_state_dict_2512417726065300465.pkl'}\n",
      "2019-03-16 17:15:02.415110, fold=1, rep=1, eta=0d 0h 17m 9s \n",
      "{'fold': 1, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.5309021472930908, 'train_time': 55.99381251598243, 'prior_train_nmll': 0.6740674376487732, 'train_nll': -753.5659790039062, 'test_nll': 117.25465393066406, 'train_mse': 0.010533473454415798, 'state_dict_file': 'model_state_dict_-8017404394728106308.pkl'}\n",
      "2019-03-16 17:16:18.621586, fold=2, rep=0, eta=0d 0h 16m 40s \n",
      "{'fold': 2, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.43062418699264526, 'train_time': 76.20340501598548, 'prior_train_nmll': 0.6857830286026001, 'train_nll': 32222.568359375, 'test_nll': 118.3277359008789, 'train_mse': 0.005895968526601791, 'state_dict_file': 'model_state_dict_3246101013380162.pkl'}\n",
      "2019-03-16 17:17:17.425128, fold=2, rep=1, eta=0d 0h 15m 15s \n",
      "{'fold': 2, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.43247342109680176, 'train_time': 58.80300408601761, 'prior_train_nmll': 0.7939095497131348, 'train_nll': -1747.483154296875, 'test_nll': 97.76771545410156, 'train_mse': 0.011274764314293861, 'state_dict_file': 'model_state_dict_8323565803649466740.pkl'}\n",
      "2019-03-16 17:18:19.811541, fold=3, rep=0, eta=0d 0h 14m 4s \n",
      "{'fold': 3, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.6295721530914307, 'train_time': 62.3832153880503, 'prior_train_nmll': 0.8206266760826111, 'train_nll': -1065.557861328125, 'test_nll': 109.39442443847656, 'train_mse': 0.01261783204972744, 'state_dict_file': 'model_state_dict_-8119314269247857149.pkl'}\n",
      "2019-03-16 17:19:29.875637, fold=3, rep=1, eta=0d 0h 13m 7s \n",
      "{'fold': 3, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.3097507357597351, 'train_time': 70.06381374702323, 'prior_train_nmll': 0.7550731301307678, 'train_nll': 13830.2705078125, 'test_nll': 85.25849914550781, 'train_mse': 0.006884776521474123, 'state_dict_file': 'model_state_dict_-4175919239926294199.pkl'}\n",
      "2019-03-16 17:20:32.866213, fold=4, rep=0, eta=0d 0h 11m 58s \n",
      "{'fold': 4, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.7110395431518555, 'train_time': 62.98734920506831, 'prior_train_nmll': 0.7913996577262878, 'train_nll': 1056600320.0, 'test_nll': 179.79840087890625, 'train_mse': 0.00969843938946724, 'state_dict_file': 'model_state_dict_5711642864646951999.pkl'}\n",
      "2019-03-16 17:21:41.711370, fold=4, rep=1, eta=0d 0h 10m 56s \n",
      "{'fold': 4, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.78504878282547, 'train_time': 68.84448327601422, 'prior_train_nmll': 0.7661463618278503, 'train_nll': 2475.90869140625, 'test_nll': 244.32037353515625, 'train_mse': 0.006602304521948099, 'state_dict_file': 'model_state_dict_6876869524660588485.pkl'}\n",
      "2019-03-16 17:22:49.820914, fold=5, rep=0, eta=0d 0h 9m 53s \n",
      "{'fold': 5, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.45913174748420715, 'train_time': 68.10635879903566, 'prior_train_nmll': 0.7843028903007507, 'train_nll': -148.95013427734375, 'test_nll': 131.08985900878906, 'train_mse': 0.009125902317464352, 'state_dict_file': 'model_state_dict_7146037368973978130.pkl'}\n",
      "2019-03-16 17:24:03.685859, fold=5, rep=1, eta=0d 0h 8m 52s \n",
      "{'fold': 5, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.33201271295547485, 'train_time': 73.86464745306876, 'prior_train_nmll': 0.8525805473327637, 'train_nll': -506.86895751953125, 'test_nll': 66.95380401611328, 'train_mse': 0.008461972698569298, 'state_dict_file': 'model_state_dict_-7152679512430884783.pkl'}\n",
      "2019-03-16 17:25:07.878082, fold=6, rep=0, eta=0d 0h 7m 44s \n",
      "{'fold': 6, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.9214968681335449, 'train_time': 64.1889861569507, 'prior_train_nmll': 0.7642678618431091, 'train_nll': -699.4254760742188, 'test_nll': 160.65614318847656, 'train_mse': 0.008810787461698055, 'state_dict_file': 'model_state_dict_1627622222620678662.pkl'}\n",
      "2019-03-16 17:26:14.085240, fold=6, rep=1, eta=0d 0h 6m 38s \n",
      "{'fold': 6, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.5847226977348328, 'train_time': 66.2068650110159, 'prior_train_nmll': 0.7983129024505615, 'train_nll': -1456.996826171875, 'test_nll': 129.2827911376953, 'train_mse': 0.009927263483405113, 'state_dict_file': 'model_state_dict_6748294000685543970.pkl'}\n",
      "2019-03-16 17:27:17.104062, fold=7, rep=0, eta=0d 0h 5m 30s \n",
      "{'fold': 7, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.6847034692764282, 'train_time': 63.01554678496905, 'prior_train_nmll': 0.6987704634666443, 'train_nll': 5324706.5, 'test_nll': 125.75717163085938, 'train_mse': 0.007746966555714607, 'state_dict_file': 'model_state_dict_-8900428962176007199.pkl'}\n",
      "2019-03-16 17:28:19.284701, fold=7, rep=1, eta=0d 0h 4m 23s \n",
      "{'fold': 7, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.9381446242332458, 'train_time': 62.18033099605236, 'prior_train_nmll': 0.6729505658149719, 'train_nll': 68190.4375, 'test_nll': 178.78997802734375, 'train_mse': 0.008727038279175758, 'state_dict_file': 'model_state_dict_-6187582708269633827.pkl'}\n",
      "2019-03-16 17:29:22.616083, fold=8, rep=0, eta=0d 0h 3m 17s \n",
      "{'fold': 8, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.3270873725414276, 'train_time': 63.32776196405757, 'prior_train_nmll': 0.7577283978462219, 'train_nll': -1667.87060546875, 'test_nll': 73.38035583496094, 'train_mse': 0.009862439706921577, 'state_dict_file': 'model_state_dict_7622770464410432834.pkl'}\n",
      "2019-03-16 17:30:37.649027, fold=8, rep=1, eta=0d 0h 2m 12s \n",
      "{'fold': 8, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.43837037682533264, 'train_time': 75.0326628300827, 'prior_train_nmll': 0.759375274181366, 'train_nll': -915.0768432617188, 'test_nll': 103.34391784667969, 'train_mse': 0.008201279677450657, 'state_dict_file': 'model_state_dict_-250416272078106495.pkl'}\n",
      "2019-03-16 17:33:09.744077, fold=9, rep=0, eta=0d 0h 1m 10s \n",
      "{'fold': 9, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.5589153170585632, 'train_time': 152.09178967704065, 'prior_train_nmll': 0.7289665937423706, 'train_nll': 72720056.0, 'test_nll': 179.6781005859375, 'train_mse': 0.002754455665126443, 'state_dict_file': 'model_state_dict_6549088493621377020.pkl'}\n",
      "2019-03-16 17:33:59.920865, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.40918123722076416, 'train_time': 50.17649793392047, 'prior_train_nmll': 0.8353244066238403, 'train_nll': 8982587.0, 'test_nll': 91.07383728027344, 'train_mse': 0.016111476346850395, 'state_dict_file': 'model_state_dict_-7917348707441377645.pkl'}\n",
      "2019-03-16 17:36:36.055273, fold=0, rep=0, eta=0d 0h 49m 26s \n",
      "{'fold': 0, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0012276610359549522, 'train_time': 156.10813623806462, 'prior_train_nmll': -1.4506126642227173, 'train_nll': -3908.754638671875, 'test_nll': 763.35986328125, 'train_mse': 0.000753997708670795, 'state_dict_file': 'model_state_dict_-1328766933172112136.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 17:38:59.123783, fold=0, rep=1, eta=0d 0h 44m 52s \n",
      "{'fold': 0, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.001852419227361679, 'train_time': 143.06824141799007, 'prior_train_nmll': -1.1709449291229248, 'train_nll': -2092.158203125, 'test_nll': 2045.0478515625, 'train_mse': 0.0006927034701220691, 'state_dict_file': 'model_state_dict_-6168461113908803357.pkl'}\n",
      "2019-03-16 17:41:22.388673, fold=1, rep=0, eta=0d 0h 41m 47s \n",
      "{'fold': 1, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0020153578370809555, 'train_time': 143.26184108702, 'prior_train_nmll': -1.2324990034103394, 'train_nll': 42904.30078125, 'test_nll': -110.72918701171875, 'train_mse': 0.0007568741566501558, 'state_dict_file': 'model_state_dict_1253948683518519559.pkl'}\n",
      "2019-03-16 17:43:51.386687, fold=1, rep=1, eta=0d 0h 39m 25s \n",
      "{'fold': 1, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0013443792704492807, 'train_time': 148.99774658295792, 'prior_train_nmll': -1.2665939331054688, 'train_nll': -2904.326904296875, 'test_nll': -709.0181884765625, 'train_mse': 0.0006496910937130451, 'state_dict_file': 'model_state_dict_-6163510604186971936.pkl'}\n",
      "2019-03-16 17:47:37.628037, fold=2, rep=0, eta=0d 0h 40m 53s \n",
      "{'fold': 2, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.002014527563005686, 'train_time': 226.2382252520183, 'prior_train_nmll': -1.2949479818344116, 'train_nll': 1113794560.0, 'test_nll': 50605.0625, 'train_mse': 0.00033526375773362815, 'state_dict_file': 'model_state_dict_-2472395858629596522.pkl'}\n",
      "2019-03-16 17:50:41.588076, fold=2, rep=1, eta=0d 0h 38m 57s \n",
      "{'fold': 2, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.003792513394728303, 'train_time': 183.9593253530329, 'prior_train_nmll': -1.207655429840088, 'train_nll': 216386.0, 'test_nll': 46533.14453125, 'train_mse': 0.000546678842511028, 'state_dict_file': 'model_state_dict_-8785999302084177927.pkl'}\n",
      "2019-03-16 17:53:11.159922, fold=3, rep=0, eta=0d 0h 35m 37s \n",
      "{'fold': 3, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0017270209500566125, 'train_time': 149.56865311297588, 'prior_train_nmll': -1.2786182165145874, 'train_nll': 10129482752.0, 'test_nll': 17315.103515625, 'train_mse': 0.000835446931887418, 'state_dict_file': 'model_state_dict_6642483322838892475.pkl'}\n",
      "2019-03-16 17:55:28.677204, fold=3, rep=1, eta=0d 0h 32m 13s \n",
      "{'fold': 3, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0015504632610827684, 'train_time': 137.5169709949987, 'prior_train_nmll': -1.2373536825180054, 'train_nll': -3265.031982421875, 'test_nll': -1882.615478515625, 'train_mse': 0.0006427128682844341, 'state_dict_file': 'model_state_dict_-4548493146169572834.pkl'}\n",
      "2019-03-16 17:57:57.211430, fold=4, rep=0, eta=0d 0h 29m 16s \n",
      "{'fold': 4, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.002431917004287243, 'train_time': 148.53107958391774, 'prior_train_nmll': -1.2785547971725464, 'train_nll': 34418.37890625, 'test_nll': 1931.360107421875, 'train_mse': 0.0005754997255280614, 'state_dict_file': 'model_state_dict_5176308033340465083.pkl'}\n",
      "2019-03-16 18:00:29.913210, fold=4, rep=1, eta=0d 0h 26m 29s \n",
      "{'fold': 4, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0026281753089278936, 'train_time': 152.70149515301455, 'prior_train_nmll': -1.2772839069366455, 'train_nll': 811.5172729492188, 'test_nll': 4021.453125, 'train_mse': 0.0006733256741426885, 'state_dict_file': 'model_state_dict_8359393016079506534.pkl'}\n",
      "2019-03-16 18:03:09.874116, fold=5, rep=0, eta=0d 0h 23m 51s \n",
      "{'fold': 5, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0021167700178921223, 'train_time': 159.9577159800101, 'prior_train_nmll': -1.23997163772583, 'train_nll': 3146.145263671875, 'test_nll': 3460.8193359375, 'train_mse': 0.0006885898765176535, 'state_dict_file': 'model_state_dict_6822112627601658058.pkl'}\n",
      "2019-03-16 18:06:09.214524, fold=5, rep=1, eta=0d 0h 21m 26s \n",
      "{'fold': 5, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0017519359244033694, 'train_time': 179.34010476293042, 'prior_train_nmll': -1.3298810720443726, 'train_nll': -134.47393798828125, 'test_nll': -302.8080139160156, 'train_mse': 0.0005408185534179211, 'state_dict_file': 'model_state_dict_1381509260532111531.pkl'}\n",
      "2019-03-16 18:08:50.801011, fold=6, rep=0, eta=0d 0h 18m 45s \n",
      "{'fold': 6, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0014531516935676336, 'train_time': 161.5833303979598, 'prior_train_nmll': -1.4000388383865356, 'train_nll': 181049.46875, 'test_nll': 26060.80859375, 'train_mse': 0.0007572477916255593, 'state_dict_file': 'model_state_dict_5611583630833570537.pkl'}\n",
      "2019-03-16 18:11:28.799922, fold=6, rep=1, eta=0d 0h 16m 3s \n",
      "{'fold': 6, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.001695691142231226, 'train_time': 157.998612313997, 'prior_train_nmll': -1.2453110218048096, 'train_nll': -3835.785400390625, 'test_nll': 17240134.0, 'train_mse': 0.0005426804418675601, 'state_dict_file': 'model_state_dict_-3082520092353255879.pkl'}\n",
      "2019-03-16 18:13:45.120975, fold=7, rep=0, eta=0d 0h 13m 15s \n",
      "{'fold': 7, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0022601927630603313, 'train_time': 136.3178356779972, 'prior_train_nmll': -1.2317646741867065, 'train_nll': -3050.7587890625, 'test_nll': -573.063232421875, 'train_mse': 0.0007720001158304513, 'state_dict_file': 'model_state_dict_3528755828343568927.pkl'}\n",
      "2019-03-16 18:16:21.999126, fold=7, rep=1, eta=0d 0h 10m 35s \n",
      "{'fold': 7, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.002027318114414811, 'train_time': 156.87744223501068, 'prior_train_nmll': -1.2286889553070068, 'train_nll': 8057.5126953125, 'test_nll': 134194.0625, 'train_mse': 0.000592307944316417, 'state_dict_file': 'model_state_dict_1172950915442010076.pkl'}\n",
      "2019-03-16 18:18:33.565870, fold=8, rep=0, eta=0d 0h 7m 51s \n",
      "{'fold': 8, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0015262147644534707, 'train_time': 131.563447145978, 'prior_train_nmll': -1.2842004299163818, 'train_nll': -2871.181640625, 'test_nll': 37248468.0, 'train_mse': 0.0009905697079375386, 'state_dict_file': 'model_state_dict_9154283016988521729.pkl'}\n",
      "2019-03-16 18:21:06.471897, fold=8, rep=1, eta=0d 0h 5m 14s \n",
      "{'fold': 8, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0015433777589350939, 'train_time': 152.9057303860318, 'prior_train_nmll': -1.2035107612609863, 'train_nll': 356464.375, 'test_nll': -43051748.0, 'train_mse': 0.0008178309071809053, 'state_dict_file': 'model_state_dict_-7783410578885451285.pkl'}\n",
      "2019-03-16 18:24:00.276174, fold=9, rep=0, eta=0d 0h 2m 37s \n",
      "{'fold': 9, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.003244179068133235, 'train_time': 173.8003756949911, 'prior_train_nmll': -1.269322395324707, 'train_nll': 11680476.0, 'test_nll': 451252.3125, 'train_mse': 0.0007439854089170694, 'state_dict_file': 'model_state_dict_-4767226036006228417.pkl'}\n",
      "2019-03-16 18:26:20.866892, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0022167020943015814, 'train_time': 140.590275676921, 'prior_train_nmll': -1.310279369354248, 'train_nll': -2047.35498046875, 'test_nll': -8439.921875, 'train_mse': 0.000806266616564244, 'state_dict_file': 'model_state_dict_7902976548146905248.pkl'}\n",
      "2019-03-16 18:28:35.548397, fold=0, rep=0, eta=0d 0h 42m 38s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.3932395577430725, 'train_time': 134.65228815400042, 'prior_train_nmll': 0.17284442484378815, 'train_nll': -519.6461791992188, 'test_nll': 141.74197387695312, 'train_mse': 0.020339662209153175, 'state_dict_file': 'model_state_dict_-8425967434992809637.pkl'}\n",
      "2019-03-16 18:30:52.930571, fold=0, rep=1, eta=0d 0h 40m 48s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.33278292417526245, 'train_time': 137.38182116800454, 'prior_train_nmll': 0.1725461632013321, 'train_nll': -997.8850708007812, 'test_nll': 106.31167602539062, 'train_mse': 0.020445289090275764, 'state_dict_file': 'model_state_dict_3109934469086347314.pkl'}\n",
      "2019-03-16 18:33:14.729448, fold=1, rep=0, eta=0d 0h 39m 5s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.5723753571510315, 'train_time': 141.7956429659389, 'prior_train_nmll': 0.11028752475976944, 'train_nll': -1145.58056640625, 'test_nll': 115.81900024414062, 'train_mse': 0.01734481193125248, 'state_dict_file': 'model_state_dict_-6348996147083705807.pkl'}\n",
      "2019-03-16 18:36:01.807085, fold=1, rep=1, eta=0d 0h 38m 43s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.2643355131149292, 'train_time': 167.0772715699859, 'prior_train_nmll': 0.1541914939880371, 'train_nll': -125298200.0, 'test_nll': 140.6761016845703, 'train_mse': 0.0156466793268919, 'state_dict_file': 'model_state_dict_2976115838609559137.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 18:39:48.471801, fold=2, rep=0, eta=0d 0h 40m 22s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.10179780423641205, 'train_time': 226.6616673640674, 'prior_train_nmll': 0.12809069454669952, 'train_nll': -930.4108276367188, 'test_nll': 24.195533752441406, 'train_mse': 0.017051054164767265, 'state_dict_file': 'model_state_dict_5836625077770178864.pkl'}\n",
      "2019-03-16 18:42:08.870085, fold=2, rep=1, eta=0d 0h 36m 51s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.1108885183930397, 'train_time': 140.39797323406674, 'prior_train_nmll': 0.1539066582918167, 'train_nll': -393.44647216796875, 'test_nll': 8.226348876953125, 'train_mse': 0.021703802049160004, 'state_dict_file': 'model_state_dict_7353871099130475635.pkl'}\n",
      "2019-03-16 18:48:15.878922, fold=3, rep=0, eta=0d 0h 40m 42s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.3360963761806488, 'train_time': 367.00559619499836, 'prior_train_nmll': 0.00011301092308713123, 'train_nll': -1972.036865234375, 'test_nll': 191.50204467773438, 'train_mse': 0.01511925458908081, 'state_dict_file': 'model_state_dict_4442025281527646356.pkl'}\n",
      "2019-03-16 18:50:35.353530, fold=3, rep=1, eta=0d 0h 36m 21s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.24836377799510956, 'train_time': 139.47428978898097, 'prior_train_nmll': 0.11819865554571152, 'train_nll': -924.8434448242188, 'test_nll': 96.21572875976562, 'train_mse': 0.021799085661768913, 'state_dict_file': 'model_state_dict_-3116335354061526603.pkl'}\n",
      "2019-03-16 18:58:33.629307, fold=4, rep=0, eta=0d 0h 39m 22s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.19599546492099762, 'train_time': 478.2725005430402, 'prior_train_nmll': 0.025783132761716843, 'train_nll': -1296.62841796875, 'test_nll': 95.88435363769531, 'train_mse': 0.011125875636935234, 'state_dict_file': 'model_state_dict_8923549467855894299.pkl'}\n",
      "2019-03-16 19:04:32.333215, fold=4, rep=1, eta=0d 0h 38m 11s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.2798973321914673, 'train_time': 358.703583098948, 'prior_train_nmll': 0.08473861217498779, 'train_nll': -10918.556640625, 'test_nll': 105.24153137207031, 'train_mse': 0.014307048171758652, 'state_dict_file': 'model_state_dict_-885036246475903843.pkl'}\n",
      "2019-03-16 19:09:57.339047, fold=5, rep=0, eta=0d 0h 35m 40s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.10908115655183792, 'train_time': 325.0026179510169, 'prior_train_nmll': 0.0622277706861496, 'train_nll': -1092.35693359375, 'test_nll': 1.073333740234375, 'train_mse': 0.016860779374837875, 'state_dict_file': 'model_state_dict_7839369310875611437.pkl'}\n",
      "2019-03-16 19:14:57.306402, fold=5, rep=1, eta=0d 0h 32m 24s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.2255885899066925, 'train_time': 299.9665438710945, 'prior_train_nmll': 0.09801635891199112, 'train_nll': 226.27227783203125, 'test_nll': 39.62053680419922, 'train_mse': 0.01789700612425804, 'state_dict_file': 'model_state_dict_728232986395348305.pkl'}\n",
      "2019-03-16 19:20:25.174006, fold=6, rep=0, eta=0d 0h 29m 6s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.08876141905784607, 'train_time': 327.86452589998953, 'prior_train_nmll': 0.09156356006860733, 'train_nll': -702.7617797851562, 'test_nll': 2.831756591796875, 'train_mse': 0.01764809340238571, 'state_dict_file': 'model_state_dict_-6666213098506117145.pkl'}\n",
      "2019-03-16 19:24:01.613847, fold=6, rep=1, eta=0d 0h 24m 43s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.2372412383556366, 'train_time': 216.43947414297145, 'prior_train_nmll': 0.1509907990694046, 'train_nll': -864.8777465820312, 'test_nll': 26.687286376953125, 'train_mse': 0.020310528576374054, 'state_dict_file': 'model_state_dict_-191941160472903670.pkl'}\n",
      "2019-03-16 19:29:15.992106, fold=7, rep=0, eta=0d 0h 20m 58s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.16865554451942444, 'train_time': 314.3750107520027, 'prior_train_nmll': 0.12057329714298248, 'train_nll': -1331.64404296875, 'test_nll': 1.8607406616210938, 'train_mse': 0.0172087624669075, 'state_dict_file': 'model_state_dict_1337353042759012659.pkl'}\n",
      "2019-03-16 19:32:57.419380, fold=7, rep=1, eta=0d 0h 16m 39s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.08936597406864166, 'train_time': 221.4268884359626, 'prior_train_nmll': 0.10890794545412064, 'train_nll': -702.3435668945312, 'test_nll': -3.7323150634765625, 'train_mse': 0.017730798572301865, 'state_dict_file': 'model_state_dict_-4406737665752490274.pkl'}\n",
      "2019-03-16 19:38:01.301039, fold=8, rep=0, eta=0d 0h 12m 38s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.03900445997714996, 'train_time': 303.87853194295894, 'prior_train_nmll': 0.0653231143951416, 'train_nll': -633.9083862304688, 'test_nll': -26.626976013183594, 'train_mse': 0.019322190433740616, 'state_dict_file': 'model_state_dict_-6728983664535263178.pkl'}\n",
      "2019-03-16 19:44:40.295038, fold=8, rep=1, eta=0d 0h 8m 42s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.03715788200497627, 'train_time': 398.9936829620274, 'prior_train_nmll': 0.033004242926836014, 'train_nll': -722.6067504882812, 'test_nll': -35.756195068359375, 'train_mse': 0.01795324683189392, 'state_dict_file': 'model_state_dict_-7305189109159148913.pkl'}\n",
      "2019-03-16 19:48:31.675307, fold=9, rep=0, eta=0d 0h 4m 19s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.042559873312711716, 'train_time': 231.3770448010182, 'prior_train_nmll': 0.07443032413721085, 'train_nll': -762.4440307617188, 'test_nll': -36.35923767089844, 'train_mse': 0.0182906836271286, 'state_dict_file': 'model_state_dict_207900400588105314.pkl'}\n",
      "2019-03-16 19:53:50.957405, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.03935829550027847, 'train_time': 319.2817978520179, 'prior_train_nmll': 0.04634098336100578, 'train_nll': -1218.927978515625, 'test_nll': -30.18268585205078, 'train_mse': 0.017830079421401024, 'state_dict_file': 'model_state_dict_-198615676271461792.pkl'}\n",
      "2019-03-16 19:58:44.905569, fold=0, rep=0, eta=0d 1h 33m 4s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.0545166730880737, 'train_time': 293.91574126505293, 'prior_train_nmll': 1.270485758781433, 'train_nll': 1073.8359375, 'test_nll': 157.25198364257812, 'train_mse': 0.5090353488922119, 'state_dict_file': 'model_state_dict_-6509637534597033100.pkl'}\n",
      "2019-03-16 20:01:47.829097, fold=0, rep=1, eta=0d 1h 11m 31s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.0891395807266235, 'train_time': 182.92316851904616, 'prior_train_nmll': 1.2806752920150757, 'train_nll': 1075.72265625, 'test_nll': 160.5229949951172, 'train_mse': 0.5130524039268494, 'state_dict_file': 'model_state_dict_2700101594247263020.pkl'}\n",
      "2019-03-16 20:04:27.811676, fold=1, rep=0, eta=0d 1h 0m 8s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.8268647193908691, 'train_time': 159.97931923007127, 'prior_train_nmll': 1.307553768157959, 'train_nll': 1113.770751953125, 'test_nll': 135.0001983642578, 'train_mse': 0.557832658290863, 'state_dict_file': 'model_state_dict_-2929372070351898934.pkl'}\n",
      "2019-03-16 20:11:12.508511, fold=1, rep=1, eta=0d 1h 9m 26s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.7783772349357605, 'train_time': 404.69633441395126, 'prior_train_nmll': 1.2972019910812378, 'train_nll': 1108.307373046875, 'test_nll': 130.86170959472656, 'train_mse': 0.5496776700019836, 'state_dict_file': 'model_state_dict_6445696905636352207.pkl'}\n",
      "2019-03-16 20:14:27.200137, fold=2, rep=0, eta=0d 1h 1m 48s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.522163987159729, 'train_time': 194.68668257200625, 'prior_train_nmll': 1.2658073902130127, 'train_nll': 1079.214111328125, 'test_nll': 183.65280151367188, 'train_mse': 0.5194718241691589, 'state_dict_file': 'model_state_dict_2438250296158067258.pkl'}\n",
      "2019-03-16 20:18:06.720513, fold=2, rep=1, eta=0d 0h 56m 36s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.3905775547027588, 'train_time': 219.51998347206973, 'prior_train_nmll': 1.2699989080429077, 'train_nll': 1076.9716796875, 'test_nll': 177.6867218017578, 'train_mse': 0.5148523449897766, 'state_dict_file': 'model_state_dict_-6239351319021552201.pkl'}\n",
      "2019-03-16 20:21:41.856476, fold=3, rep=0, eta=0d 0h 51m 43s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.8387793302536011, 'train_time': 215.13278413703665, 'prior_train_nmll': 1.2988377809524536, 'train_nll': 1093.83935546875, 'test_nll': 138.18844604492188, 'train_mse': 0.5313745141029358, 'state_dict_file': 'model_state_dict_7141667098977124233.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 20:25:08.580170, fold=3, rep=1, eta=0d 0h 46m 56s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.8082998394966125, 'train_time': 206.7233641789062, 'prior_train_nmll': 1.2874209880828857, 'train_nll': 1089.19775390625, 'test_nll': 135.52880859375, 'train_mse': 0.5285563468933105, 'state_dict_file': 'model_state_dict_7732197496245542916.pkl'}\n",
      "2019-03-16 20:29:03.242505, fold=4, rep=0, eta=0d 0h 43m 1s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.5192204117774963, 'train_time': 234.65909323003143, 'prior_train_nmll': 1.3100636005401611, 'train_nll': 1112.269287109375, 'test_nll': 114.26042938232422, 'train_mse': 0.5515456795692444, 'state_dict_file': 'model_state_dict_-628630097177456058.pkl'}\n",
      "2019-03-16 20:32:26.038980, fold=4, rep=1, eta=0d 0h 38m 35s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.5150270462036133, 'train_time': 202.79611020407174, 'prior_train_nmll': 1.3080652952194214, 'train_nll': 1111.36474609375, 'test_nll': 114.97632598876953, 'train_mse': 0.5498210191726685, 'state_dict_file': 'model_state_dict_6232569870750488982.pkl'}\n",
      "2019-03-16 20:34:40.763056, fold=5, rep=0, eta=0d 0h 33m 24s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.5929841995239258, 'train_time': 134.71790533897, 'prior_train_nmll': 1.2439550161361694, 'train_nll': 1052.2635498046875, 'test_nll': 201.07595825195312, 'train_mse': 0.48792439699172974, 'state_dict_file': 'model_state_dict_-6607854087019360775.pkl'}\n",
      "2019-03-16 20:37:02.075678, fold=5, rep=1, eta=0d 0h 28m 47s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.5898243188858032, 'train_time': 141.31229774095118, 'prior_train_nmll': 1.2456094026565552, 'train_nll': 1047.0244140625, 'test_nll': 199.40740966796875, 'train_mse': 0.480694442987442, 'state_dict_file': 'model_state_dict_6027970676218304905.pkl'}\n",
      "2019-03-16 20:41:02.079649, fold=6, rep=0, eta=0d 0h 25m 24s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.2362451553344727, 'train_time': 240.0007253329968, 'prior_train_nmll': 1.2719533443450928, 'train_nll': 1109.7559814453125, 'test_nll': 164.565673828125, 'train_mse': 0.5605046153068542, 'state_dict_file': 'model_state_dict_9177115580300549415.pkl'}\n",
      "2019-03-16 20:44:49.276974, fold=6, rep=1, eta=0d 0h 21m 50s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.5149507522583008, 'train_time': 227.1969768969575, 'prior_train_nmll': 1.2795816659927368, 'train_nll': 1116.1446533203125, 'test_nll': 177.94775390625, 'train_mse': 0.5669808387756348, 'state_dict_file': 'model_state_dict_4432382435842291537.pkl'}\n",
      "2019-03-16 20:47:46.093481, fold=7, rep=0, eta=0d 0h 17m 58s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.597987413406372, 'train_time': 176.8111330299871, 'prior_train_nmll': 1.249582052230835, 'train_nll': 1058.192138671875, 'test_nll': 184.38528442382812, 'train_mse': 0.49205222725868225, 'state_dict_file': 'model_state_dict_-2887531277619798373.pkl'}\n",
      "2019-03-16 20:51:01.835204, fold=7, rep=1, eta=0d 0h 14m 17s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.5141324996948242, 'train_time': 195.7414006969193, 'prior_train_nmll': 1.250575304031372, 'train_nll': 1058.664794921875, 'test_nll': 182.0358123779297, 'train_mse': 0.4961441159248352, 'state_dict_file': 'model_state_dict_6768224500000528918.pkl'}\n",
      "2019-03-16 20:54:31.302205, fold=8, rep=0, eta=0d 0h 10m 42s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.5409173369407654, 'train_time': 209.4620050659869, 'prior_train_nmll': 1.3095515966415405, 'train_nll': 1123.7730712890625, 'test_nll': 117.095703125, 'train_mse': 0.5659506320953369, 'state_dict_file': 'model_state_dict_1813635408411616231.pkl'}\n",
      "2019-03-16 20:57:31.551911, fold=8, rep=1, eta=0d 0h 7m 4s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.6433196067810059, 'train_time': 180.24919080198742, 'prior_train_nmll': 1.3057726621627808, 'train_nll': 1123.736572265625, 'test_nll': 123.25755310058594, 'train_mse': 0.5696474313735962, 'state_dict_file': 'model_state_dict_2954571903160382064.pkl'}\n",
      "2019-03-16 21:00:14.842859, fold=9, rep=0, eta=0d 0h 3m 29s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.2291048765182495, 'train_time': 163.28760698006954, 'prior_train_nmll': 1.2624658346176147, 'train_nll': 1058.8355712890625, 'test_nll': 164.18296813964844, 'train_mse': 0.4861425459384918, 'state_dict_file': 'model_state_dict_9022865572372386669.pkl'}\n",
      "2019-03-16 21:04:50.956877, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.3202711343765259, 'train_time': 276.11372263508383, 'prior_train_nmll': 1.2569892406463623, 'train_nll': 1059.398193359375, 'test_nll': 167.9287872314453, 'train_mse': 0.48804646730422974, 'state_dict_file': 'model_state_dict_-3281827874826445049.pkl'}\n",
      "2019-03-16 21:24:25.869209, fold=0, rep=0, eta=0d 6h 12m 2s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.046494629234075546, 'train_time': 1174.873012644006, 'prior_train_nmll': 0.04618096724152565, 'train_nll': -2745.3759765625, 'test_nll': -22.6875, 'train_mse': 0.016726404428482056, 'state_dict_file': 'model_state_dict_3041930715601302727.pkl'}\n",
      "2019-03-16 21:43:42.652696, fold=0, rep=1, eta=0d 5h 49m 44s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.04388735070824623, 'train_time': 1156.7831204429967, 'prior_train_nmll': 0.024647746235132217, 'train_nll': -3367.55615234375, 'test_nll': -13.448257446289062, 'train_mse': 0.020146263763308525, 'state_dict_file': 'model_state_dict_1023646031040224627.pkl'}\n",
      "2019-03-16 21:56:19.311423, fold=1, rep=0, eta=0d 4h 51m 40s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.054037775844335556, 'train_time': 756.6555300550535, 'prior_train_nmll': 0.050320252776145935, 'train_nll': 3567.23876953125, 'test_nll': -20.227706909179688, 'train_mse': 0.02088610827922821, 'state_dict_file': 'model_state_dict_-7514956394232736782.pkl'}\n",
      "2019-03-16 22:07:43.242342, fold=1, rep=1, eta=0d 4h 11m 28s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.06630677729845047, 'train_time': 683.9306093569612, 'prior_train_nmll': 0.08266996592283249, 'train_nll': 1492.1524658203125, 'test_nll': 11.786674499511719, 'train_mse': 0.02388131432235241, 'state_dict_file': 'model_state_dict_3529340652437905011.pkl'}\n",
      "2019-03-16 22:28:54.669635, fold=2, rep=0, eta=0d 4h 12m 11s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.05129827558994293, 'train_time': 1271.4240251170704, 'prior_train_nmll': 0.04776779189705849, 'train_nll': -1076.2208251953125, 'test_nll': -6.6652679443359375, 'train_mse': 0.018876302987337112, 'state_dict_file': 'model_state_dict_-6829790629016613931.pkl'}\n",
      "2019-03-16 22:38:18.354114, fold=2, rep=1, eta=0d 3h 38m 3s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.06625482439994812, 'train_time': 563.683750999975, 'prior_train_nmll': 0.09731028974056244, 'train_nll': 7501.13427734375, 'test_nll': 10.631179809570312, 'train_mse': 0.025607174262404442, 'state_dict_file': 'model_state_dict_6952228952744705488.pkl'}\n",
      "2019-03-16 22:50:02.667160, fold=3, rep=0, eta=0d 3h 15m 21s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.03762441128492355, 'train_time': 704.3098590599839, 'prior_train_nmll': 0.11320759356021881, 'train_nll': 35742.1328125, 'test_nll': -33.511932373046875, 'train_mse': 0.025408508256077766, 'state_dict_file': 'model_state_dict_483424908921672249.pkl'}\n",
      "2019-03-16 23:04:21.112323, fold=3, rep=1, eta=0d 2h 59m 15s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.0472995825111866, 'train_time': 858.4447440669173, 'prior_train_nmll': 0.13123780488967896, 'train_nll': 18407.763671875, 'test_nll': -19.188766479492188, 'train_mse': 0.02260175161063671, 'state_dict_file': 'model_state_dict_4262636960961645847.pkl'}\n",
      "2019-03-16 23:26:29.958392, fold=4, rep=0, eta=0d 2h 53m 7s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.0692625641822815, 'train_time': 1328.8422542209737, 'prior_train_nmll': 0.04814962297677994, 'train_nll': 13169.166015625, 'test_nll': 17.5670166015625, 'train_mse': 0.018495161086320877, 'state_dict_file': 'model_state_dict_-3737935813067514493.pkl'}\n",
      "2019-03-16 23:38:52.731401, fold=4, rep=1, eta=0d 2h 34m 1s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.05593959987163544, 'train_time': 742.7726665940136, 'prior_train_nmll': 0.038691699504852295, 'train_nll': -519.9422607421875, 'test_nll': 6.2329864501953125, 'train_mse': 0.02130872756242752, 'state_dict_file': 'model_state_dict_-8334273854518432579.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 23:58:23.441453, fold=5, rep=0, eta=0d 2h 21m 59s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.04358627647161484, 'train_time': 1170.7068703579716, 'prior_train_nmll': 0.08745770901441574, 'train_nll': 1733.0592041015625, 'test_nll': -25.13720703125, 'train_mse': 0.0214262455701828, 'state_dict_file': 'model_state_dict_6758133343800201166.pkl'}\n",
      "2019-03-17 00:13:10.581445, fold=5, rep=1, eta=0d 2h 5m 33s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.04177720099687576, 'train_time': 887.1395943480311, 'prior_train_nmll': 0.14173565804958344, 'train_nll': -1636.2767333984375, 'test_nll': -29.716812133789062, 'train_mse': 0.027276115491986275, 'state_dict_file': 'model_state_dict_8372214585885256749.pkl'}\n",
      "2019-03-17 00:23:48.712351, fold=6, rep=0, eta=0d 1h 47m 8s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.059028226882219315, 'train_time': 638.1277388989693, 'prior_train_nmll': 0.06730644404888153, 'train_nll': -1619.2830810546875, 'test_nll': -0.0647125244140625, 'train_mse': 0.024221930652856827, 'state_dict_file': 'model_state_dict_-3142926522730086806.pkl'}\n",
      "2019-03-17 00:42:23.439640, fold=6, rep=1, eta=0d 1h 33m 13s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.05081271007657051, 'train_time': 1114.7268693549559, 'prior_train_nmll': 0.08160528540611267, 'train_nll': -1203.0731201171875, 'test_nll': -12.952407836914062, 'train_mse': 0.021834127604961395, 'state_dict_file': 'model_state_dict_-117726306867741720.pkl'}\n",
      "2019-03-17 00:58:30.271894, fold=7, rep=0, eta=0d 1h 17m 53s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.060679689049720764, 'train_time': 966.8290007730247, 'prior_train_nmll': 0.031174715608358383, 'train_nll': 33016.73828125, 'test_nll': 4.1878204345703125, 'train_mse': 0.02029009908437729, 'state_dict_file': 'model_state_dict_-7898324877810677894.pkl'}\n",
      "2019-03-17 01:18:53.761663, fold=7, rep=1, eta=0d 1h 3m 30s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.06395066529512405, 'train_time': 1223.489382144995, 'prior_train_nmll': 0.056267570704221725, 'train_nll': -1903.4417724609375, 'test_nll': 6.6984405517578125, 'train_mse': 0.01945304311811924, 'state_dict_file': 'model_state_dict_222557329507860691.pkl'}\n",
      "2019-03-17 01:33:34.552527, fold=8, rep=0, eta=0d 0h 47m 25s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.03477467596530914, 'train_time': 880.7868062499911, 'prior_train_nmll': 0.0667576715350151, 'train_nll': -3022.287109375, 'test_nll': -46.94384765625, 'train_mse': 0.01933920383453369, 'state_dict_file': 'model_state_dict_5016723689222484606.pkl'}\n",
      "2019-03-17 01:50:11.921368, fold=8, rep=1, eta=0d 0h 31m 42s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.04221637547016144, 'train_time': 997.3685051430948, 'prior_train_nmll': 0.04247452691197395, 'train_nll': 10253.8310546875, 'test_nll': -30.235214233398438, 'train_mse': 0.01931542344391346, 'state_dict_file': 'model_state_dict_-74060502897456956.pkl'}\n",
      "2019-03-17 02:08:39.583266, fold=9, rep=0, eta=0d 0h 15m 59s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.03346605226397514, 'train_time': 1107.6585528010037, 'prior_train_nmll': 0.06344756484031677, 'train_nll': -2749.140625, 'test_nll': -44.68408203125, 'train_mse': 0.019100824370980263, 'state_dict_file': 'model_state_dict_-6620002810499880486.pkl'}\n",
      "2019-03-17 02:32:03.693043, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.03725513443350792, 'train_time': 1404.1094166529365, 'prior_train_nmll': 0.07899001985788345, 'train_nll': 3091.82373046875, 'test_nll': -33.514556884765625, 'train_mse': 0.017240729182958603, 'state_dict_file': 'model_state_dict_-1909482001753533919.pkl'}\n",
      "2019-03-17 02:40:47.104329, fold=0, rep=0, eta=0d 2h 45m 44s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.2153247892856598, 'train_time': 523.3808239339851, 'prior_train_nmll': 0.530367910861969, 'train_nll': 99.414794921875, 'test_nll': 119.73600769042969, 'train_mse': 0.05275202915072441, 'state_dict_file': 'model_state_dict_-7609031331282600923.pkl'}\n",
      "2019-03-17 02:52:35.980532, fold=0, rep=1, eta=0d 3h 4m 50s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.22690288722515106, 'train_time': 708.8758392080199, 'prior_train_nmll': 0.45438405871391296, 'train_nll': -26.44091796875, 'test_nll': 160.29928588867188, 'train_mse': 0.04445524513721466, 'state_dict_file': 'model_state_dict_-2566619435621158921.pkl'}\n",
      "2019-03-17 03:02:49.545127, fold=1, rep=0, eta=0d 2h 54m 19s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.3444783389568329, 'train_time': 613.561466280953, 'prior_train_nmll': 0.48867419362068176, 'train_nll': 31.3497314453125, 'test_nll': 176.4251708984375, 'train_mse': 0.04794849082827568, 'state_dict_file': 'model_state_dict_-8449148210040149760.pkl'}\n",
      "2019-03-17 03:08:01.489848, fold=1, rep=1, eta=0d 2h 23m 51s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.24019408226013184, 'train_time': 311.9441243340261, 'prior_train_nmll': 0.5938705205917358, 'train_nll': 376.0242919921875, 'test_nll': 108.05179595947266, 'train_mse': 0.08021409809589386, 'state_dict_file': 'model_state_dict_-6870817539687863119.pkl'}\n",
      "2019-03-17 03:18:01.065056, fold=2, rep=0, eta=0d 2h 17m 52s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.22658701241016388, 'train_time': 599.5718541160459, 'prior_train_nmll': 0.5090373158454895, 'train_nll': 59.741455078125, 'test_nll': 127.58946228027344, 'train_mse': 0.05019012466073036, 'state_dict_file': 'model_state_dict_2958298472793669724.pkl'}\n",
      "2019-03-17 03:37:45.652532, fold=2, rep=1, eta=0d 2h 33m 17s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2857530117034912, 'train_time': 1184.58714619698, 'prior_train_nmll': 0.508533239364624, 'train_nll': -252.111083984375, 'test_nll': 202.59078979492188, 'train_mse': 0.03165300190448761, 'state_dict_file': 'model_state_dict_5040195194206612216.pkl'}\n",
      "2019-03-17 03:51:36.665231, fold=3, rep=0, eta=0d 2h 27m 44s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.2750524878501892, 'train_time': 831.0093742309837, 'prior_train_nmll': 0.44422659277915955, 'train_nll': -110.7611083984375, 'test_nll': 204.06344604492188, 'train_mse': 0.03928288072347641, 'state_dict_file': 'model_state_dict_7194092492420008955.pkl'}\n",
      "2019-03-17 04:08:55.027633, fold=3, rep=1, eta=0d 2h 25m 16s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2910401523113251, 'train_time': 1038.3620703070192, 'prior_train_nmll': 0.4894682765007019, 'train_nll': -90.9290771484375, 'test_nll': 212.74395751953125, 'train_mse': 0.04048491269350052, 'state_dict_file': 'model_state_dict_-2104684984168198633.pkl'}\n",
      "2019-03-17 04:21:15.883222, fold=4, rep=0, eta=0d 2h 13m 28s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.27007588744163513, 'train_time': 740.8522486800794, 'prior_train_nmll': 0.4845280051231384, 'train_nll': -45.8582763671875, 'test_nll': 167.03646850585938, 'train_mse': 0.04420095682144165, 'state_dict_file': 'model_state_dict_-8130847377363174253.pkl'}\n",
      "2019-03-17 04:32:43.462291, fold=4, rep=1, eta=0d 2h 0m 39s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2648388743400574, 'train_time': 687.5786374050658, 'prior_train_nmll': 0.4870985448360443, 'train_nll': -10.2589111328125, 'test_nll': 157.12899780273438, 'train_mse': 0.04463343322277069, 'state_dict_file': 'model_state_dict_-242075918700283117.pkl'}\n",
      "2019-03-17 04:55:37.243715, fold=5, rep=0, eta=0d 1h 57m 27s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.4096619188785553, 'train_time': 1373.7772542750463, 'prior_train_nmll': 0.44235455989837646, 'train_nll': -330.8912353515625, 'test_nll': 284.91845703125, 'train_mse': 0.02701178565621376, 'state_dict_file': 'model_state_dict_2767322322882826.pkl'}\n",
      "2019-03-17 05:07:48.885592, fold=5, rep=1, eta=0d 1h 43m 50s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.3794260323047638, 'train_time': 731.6415248119738, 'prior_train_nmll': 0.43038246035575867, 'train_nll': -153.28271484375, 'test_nll': 244.1433868408203, 'train_mse': 0.036472298204898834, 'state_dict_file': 'model_state_dict_-8124618842634228314.pkl'}\n",
      "2019-03-17 05:19:00.786736, fold=6, rep=0, eta=0d 1h 29m 53s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.24976825714111328, 'train_time': 671.8978435979225, 'prior_train_nmll': 0.5190816521644592, 'train_nll': -15.98876953125, 'test_nll': 146.14303588867188, 'train_mse': 0.04563242569565773, 'state_dict_file': 'model_state_dict_5652208934352475848.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-17 05:32:45.864130, fold=6, rep=1, eta=0d 1h 17m 26s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.29112425446510315, 'train_time': 825.077078782022, 'prior_train_nmll': 0.46508094668388367, 'train_nll': -144.9638671875, 'test_nll': 193.7093505859375, 'train_mse': 0.037658654153347015, 'state_dict_file': 'model_state_dict_-3042194656190362366.pkl'}\n",
      "2019-03-17 05:50:04.055795, fold=7, rep=0, eta=0d 1h 6m 0s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.29162806272506714, 'train_time': 1038.1808161300141, 'prior_train_nmll': 0.44287392497062683, 'train_nll': -165.510986328125, 'test_nll': 167.47731018066406, 'train_mse': 0.03565804660320282, 'state_dict_file': 'model_state_dict_-876285943403650734.pkl'}\n",
      "2019-03-17 06:05:59.565592, fold=7, rep=1, eta=0d 0h 53m 28s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2606983780860901, 'train_time': 955.5094747979892, 'prior_train_nmll': 0.4788036346435547, 'train_nll': -115.119384765625, 'test_nll': 163.36483764648438, 'train_mse': 0.0391848050057888, 'state_dict_file': 'model_state_dict_-2305825013345228209.pkl'}\n",
      "2019-03-17 06:18:40.536192, fold=8, rep=0, eta=0d 0h 39m 59s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.31779202818870544, 'train_time': 760.9671591949882, 'prior_train_nmll': 0.5001834034919739, 'train_nll': -39.88232421875, 'test_nll': 204.34002685546875, 'train_mse': 0.04362396150827408, 'state_dict_file': 'model_state_dict_-5180184697674219611.pkl'}\n",
      "2019-03-17 06:37:12.449138, fold=8, rep=1, eta=0d 0h 27m 14s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.26559552550315857, 'train_time': 1111.9126205779612, 'prior_train_nmll': 0.4344623386859894, 'train_nll': -264.33251953125, 'test_nll': 191.72523498535156, 'train_mse': 0.030487272888422012, 'state_dict_file': 'model_state_dict_1247197165379892254.pkl'}\n",
      "2019-03-17 06:50:56.800342, fold=9, rep=0, eta=0d 0h 13m 37s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.2465350329875946, 'train_time': 824.3478834109847, 'prior_train_nmll': 0.4716823995113373, 'train_nll': -110.6832275390625, 'test_nll': 160.50843811035156, 'train_mse': 0.03932274132966995, 'state_dict_file': 'model_state_dict_2982344425001775080.pkl'}\n",
      "2019-03-17 07:07:46.372895, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2401200532913208, 'train_time': 1009.5722141109873, 'prior_train_nmll': 0.49148887395858765, 'train_nll': -124.615478515625, 'test_nll': 145.9842529296875, 'train_mse': 0.03804611414670944, 'state_dict_file': 'model_state_dict_-638053533133345781.pkl'}\n",
      "2019-03-17 07:55:38.458525, fold=0, rep=0, eta=0d 15h 9m 28s \n",
      "{'fold': 0, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.020586419850587845, 'train_time': 2872.0378410769626, 'prior_train_nmll': -0.2255227267742157, 'train_nll': -2989.19287109375, 'test_nll': -164.87142944335938, 'train_mse': 0.0018306989222764969, 'state_dict_file': 'model_state_dict_-2446471872400458474.pkl'}\n",
      "2019-03-17 08:54:15.939107, fold=0, rep=1, eta=0d 15h 58m 25s \n",
      "{'fold': 0, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.02532120980322361, 'train_time': 3517.4802061410155, 'prior_train_nmll': -0.34150752425193787, 'train_nll': 10112.404296875, 'test_nll': 74.21835327148438, 'train_mse': 0.0003580099728424102, 'state_dict_file': 'model_state_dict_5292581007092756666.pkl'}\n",
      "2019-03-17 09:42:42.785783, fold=1, rep=0, eta=0d 14h 37m 59s \n",
      "{'fold': 1, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.04545779153704643, 'train_time': 2906.841232025996, 'prior_train_nmll': -0.3094496726989746, 'train_nll': -3243.38427734375, 'test_nll': 34.01806640625, 'train_mse': 0.0013068990083411336, 'state_dict_file': 'model_state_dict_-2119406410749270582.pkl'}\n",
      "2019-03-17 10:49:43.491711, fold=1, rep=1, eta=0d 14h 47m 48s \n",
      "{'fold': 1, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.053849250078201294, 'train_time': 4020.7053859679727, 'prior_train_nmll': -0.41408592462539673, 'train_nll': 2053397248.0, 'test_nll': 978.8646240234375, 'train_mse': 0.00041789101669564843, 'state_dict_file': 'model_state_dict_6088061456114734270.pkl'}\n",
      "2019-03-17 11:56:45.631244, fold=2, rep=0, eta=0d 14h 26m 57s \n",
      "{'fold': 2, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.04734834283590317, 'train_time': 4022.1319154939847, 'prior_train_nmll': -0.3387273848056793, 'train_nll': 29045.490234375, 'test_nll': 923.255859375, 'train_mse': 0.0002447341976221651, 'state_dict_file': 'model_state_dict_-7263304326643703012.pkl'}\n",
      "2019-03-17 12:40:53.075294, fold=2, rep=1, eta=0d 12h 57m 15s \n",
      "{'fold': 2, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.04063461348414421, 'train_time': 2647.4436870969366, 'prior_train_nmll': -0.2250477820634842, 'train_nll': -3645.7177734375, 'test_nll': 276.85516357421875, 'train_mse': 0.000702439050655812, 'state_dict_file': 'model_state_dict_-6619036071570073128.pkl'}\n",
      "2019-03-17 13:37:15.706172, fold=3, rep=0, eta=0d 12h 3m 20s \n",
      "{'fold': 3, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.07725167274475098, 'train_time': 3382.62563279795, 'prior_train_nmll': -0.30047792196273804, 'train_nll': -4881.208984375, 'test_nll': 570.321533203125, 'train_mse': 0.0015714741311967373, 'state_dict_file': 'model_state_dict_-2962252920231460936.pkl'}\n",
      "2019-03-17 14:24:18.231790, fold=3, rep=1, eta=0d 10h 54m 47s \n",
      "{'fold': 3, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.08038511127233505, 'train_time': 2822.525262512965, 'prior_train_nmll': -0.3459133505821228, 'train_nll': -3577.41455078125, 'test_nll': 697.0670166015625, 'train_mse': 0.0007970031583681703, 'state_dict_file': 'model_state_dict_8838354834468179926.pkl'}\n",
      "2019-03-17 15:42:30.576952, fold=4, rep=0, eta=0d 10h 29m 7s \n",
      "{'fold': 4, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.038606029003858566, 'train_time': 4692.339256075094, 'prior_train_nmll': -0.24307124316692352, 'train_nll': 17359142.0, 'test_nll': 205.89932250976562, 'train_mse': 0.0007084375247359276, 'state_dict_file': 'model_state_dict_-959622480366835686.pkl'}\n",
      "2019-03-17 16:54:14.889124, fold=4, rep=1, eta=0d 9h 46m 28s \n",
      "{'fold': 4, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.03869284316897392, 'train_time': 4304.311839670991, 'prior_train_nmll': -0.2841443121433258, 'train_nll': 4144864.5, 'test_nll': 129.91220092773438, 'train_mse': 0.0003663790412247181, 'state_dict_file': 'model_state_dict_-6907845755968772654.pkl'}\n",
      "2019-03-17 17:34:22.677978, fold=5, rep=0, eta=0d 8h 32m 40s \n",
      "{'fold': 5, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.023774608969688416, 'train_time': 2407.7836689959513, 'prior_train_nmll': -0.2714884579181671, 'train_nll': -2624.3134765625, 'test_nll': -161.30606079101562, 'train_mse': 0.0026514935307204723, 'state_dict_file': 'model_state_dict_-2892601764377673090.pkl'}\n",
      "2019-03-17 18:29:16.934464, fold=5, rep=1, eta=0d 7h 34m 20s \n",
      "{'fold': 5, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.03314725309610367, 'train_time': 3294.256101410021, 'prior_train_nmll': -0.3017530143260956, 'train_nll': -3751.50048828125, 'test_nll': -128.69046020507812, 'train_mse': 0.0006337125087156892, 'state_dict_file': 'model_state_dict_-679308468912929177.pkl'}\n",
      "2019-03-17 19:41:20.427166, fold=6, rep=0, eta=0d 6h 45m 46s \n",
      "{'fold': 6, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.02278176322579384, 'train_time': 4323.487459921977, 'prior_train_nmll': -0.35856783390045166, 'train_nll': 4883502.5, 'test_nll': -8.55572509765625, 'train_mse': 0.0010587210999801755, 'state_dict_file': 'model_state_dict_9215689806627066507.pkl'}\n",
      "2019-03-17 20:48:41.541746, fold=6, rep=1, eta=0d 5h 51m 49s \n",
      "{'fold': 6, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.02283502370119095, 'train_time': 4041.114244194003, 'prior_train_nmll': -0.332590252161026, 'train_nll': -4862.4970703125, 'test_nll': -51.050933837890625, 'train_mse': 0.0002258984459331259, 'state_dict_file': 'model_state_dict_-4949901887659071017.pkl'}\n",
      "2019-03-17 21:54:42.571690, fold=7, rep=0, eta=0d 4h 55m 38s \n",
      "{'fold': 7, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.04064834862947464, 'train_time': 3961.0245485759806, 'prior_train_nmll': -0.19981729984283447, 'train_nll': 19843.24609375, 'test_nll': -98.81393432617188, 'train_mse': 0.00026631445507518947, 'state_dict_file': 'model_state_dict_-836049127677496450.pkl'}\n",
      "2019-03-17 22:34:35.822693, fold=7, rep=1, eta=0d 3h 51m 42s \n",
      "{'fold': 7, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.019999852403998375, 'train_time': 2393.250603144057, 'prior_train_nmll': -0.2052089124917984, 'train_nll': -2546.52490234375, 'test_nll': -170.84054565429688, 'train_mse': 0.0031697896774858236, 'state_dict_file': 'model_state_dict_-1866157382194768509.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-17 23:45:04.781789, fold=8, rep=0, eta=0d 2h 55m 59s \n",
      "{'fold': 8, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.025813965126872063, 'train_time': 4228.947037538979, 'prior_train_nmll': -0.2600017786026001, 'train_nll': -5229.9189453125, 'test_nll': -94.48910522460938, 'train_mse': 0.0011938211973756552, 'state_dict_file': 'model_state_dict_-655316556385848474.pkl'}\n",
      "2019-03-18 00:26:33.872666, fold=8, rep=1, eta=0d 1h 55m 25s \n",
      "{'fold': 8, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.016700752079486847, 'train_time': 2489.0905095019843, 'prior_train_nmll': -0.1977592408657074, 'train_nll': -2571.8017578125, 'test_nll': -177.2943115234375, 'train_mse': 0.0029408272821456194, 'state_dict_file': 'model_state_dict_-6923132928327695668.pkl'}\n",
      "2019-03-18 01:20:57.018035, fold=9, rep=0, eta=0d 0h 57m 32s \n",
      "{'fold': 9, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.051896993070840836, 'train_time': 3263.136807493982, 'prior_train_nmll': -0.24873818457126617, 'train_nll': -3993.47509765625, 'test_nll': 81.205078125, 'train_mse': 0.0005783208180218935, 'state_dict_file': 'model_state_dict_-8271784189816178677.pkl'}\n",
      "2019-03-18 02:30:30.153492, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.043836578726768494, 'train_time': 4173.135086619994, 'prior_train_nmll': -0.3523392975330353, 'train_nll': 29536.41015625, 'test_nll': 545.309814453125, 'train_mse': 0.0002976350369863212, 'state_dict_file': 'model_state_dict_6559056478429304278.pkl'}\n",
      "2019-03-18 04:03:30.215535, fold=0, rep=0, eta=1d 5h 26m 59s \n",
      "{'fold': 0, 'repeat': 0, 'n': 3338, 'd': 17, 'mse': 0.5871098041534424, 'train_time': 5579.964713861933, 'prior_train_nmll': 0.879579484462738, 'train_nll': 1796.0101318359375, 'test_nll': 426.23162841796875, 'train_mse': 0.16730615496635437, 'state_dict_file': 'model_state_dict_490602208582316812.pkl'}\n",
      "2019-03-18 05:49:10.266575, fold=0, rep=1, eta=1d 5h 48m 0s \n",
      "{'fold': 0, 'repeat': 1, 'n': 3338, 'd': 17, 'mse': 0.7436127662658691, 'train_time': 6340.050643499009, 'prior_train_nmll': 0.8842068910598755, 'train_nll': 1720.028564453125, 'test_nll': 522.05908203125, 'train_mse': 0.15804541110992432, 'state_dict_file': 'model_state_dict_3678717484877942084.pkl'}\n",
      "2019-03-18 08:07:30.449766, fold=1, rep=0, eta=1d 7h 49m 41s \n",
      "{'fold': 1, 'repeat': 0, 'n': 3338, 'd': 17, 'mse': 0.6359672546386719, 'train_time': 8300.177167467074, 'prior_train_nmll': 0.8704572319984436, 'train_nll': 1643.6578369140625, 'test_nll': 492.5637512207031, 'train_mse': 0.1488538235425949, 'state_dict_file': 'model_state_dict_-1364205099322925238.pkl'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-045badb2557c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcg_tolerance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         result = rp_experiments.run_experiment(training_routines.train_additive_rp_gp, options, \n\u001b[0;32m----> 8\u001b[0;31m                                      dataset=dataset, split=0.1, cv=True, repeats=2)\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/rp_experiments.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(training_routine, training_options, dataset, split, cv, addl_metrics, repeats, error_repeats)\u001b[0m\n\u001b[1;32m    136\u001b[0m                     ret = training_routine(trainX, trainY, testX,\n\u001b[1;32m    137\u001b[0m                                                             \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                                                             **training_options)\n\u001b[0m\u001b[1;32m    139\u001b[0m                     \u001b[0mmodel_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                     \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/training_routines.py\u001b[0m in \u001b[0;36mtrain_additive_rp_gp\u001b[0;34m(trainX, trainY, testX, testY, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_additive_rp_gp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_exact_gp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/training_routines.py\u001b[0m in \u001b[0;36mtrain_exact_gp\u001b[0;34m(trainX, trainY, testX, testY, rp, k, J, ard, activation, optimizer, n_epochs, lr, verbose, patience, smooth, noise_prior, ski, grid_ratio, grid_size, learn_weights, additive)\u001b[0m\n\u001b[1;32m    229\u001b[0m                          \u001b[0misloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                          smooth=smooth)\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mtrain_to_convergence\u001b[0;34m(model, xs, ys, optimizer, lr, objective, max_iter, verbose, patience, conv_tol, check_conv, smooth, isloss, batch_size)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch {}, iter {}, loss {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "options = dict(verbose=False, ard=False, activation=None, optimizer='adam',\n",
    "               n_epochs=1000, lr=0.1, patience=20, k=1, J=20, smooth=True, \n",
    "               noise_prior=True, learn_weights=True)\n",
    "datasets = rp_experiments.get_small_datasets() + rp_experiments.get_medium_datasets()\n",
    "for dataset in datasets:\n",
    "    with gpytorch.settings.cg_tolerance(0.01):\n",
    "        result = rp_experiments.run_experiment(training_routines.train_additive_rp_gp, options, \n",
    "                                     dataset=dataset, split=0.1, cv=True, repeats=2)\n",
    "    result['RP'] = True\n",
    "    result['k'] = 1\n",
    "    result['J'] = 20\n",
    "    result['dataset'] = dataset\n",
    "    result['options'] = json.dumps(options)\n",
    "    result['learn_weights'] = True\n",
    "    df = pd.concat([df, result])\n",
    "    df.to_csv('./learning_weights_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py:184: RuntimeWarning: Mean of empty slice.\n",
      "  ma[i] = losses[i-patience+1:i+1].mean()\n",
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ian/gpytorch/gpytorch/models/exact_gp.py:190: UserWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  \"The input matches the stored training data. Did you forget to call model.train()?\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:02:49.089445, fold=0, rep=0, eta=0d 0h 4m 6s \n",
      "{'fold': 0, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.539741039276123, 'train_time': 12.968127058004029, 'prior_train_nmll': 1.5086475610733032, 'train_nll': 21.77501678466797, 'test_nll': 3.524848222732544, 'train_mse': 0.42348822951316833, 'state_dict_file': 'model_state_dict_5481701310660915877.pkl'}\n",
      "2019-03-16 15:03:00.186879, fold=0, rep=1, eta=0d 0h 3m 36s \n",
      "{'fold': 0, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.19903410971164703, 'train_time': 11.097243868978694, 'prior_train_nmll': 1.504563570022583, 'train_nll': 20.27582550048828, 'test_nll': 2.9719150066375732, 'train_mse': 0.3344864249229431, 'state_dict_file': 'model_state_dict_-2282922694711208705.pkl'}\n",
      "2019-03-16 15:03:09.763946, fold=1, rep=0, eta=0d 0h 3m 10s \n",
      "{'fold': 1, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.5020298361778259, 'train_time': 9.572137424955145, 'prior_train_nmll': 1.4897948503494263, 'train_nll': 11.776873588562012, 'test_nll': 3.2702648639678955, 'train_mse': 0.09221820533275604, 'state_dict_file': 'model_state_dict_7033948814869623064.pkl'}\n",
      "2019-03-16 15:03:21.731063, fold=1, rep=1, eta=0d 0h 3m 2s \n",
      "{'fold': 1, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.34153851866722107, 'train_time': 11.966880692983977, 'prior_train_nmll': 1.470295786857605, 'train_nll': 12.658514022827148, 'test_nll': 2.866973876953125, 'train_mse': 0.11608357727527618, 'state_dict_file': 'model_state_dict_1226232652619179556.pkl'}\n",
      "2019-03-16 15:03:31.400950, fold=2, rep=0, eta=0d 0h 2m 45s \n",
      "{'fold': 2, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.875917911529541, 'train_time': 9.665900059044361, 'prior_train_nmll': 1.4607353210449219, 'train_nll': 18.231178283691406, 'test_nll': 4.275941848754883, 'train_mse': 0.2515183687210083, 'state_dict_file': 'model_state_dict_4183148556585905819.pkl'}\n",
      "2019-03-16 15:03:40.961736, fold=2, rep=1, eta=0d 0h 2m 31s \n",
      "{'fold': 2, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.7476749420166016, 'train_time': 9.560563714941964, 'prior_train_nmll': 1.4689013957977295, 'train_nll': 19.264476776123047, 'test_nll': 3.934051275253296, 'train_mse': 0.2995811104774475, 'state_dict_file': 'model_state_dict_5635698873257492025.pkl'}\n",
      "2019-03-16 15:03:53.292640, fold=3, rep=0, eta=0d 0h 2m 23s \n",
      "{'fold': 3, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 1.9047565460205078, 'train_time': 12.325028069899417, 'prior_train_nmll': 1.411368727684021, 'train_nll': 6.062746047973633, 'test_nll': 5.6391520500183105, 'train_mse': 0.03723924234509468, 'state_dict_file': 'model_state_dict_-2090655822804753852.pkl'}\n",
      "2019-03-16 15:04:07.558766, fold=3, rep=1, eta=0d 0h 2m 17s \n",
      "{'fold': 3, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 1.9637326002120972, 'train_time': 14.265895630931482, 'prior_train_nmll': 1.3612067699432373, 'train_nll': 14.118461608886719, 'test_nll': 6.173206329345703, 'train_mse': 0.14822739362716675, 'state_dict_file': 'model_state_dict_-139825982313006691.pkl'}\n",
      "2019-03-16 15:04:12.112347, fold=4, rep=0, eta=0d 0h 1m 57s \n",
      "{'fold': 4, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 3.1678683757781982, 'train_time': 4.548558574984781, 'prior_train_nmll': 1.1562132835388184, 'train_nll': 0.20898056030273438, 'test_nll': 29.2530574798584, 'train_mse': 0.02061283215880394, 'state_dict_file': 'model_state_dict_-9022249589547738386.pkl'}\n",
      "2019-03-16 15:04:16.686932, fold=4, rep=1, eta=0d 0h 1m 40s \n",
      "{'fold': 4, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 3.7492246627807617, 'train_time': 4.574275991995819, 'prior_train_nmll': 1.0731585025787354, 'train_nll': -1.7145805358886719, 'test_nll': 50.182594299316406, 'train_mse': 0.017721785232424736, 'state_dict_file': 'model_state_dict_-1101573603144495157.pkl'}\n",
      "2019-03-16 15:04:24.021874, fold=5, rep=0, eta=0d 0h 1m 28s \n",
      "{'fold': 5, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.220115527510643, 'train_time': 7.330599689972587, 'prior_train_nmll': 1.520897626876831, 'train_nll': 18.10640525817871, 'test_nll': 2.7005624771118164, 'train_mse': 0.23199263215065002, 'state_dict_file': 'model_state_dict_-7117110953417941374.pkl'}\n",
      "2019-03-16 15:04:32.465532, fold=5, rep=1, eta=0d 0h 1m 17s \n",
      "{'fold': 5, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.21240663528442383, 'train_time': 8.443434390006587, 'prior_train_nmll': 1.4965742826461792, 'train_nll': 18.584548950195312, 'test_nll': 2.634784698486328, 'train_mse': 0.266084760427475, 'state_dict_file': 'model_state_dict_6151956646786945680.pkl'}\n",
      "2019-03-16 15:04:39.849303, fold=6, rep=0, eta=0d 0h 1m 6s \n",
      "{'fold': 6, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 1.1073025465011597, 'train_time': 7.378614210989326, 'prior_train_nmll': 1.4791359901428223, 'train_nll': 14.548095703125, 'test_nll': 4.660927772521973, 'train_mse': 0.1484179049730301, 'state_dict_file': 'model_state_dict_-2583685294879070532.pkl'}\n",
      "2019-03-16 15:04:47.017456, fold=6, rep=1, eta=0d 0h 0m 56s \n",
      "{'fold': 6, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.7891602516174316, 'train_time': 7.167929287999868, 'prior_train_nmll': 1.474351167678833, 'train_nll': 20.349529266357422, 'test_nll': 3.9449942111968994, 'train_mse': 0.35370272397994995, 'state_dict_file': 'model_state_dict_-4830500014261765795.pkl'}\n",
      "2019-03-16 15:05:01.094708, fold=7, rep=0, eta=0d 0h 0m 48s \n",
      "{'fold': 7, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.22581282258033752, 'train_time': 14.071898718946613, 'prior_train_nmll': 1.432175636291504, 'train_nll': 17.55820655822754, 'test_nll': 1.8668408393859863, 'train_mse': 0.21040408313274384, 'state_dict_file': 'model_state_dict_-5094700764478834962.pkl'}\n",
      "2019-03-16 15:05:12.826049, fold=7, rep=1, eta=0d 0h 0m 39s \n",
      "{'fold': 7, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.2091527283191681, 'train_time': 11.731153319007717, 'prior_train_nmll': 1.4728076457977295, 'train_nll': 18.197843551635742, 'test_nll': 1.8587431907653809, 'train_mse': 0.21883925795555115, 'state_dict_file': 'model_state_dict_-9199351250203808717.pkl'}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\\n    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 263, in train_additive_rp_gp\n",
      "    return train_exact_gp(trainX, trainY, testX, testY, rp=True, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 91, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:08:25.327567, fold=0, rep=0, eta=0d 0h 5m 40s \n",
      "{'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.4536058902740479, 'train_time': 17.900123110972345, 'prior_train_nmll': 1.0528591871261597, 'train_nll': -1.2246627807617188, 'test_nll': 31.79458236694336, 'train_mse': 0.0164335910230875, 'state_dict_file': 'model_state_dict_2735516277037853461.pkl'}\n",
      "2019-03-16 15:08:45.384507, fold=0, rep=1, eta=0d 0h 5m 41s \n",
      "{'fold': 0, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.2620031833648682, 'train_time': 20.056727370945737, 'prior_train_nmll': 1.0191502571105957, 'train_nll': -6.0626678466796875, 'test_nll': 29.829910278320312, 'train_mse': 0.013760607689619064, 'state_dict_file': 'model_state_dict_3885118029543572562.pkl'}\n",
      "2019-03-16 15:08:57.628880, fold=1, rep=0, eta=0d 0h 4m 44s \n",
      "{'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1141897439956665, 'train_time': 12.241445621009916, 'prior_train_nmll': 1.0472252368927002, 'train_nll': 19.362468719482422, 'test_nll': 20.233734130859375, 'train_mse': 0.03740076348185539, 'state_dict_file': 'model_state_dict_-665659854449504531.pkl'}\n",
      "2019-03-16 15:09:14.573136, fold=1, rep=1, eta=0d 0h 4m 28s \n",
      "{'fold': 1, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.4251911640167236, 'train_time': 16.944036686909385, 'prior_train_nmll': 1.0802438259124756, 'train_nll': 21.08556365966797, 'test_nll': 20.468101501464844, 'train_mse': 0.0377255342900753, 'state_dict_file': 'model_state_dict_4170868110569443453.pkl'}\n",
      "2019-03-16 15:09:29.448768, fold=2, rep=0, eta=0d 0h 4m 6s \n",
      "{'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.6070430278778076, 'train_time': 14.872497632051818, 'prior_train_nmll': 0.9888104796409607, 'train_nll': 17.6124267578125, 'test_nll': 30.117900848388672, 'train_mse': 0.03714125603437424, 'state_dict_file': 'model_state_dict_2075754474969790559.pkl'}\n",
      "2019-03-16 15:09:45.353428, fold=2, rep=1, eta=0d 0h 3m 48s \n",
      "{'fold': 2, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.696716547012329, 'train_time': 15.904438728000969, 'prior_train_nmll': 0.9993088841438293, 'train_nll': 17.137527465820312, 'test_nll': 32.198097229003906, 'train_mse': 0.03623097762465477, 'state_dict_file': 'model_state_dict_7701855412881109887.pkl'}\n",
      "2019-03-16 15:09:57.250530, fold=3, rep=0, eta=0d 0h 3m 23s \n",
      "{'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2124135494232178, 'train_time': 11.893827205058187, 'prior_train_nmll': 1.0946540832519531, 'train_nll': 20.04468536376953, 'test_nll': 22.83242416381836, 'train_mse': 0.036635056138038635, 'state_dict_file': 'model_state_dict_-2822867626615900893.pkl'}\n",
      "2019-03-16 15:10:12.712355, fold=3, rep=1, eta=0d 0h 3m 7s \n",
      "{'fold': 3, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.998009204864502, 'train_time': 15.461602723109536, 'prior_train_nmll': 1.0535153150558472, 'train_nll': 21.60326385498047, 'test_nll': 20.097370147705078, 'train_mse': 0.03785321116447449, 'state_dict_file': 'model_state_dict_3154871282069635497.pkl'}\n",
      "2019-03-16 15:10:26.031205, fold=4, rep=0, eta=0d 0h 2m 49s \n",
      "{'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.8086762428283691, 'train_time': 13.315338790067472, 'prior_train_nmll': 1.0744892358779907, 'train_nll': 20.72333526611328, 'test_nll': 14.278376579284668, 'train_mse': 0.03716084733605385, 'state_dict_file': 'model_state_dict_-181657369454269136.pkl'}\n",
      "2019-03-16 15:10:38.032065, fold=4, rep=1, eta=0d 0h 2m 30s \n",
      "{'fold': 4, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.6427204012870789, 'train_time': 12.000615125056356, 'prior_train_nmll': 1.0594358444213867, 'train_nll': 20.9952392578125, 'test_nll': 13.741243362426758, 'train_mse': 0.03762785717844963, 'state_dict_file': 'model_state_dict_2527346664171458536.pkl'}\n",
      "2019-03-16 15:10:50.120068, fold=5, rep=0, eta=0d 0h 2m 13s \n",
      "{'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2778680324554443, 'train_time': 12.08456159895286, 'prior_train_nmll': 1.0818887948989868, 'train_nll': 13.418785095214844, 'test_nll': 23.254024505615234, 'train_mse': 0.027772871777415276, 'state_dict_file': 'model_state_dict_2366321473454518303.pkl'}\n",
      "2019-03-16 15:11:03.243500, fold=5, rep=1, eta=0d 0h 1m 57s \n",
      "{'fold': 5, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.1053333282470703, 'train_time': 13.123203765950166, 'prior_train_nmll': 0.9840680360794067, 'train_nll': 7.906623840332031, 'test_nll': 24.36137580871582, 'train_mse': 0.025125376880168915, 'state_dict_file': 'model_state_dict_3317951095216557727.pkl'}\n",
      "2019-03-16 15:11:21.152761, fold=6, rep=0, eta=0d 0h 1m 44s \n",
      "{'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3475751876831055, 'train_time': 17.90623406402301, 'prior_train_nmll': 1.0019818544387817, 'train_nll': -3.2494049072265625, 'test_nll': 28.381027221679688, 'train_mse': 0.015490429475903511, 'state_dict_file': 'model_state_dict_6706300528422674956.pkl'}\n",
      "2019-03-16 15:11:35.539381, fold=6, rep=1, eta=0d 0h 1m 29s \n",
      "{'fold': 6, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.474230408668518, 'train_time': 14.386161755071953, 'prior_train_nmll': 1.0318595170974731, 'train_nll': -2.0582962036132812, 'test_nll': 30.265037536621094, 'train_mse': 0.01628510281443596, 'state_dict_file': 'model_state_dict_-5471822538068318969.pkl'}\n",
      "2019-03-16 15:11:51.257028, fold=7, rep=0, eta=0d 0h 1m 14s \n",
      "{'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.742012083530426, 'train_time': 15.714212808990851, 'prior_train_nmll': 1.1136126518249512, 'train_nll': 23.209884643554688, 'test_nll': 14.170076370239258, 'train_mse': 0.0387934111058712, 'state_dict_file': 'model_state_dict_-2572019818479950011.pkl'}\n",
      "2019-03-16 15:12:09.699274, fold=7, rep=1, eta=0d 0h 1m 0s \n",
      "{'fold': 7, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.5658707022666931, 'train_time': 18.441998352995142, 'prior_train_nmll': 1.0989738702774048, 'train_nll': 21.99810028076172, 'test_nll': 12.246196746826172, 'train_mse': 0.03614334762096405, 'state_dict_file': 'model_state_dict_2937591179076678606.pkl'}\n",
      "2019-03-16 15:12:25.519940, fold=8, rep=0, eta=0d 0h 0m 45s \n",
      "{'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 2.1262354850769043, 'train_time': 15.817443760926835, 'prior_train_nmll': 1.0536372661590576, 'train_nll': 20.265350341796875, 'test_nll': 32.458744049072266, 'train_mse': 0.038448575884103775, 'state_dict_file': 'model_state_dict_2119607228814835471.pkl'}\n",
      "2019-03-16 15:12:42.927706, fold=8, rep=1, eta=0d 0h 0m 30s \n",
      "{'fold': 8, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.4746137857437134, 'train_time': 17.40755514195189, 'prior_train_nmll': 1.01187002658844, 'train_nll': 19.261600494384766, 'test_nll': 27.058696746826172, 'train_mse': 0.03761899098753929, 'state_dict_file': 'model_state_dict_-3258460892715794207.pkl'}\n",
      "2019-03-16 15:13:00.158603, fold=9, rep=0, eta=0d 0h 0m 15s \n",
      "{'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.5440942645072937, 'train_time': 17.22728738700971, 'prior_train_nmll': 1.090135097503662, 'train_nll': 21.13345718383789, 'test_nll': 11.32058334350586, 'train_mse': 0.038186196237802505, 'state_dict_file': 'model_state_dict_2633690366260352703.pkl'}\n",
      "2019-03-16 15:13:12.951591, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.7205531001091003, 'train_time': 12.792775274021551, 'prior_train_nmll': 1.0993486642837524, 'train_nll': 24.524723052978516, 'test_nll': 14.556057929992676, 'train_mse': 0.038929082453250885, 'state_dict_file': 'model_state_dict_6325980663460822712.pkl'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/ipykernel_launcher.py:15: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:13:24.418215, fold=0, rep=0, eta=0d 0h 3m 37s \n",
      "{'fold': 0, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.06646811217069626, 'train_time': 11.443595984950662, 'prior_train_nmll': -0.03912442550063133, 'train_nll': -421.0924072265625, 'test_nll': 1.8512744903564453, 'train_mse': 7.906831160653383e-05, 'state_dict_file': 'model_state_dict_7111611273834999375.pkl'}\n",
      "2019-03-16 15:13:33.106216, fold=0, rep=1, eta=0d 0h 3m 1s \n",
      "{'fold': 0, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.01964564435184002, 'train_time': 8.687782558030449, 'prior_train_nmll': 0.0731954351067543, 'train_nll': -489.7598876953125, 'test_nll': -6.138891220092773, 'train_mse': 9.490145748713985e-05, 'state_dict_file': 'model_state_dict_6645346290150733212.pkl'}\n",
      "2019-03-16 15:13:45.037882, fold=1, rep=0, eta=0d 0h 3m 1s \n",
      "{'fold': 1, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.02842322736978531, 'train_time': 11.928161144955084, 'prior_train_nmll': -0.12236659228801727, 'train_nll': -491.32342529296875, 'test_nll': -8.587963104248047, 'train_mse': 9.857933036983013e-05, 'state_dict_file': 'model_state_dict_2896289302706431466.pkl'}\n",
      "2019-03-16 15:13:55.351874, fold=1, rep=1, eta=0d 0h 2m 49s \n",
      "{'fold': 1, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.04248085245490074, 'train_time': 10.313765114056878, 'prior_train_nmll': -0.23038139939308167, 'train_nll': 32659.892578125, 'test_nll': -4.005521774291992, 'train_mse': 0.0001137209328589961, 'state_dict_file': 'model_state_dict_2962023097274472735.pkl'}\n",
      "2019-03-16 15:14:05.327756, fold=2, rep=0, eta=0d 0h 2m 37s \n",
      "{'fold': 2, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.007843798026442528, 'train_time': 9.972585230018012, 'prior_train_nmll': -0.09697945415973663, 'train_nll': -479.21746826171875, 'test_nll': -13.310068130493164, 'train_mse': 0.0001251647772733122, 'state_dict_file': 'model_state_dict_7036536336670674910.pkl'}\n",
      "2019-03-16 15:14:15.763127, fold=2, rep=1, eta=0d 0h 2m 26s \n",
      "{'fold': 2, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.006345918867737055, 'train_time': 10.435079991002567, 'prior_train_nmll': -0.011826950125396252, 'train_nll': -523.5498657226562, 'test_nll': -11.795454025268555, 'train_mse': 0.00010576523345662281, 'state_dict_file': 'model_state_dict_-3336394290217259031.pkl'}\n",
      "2019-03-16 15:14:26.828311, fold=3, rep=0, eta=0d 0h 2m 17s \n",
      "{'fold': 3, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.011784224770963192, 'train_time': 11.061341812950559, 'prior_train_nmll': -0.11200305819511414, 'train_nll': -481.75994873046875, 'test_nll': -11.94619369506836, 'train_mse': 7.629208994330838e-05, 'state_dict_file': 'model_state_dict_8173414427730786542.pkl'}\n",
      "2019-03-16 15:14:35.682577, fold=3, rep=1, eta=0d 0h 2m 4s \n",
      "{'fold': 3, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.011318080127239227, 'train_time': 8.854043095023371, 'prior_train_nmll': -0.08734975010156631, 'train_nll': -424.96746826171875, 'test_nll': -10.084842681884766, 'train_mse': 8.505758160026744e-05, 'state_dict_file': 'model_state_dict_2248549458607619936.pkl'}\n",
      "2019-03-16 15:14:45.367028, fold=4, rep=0, eta=0d 0h 1m 52s \n",
      "{'fold': 4, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.004708151798695326, 'train_time': 9.680957228061743, 'prior_train_nmll': -0.14103274047374725, 'train_nll': -563.3156127929688, 'test_nll': -13.024282455444336, 'train_mse': 8.465926657663658e-05, 'state_dict_file': 'model_state_dict_-8500811981912954067.pkl'}\n",
      "2019-03-16 15:14:59.409340, fold=4, rep=1, eta=0d 0h 1m 46s \n",
      "{'fold': 4, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.004253069870173931, 'train_time': 14.04208273801487, 'prior_train_nmll': -0.006590200588107109, 'train_nll': -412.6546630859375, 'test_nll': -11.984121322631836, 'train_mse': 8.314786828123033e-05, 'state_dict_file': 'model_state_dict_-379746806537483363.pkl'}\n",
      "2019-03-16 15:15:12.524035, fold=5, rep=0, eta=0d 0h 1m 37s \n",
      "{'fold': 5, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.012241813354194164, 'train_time': 13.111360101029277, 'prior_train_nmll': -0.0015564586501568556, 'train_nll': -570.0518188476562, 'test_nll': -10.324579238891602, 'train_mse': 0.00010650255717337132, 'state_dict_file': 'model_state_dict_-3970493034372201366.pkl'}\n",
      "2019-03-16 15:15:23.906926, fold=5, rep=1, eta=0d 0h 1m 27s \n",
      "{'fold': 5, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.021960170939564705, 'train_time': 11.382673728978261, 'prior_train_nmll': -0.09339595586061478, 'train_nll': -482.41107177734375, 'test_nll': -12.937227249145508, 'train_mse': 0.00012702947424259037, 'state_dict_file': 'model_state_dict_3964469567244374108.pkl'}\n",
      "2019-03-16 15:15:35.612329, fold=6, rep=0, eta=0d 0h 1m 16s \n",
      "{'fold': 6, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.017526449635624886, 'train_time': 11.701983125065453, 'prior_train_nmll': -0.006961884908378124, 'train_nll': -470.88592529296875, 'test_nll': -7.723653793334961, 'train_mse': 7.10674503352493e-05, 'state_dict_file': 'model_state_dict_9171712479154756156.pkl'}\n",
      "2019-03-16 15:15:46.855738, fold=6, rep=1, eta=0d 0h 1m 5s \n",
      "{'fold': 6, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.012603861279785633, 'train_time': 11.243186621111818, 'prior_train_nmll': -0.19596131145954132, 'train_nll': -472.4881591796875, 'test_nll': -12.101091384887695, 'train_mse': 0.00011338565673213452, 'state_dict_file': 'model_state_dict_-7420673884544297876.pkl'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py:63: UserWarning: NaNs encountered in preconditioner computation. Attempting to continue without preconditioning.\n",
      "  \"NaNs encountered in preconditioner computation. Attempting to continue without preconditioning.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:15:58.139799, fold=7, rep=0, eta=0d 0h 0m 55s \n",
      "{'fold': 7, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.09353262931108475, 'train_time': 11.280552237993106, 'prior_train_nmll': -0.2574930489063263, 'train_nll': -469.515869140625, 'test_nll': 0.3037242889404297, 'train_mse': 0.00011069536412833259, 'state_dict_file': 'model_state_dict_1959115837024645230.pkl'}\n",
      "2019-03-16 15:16:09.391290, fold=7, rep=1, eta=0d 0h 0m 44s \n",
      "{'fold': 7, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.12539328634738922, 'train_time': 11.251142958062701, 'prior_train_nmll': -0.07086212933063507, 'train_nll': -468.983154296875, 'test_nll': 6.032941818237305, 'train_mse': 0.0001313518878305331, 'state_dict_file': 'model_state_dict_-6016895112307526631.pkl'}\n",
      "2019-03-16 15:16:18.926324, fold=8, rep=0, eta=0d 0h 0m 32s \n",
      "{'fold': 8, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.008294757455587387, 'train_time': 9.529598235036246, 'prior_train_nmll': -0.055480439215898514, 'train_nll': 370637.78125, 'test_nll': -14.655086517333984, 'train_mse': 0.00016185393906198442, 'state_dict_file': 'model_state_dict_-8517373448206723107.pkl'}\n",
      "2019-03-16 15:16:26.950638, fold=8, rep=1, eta=0d 0h 0m 21s \n",
      "{'fold': 8, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.03327471390366554, 'train_time': 8.024073182954453, 'prior_train_nmll': 0.06939384341239929, 'train_nll': nan, 'test_nll': -11.709680557250977, 'train_mse': 8.456816431134939e-05, 'state_dict_file': 'model_state_dict_-8241110142538878723.pkl'}\n",
      "2019-03-16 15:16:36.003087, fold=9, rep=0, eta=0d 0h 0m 10s \n",
      "{'fold': 9, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.0020079880487173796, 'train_time': 9.049229486961849, 'prior_train_nmll': 0.009401745162904263, 'train_nll': -581.665771484375, 'test_nll': -5.650827884674072, 'train_mse': 0.0001571656612213701, 'state_dict_file': 'model_state_dict_6764511418767222114.pkl'}\n",
      "2019-03-16 15:16:44.690026, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.014204630628228188, 'train_time': 8.68666641798336, 'prior_train_nmll': -0.045478880405426025, 'train_nll': -540.1359252929688, 'test_nll': -5.44662618637085, 'train_mse': 0.00011137364344904199, 'state_dict_file': 'model_state_dict_6573282218701052243.pkl'}\n",
      "2019-03-16 15:17:09.438774, fold=0, rep=0, eta=0d 0h 7m 49s \n",
      "{'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.07674781233072281, 'train_time': 24.726253519998863, 'prior_train_nmll': 0.19591230154037476, 'train_nll': -129.85110473632812, 'test_nll': 1.353438377380371, 'train_mse': 0.0034556190948933363, 'state_dict_file': 'model_state_dict_-3044418744805417849.pkl'}\n",
      "2019-03-16 15:17:32.288913, fold=0, rep=1, eta=0d 0h 7m 8s \n",
      "{'fold': 0, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.07888510823249817, 'train_time': 22.849917484098114, 'prior_train_nmll': 0.16557574272155762, 'train_nll': -130.827880859375, 'test_nll': 2.23372745513916, 'train_mse': 0.003686628071591258, 'state_dict_file': 'model_state_dict_-7246927397375080009.pkl'}\n",
      "2019-03-16 15:18:07.349614, fold=1, rep=0, eta=0d 0h 7m 48s \n",
      "{'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4003998041152954, 'train_time': 35.05731719301548, 'prior_train_nmll': 0.11715678125619888, 'train_nll': -166.95724487304688, 'test_nll': 30.27404022216797, 'train_mse': 0.0018291500164195895, 'state_dict_file': 'model_state_dict_-4900383991779304560.pkl'}\n",
      "2019-03-16 15:18:28.144916, fold=1, rep=1, eta=0d 0h 6m 53s \n",
      "{'fold': 1, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.4053459167480469, 'train_time': 20.79502452700399, 'prior_train_nmll': 0.17925803363323212, 'train_nll': -157.89389038085938, 'test_nll': 34.064598083496094, 'train_mse': 0.0021136971190571785, 'state_dict_file': 'model_state_dict_-4566165215277838971.pkl'}\n",
      "2019-03-16 15:18:51.964656, fold=2, rep=0, eta=0d 0h 6m 21s \n",
      "{'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.16383397579193115, 'train_time': 23.81669995491393, 'prior_train_nmll': 0.24114713072776794, 'train_nll': -193.07769775390625, 'test_nll': 24.958847045898438, 'train_mse': 0.0009770491160452366, 'state_dict_file': 'model_state_dict_5555835767946574541.pkl'}\n",
      "2019-03-16 15:19:24.238948, fold=2, rep=1, eta=0d 0h 6m 12s \n",
      "{'fold': 2, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.11299850791692734, 'train_time': 32.27407090191264, 'prior_train_nmll': 0.16501373052597046, 'train_nll': -211.78485107421875, 'test_nll': 26.318532943725586, 'train_mse': 0.0008840680820867419, 'state_dict_file': 'model_state_dict_-8231785232139634040.pkl'}\n",
      "2019-03-16 15:19:50.472747, fold=3, rep=0, eta=0d 0h 5m 44s \n",
      "{'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1785510778427124, 'train_time': 26.230446630972438, 'prior_train_nmll': 0.16744382679462433, 'train_nll': -135.30718994140625, 'test_nll': 24.519710540771484, 'train_mse': 0.0031950934790074825, 'state_dict_file': 'model_state_dict_5487979886011040892.pkl'}\n",
      "2019-03-16 15:20:12.921165, fold=3, rep=1, eta=0d 0h 5m 12s \n",
      "{'fold': 3, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.1449941247701645, 'train_time': 22.44816445792094, 'prior_train_nmll': 0.2268034964799881, 'train_nll': -129.33514404296875, 'test_nll': 11.483116149902344, 'train_mse': 0.003347812918946147, 'state_dict_file': 'model_state_dict_4606012838443498414.pkl'}\n",
      "2019-03-16 15:20:47.166934, fold=4, rep=0, eta=0d 0h 4m 56s \n",
      "{'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.17970404028892517, 'train_time': 34.24231459503062, 'prior_train_nmll': 0.11189276725053787, 'train_nll': -157.7156982421875, 'test_nll': 32.35161590576172, 'train_mse': 0.0023395561147481203, 'state_dict_file': 'model_state_dict_1560201305293163102.pkl'}\n",
      "2019-03-16 15:21:14.393776, fold=4, rep=1, eta=0d 0h 4m 29s \n",
      "{'fold': 4, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.1303805708885193, 'train_time': 27.226597090018913, 'prior_train_nmll': 0.113428995013237, 'train_nll': -145.55422973632812, 'test_nll': 21.765653610229492, 'train_mse': 0.002835444640368223, 'state_dict_file': 'model_state_dict_3633931001364488134.pkl'}\n",
      "2019-03-16 15:21:44.617233, fold=5, rep=0, eta=0d 0h 4m 5s \n",
      "{'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.14509451389312744, 'train_time': 30.219327123020776, 'prior_train_nmll': 0.13375018537044525, 'train_nll': -142.75009155273438, 'test_nll': 14.188505172729492, 'train_mse': 0.0030396936926990747, 'state_dict_file': 'model_state_dict_-1894321863873855773.pkl'}\n",
      "2019-03-16 15:22:16.013757, fold=5, rep=1, eta=0d 0h 3m 40s \n",
      "{'fold': 5, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.30218517780303955, 'train_time': 31.396306459093466, 'prior_train_nmll': 0.10466479510068893, 'train_nll': -141.84588623046875, 'test_nll': 24.5684814453125, 'train_mse': 0.003105483716353774, 'state_dict_file': 'model_state_dict_-7489452874987659268.pkl'}\n",
      "2019-03-16 15:22:57.391514, fold=6, rep=0, eta=0d 0h 3m 20s \n",
      "{'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.11550398170948029, 'train_time': 41.374249481013976, 'prior_train_nmll': 0.12582990527153015, 'train_nll': -141.46014404296875, 'test_nll': 13.344560623168945, 'train_mse': 0.0032666611950844526, 'state_dict_file': 'model_state_dict_5544691810273984125.pkl'}\n",
      "2019-03-16 15:23:32.787762, fold=6, rep=1, eta=0d 0h 2m 54s \n",
      "{'fold': 6, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.14590716361999512, 'train_time': 35.39594251406379, 'prior_train_nmll': 0.12447454780340195, 'train_nll': -138.85888671875, 'test_nll': 12.775487899780273, 'train_mse': 0.003237449796870351, 'state_dict_file': 'model_state_dict_606438358643688825.pkl'}\n",
      "2019-03-16 15:23:53.984976, fold=7, rep=0, eta=0d 0h 2m 23s \n",
      "{'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1631755232810974, 'train_time': 21.193433068925515, 'prior_train_nmll': 0.18006743490695953, 'train_nll': -131.16952514648438, 'test_nll': 9.335939407348633, 'train_mse': 0.0036007973831146955, 'state_dict_file': 'model_state_dict_-7840802811885394083.pkl'}\n",
      "2019-03-16 15:24:19.681972, fold=7, rep=1, eta=0d 0h 1m 53s \n",
      "{'fold': 7, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.10911069065332413, 'train_time': 25.69675993709825, 'prior_train_nmll': 0.20793035626411438, 'train_nll': -134.42816162109375, 'test_nll': 11.983678817749023, 'train_mse': 0.003438358660787344, 'state_dict_file': 'model_state_dict_3780314593244641487.pkl'}\n",
      "2019-03-16 15:24:39.828874, fold=8, rep=0, eta=0d 0h 1m 23s \n",
      "{'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4815264046192169, 'train_time': 20.142901577986777, 'prior_train_nmll': 0.18033990263938904, 'train_nll': -121.69113159179688, 'test_nll': 25.305492401123047, 'train_mse': 0.0040757195092737675, 'state_dict_file': 'model_state_dict_-6780372710784218377.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:25:15.026148, fold=8, rep=1, eta=0d 0h 0m 56s \n",
      "{'fold': 8, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.33556902408599854, 'train_time': 35.19703944702633, 'prior_train_nmll': 0.08283122628927231, 'train_nll': -141.48379516601562, 'test_nll': 17.182613372802734, 'train_mse': 0.00313000101596117, 'state_dict_file': 'model_state_dict_-2630996341086971605.pkl'}\n",
      "2019-03-16 15:25:40.595463, fold=9, rep=0, eta=0d 0h 0m 28s \n",
      "{'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.07682517170906067, 'train_time': 25.56570063403342, 'prior_train_nmll': 0.1806652992963791, 'train_nll': -158.50466918945312, 'test_nll': 4.4984025955200195, 'train_mse': 0.0022535312455147505, 'state_dict_file': 'model_state_dict_-4608979545482689361.pkl'}\n",
      "2019-03-16 15:26:13.486946, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.09440872073173523, 'train_time': 32.89124977705069, 'prior_train_nmll': 0.11620782315731049, 'train_nll': -158.94354248046875, 'test_nll': 9.416964530944824, 'train_mse': 0.002330068964511156, 'state_dict_file': 'model_state_dict_-3457616739981792964.pkl'}\n",
      "2019-03-16 15:26:26.953439, fold=0, rep=0, eta=0d 0h 4m 15s \n",
      "{'fold': 0, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.11789390444755554, 'train_time': 13.444154226919636, 'prior_train_nmll': 0.14323706924915314, 'train_nll': -290.60565185546875, 'test_nll': 22.789630889892578, 'train_mse': 0.004618191625922918, 'state_dict_file': 'model_state_dict_-8727332891634146916.pkl'}\n",
      "2019-03-16 15:26:43.724868, fold=0, rep=1, eta=0d 0h 4m 31s \n",
      "{'fold': 0, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.1306280791759491, 'train_time': 16.771220532944426, 'prior_train_nmll': 0.1757945865392685, 'train_nll': -100.11489868164062, 'test_nll': 17.3427734375, 'train_mse': 0.010622943751513958, 'state_dict_file': 'model_state_dict_6014131882630994109.pkl'}\n",
      "2019-03-16 15:26:53.510583, fold=1, rep=0, eta=0d 0h 3m 46s \n",
      "{'fold': 1, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.24796609580516815, 'train_time': 9.782406347920187, 'prior_train_nmll': 0.22181101143360138, 'train_nll': -96.38888549804688, 'test_nll': 31.81417465209961, 'train_mse': 0.011045273393392563, 'state_dict_file': 'model_state_dict_1522775367259040963.pkl'}\n",
      "2019-03-16 15:27:10.542450, fold=1, rep=1, eta=0d 0h 3m 48s \n",
      "{'fold': 1, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.1476411670446396, 'train_time': 17.031622952083126, 'prior_train_nmll': 0.20377884805202484, 'train_nll': -114.70819091796875, 'test_nll': 9.404324531555176, 'train_mse': 0.008820217102766037, 'state_dict_file': 'model_state_dict_-6229500361426842652.pkl'}\n",
      "2019-03-16 15:27:23.494098, fold=2, rep=0, eta=0d 0h 3m 29s \n",
      "{'fold': 2, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.2857395112514496, 'train_time': 12.94862608902622, 'prior_train_nmll': 0.18810579180717468, 'train_nll': -108.75752258300781, 'test_nll': 39.49673843383789, 'train_mse': 0.008304191753268242, 'state_dict_file': 'model_state_dict_8107480207291141115.pkl'}\n",
      "2019-03-16 15:27:33.461188, fold=2, rep=1, eta=0d 0h 3m 6s \n",
      "{'fold': 2, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.08399657905101776, 'train_time': 9.96687445603311, 'prior_train_nmll': 0.2695463001728058, 'train_nll': -60.97834777832031, 'test_nll': 3.2541351318359375, 'train_mse': 0.01828564703464508, 'state_dict_file': 'model_state_dict_2933072305021292093.pkl'}\n",
      "2019-03-16 15:27:58.113265, fold=3, rep=0, eta=0d 0h 3m 14s \n",
      "{'fold': 3, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.19587373733520508, 'train_time': 24.648804898024537, 'prior_train_nmll': 0.17387056350708008, 'train_nll': -318.72418212890625, 'test_nll': 52.624473571777344, 'train_mse': 0.004042312502861023, 'state_dict_file': 'model_state_dict_4291660968179508575.pkl'}\n",
      "2019-03-16 15:28:07.190013, fold=3, rep=1, eta=0d 0h 2m 50s \n",
      "{'fold': 3, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.1494976431131363, 'train_time': 9.0765150670195, 'prior_train_nmll': 0.25385376811027527, 'train_nll': -523.0758056640625, 'test_nll': 32.83576965332031, 'train_mse': 0.007315021939575672, 'state_dict_file': 'model_state_dict_-2398383766201197142.pkl'}\n",
      "2019-03-16 15:28:17.641698, fold=4, rep=0, eta=0d 0h 2m 31s \n",
      "{'fold': 4, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.05680370703339577, 'train_time': 10.448293326073326, 'prior_train_nmll': 0.24771606922149658, 'train_nll': -70.77737426757812, 'test_nll': -0.8474740982055664, 'train_mse': 0.015203275717794895, 'state_dict_file': 'model_state_dict_3732024596101611030.pkl'}\n",
      "2019-03-16 15:28:32.048033, fold=4, rep=1, eta=0d 0h 2m 18s \n",
      "{'fold': 4, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.13110104203224182, 'train_time': 14.405673600034788, 'prior_train_nmll': 0.26379942893981934, 'train_nll': 1113.898193359375, 'test_nll': 24.79666519165039, 'train_mse': 0.0037915415596216917, 'state_dict_file': 'model_state_dict_-1284372468888936085.pkl'}\n",
      "2019-03-16 15:28:47.687372, fold=5, rep=0, eta=0d 0h 2m 6s \n",
      "{'fold': 5, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.2785254418849945, 'train_time': 15.636283530038781, 'prior_train_nmll': 0.23775146901607513, 'train_nll': -139.86546325683594, 'test_nll': 48.36687088012695, 'train_mse': 0.006499435752630234, 'state_dict_file': 'model_state_dict_1918470254736981342.pkl'}\n",
      "2019-03-16 15:28:58.383570, fold=5, rep=1, eta=0d 0h 1m 49s \n",
      "{'fold': 5, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.2864825427532196, 'train_time': 10.695959764998406, 'prior_train_nmll': 0.21699956059455872, 'train_nll': 996581.8125, 'test_nll': 40.61200714111328, 'train_mse': 0.006445042788982391, 'state_dict_file': 'model_state_dict_-1480370495132933584.pkl'}\n",
      "2019-03-16 15:29:13.098388, fold=6, rep=0, eta=0d 0h 1m 36s \n",
      "{'fold': 6, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.062278952449560165, 'train_time': 14.711358343949541, 'prior_train_nmll': 0.16927313804626465, 'train_nll': -3938.583251953125, 'test_nll': 1.033543586730957, 'train_mse': 0.0065732914954423904, 'state_dict_file': 'model_state_dict_1765368482900883118.pkl'}\n",
      "2019-03-16 15:29:24.193555, fold=6, rep=1, eta=0d 0h 1m 21s \n",
      "{'fold': 6, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.08597519993782043, 'train_time': 11.09494924498722, 'prior_train_nmll': 0.2990030348300934, 'train_nll': -539.351806640625, 'test_nll': 9.879986763000488, 'train_mse': 0.004440425429493189, 'state_dict_file': 'model_state_dict_9210931607272688912.pkl'}\n",
      "2019-03-16 15:29:37.843016, fold=7, rep=0, eta=0d 0h 1m 8s \n",
      "{'fold': 7, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.15413036942481995, 'train_time': 13.64595982094761, 'prior_train_nmll': 0.2262987196445465, 'train_nll': -418.572265625, 'test_nll': 25.575946807861328, 'train_mse': 0.0050254506058990955, 'state_dict_file': 'model_state_dict_2708068879008630306.pkl'}\n",
      "2019-03-16 15:29:48.471692, fold=7, rep=1, eta=0d 0h 0m 53s \n",
      "{'fold': 7, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.09217363595962524, 'train_time': 10.62841473997105, 'prior_train_nmll': 0.20718933641910553, 'train_nll': 145.8806610107422, 'test_nll': 4.4706010818481445, 'train_mse': 0.007891656830906868, 'state_dict_file': 'model_state_dict_-8350551446057374363.pkl'}\n",
      "2019-03-16 15:30:03.178878, fold=8, rep=0, eta=0d 0h 0m 40s \n",
      "{'fold': 8, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.07127220928668976, 'train_time': 14.703750757034868, 'prior_train_nmll': 0.195535346865654, 'train_nll': -20724.99609375, 'test_nll': 2.8907432556152344, 'train_mse': 0.009371604770421982, 'state_dict_file': 'model_state_dict_8120506026716141257.pkl'}\n",
      "2019-03-16 15:30:15.685027, fold=8, rep=1, eta=0d 0h 0m 26s \n",
      "{'fold': 8, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.11627344787120819, 'train_time': 12.505869994056411, 'prior_train_nmll': 0.16292497515678406, 'train_nll': -88.00514221191406, 'test_nll': 12.033360481262207, 'train_mse': 0.011944709345698357, 'state_dict_file': 'model_state_dict_7679068975029686840.pkl'}\n",
      "2019-03-16 15:30:27.772184, fold=9, rep=0, eta=0d 0h 0m 13s \n",
      "{'fold': 9, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.1052679494023323, 'train_time': 12.083937440998852, 'prior_train_nmll': 0.2569026052951813, 'train_nll': -252.12237548828125, 'test_nll': 9.21279525756836, 'train_mse': 0.005787996109575033, 'state_dict_file': 'model_state_dict_-7225468838248611543.pkl'}\n",
      "2019-03-16 15:30:43.891026, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.15256686508655548, 'train_time': 16.11824518290814, 'prior_train_nmll': 0.1602586805820465, 'train_nll': -360.14764404296875, 'test_nll': 50.37119674682617, 'train_mse': 0.0021913691889494658, 'state_dict_file': 'model_state_dict_-4504456524554414056.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:31:09.603158, fold=0, rep=0, eta=0d 0h 8m 8s \n",
      "{'fold': 0, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.7551144361495972, 'train_time': 25.688570062979124, 'prior_train_nmll': 0.7126064300537109, 'train_nll': -584.4906616210938, 'test_nll': 87.87945556640625, 'train_mse': 0.00010283885785611346, 'state_dict_file': 'model_state_dict_4455568237930429054.pkl'}\n",
      "2019-03-16 15:31:38.978203, fold=0, rep=1, eta=0d 0h 8m 15s \n",
      "{'fold': 0, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.7222751379013062, 'train_time': 29.37481420894619, 'prior_train_nmll': 0.7358039021492004, 'train_nll': -616.7403564453125, 'test_nll': 90.58358764648438, 'train_mse': 4.541098314803094e-05, 'state_dict_file': 'model_state_dict_-2290775982462551909.pkl'}\n",
      "2019-03-16 15:32:12.662365, fold=1, rep=0, eta=0d 0h 8m 22s \n",
      "{'fold': 1, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.0703811645507812, 'train_time': 33.68113467597868, 'prior_train_nmll': 0.6798458099365234, 'train_nll': -753.834228515625, 'test_nll': 128.09048461914062, 'train_mse': 0.00010342026507714763, 'state_dict_file': 'model_state_dict_-8329655536038404598.pkl'}\n",
      "2019-03-16 15:32:48.920869, fold=1, rep=1, eta=0d 0h 8m 20s \n",
      "{'fold': 1, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.9082948565483093, 'train_time': 36.25825509999413, 'prior_train_nmll': 0.7593631148338318, 'train_nll': -759.1567993164062, 'test_nll': 110.1688232421875, 'train_mse': 8.553153747925535e-05, 'state_dict_file': 'model_state_dict_7134577646007320776.pkl'}\n",
      "2019-03-16 15:33:12.969639, fold=2, rep=0, eta=0d 0h 7m 27s \n",
      "{'fold': 2, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.296911358833313, 'train_time': 24.04568806407042, 'prior_train_nmll': 0.7708112597465515, 'train_nll': -541.64404296875, 'test_nll': 152.62075805664062, 'train_mse': 7.741135050309822e-05, 'state_dict_file': 'model_state_dict_6268441109897381762.pkl'}\n",
      "2019-03-16 15:33:42.709866, fold=2, rep=1, eta=0d 0h 6m 57s \n",
      "{'fold': 2, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.0893834829330444, 'train_time': 29.739985967054963, 'prior_train_nmll': 0.7007604241371155, 'train_nll': -614.6353149414062, 'test_nll': 125.25387573242188, 'train_mse': 3.9850881876191124e-05, 'state_dict_file': 'model_state_dict_2133475985478517786.pkl'}\n",
      "2019-03-16 15:34:14.996745, fold=3, rep=0, eta=0d 0h 6m 32s \n",
      "{'fold': 3, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.9844121932983398, 'train_time': 32.28371838491876, 'prior_train_nmll': 0.661144495010376, 'train_nll': -675.4699096679688, 'test_nll': 130.60711669921875, 'train_mse': 5.40866021765396e-05, 'state_dict_file': 'model_state_dict_-1535912958207282431.pkl'}\n",
      "2019-03-16 15:34:37.128128, fold=3, rep=1, eta=0d 0h 5m 49s \n",
      "{'fold': 3, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.0481107234954834, 'train_time': 22.13075705105439, 'prior_train_nmll': 0.7110598087310791, 'train_nll': -476.008544921875, 'test_nll': 95.57892608642578, 'train_mse': 7.021709461696446e-05, 'state_dict_file': 'model_state_dict_5525023073383592872.pkl'}\n",
      "2019-03-16 15:35:00.355372, fold=4, rep=0, eta=0d 0h 5m 13s \n",
      "{'fold': 4, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.8499759435653687, 'train_time': 23.224011969054118, 'prior_train_nmll': 0.743695855140686, 'train_nll': -693.3444213867188, 'test_nll': 102.793701171875, 'train_mse': 7.521772931795567e-05, 'state_dict_file': 'model_state_dict_9092348534082272243.pkl'}\n",
      "2019-03-16 15:35:23.794862, fold=4, rep=1, eta=0d 0h 4m 39s \n",
      "{'fold': 4, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.6926666498184204, 'train_time': 23.439255125937052, 'prior_train_nmll': 0.7405940294265747, 'train_nll': -421.31793212890625, 'test_nll': 78.00595092773438, 'train_mse': 6.025735638104379e-05, 'state_dict_file': 'model_state_dict_-418907118720706372.pkl'}\n",
      "2019-03-16 15:35:47.761277, fold=5, rep=0, eta=0d 0h 4m 8s \n",
      "{'fold': 5, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.2594090700149536, 'train_time': 23.96323306998238, 'prior_train_nmll': 0.6386882662773132, 'train_nll': -457.87017822265625, 'test_nll': 151.74432373046875, 'train_mse': 7.130642188712955e-05, 'state_dict_file': 'model_state_dict_5613444250265913366.pkl'}\n",
      "2019-03-16 15:36:10.806678, fold=5, rep=1, eta=0d 0h 3m 37s \n",
      "{'fold': 5, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.1219291687011719, 'train_time': 23.044891946949065, 'prior_train_nmll': 0.7209460735321045, 'train_nll': -370.87640380859375, 'test_nll': 123.20303344726562, 'train_mse': 9.062772005563602e-05, 'state_dict_file': 'model_state_dict_329489515810668737.pkl'}\n",
      "2019-03-16 15:36:30.439326, fold=6, rep=0, eta=0d 0h 3m 6s \n",
      "{'fold': 6, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.45979148149490356, 'train_time': 19.6294462189544, 'prior_train_nmll': 0.6615974307060242, 'train_nll': -400.86822509765625, 'test_nll': 65.02179718017578, 'train_mse': 8.061746484600008e-05, 'state_dict_file': 'model_state_dict_3984880507060306263.pkl'}\n",
      "2019-03-16 15:37:01.829238, fold=6, rep=1, eta=0d 0h 2m 41s \n",
      "{'fold': 6, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.5199680328369141, 'train_time': 31.389673092984594, 'prior_train_nmll': 0.7164275050163269, 'train_nll': -541.0924682617188, 'test_nll': 76.4832992553711, 'train_mse': 7.29259118088521e-05, 'state_dict_file': 'model_state_dict_6662698990038638309.pkl'}\n",
      "2019-03-16 15:37:23.838071, fold=7, rep=0, eta=0d 0h 2m 13s \n",
      "{'fold': 7, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.2855424880981445, 'train_time': 22.005659568938427, 'prior_train_nmll': 0.689614474773407, 'train_nll': -495.934326171875, 'test_nll': 130.0301055908203, 'train_mse': 6.724111881339923e-05, 'state_dict_file': 'model_state_dict_2287707868977831991.pkl'}\n",
      "2019-03-16 15:37:52.113653, fold=7, rep=1, eta=0d 0h 1m 47s \n",
      "{'fold': 7, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.225441813468933, 'train_time': 28.275324402027763, 'prior_train_nmll': 0.7535302042961121, 'train_nll': -567.2587280273438, 'test_nll': 123.44178771972656, 'train_mse': 7.114093023119494e-05, 'state_dict_file': 'model_state_dict_6312376881399729860.pkl'}\n",
      "2019-03-16 15:38:24.282835, fold=8, rep=0, eta=0d 0h 1m 21s \n",
      "{'fold': 8, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.3090081214904785, 'train_time': 32.16594181000255, 'prior_train_nmll': 0.7601162195205688, 'train_nll': 75.15928649902344, 'test_nll': 136.9474334716797, 'train_mse': 5.205488923820667e-05, 'state_dict_file': 'model_state_dict_4131333146659273476.pkl'}\n",
      "2019-03-16 15:38:53.777174, fold=8, rep=1, eta=0d 0h 0m 54s \n",
      "{'fold': 8, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.2111293077468872, 'train_time': 29.494089846033603, 'prior_train_nmll': 0.6112164258956909, 'train_nll': -653.0634155273438, 'test_nll': 154.0265350341797, 'train_mse': 0.00010690264025470242, 'state_dict_file': 'model_state_dict_-4949772816813952542.pkl'}\n",
      "2019-03-16 15:39:16.508323, fold=9, rep=0, eta=0d 0h 0m 26s \n",
      "{'fold': 9, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.8679459691047668, 'train_time': 22.72783257998526, 'prior_train_nmll': 0.7547544836997986, 'train_nll': -379.67401123046875, 'test_nll': 58.10493469238281, 'train_mse': 9.890246292343363e-05, 'state_dict_file': 'model_state_dict_-3641998751299635607.pkl'}\n",
      "2019-03-16 15:39:39.387315, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.0743296146392822, 'train_time': 22.878749686060473, 'prior_train_nmll': 0.673466145992279, 'train_nll': -477.92852783203125, 'test_nll': 70.71385955810547, 'train_mse': 0.00010433770512463525, 'state_dict_file': 'model_state_dict_-8041504445266061421.pkl'}\n",
      "2019-03-16 15:40:05.023270, fold=0, rep=0, eta=0d 0h 8m 6s \n",
      "{'fold': 0, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.08127428591251373, 'train_time': 25.612846617004834, 'prior_train_nmll': 0.35563674569129944, 'train_nll': -33.83099365234375, 'test_nll': 2.6324291229248047, 'train_mse': 0.02815677598118782, 'state_dict_file': 'model_state_dict_-6930679057670981396.pkl'}\n",
      "2019-03-16 15:40:25.178664, fold=0, rep=1, eta=0d 0h 6m 51s \n",
      "{'fold': 0, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.10106579214334488, 'train_time': 20.155142807983793, 'prior_train_nmll': 0.38160860538482666, 'train_nll': -17.073989868164062, 'test_nll': 7.172553062438965, 'train_mse': 0.03605324774980545, 'state_dict_file': 'model_state_dict_8449349134329220440.pkl'}\n",
      "2019-03-16 15:40:41.137324, fold=1, rep=0, eta=0d 0h 5m 49s \n",
      "{'fold': 1, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.28255635499954224, 'train_time': 15.95566549594514, 'prior_train_nmll': 0.36304351687431335, 'train_nll': -23.20050048828125, 'test_nll': 34.02984619140625, 'train_mse': 0.03501855954527855, 'state_dict_file': 'model_state_dict_-947563007401739610.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:40:59.920679, fold=1, rep=1, eta=0d 0h 5m 22s \n",
      "{'fold': 1, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.2459728866815567, 'train_time': 18.782648036023602, 'prior_train_nmll': 0.30793383717536926, 'train_nll': -28.801055908203125, 'test_nll': 30.053295135498047, 'train_mse': 0.03230217844247818, 'state_dict_file': 'model_state_dict_6455621186431643394.pkl'}\n",
      "2019-03-16 15:41:14.523170, fold=2, rep=0, eta=0d 0h 4m 45s \n",
      "{'fold': 2, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.10418874770402908, 'train_time': 14.599433812079951, 'prior_train_nmll': 0.4755271375179291, 'train_nll': 8.602142333984375, 'test_nll': 5.556902885437012, 'train_mse': 0.04854276403784752, 'state_dict_file': 'model_state_dict_1044435379693084194.pkl'}\n",
      "2019-03-16 15:41:31.312268, fold=2, rep=1, eta=0d 0h 4m 21s \n",
      "{'fold': 2, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.11320599168539047, 'train_time': 16.788610424962826, 'prior_train_nmll': 0.42787688970565796, 'train_nll': -1.7401580810546875, 'test_nll': 7.139056205749512, 'train_mse': 0.0429733544588089, 'state_dict_file': 'model_state_dict_2526581307547344453.pkl'}\n",
      "2019-03-16 15:41:54.967998, fold=3, rep=0, eta=0d 0h 4m 11s \n",
      "{'fold': 3, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.17325210571289062, 'train_time': 23.65261150605511, 'prior_train_nmll': 0.3818185329437256, 'train_nll': -17.94451904296875, 'test_nll': 18.879047393798828, 'train_mse': 0.03541218116879463, 'state_dict_file': 'model_state_dict_-5315879208805682692.pkl'}\n",
      "2019-03-16 15:42:14.214522, fold=3, rep=1, eta=0d 0h 3m 52s \n",
      "{'fold': 3, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.2272540032863617, 'train_time': 19.24586016696412, 'prior_train_nmll': 0.39108413457870483, 'train_nll': -16.427841186523438, 'test_nll': 21.044679641723633, 'train_mse': 0.036710064858198166, 'state_dict_file': 'model_state_dict_2974833196508292017.pkl'}\n",
      "2019-03-16 15:42:43.822162, fold=4, rep=0, eta=0d 0h 3m 45s \n",
      "{'fold': 4, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.22048985958099365, 'train_time': 29.60454150394071, 'prior_train_nmll': 0.3862069249153137, 'train_nll': -14.611297607421875, 'test_nll': 12.05073356628418, 'train_mse': 0.03728117421269417, 'state_dict_file': 'model_state_dict_2356759668293110222.pkl'}\n",
      "2019-03-16 15:43:02.075494, fold=4, rep=1, eta=0d 0h 3m 22s \n",
      "{'fold': 4, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.55137038230896, 'train_time': 18.253093188046478, 'prior_train_nmll': 0.3730930685997009, 'train_nll': -24.974517822265625, 'test_nll': 22.390653610229492, 'train_mse': 0.03366518393158913, 'state_dict_file': 'model_state_dict_-1716492403852068825.pkl'}\n",
      "2019-03-16 15:43:28.242907, fold=5, rep=0, eta=0d 0h 3m 7s \n",
      "{'fold': 5, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.24381060898303986, 'train_time': 26.164194534067065, 'prior_train_nmll': 0.3453653156757355, 'train_nll': -49.05877685546875, 'test_nll': 22.264394760131836, 'train_mse': 0.022689232602715492, 'state_dict_file': 'model_state_dict_-9186549040399170472.pkl'}\n",
      "2019-03-16 15:43:57.214814, fold=5, rep=1, eta=0d 0h 2m 51s \n",
      "{'fold': 5, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.21749621629714966, 'train_time': 28.97165673004929, 'prior_train_nmll': 0.32471969723701477, 'train_nll': -57.205718994140625, 'test_nll': 25.164287567138672, 'train_mse': 0.02121068723499775, 'state_dict_file': 'model_state_dict_-7356762167492194514.pkl'}\n",
      "2019-03-16 15:44:20.326833, fold=6, rep=0, eta=0d 0h 2m 31s \n",
      "{'fold': 6, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.13331755995750427, 'train_time': 23.108822223963216, 'prior_train_nmll': 0.4071260392665863, 'train_nll': 0.588775634765625, 'test_nll': 8.533586502075195, 'train_mse': 0.046675462275743484, 'state_dict_file': 'model_state_dict_2780785250886904835.pkl'}\n",
      "2019-03-16 15:44:42.727446, fold=6, rep=1, eta=0d 0h 2m 9s \n",
      "{'fold': 6, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.11602414399385452, 'train_time': 22.400358195998706, 'prior_train_nmll': 0.38113853335380554, 'train_nll': -22.499252319335938, 'test_nll': 7.242170333862305, 'train_mse': 0.033895984292030334, 'state_dict_file': 'model_state_dict_6523900300592310619.pkl'}\n",
      "2019-03-16 15:45:00.801877, fold=7, rep=0, eta=0d 0h 1m 47s \n",
      "{'fold': 7, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.19987328350543976, 'train_time': 18.071133263991214, 'prior_train_nmll': 0.35128816962242126, 'train_nll': -38.22235107421875, 'test_nll': 20.64440155029297, 'train_mse': 0.028038332238793373, 'state_dict_file': 'model_state_dict_5004434242482060644.pkl'}\n",
      "2019-03-16 15:45:31.162089, fold=7, rep=1, eta=0d 0h 1m 27s \n",
      "{'fold': 7, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.22823719680309296, 'train_time': 30.359961192007177, 'prior_train_nmll': 0.3618234694004059, 'train_nll': -48.76409912109375, 'test_nll': 30.789064407348633, 'train_mse': 0.023778270930051804, 'state_dict_file': 'model_state_dict_-811092246662088479.pkl'}\n",
      "2019-03-16 15:45:57.843823, fold=8, rep=0, eta=0d 0h 1m 6s \n",
      "{'fold': 8, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.23404784500598907, 'train_time': 26.67803603899665, 'prior_train_nmll': 0.3535696864128113, 'train_nll': -29.924896240234375, 'test_nll': 19.88419532775879, 'train_mse': 0.030131202191114426, 'state_dict_file': 'model_state_dict_1856414756970374709.pkl'}\n",
      "2019-03-16 15:46:18.758632, fold=8, rep=1, eta=0d 0h 0m 44s \n",
      "{'fold': 8, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.26756370067596436, 'train_time': 20.9145144529175, 'prior_train_nmll': 0.3140451908111572, 'train_nll': -40.318603515625, 'test_nll': 21.97804832458496, 'train_mse': 0.028135107830166817, 'state_dict_file': 'model_state_dict_7401324554808928670.pkl'}\n",
      "2019-03-16 15:46:46.457645, fold=9, rep=0, eta=0d 0h 0m 22s \n",
      "{'fold': 9, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.3772881329059601, 'train_time': 27.694721976993605, 'prior_train_nmll': 0.3374955654144287, 'train_nll': -57.67124938964844, 'test_nll': 19.932231903076172, 'train_mse': 0.021161295473575592, 'state_dict_file': 'model_state_dict_649589048319757783.pkl'}\n",
      "2019-03-16 15:47:08.552540, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.2966914772987366, 'train_time': 22.0946414610371, 'prior_train_nmll': 0.3290421962738037, 'train_nll': -50.461029052734375, 'test_nll': 13.534858703613281, 'train_mse': 0.023778455331921577, 'state_dict_file': 'model_state_dict_-6060459507490378253.pkl'}\n",
      "2019-03-16 15:47:29.671270, fold=0, rep=0, eta=0d 0h 6m 40s \n",
      "{'fold': 0, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.0025246809236705303, 'train_time': 21.09469260601327, 'prior_train_nmll': -0.37944793701171875, 'train_nll': -1699.3065185546875, 'test_nll': -118.31575012207031, 'train_mse': 0.0024744144175201654, 'state_dict_file': 'model_state_dict_8729572830191086759.pkl'}\n",
      "2019-03-16 15:47:52.269095, fold=0, rep=1, eta=0d 0h 6m 33s \n",
      "{'fold': 0, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.00686614541336894, 'train_time': 22.597600847948343, 'prior_train_nmll': -0.3854496479034424, 'train_nll': -1117.2186279296875, 'test_nll': 244.74966430664062, 'train_mse': 0.003900039941072464, 'state_dict_file': 'model_state_dict_-6871848619414597464.pkl'}\n",
      "2019-03-16 15:48:16.413161, fold=1, rep=0, eta=0d 0h 6m 24s \n",
      "{'fold': 1, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.013560356572270393, 'train_time': 24.140862058033235, 'prior_train_nmll': -0.2952876091003418, 'train_nll': -1331.014892578125, 'test_nll': 777.46142578125, 'train_mse': 0.007851757109165192, 'state_dict_file': 'model_state_dict_-2309291410258005013.pkl'}\n",
      "2019-03-16 15:48:37.428238, fold=1, rep=1, eta=0d 0h 5m 55s \n",
      "{'fold': 1, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.0047750528901815414, 'train_time': 21.014448091969825, 'prior_train_nmll': -0.5949133634567261, 'train_nll': -1161.0594482421875, 'test_nll': 2984.35302734375, 'train_mse': 0.0008823430398479104, 'state_dict_file': 'model_state_dict_2401513264924647113.pkl'}\n",
      "2019-03-16 15:48:59.968177, fold=2, rep=0, eta=0d 0h 5m 34s \n",
      "{'fold': 2, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.020220234990119934, 'train_time': 22.536625702981837, 'prior_train_nmll': -0.4348994791507721, 'train_nll': -1527.7901611328125, 'test_nll': 284.76995849609375, 'train_mse': 0.011009889654815197, 'state_dict_file': 'model_state_dict_-1159408398957603098.pkl'}\n",
      "2019-03-16 15:49:19.501564, fold=2, rep=1, eta=0d 0h 5m 5s \n",
      "{'fold': 2, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.005873601883649826, 'train_time': 19.532778071938083, 'prior_train_nmll': -0.3222455382347107, 'train_nll': -1386.0850830078125, 'test_nll': -8.677989959716797, 'train_mse': 0.004106548149138689, 'state_dict_file': 'model_state_dict_-2502827247635719037.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:49:38.049125, fold=3, rep=0, eta=0d 0h 4m 37s \n",
      "{'fold': 3, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.004673239775002003, 'train_time': 18.544197222101502, 'prior_train_nmll': -0.30420801043510437, 'train_nll': -1626.6341552734375, 'test_nll': 114.39070129394531, 'train_mse': 0.004086801316589117, 'state_dict_file': 'model_state_dict_-4789496634876534802.pkl'}\n",
      "2019-03-16 15:50:01.060718, fold=3, rep=1, eta=0d 0h 4m 18s \n",
      "{'fold': 3, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.0018997184233739972, 'train_time': 23.011358499061316, 'prior_train_nmll': -0.6581302285194397, 'train_nll': -1517.4185791015625, 'test_nll': 4564.28173828125, 'train_mse': 0.00031270389445126057, 'state_dict_file': 'model_state_dict_-3783618945754078518.pkl'}\n",
      "2019-03-16 15:50:24.329071, fold=4, rep=0, eta=0d 0h 3m 59s \n",
      "{'fold': 4, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.002319477265700698, 'train_time': 23.264144022017717, 'prior_train_nmll': -0.5380138158798218, 'train_nll': -1668.9764404296875, 'test_nll': 3232001.5, 'train_mse': 0.0013052435824647546, 'state_dict_file': 'model_state_dict_-7381712259507408426.pkl'}\n",
      "2019-03-16 15:50:47.313808, fold=4, rep=1, eta=0d 0h 3m 38s \n",
      "{'fold': 4, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.006280004512518644, 'train_time': 22.984464750043117, 'prior_train_nmll': -0.4975552260875702, 'train_nll': -1064.3726806640625, 'test_nll': 40.417118072509766, 'train_mse': 0.0031258396338671446, 'state_dict_file': 'model_state_dict_-2355946802573022983.pkl'}\n",
      "2019-03-16 15:51:09.370245, fold=5, rep=0, eta=0d 0h 3m 17s \n",
      "{'fold': 5, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.03851183503866196, 'train_time': 22.05329912097659, 'prior_train_nmll': -0.3982175588607788, 'train_nll': -1461.4932861328125, 'test_nll': 4224.47802734375, 'train_mse': 0.004547898657619953, 'state_dict_file': 'model_state_dict_-9196356401681594751.pkl'}\n",
      "2019-03-16 15:51:29.894920, fold=5, rep=1, eta=0d 0h 2m 54s \n",
      "{'fold': 5, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.013632317073643208, 'train_time': 20.524419059976935, 'prior_train_nmll': -0.34118416905403137, 'train_nll': -1565.505615234375, 'test_nll': 47917540.0, 'train_mse': 0.0009887071792036295, 'state_dict_file': 'model_state_dict_1182561391787853215.pkl'}\n",
      "2019-03-16 15:51:54.244032, fold=6, rep=0, eta=0d 0h 2m 33s \n",
      "{'fold': 6, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.07573188096284866, 'train_time': 24.346004558959976, 'prior_train_nmll': -0.6763780117034912, 'train_nll': 12813.607421875, 'test_nll': 7279.24462890625, 'train_mse': 0.011045643128454685, 'state_dict_file': 'model_state_dict_5409659151953035221.pkl'}\n",
      "2019-03-16 15:52:18.420298, fold=6, rep=1, eta=0d 0h 2m 12s \n",
      "{'fold': 6, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.061922743916511536, 'train_time': 24.176055460004136, 'prior_train_nmll': -0.6234166622161865, 'train_nll': -1726.7867431640625, 'test_nll': 6328223.5, 'train_mse': 0.0015806620940566063, 'state_dict_file': 'model_state_dict_5222800440787279302.pkl'}\n",
      "2019-03-16 15:52:45.451396, fold=7, rep=0, eta=0d 0h 1m 52s \n",
      "{'fold': 7, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.014007069170475006, 'train_time': 27.02758898306638, 'prior_train_nmll': -0.5160716772079468, 'train_nll': -1599.997314453125, 'test_nll': -88.72280883789062, 'train_mse': 0.004638273734599352, 'state_dict_file': 'model_state_dict_-644872492409844050.pkl'}\n",
      "2019-03-16 15:53:08.341370, fold=7, rep=1, eta=0d 0h 1m 29s \n",
      "{'fold': 7, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.011149139143526554, 'train_time': 22.88967419404071, 'prior_train_nmll': -0.3751545548439026, 'train_nll': -1534.0589599609375, 'test_nll': 254.30941772460938, 'train_mse': 0.0029394966550171375, 'state_dict_file': 'model_state_dict_-8432637169181050082.pkl'}\n",
      "2019-03-16 15:53:29.796366, fold=8, rep=0, eta=0d 0h 1m 7s \n",
      "{'fold': 8, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.01538991741836071, 'train_time': 21.451500077033415, 'prior_train_nmll': -0.48354193568229675, 'train_nll': -1398.4549560546875, 'test_nll': 1178619.125, 'train_mse': 0.006960059516131878, 'state_dict_file': 'model_state_dict_-7294170671551055859.pkl'}\n",
      "2019-03-16 15:53:53.146198, fold=8, rep=1, eta=0d 0h 0m 44s \n",
      "{'fold': 8, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.0027808004524558783, 'train_time': 23.349596804007888, 'prior_train_nmll': -0.48327508568763733, 'train_nll': -1654.2154541015625, 'test_nll': 8115197.5, 'train_mse': 0.0009083718759939075, 'state_dict_file': 'model_state_dict_-1087263307368667676.pkl'}\n",
      "2019-03-16 15:54:17.825119, fold=9, rep=0, eta=0d 0h 0m 22s \n",
      "{'fold': 9, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.009536021389067173, 'train_time': 24.67561650101561, 'prior_train_nmll': -0.41331905126571655, 'train_nll': -1763.4765625, 'test_nll': 427704.9375, 'train_mse': 0.00972032267600298, 'state_dict_file': 'model_state_dict_-3419506697941858556.pkl'}\n",
      "2019-03-16 15:54:42.454032, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.0037599769420921803, 'train_time': 24.628609100007452, 'prior_train_nmll': -0.2860187590122223, 'train_nll': -1090.2294921875, 'test_nll': 157141312.0, 'train_mse': 0.0027943497989326715, 'state_dict_file': 'model_state_dict_5001816524376734273.pkl'}\n",
      "2019-03-16 15:55:09.458866, fold=0, rep=0, eta=0d 0h 8m 32s \n",
      "{'fold': 0, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.11392232030630112, 'train_time': 26.98102852399461, 'prior_train_nmll': 0.17406989634037018, 'train_nll': -151.4163818359375, 'test_nll': 16.773189544677734, 'train_mse': 0.017773212864995003, 'state_dict_file': 'model_state_dict_1771873671086591898.pkl'}\n",
      "2019-03-16 15:55:32.309157, fold=0, rep=1, eta=0d 0h 7m 28s \n",
      "{'fold': 0, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.12444359064102173, 'train_time': 22.850042975973338, 'prior_train_nmll': 0.19932827353477478, 'train_nll': -113.99179077148438, 'test_nll': 21.193317413330078, 'train_mse': 0.02428743802011013, 'state_dict_file': 'model_state_dict_-6672282064641935122.pkl'}\n",
      "2019-03-16 15:56:13.256872, fold=1, rep=0, eta=0d 0h 8m 34s \n",
      "{'fold': 1, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.17407388985157013, 'train_time': 40.944431648007594, 'prior_train_nmll': 0.12478956580162048, 'train_nll': -226.604736328125, 'test_nll': 54.18091583251953, 'train_mse': 0.013462276197969913, 'state_dict_file': 'model_state_dict_-3595850344533003598.pkl'}\n",
      "2019-03-16 15:56:50.751683, fold=1, rep=1, eta=0d 0h 8m 33s \n",
      "{'fold': 1, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.1949479877948761, 'train_time': 37.49451517208945, 'prior_train_nmll': 0.115674689412117, 'train_nll': -233.65447998046875, 'test_nll': 61.980018615722656, 'train_mse': 0.010434665717184544, 'state_dict_file': 'model_state_dict_-3408056285208681650.pkl'}\n",
      "2019-03-16 15:57:17.898697, fold=2, rep=0, eta=0d 0h 7m 46s \n",
      "{'fold': 2, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.1586117148399353, 'train_time': 27.14347456896212, 'prior_train_nmll': 0.14078684151172638, 'train_nll': -465.1041259765625, 'test_nll': 45.70750045776367, 'train_mse': 0.018014531582593918, 'state_dict_file': 'model_state_dict_-8173652860056193171.pkl'}\n",
      "2019-03-16 15:57:50.494101, fold=2, rep=1, eta=0d 0h 7m 18s \n",
      "{'fold': 2, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.14228394627571106, 'train_time': 32.59518493700307, 'prior_train_nmll': 0.15751047432422638, 'train_nll': -150.25201416015625, 'test_nll': 34.45951461791992, 'train_mse': 0.018629448488354683, 'state_dict_file': 'model_state_dict_126823933648032168.pkl'}\n",
      "2019-03-16 15:58:24.874586, fold=3, rep=0, eta=0d 0h 6m 53s \n",
      "{'fold': 3, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.14089304208755493, 'train_time': 34.376993611920625, 'prior_train_nmll': 0.1737264096736908, 'train_nll': -159.68963623046875, 'test_nll': 33.060726165771484, 'train_mse': 0.017319737002253532, 'state_dict_file': 'model_state_dict_-2925961855644428048.pkl'}\n",
      "2019-03-16 15:58:54.914958, fold=3, rep=1, eta=0d 0h 6m 18s \n",
      "{'fold': 3, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.1925637274980545, 'train_time': 30.040118752978742, 'prior_train_nmll': 0.11731527000665665, 'train_nll': 268722.28125, 'test_nll': 62.176456451416016, 'train_mse': 0.00925293006002903, 'state_dict_file': 'model_state_dict_7472931209694835466.pkl'}\n",
      "2019-03-16 15:59:22.444258, fold=4, rep=0, eta=0d 0h 5m 42s \n",
      "{'fold': 4, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.12714651226997375, 'train_time': 27.52577621396631, 'prior_train_nmll': 0.19026900827884674, 'train_nll': -102.75039672851562, 'test_nll': 20.442825317382812, 'train_mse': 0.02480829320847988, 'state_dict_file': 'model_state_dict_-145622379304072577.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 15:59:51.382985, fold=4, rep=1, eta=0d 0h 5m 8s \n",
      "{'fold': 4, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.1724410355091095, 'train_time': 28.938469129963778, 'prior_train_nmll': 0.1446312814950943, 'train_nll': -169.029052734375, 'test_nll': 50.61069107055664, 'train_mse': 0.016567975282669067, 'state_dict_file': 'model_state_dict_-6988182838259350375.pkl'}\n",
      "2019-03-16 16:00:26.157582, fold=5, rep=0, eta=0d 0h 4m 41s \n",
      "{'fold': 5, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.08835114538669586, 'train_time': 34.771137755014934, 'prior_train_nmll': 0.16706109046936035, 'train_nll': -118.30990600585938, 'test_nll': 10.056137084960938, 'train_mse': 0.02344626747071743, 'state_dict_file': 'model_state_dict_5908412381593831531.pkl'}\n",
      "2019-03-16 16:00:59.531087, fold=5, rep=1, eta=0d 0h 4m 11s \n",
      "{'fold': 5, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.08967592567205429, 'train_time': 33.37325592292473, 'prior_train_nmll': 0.17613078653812408, 'train_nll': 1619.868408203125, 'test_nll': 11.369644165039062, 'train_mse': 0.01663113757967949, 'state_dict_file': 'model_state_dict_-4644375233706760931.pkl'}\n",
      "2019-03-16 16:01:28.839007, fold=6, rep=0, eta=0d 0h 3m 38s \n",
      "{'fold': 6, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.10656015574932098, 'train_time': 29.304404860944487, 'prior_train_nmll': 0.17072910070419312, 'train_nll': -138.586669921875, 'test_nll': 15.162673950195312, 'train_mse': 0.01914430409669876, 'state_dict_file': 'model_state_dict_-6696308288339615077.pkl'}\n",
      "2019-03-16 16:02:01.357256, fold=6, rep=1, eta=0d 0h 3m 8s \n",
      "{'fold': 6, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.10437855869531631, 'train_time': 32.51801067695487, 'prior_train_nmll': 0.15665321052074432, 'train_nll': -187.0146484375, 'test_nll': 15.242721557617188, 'train_mse': 0.015091383829712868, 'state_dict_file': 'model_state_dict_1422993420796903884.pkl'}\n",
      "2019-03-16 16:02:30.389151, fold=7, rep=0, eta=0d 0h 2m 35s \n",
      "{'fold': 7, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.2600601017475128, 'train_time': 29.028322221012786, 'prior_train_nmll': 0.13333353400230408, 'train_nll': -132.44918823242188, 'test_nll': 66.43347930908203, 'train_mse': 0.020972033962607384, 'state_dict_file': 'model_state_dict_-3504028218200932388.pkl'}\n",
      "2019-03-16 16:03:12.633008, fold=7, rep=1, eta=0d 0h 2m 7s \n",
      "{'fold': 7, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.32972365617752075, 'train_time': 42.2436333730584, 'prior_train_nmll': 0.10313092917203903, 'train_nll': -177.61993408203125, 'test_nll': 101.80606079101562, 'train_mse': 0.016370074823498726, 'state_dict_file': 'model_state_dict_4652636463916008420.pkl'}\n",
      "2019-03-16 16:04:03.485771, fold=8, rep=0, eta=0d 0h 1m 39s \n",
      "{'fold': 8, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.12107175588607788, 'train_time': 50.84922797197942, 'prior_train_nmll': 0.14314207434654236, 'train_nll': -13.0506591796875, 'test_nll': 23.206005096435547, 'train_mse': 0.020234571769833565, 'state_dict_file': 'model_state_dict_-4091703430369849128.pkl'}\n",
      "2019-03-16 16:04:30.375567, fold=8, rep=1, eta=0d 0h 1m 5s \n",
      "{'fold': 8, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.1977182924747467, 'train_time': 26.889534714980982, 'prior_train_nmll': 0.13880683481693268, 'train_nll': -155.739990234375, 'test_nll': 56.956138610839844, 'train_mse': 0.018002184107899666, 'state_dict_file': 'model_state_dict_-2032982154520199096.pkl'}\n",
      "2019-03-16 16:05:28.720095, fold=9, rep=0, eta=0d 0h 0m 34s \n",
      "{'fold': 9, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.1227860152721405, 'train_time': 58.34089491004124, 'prior_train_nmll': 0.1409958451986313, 'train_nll': -21.707061767578125, 'test_nll': 26.88729476928711, 'train_mse': 0.011370742693543434, 'state_dict_file': 'model_state_dict_-200739261493315471.pkl'}\n",
      "2019-03-16 16:06:13.167945, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.16673994064331055, 'train_time': 44.44759537500795, 'prior_train_nmll': 0.11096570640802383, 'train_nll': -199.34649658203125, 'test_nll': 34.366783142089844, 'train_mse': 0.016130022704601288, 'state_dict_file': 'model_state_dict_-6047274387754345528.pkl'}\n",
      "2019-03-16 16:06:57.244473, fold=0, rep=0, eta=0d 0h 13m 56s \n",
      "{'fold': 0, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.1406373530626297, 'train_time': 44.05053236708045, 'prior_train_nmll': 0.17434991896152496, 'train_nll': -77.96237182617188, 'test_nll': 35.399532318115234, 'train_mse': 0.003950240556150675, 'state_dict_file': 'model_state_dict_-6350674967322665025.pkl'}\n",
      "2019-03-16 16:07:38.306188, fold=0, rep=1, eta=0d 0h 12m 46s \n",
      "{'fold': 0, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.15979863703250885, 'train_time': 41.061474314075895, 'prior_train_nmll': 0.23833543062210083, 'train_nll': -370.2430114746094, 'test_nll': 39.10065841674805, 'train_mse': 0.00573227321729064, 'state_dict_file': 'model_state_dict_3317749488168573405.pkl'}\n",
      "2019-03-16 16:08:18.737783, fold=1, rep=0, eta=0d 0h 11m 51s \n",
      "{'fold': 1, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.12758983671665192, 'train_time': 40.42829534201883, 'prior_train_nmll': 0.24341146647930145, 'train_nll': -439.8300476074219, 'test_nll': 76.78819274902344, 'train_mse': 0.005233184900134802, 'state_dict_file': 'model_state_dict_-5439291334689352492.pkl'}\n",
      "2019-03-16 16:09:00.033919, fold=1, rep=1, eta=0d 0h 11m 7s \n",
      "{'fold': 1, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.10516941547393799, 'train_time': 41.29586578998715, 'prior_train_nmll': 0.28663235902786255, 'train_nll': 5855.3173828125, 'test_nll': 39.061161041259766, 'train_mse': 0.005239289253950119, 'state_dict_file': 'model_state_dict_-6999366384737615163.pkl'}\n",
      "2019-03-16 16:09:35.526960, fold=2, rep=0, eta=0d 0h 10m 7s \n",
      "{'fold': 2, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.2802201509475708, 'train_time': 35.48999300401192, 'prior_train_nmll': 0.23457874357700348, 'train_nll': -467.6391906738281, 'test_nll': 89.7015380859375, 'train_mse': 0.006542632821947336, 'state_dict_file': 'model_state_dict_2018421074638962969.pkl'}\n",
      "2019-03-16 16:10:23.845605, fold=2, rep=1, eta=0d 0h 9m 44s \n",
      "{'fold': 2, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.20331647992134094, 'train_time': 48.31840003200341, 'prior_train_nmll': 0.2476080060005188, 'train_nll': -457.7607727050781, 'test_nll': 98.37507629394531, 'train_mse': 0.004693666007369757, 'state_dict_file': 'model_state_dict_-5780949218782133261.pkl'}\n",
      "2019-03-16 16:11:05.270859, fold=3, rep=0, eta=0d 0h 9m 2s \n",
      "{'fold': 3, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.26717475056648254, 'train_time': 41.42182444105856, 'prior_train_nmll': 0.14882448315620422, 'train_nll': -707.547119140625, 'test_nll': 121.28274536132812, 'train_mse': 0.004450026899576187, 'state_dict_file': 'model_state_dict_8275287097312356955.pkl'}\n",
      "2019-03-16 16:11:46.550701, fold=3, rep=1, eta=0d 0h 8m 20s \n",
      "{'fold': 3, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.20587673783302307, 'train_time': 41.27957691100892, 'prior_train_nmll': 0.14440728724002838, 'train_nll': 5396.279296875, 'test_nll': 92.80891418457031, 'train_mse': 0.005215902347117662, 'state_dict_file': 'model_state_dict_5472255642912601257.pkl'}\n",
      "2019-03-16 16:12:35.549028, fold=4, rep=0, eta=0d 0h 7m 47s \n",
      "{'fold': 4, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.2523859143257141, 'train_time': 48.994772939011455, 'prior_train_nmll': 0.2139149308204651, 'train_nll': -428.3529357910156, 'test_nll': 68.63706970214844, 'train_mse': 0.00477692810818553, 'state_dict_file': 'model_state_dict_185522896563692389.pkl'}\n",
      "2019-03-16 16:13:18.298993, fold=4, rep=1, eta=0d 0h 7m 5s \n",
      "{'fold': 4, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.22401225566864014, 'train_time': 42.749720401945524, 'prior_train_nmll': 0.21433116495609283, 'train_nll': -364248.125, 'test_nll': 73.46791076660156, 'train_mse': 0.004925157409161329, 'state_dict_file': 'model_state_dict_-7731946865093898761.pkl'}\n",
      "2019-03-16 16:14:04.195867, fold=5, rep=0, eta=0d 0h 6m 25s \n",
      "{'fold': 5, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.17780335247516632, 'train_time': 45.89340811804868, 'prior_train_nmll': 0.2195064127445221, 'train_nll': -984.79052734375, 'test_nll': 54.52536392211914, 'train_mse': 0.004033313132822514, 'state_dict_file': 'model_state_dict_839082211661046915.pkl'}\n",
      "2019-03-16 16:14:47.794601, fold=5, rep=1, eta=0d 0h 5m 43s \n",
      "{'fold': 5, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.1185406893491745, 'train_time': 43.59846407198347, 'prior_train_nmll': 0.18531741201877594, 'train_nll': -636.4403076171875, 'test_nll': 34.44002914428711, 'train_mse': 0.004202378913760185, 'state_dict_file': 'model_state_dict_2657733512494709757.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 16:15:32.478941, fold=6, rep=0, eta=0d 0h 5m 1s \n",
      "{'fold': 6, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.1643943041563034, 'train_time': 44.68115670001134, 'prior_train_nmll': 0.22544990479946136, 'train_nll': -633.69921875, 'test_nll': 67.48878479003906, 'train_mse': 0.004811123479157686, 'state_dict_file': 'model_state_dict_-2682496804543518167.pkl'}\n",
      "2019-03-16 16:16:17.036576, fold=6, rep=1, eta=0d 0h 4m 18s \n",
      "{'fold': 6, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.13831764459609985, 'train_time': 44.55739258194808, 'prior_train_nmll': 0.2290034145116806, 'train_nll': -744.3709716796875, 'test_nll': 56.61843490600586, 'train_mse': 0.005006663501262665, 'state_dict_file': 'model_state_dict_-4768241488200235452.pkl'}\n",
      "2019-03-16 16:17:11.963603, fold=7, rep=0, eta=0d 0h 3m 39s \n",
      "{'fold': 7, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.2620334327220917, 'train_time': 54.92352542595472, 'prior_train_nmll': 0.20642919838428497, 'train_nll': -657.6129150390625, 'test_nll': 87.19322204589844, 'train_mse': 0.003367049153894186, 'state_dict_file': 'model_state_dict_-2638286904156489157.pkl'}\n",
      "2019-03-16 16:17:51.082026, fold=7, rep=1, eta=0d 0h 2m 54s \n",
      "{'fold': 7, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.1321890950202942, 'train_time': 39.11812567396555, 'prior_train_nmll': 0.2095034271478653, 'train_nll': -226.32913208007812, 'test_nll': 31.233394622802734, 'train_mse': 0.0076355598866939545, 'state_dict_file': 'model_state_dict_6953030707443113655.pkl'}\n",
      "2019-03-16 16:18:29.103638, fold=8, rep=0, eta=0d 0h 2m 9s \n",
      "{'fold': 8, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.19893696904182434, 'train_time': 38.01844265602995, 'prior_train_nmll': 0.26126956939697266, 'train_nll': -181.50588989257812, 'test_nll': 64.07026672363281, 'train_mse': 0.006530562415719032, 'state_dict_file': 'model_state_dict_4109449124248256255.pkl'}\n",
      "2019-03-16 16:19:07.496575, fold=8, rep=1, eta=0d 0h 1m 26s \n",
      "{'fold': 8, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.10744662582874298, 'train_time': 38.39265062205959, 'prior_train_nmll': 0.2084430307149887, 'train_nll': -1633.9302978515625, 'test_nll': 20.856952667236328, 'train_mse': 0.005716734565794468, 'state_dict_file': 'model_state_dict_7954601859813550344.pkl'}\n",
      "2019-03-16 16:19:45.501368, fold=9, rep=0, eta=0d 0h 0m 42s \n",
      "{'fold': 9, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.12690497934818268, 'train_time': 38.00157074199524, 'prior_train_nmll': 0.19915923476219177, 'train_nll': -368.8572998046875, 'test_nll': 39.25920104980469, 'train_mse': 0.006978449411690235, 'state_dict_file': 'model_state_dict_-7304403770821877721.pkl'}\n",
      "2019-03-16 16:20:22.121032, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.23419718444347382, 'train_time': 36.61940222000703, 'prior_train_nmll': 0.2054329514503479, 'train_nll': 3978.850341796875, 'test_nll': 58.634727478027344, 'train_mse': 0.006324142683297396, 'state_dict_file': 'model_state_dict_-6695017290534985104.pkl'}\n",
      "2019-03-16 16:22:19.453963, fold=0, rep=0, eta=0d 0h 37m 8s \n",
      "{'fold': 0, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.4141228199005127, 'train_time': 117.3050604229793, 'prior_train_nmll': 1.0127620697021484, 'train_nll': -93.68911743164062, 'test_nll': 239.6928253173828, 'train_mse': 0.012759950943291187, 'state_dict_file': 'model_state_dict_7867299116898803997.pkl'}\n",
      "2019-03-16 16:24:36.193065, fold=0, rep=1, eta=0d 0h 38m 6s \n",
      "{'fold': 0, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.4790899753570557, 'train_time': 136.73887651099358, 'prior_train_nmll': 0.9092207551002502, 'train_nll': -129.64938354492188, 'test_nll': 269.77801513671875, 'train_mse': 0.01140739768743515, 'state_dict_file': 'model_state_dict_849772071826303898.pkl'}\n",
      "2019-03-16 16:26:15.424556, fold=1, rep=0, eta=0d 0h 33m 21s \n",
      "{'fold': 1, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.9098286628723145, 'train_time': 99.22802049305756, 'prior_train_nmll': 0.974236011505127, 'train_nll': -106.50137329101562, 'test_nll': 277.44818115234375, 'train_mse': 0.011666769161820412, 'state_dict_file': 'model_state_dict_-3652224077926875203.pkl'}\n",
      "2019-03-16 16:28:00.661837, fold=1, rep=1, eta=0d 0h 30m 34s \n",
      "{'fold': 1, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.8528128862380981, 'train_time': 105.23703644692432, 'prior_train_nmll': 0.9673808813095093, 'train_nll': -103.91629028320312, 'test_nll': 344.6268310546875, 'train_mse': 0.01374739222228527, 'state_dict_file': 'model_state_dict_-7416870123927320616.pkl'}\n",
      "2019-03-16 16:30:12.118618, fold=2, rep=0, eta=0d 0h 29m 29s \n",
      "{'fold': 2, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.4378724098205566, 'train_time': 131.45336289703846, 'prior_train_nmll': 0.9811270833015442, 'train_nll': -103.87088012695312, 'test_nll': 210.83641052246094, 'train_mse': 0.011904753744602203, 'state_dict_file': 'model_state_dict_2023856577887823546.pkl'}\n",
      "2019-03-16 16:32:06.020670, fold=2, rep=1, eta=0d 0h 27m 22s \n",
      "{'fold': 2, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.3545629978179932, 'train_time': 113.90175089996774, 'prior_train_nmll': 0.9240682721138, 'train_nll': -108.66171264648438, 'test_nll': 231.56414794921875, 'train_mse': 0.012009416706860065, 'state_dict_file': 'model_state_dict_-8838288344178290494.pkl'}\n",
      "2019-03-16 16:33:56.401350, fold=3, rep=0, eta=0d 0h 25m 12s \n",
      "{'fold': 3, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 2.1583526134490967, 'train_time': 110.3775150309084, 'prior_train_nmll': 0.9403313994407654, 'train_nll': -104.27761840820312, 'test_nll': 334.639404296875, 'train_mse': 0.012353683821856976, 'state_dict_file': 'model_state_dict_7187732174151903888.pkl'}\n",
      "2019-03-16 16:35:52.815216, fold=3, rep=1, eta=0d 0h 23m 16s \n",
      "{'fold': 3, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 2.339597702026367, 'train_time': 116.41352381499019, 'prior_train_nmll': 0.9181825518608093, 'train_nll': -124.66122436523438, 'test_nll': 381.7451171875, 'train_mse': 0.01151000801473856, 'state_dict_file': 'model_state_dict_-4341514196387778211.pkl'}\n",
      "2019-03-16 16:37:35.453868, fold=4, rep=0, eta=0d 0h 21m 2s \n",
      "{'fold': 4, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.9176911115646362, 'train_time': 102.63470020890236, 'prior_train_nmll': 0.9637883901596069, 'train_nll': -105.12429809570312, 'test_nll': 335.18707275390625, 'train_mse': 0.01232060045003891, 'state_dict_file': 'model_state_dict_-5368432392140436347.pkl'}\n",
      "2019-03-16 16:39:01.736115, fold=4, rep=1, eta=0d 0h 18m 39s \n",
      "{'fold': 4, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.6786859035491943, 'train_time': 86.28199304197915, 'prior_train_nmll': 0.9108984470367432, 'train_nll': -116.53866577148438, 'test_nll': 300.4266052246094, 'train_mse': 0.012900236994028091, 'state_dict_file': 'model_state_dict_-1391349725188029534.pkl'}\n",
      "2019-03-16 16:41:02.237271, fold=5, rep=0, eta=0d 0h 16m 54s \n",
      "{'fold': 5, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.648552417755127, 'train_time': 120.4976633999031, 'prior_train_nmll': 0.9218929409980774, 'train_nll': -155.74649047851562, 'test_nll': 319.45263671875, 'train_mse': 0.00931717362254858, 'state_dict_file': 'model_state_dict_-7821658022649644733.pkl'}\n",
      "2019-03-16 16:42:16.556483, fold=5, rep=1, eta=0d 0h 14m 36s \n",
      "{'fold': 5, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.9868830442428589, 'train_time': 74.31895980599802, 'prior_train_nmll': 0.9927984476089478, 'train_nll': -107.40567016601562, 'test_nll': 278.04296875, 'train_mse': 0.011636706069111824, 'state_dict_file': 'model_state_dict_-3433343680978836901.pkl'}\n",
      "2019-03-16 16:44:35.846153, fold=6, rep=0, eta=0d 0h 13m 2s \n",
      "{'fold': 6, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.3390412330627441, 'train_time': 139.28613443707582, 'prior_train_nmll': 0.9353432655334473, 'train_nll': -166.24447631835938, 'test_nll': 245.0000457763672, 'train_mse': 0.008159130811691284, 'state_dict_file': 'model_state_dict_4150057987464401164.pkl'}\n",
      "2019-03-16 16:46:35.450353, fold=6, rep=1, eta=0d 0h 11m 14s \n",
      "{'fold': 6, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.3813440799713135, 'train_time': 119.6038895950187, 'prior_train_nmll': 0.9010919332504272, 'train_nll': -171.14913940429688, 'test_nll': 246.0154571533203, 'train_mse': 0.008366131223738194, 'state_dict_file': 'model_state_dict_5658920921027790168.pkl'}\n",
      "2019-03-16 16:48:33.012740, fold=7, rep=0, eta=0d 0h 9m 23s \n",
      "{'fold': 7, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.0896916389465332, 'train_time': 117.55876539205201, 'prior_train_nmll': 0.942841649055481, 'train_nll': -73.270751953125, 'test_nll': 172.48997497558594, 'train_mse': 0.01539577730000019, 'state_dict_file': 'model_state_dict_-3560199524026181322.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 16:50:26.296606, fold=7, rep=1, eta=0d 0h 7m 31s \n",
      "{'fold': 7, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.1789768934249878, 'train_time': 113.28359137196094, 'prior_train_nmll': 0.9860050082206726, 'train_nll': -28.3173828125, 'test_nll': 158.3406524658203, 'train_mse': 0.01896815001964569, 'state_dict_file': 'model_state_dict_-8394055197699233599.pkl'}\n",
      "2019-03-16 16:53:02.743707, fold=8, rep=0, eta=0d 0h 5m 45s \n",
      "{'fold': 8, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 2.4667649269104004, 'train_time': 156.4438734789146, 'prior_train_nmll': 0.902582585811615, 'train_nll': -82.21484375, 'test_nll': 380.3978271484375, 'train_mse': 0.01582585833966732, 'state_dict_file': 'model_state_dict_8695866091731253082.pkl'}\n",
      "2019-03-16 16:55:03.235284, fold=8, rep=1, eta=0d 0h 3m 51s \n",
      "{'fold': 8, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 2.0212414264678955, 'train_time': 120.49088609591126, 'prior_train_nmll': 0.9633032083511353, 'train_nll': -85.27529907226562, 'test_nll': 294.12237548828125, 'train_mse': 0.01482625026255846, 'state_dict_file': 'model_state_dict_2387182587207907637.pkl'}\n",
      "2019-03-16 16:56:54.745930, fold=9, rep=0, eta=0d 0h 1m 55s \n",
      "{'fold': 9, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.5000429153442383, 'train_time': 111.50693470996339, 'prior_train_nmll': 0.9752531051635742, 'train_nll': -58.58087158203125, 'test_nll': 206.08541870117188, 'train_mse': 0.016685254871845245, 'state_dict_file': 'model_state_dict_-5345454817978472647.pkl'}\n",
      "2019-03-16 16:59:25.303316, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.3956069946289062, 'train_time': 150.55709788005333, 'prior_train_nmll': 0.9477730393409729, 'train_nll': -92.95098876953125, 'test_nll': 195.60394287109375, 'train_mse': 0.014403334818780422, 'state_dict_file': 'model_state_dict_-334664140252720834.pkl'}\n",
      "2019-03-16 16:59:56.339008, fold=0, rep=0, eta=0d 0h 9m 49s \n",
      "{'fold': 0, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.2097003012895584, 'train_time': 31.0064683459932, 'prior_train_nmll': 0.3865593373775482, 'train_nll': -232.60006713867188, 'test_nll': 52.76676940917969, 'train_mse': 0.01298761647194624, 'state_dict_file': 'model_state_dict_1992043468515185699.pkl'}\n",
      "2019-03-16 17:00:27.823577, fold=0, rep=1, eta=0d 0h 9m 22s \n",
      "{'fold': 0, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.16951967775821686, 'train_time': 31.48430189699866, 'prior_train_nmll': 0.39962732791900635, 'train_nll': -256.0716247558594, 'test_nll': 52.52117919921875, 'train_mse': 0.011384747922420502, 'state_dict_file': 'model_state_dict_7785103842510130407.pkl'}\n",
      "2019-03-16 17:01:01.301149, fold=1, rep=0, eta=0d 0h 9m 3s \n",
      "{'fold': 1, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.2572002708911896, 'train_time': 33.474456968018785, 'prior_train_nmll': 0.3368043899536133, 'train_nll': -349.6499328613281, 'test_nll': 77.28097534179688, 'train_mse': 0.009748613461852074, 'state_dict_file': 'model_state_dict_1082283794725101641.pkl'}\n",
      "2019-03-16 17:01:30.099986, fold=1, rep=1, eta=0d 0h 8m 19s \n",
      "{'fold': 1, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.25418365001678467, 'train_time': 28.798552133957855, 'prior_train_nmll': 0.4049648940563202, 'train_nll': 50.739837646484375, 'test_nll': 69.13827514648438, 'train_mse': 0.01338245440274477, 'state_dict_file': 'model_state_dict_-5164977340315114184.pkl'}\n",
      "2019-03-16 17:02:04.887474, fold=2, rep=0, eta=0d 0h 7m 58s \n",
      "{'fold': 2, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.24536973237991333, 'train_time': 34.78437959996518, 'prior_train_nmll': 0.42198076844215393, 'train_nll': -275.3241882324219, 'test_nll': 59.35009002685547, 'train_mse': 0.009337215684354305, 'state_dict_file': 'model_state_dict_-2068852555333946321.pkl'}\n",
      "2019-03-16 17:02:31.931811, fold=2, rep=1, eta=0d 0h 7m 15s \n",
      "{'fold': 2, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.19435346126556396, 'train_time': 27.043871904024854, 'prior_train_nmll': 0.4718557894229889, 'train_nll': -195.02743530273438, 'test_nll': 53.679771423339844, 'train_mse': 0.017600731924176216, 'state_dict_file': 'model_state_dict_2106598072742245387.pkl'}\n",
      "2019-03-16 17:03:27.504272, fold=3, rep=0, eta=0d 0h 7m 29s \n",
      "{'fold': 3, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.23812657594680786, 'train_time': 55.56932054599747, 'prior_train_nmll': 0.357564777135849, 'train_nll': -346.8020935058594, 'test_nll': 85.86631774902344, 'train_mse': 0.007063053082674742, 'state_dict_file': 'model_state_dict_-4705749008631694063.pkl'}\n",
      "2019-03-16 17:04:02.271825, fold=3, rep=1, eta=0d 0h 6m 55s \n",
      "{'fold': 3, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.1369646191596985, 'train_time': 34.76732385600917, 'prior_train_nmll': 0.35061898827552795, 'train_nll': -169.75454711914062, 'test_nll': 38.01734924316406, 'train_mse': 0.009997034445405006, 'state_dict_file': 'model_state_dict_-8396658373502201495.pkl'}\n",
      "2019-03-16 17:04:35.360243, fold=4, rep=0, eta=0d 0h 6m 18s \n",
      "{'fold': 4, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.19692103564739227, 'train_time': 33.08485161699355, 'prior_train_nmll': 0.41985759139060974, 'train_nll': -262.5577697753906, 'test_nll': 55.498748779296875, 'train_mse': 0.011594617739319801, 'state_dict_file': 'model_state_dict_-1888436802293793367.pkl'}\n",
      "2019-03-16 17:05:04.464942, fold=4, rep=1, eta=0d 0h 5m 39s \n",
      "{'fold': 4, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.2200087308883667, 'train_time': 29.10442486696411, 'prior_train_nmll': 0.4066065847873688, 'train_nll': -242.92404174804688, 'test_nll': 63.17870330810547, 'train_mse': 0.013664613477885723, 'state_dict_file': 'model_state_dict_6597383531310102284.pkl'}\n",
      "2019-03-16 17:05:35.000107, fold=5, rep=0, eta=0d 0h 5m 2s \n",
      "{'fold': 5, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.19061371684074402, 'train_time': 30.531998469028622, 'prior_train_nmll': 0.45805710554122925, 'train_nll': -174.49105834960938, 'test_nll': 41.26607894897461, 'train_mse': 0.01587134413421154, 'state_dict_file': 'model_state_dict_3494528373955860348.pkl'}\n",
      "2019-03-16 17:06:03.988610, fold=5, rep=1, eta=0d 0h 4m 25s \n",
      "{'fold': 5, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.15333937108516693, 'train_time': 28.98780204099603, 'prior_train_nmll': 0.40378206968307495, 'train_nll': -211.84231567382812, 'test_nll': 43.38145446777344, 'train_mse': 0.013789392076432705, 'state_dict_file': 'model_state_dict_-8062593438545614505.pkl'}\n",
      "2019-03-16 17:07:01.298472, fold=6, rep=0, eta=0d 0h 4m 5s \n",
      "{'fold': 6, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.400647908449173, 'train_time': 57.30670556204859, 'prior_train_nmll': 0.3293085992336273, 'train_nll': -134.72561645507812, 'test_nll': 88.1484375, 'train_mse': 0.007100233342498541, 'state_dict_file': 'model_state_dict_4517752403205142671.pkl'}\n",
      "2019-03-16 17:07:49.149114, fold=6, rep=1, eta=0d 0h 3m 35s \n",
      "{'fold': 6, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.5399431586265564, 'train_time': 47.85033102205489, 'prior_train_nmll': 0.37238726019859314, 'train_nll': -308.2693786621094, 'test_nll': 114.95220947265625, 'train_mse': 0.009153407998383045, 'state_dict_file': 'model_state_dict_1980152420034258115.pkl'}\n",
      "2019-03-16 17:08:16.684792, fold=7, rep=0, eta=0d 0h 2m 57s \n",
      "{'fold': 7, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.26220765709877014, 'train_time': 27.532396293012425, 'prior_train_nmll': 0.4367961287498474, 'train_nll': -205.05587768554688, 'test_nll': 44.370361328125, 'train_mse': 0.015364776365458965, 'state_dict_file': 'model_state_dict_-5232915558805330759.pkl'}\n",
      "2019-03-16 17:08:46.076080, fold=7, rep=1, eta=0d 0h 2m 20s \n",
      "{'fold': 7, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.22348977625370026, 'train_time': 29.391035001026466, 'prior_train_nmll': 0.37271150946617126, 'train_nll': -226.32827758789062, 'test_nll': 35.87563705444336, 'train_mse': 0.014695785008370876, 'state_dict_file': 'model_state_dict_2676517399163477771.pkl'}\n",
      "2019-03-16 17:09:14.451389, fold=8, rep=0, eta=0d 0h 1m 43s \n",
      "{'fold': 8, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.22930443286895752, 'train_time': 28.371696457033977, 'prior_train_nmll': 0.3998384475708008, 'train_nll': -175.42282104492188, 'test_nll': 50.31402587890625, 'train_mse': 0.01730787754058838, 'state_dict_file': 'model_state_dict_-2005494686572110605.pkl'}\n",
      "2019-03-16 17:09:47.867007, fold=8, rep=1, eta=0d 0h 1m 9s \n",
      "{'fold': 8, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.22791634500026703, 'train_time': 33.41495796502568, 'prior_train_nmll': 0.49088120460510254, 'train_nll': -249.21932983398438, 'test_nll': 50.218353271484375, 'train_mse': 0.012249787338078022, 'state_dict_file': 'model_state_dict_-3802677012907239239.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 17:10:14.875619, fold=9, rep=0, eta=0d 0h 0m 34s \n",
      "{'fold': 9, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.1901572197675705, 'train_time': 27.005351635045372, 'prior_train_nmll': 0.37345749139785767, 'train_nll': -202.61114501953125, 'test_nll': 56.70737838745117, 'train_mse': 0.016008496284484863, 'state_dict_file': 'model_state_dict_6204907276785055143.pkl'}\n",
      "2019-03-16 17:10:44.955468, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.1900961995124817, 'train_time': 30.07951354107354, 'prior_train_nmll': 0.41003742814064026, 'train_nll': -254.7215576171875, 'test_nll': 64.25230407714844, 'train_mse': 0.012227339670062065, 'state_dict_file': 'model_state_dict_1873593972695175368.pkl'}\n",
      "2019-03-16 17:11:48.495904, fold=0, rep=0, eta=0d 0h 20m 6s \n",
      "{'fold': 0, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.7408064007759094, 'train_time': 63.510896897059865, 'prior_train_nmll': 0.7592323422431946, 'train_nll': 1468.589599609375, 'test_nll': 148.31167602539062, 'train_mse': 0.00874989666044712, 'state_dict_file': 'model_state_dict_2285637333798083509.pkl'}\n",
      "2019-03-16 17:12:59.485794, fold=0, rep=1, eta=0d 0h 20m 10s \n",
      "{'fold': 0, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.7387932538986206, 'train_time': 70.9896059599705, 'prior_train_nmll': 0.7762901186943054, 'train_nll': 1246.440185546875, 'test_nll': 210.5229034423828, 'train_mse': 0.008629160933196545, 'state_dict_file': 'model_state_dict_-3425945511495139781.pkl'}\n",
      "2019-03-16 17:14:06.421019, fold=1, rep=0, eta=0d 0h 19m 1s \n",
      "{'fold': 1, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.5970796942710876, 'train_time': 66.93203893501777, 'prior_train_nmll': 0.7251006364822388, 'train_nll': -1704.333251953125, 'test_nll': 144.39329528808594, 'train_mse': 0.008740744553506374, 'state_dict_file': 'model_state_dict_2512417726065300465.pkl'}\n",
      "2019-03-16 17:15:02.415110, fold=1, rep=1, eta=0d 0h 17m 9s \n",
      "{'fold': 1, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.5309021472930908, 'train_time': 55.99381251598243, 'prior_train_nmll': 0.6740674376487732, 'train_nll': -753.5659790039062, 'test_nll': 117.25465393066406, 'train_mse': 0.010533473454415798, 'state_dict_file': 'model_state_dict_-8017404394728106308.pkl'}\n",
      "2019-03-16 17:16:18.621586, fold=2, rep=0, eta=0d 0h 16m 40s \n",
      "{'fold': 2, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.43062418699264526, 'train_time': 76.20340501598548, 'prior_train_nmll': 0.6857830286026001, 'train_nll': 32222.568359375, 'test_nll': 118.3277359008789, 'train_mse': 0.005895968526601791, 'state_dict_file': 'model_state_dict_3246101013380162.pkl'}\n",
      "2019-03-16 17:17:17.425128, fold=2, rep=1, eta=0d 0h 15m 15s \n",
      "{'fold': 2, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.43247342109680176, 'train_time': 58.80300408601761, 'prior_train_nmll': 0.7939095497131348, 'train_nll': -1747.483154296875, 'test_nll': 97.76771545410156, 'train_mse': 0.011274764314293861, 'state_dict_file': 'model_state_dict_8323565803649466740.pkl'}\n",
      "2019-03-16 17:18:19.811541, fold=3, rep=0, eta=0d 0h 14m 4s \n",
      "{'fold': 3, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.6295721530914307, 'train_time': 62.3832153880503, 'prior_train_nmll': 0.8206266760826111, 'train_nll': -1065.557861328125, 'test_nll': 109.39442443847656, 'train_mse': 0.01261783204972744, 'state_dict_file': 'model_state_dict_-8119314269247857149.pkl'}\n",
      "2019-03-16 17:19:29.875637, fold=3, rep=1, eta=0d 0h 13m 7s \n",
      "{'fold': 3, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.3097507357597351, 'train_time': 70.06381374702323, 'prior_train_nmll': 0.7550731301307678, 'train_nll': 13830.2705078125, 'test_nll': 85.25849914550781, 'train_mse': 0.006884776521474123, 'state_dict_file': 'model_state_dict_-4175919239926294199.pkl'}\n",
      "2019-03-16 17:20:32.866213, fold=4, rep=0, eta=0d 0h 11m 58s \n",
      "{'fold': 4, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.7110395431518555, 'train_time': 62.98734920506831, 'prior_train_nmll': 0.7913996577262878, 'train_nll': 1056600320.0, 'test_nll': 179.79840087890625, 'train_mse': 0.00969843938946724, 'state_dict_file': 'model_state_dict_5711642864646951999.pkl'}\n",
      "2019-03-16 17:21:41.711370, fold=4, rep=1, eta=0d 0h 10m 56s \n",
      "{'fold': 4, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.78504878282547, 'train_time': 68.84448327601422, 'prior_train_nmll': 0.7661463618278503, 'train_nll': 2475.90869140625, 'test_nll': 244.32037353515625, 'train_mse': 0.006602304521948099, 'state_dict_file': 'model_state_dict_6876869524660588485.pkl'}\n",
      "2019-03-16 17:22:49.820914, fold=5, rep=0, eta=0d 0h 9m 53s \n",
      "{'fold': 5, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.45913174748420715, 'train_time': 68.10635879903566, 'prior_train_nmll': 0.7843028903007507, 'train_nll': -148.95013427734375, 'test_nll': 131.08985900878906, 'train_mse': 0.009125902317464352, 'state_dict_file': 'model_state_dict_7146037368973978130.pkl'}\n",
      "2019-03-16 17:24:03.685859, fold=5, rep=1, eta=0d 0h 8m 52s \n",
      "{'fold': 5, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.33201271295547485, 'train_time': 73.86464745306876, 'prior_train_nmll': 0.8525805473327637, 'train_nll': -506.86895751953125, 'test_nll': 66.95380401611328, 'train_mse': 0.008461972698569298, 'state_dict_file': 'model_state_dict_-7152679512430884783.pkl'}\n",
      "2019-03-16 17:25:07.878082, fold=6, rep=0, eta=0d 0h 7m 44s \n",
      "{'fold': 6, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.9214968681335449, 'train_time': 64.1889861569507, 'prior_train_nmll': 0.7642678618431091, 'train_nll': -699.4254760742188, 'test_nll': 160.65614318847656, 'train_mse': 0.008810787461698055, 'state_dict_file': 'model_state_dict_1627622222620678662.pkl'}\n",
      "2019-03-16 17:26:14.085240, fold=6, rep=1, eta=0d 0h 6m 38s \n",
      "{'fold': 6, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.5847226977348328, 'train_time': 66.2068650110159, 'prior_train_nmll': 0.7983129024505615, 'train_nll': -1456.996826171875, 'test_nll': 129.2827911376953, 'train_mse': 0.009927263483405113, 'state_dict_file': 'model_state_dict_6748294000685543970.pkl'}\n",
      "2019-03-16 17:27:17.104062, fold=7, rep=0, eta=0d 0h 5m 30s \n",
      "{'fold': 7, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.6847034692764282, 'train_time': 63.01554678496905, 'prior_train_nmll': 0.6987704634666443, 'train_nll': 5324706.5, 'test_nll': 125.75717163085938, 'train_mse': 0.007746966555714607, 'state_dict_file': 'model_state_dict_-8900428962176007199.pkl'}\n",
      "2019-03-16 17:28:19.284701, fold=7, rep=1, eta=0d 0h 4m 23s \n",
      "{'fold': 7, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.9381446242332458, 'train_time': 62.18033099605236, 'prior_train_nmll': 0.6729505658149719, 'train_nll': 68190.4375, 'test_nll': 178.78997802734375, 'train_mse': 0.008727038279175758, 'state_dict_file': 'model_state_dict_-6187582708269633827.pkl'}\n",
      "2019-03-16 17:29:22.616083, fold=8, rep=0, eta=0d 0h 3m 17s \n",
      "{'fold': 8, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.3270873725414276, 'train_time': 63.32776196405757, 'prior_train_nmll': 0.7577283978462219, 'train_nll': -1667.87060546875, 'test_nll': 73.38035583496094, 'train_mse': 0.009862439706921577, 'state_dict_file': 'model_state_dict_7622770464410432834.pkl'}\n",
      "2019-03-16 17:30:37.649027, fold=8, rep=1, eta=0d 0h 2m 12s \n",
      "{'fold': 8, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.43837037682533264, 'train_time': 75.0326628300827, 'prior_train_nmll': 0.759375274181366, 'train_nll': -915.0768432617188, 'test_nll': 103.34391784667969, 'train_mse': 0.008201279677450657, 'state_dict_file': 'model_state_dict_-250416272078106495.pkl'}\n",
      "2019-03-16 17:33:09.744077, fold=9, rep=0, eta=0d 0h 1m 10s \n",
      "{'fold': 9, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.5589153170585632, 'train_time': 152.09178967704065, 'prior_train_nmll': 0.7289665937423706, 'train_nll': 72720056.0, 'test_nll': 179.6781005859375, 'train_mse': 0.002754455665126443, 'state_dict_file': 'model_state_dict_6549088493621377020.pkl'}\n",
      "2019-03-16 17:33:59.920865, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.40918123722076416, 'train_time': 50.17649793392047, 'prior_train_nmll': 0.8353244066238403, 'train_nll': 8982587.0, 'test_nll': 91.07383728027344, 'train_mse': 0.016111476346850395, 'state_dict_file': 'model_state_dict_-7917348707441377645.pkl'}\n",
      "2019-03-16 17:36:36.055273, fold=0, rep=0, eta=0d 0h 49m 26s \n",
      "{'fold': 0, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0012276610359549522, 'train_time': 156.10813623806462, 'prior_train_nmll': -1.4506126642227173, 'train_nll': -3908.754638671875, 'test_nll': 763.35986328125, 'train_mse': 0.000753997708670795, 'state_dict_file': 'model_state_dict_-1328766933172112136.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 17:38:59.123783, fold=0, rep=1, eta=0d 0h 44m 52s \n",
      "{'fold': 0, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.001852419227361679, 'train_time': 143.06824141799007, 'prior_train_nmll': -1.1709449291229248, 'train_nll': -2092.158203125, 'test_nll': 2045.0478515625, 'train_mse': 0.0006927034701220691, 'state_dict_file': 'model_state_dict_-6168461113908803357.pkl'}\n",
      "2019-03-16 17:41:22.388673, fold=1, rep=0, eta=0d 0h 41m 47s \n",
      "{'fold': 1, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0020153578370809555, 'train_time': 143.26184108702, 'prior_train_nmll': -1.2324990034103394, 'train_nll': 42904.30078125, 'test_nll': -110.72918701171875, 'train_mse': 0.0007568741566501558, 'state_dict_file': 'model_state_dict_1253948683518519559.pkl'}\n",
      "2019-03-16 17:43:51.386687, fold=1, rep=1, eta=0d 0h 39m 25s \n",
      "{'fold': 1, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0013443792704492807, 'train_time': 148.99774658295792, 'prior_train_nmll': -1.2665939331054688, 'train_nll': -2904.326904296875, 'test_nll': -709.0181884765625, 'train_mse': 0.0006496910937130451, 'state_dict_file': 'model_state_dict_-6163510604186971936.pkl'}\n",
      "2019-03-16 17:47:37.628037, fold=2, rep=0, eta=0d 0h 40m 53s \n",
      "{'fold': 2, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.002014527563005686, 'train_time': 226.2382252520183, 'prior_train_nmll': -1.2949479818344116, 'train_nll': 1113794560.0, 'test_nll': 50605.0625, 'train_mse': 0.00033526375773362815, 'state_dict_file': 'model_state_dict_-2472395858629596522.pkl'}\n",
      "2019-03-16 17:50:41.588076, fold=2, rep=1, eta=0d 0h 38m 57s \n",
      "{'fold': 2, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.003792513394728303, 'train_time': 183.9593253530329, 'prior_train_nmll': -1.207655429840088, 'train_nll': 216386.0, 'test_nll': 46533.14453125, 'train_mse': 0.000546678842511028, 'state_dict_file': 'model_state_dict_-8785999302084177927.pkl'}\n",
      "2019-03-16 17:53:11.159922, fold=3, rep=0, eta=0d 0h 35m 37s \n",
      "{'fold': 3, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0017270209500566125, 'train_time': 149.56865311297588, 'prior_train_nmll': -1.2786182165145874, 'train_nll': 10129482752.0, 'test_nll': 17315.103515625, 'train_mse': 0.000835446931887418, 'state_dict_file': 'model_state_dict_6642483322838892475.pkl'}\n",
      "2019-03-16 17:55:28.677204, fold=3, rep=1, eta=0d 0h 32m 13s \n",
      "{'fold': 3, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0015504632610827684, 'train_time': 137.5169709949987, 'prior_train_nmll': -1.2373536825180054, 'train_nll': -3265.031982421875, 'test_nll': -1882.615478515625, 'train_mse': 0.0006427128682844341, 'state_dict_file': 'model_state_dict_-4548493146169572834.pkl'}\n",
      "2019-03-16 17:57:57.211430, fold=4, rep=0, eta=0d 0h 29m 16s \n",
      "{'fold': 4, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.002431917004287243, 'train_time': 148.53107958391774, 'prior_train_nmll': -1.2785547971725464, 'train_nll': 34418.37890625, 'test_nll': 1931.360107421875, 'train_mse': 0.0005754997255280614, 'state_dict_file': 'model_state_dict_5176308033340465083.pkl'}\n",
      "2019-03-16 18:00:29.913210, fold=4, rep=1, eta=0d 0h 26m 29s \n",
      "{'fold': 4, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0026281753089278936, 'train_time': 152.70149515301455, 'prior_train_nmll': -1.2772839069366455, 'train_nll': 811.5172729492188, 'test_nll': 4021.453125, 'train_mse': 0.0006733256741426885, 'state_dict_file': 'model_state_dict_8359393016079506534.pkl'}\n",
      "2019-03-16 18:03:09.874116, fold=5, rep=0, eta=0d 0h 23m 51s \n",
      "{'fold': 5, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0021167700178921223, 'train_time': 159.9577159800101, 'prior_train_nmll': -1.23997163772583, 'train_nll': 3146.145263671875, 'test_nll': 3460.8193359375, 'train_mse': 0.0006885898765176535, 'state_dict_file': 'model_state_dict_6822112627601658058.pkl'}\n",
      "2019-03-16 18:06:09.214524, fold=5, rep=1, eta=0d 0h 21m 26s \n",
      "{'fold': 5, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0017519359244033694, 'train_time': 179.34010476293042, 'prior_train_nmll': -1.3298810720443726, 'train_nll': -134.47393798828125, 'test_nll': -302.8080139160156, 'train_mse': 0.0005408185534179211, 'state_dict_file': 'model_state_dict_1381509260532111531.pkl'}\n",
      "2019-03-16 18:08:50.801011, fold=6, rep=0, eta=0d 0h 18m 45s \n",
      "{'fold': 6, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0014531516935676336, 'train_time': 161.5833303979598, 'prior_train_nmll': -1.4000388383865356, 'train_nll': 181049.46875, 'test_nll': 26060.80859375, 'train_mse': 0.0007572477916255593, 'state_dict_file': 'model_state_dict_5611583630833570537.pkl'}\n",
      "2019-03-16 18:11:28.799922, fold=6, rep=1, eta=0d 0h 16m 3s \n",
      "{'fold': 6, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.001695691142231226, 'train_time': 157.998612313997, 'prior_train_nmll': -1.2453110218048096, 'train_nll': -3835.785400390625, 'test_nll': 17240134.0, 'train_mse': 0.0005426804418675601, 'state_dict_file': 'model_state_dict_-3082520092353255879.pkl'}\n",
      "2019-03-16 18:13:45.120975, fold=7, rep=0, eta=0d 0h 13m 15s \n",
      "{'fold': 7, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0022601927630603313, 'train_time': 136.3178356779972, 'prior_train_nmll': -1.2317646741867065, 'train_nll': -3050.7587890625, 'test_nll': -573.063232421875, 'train_mse': 0.0007720001158304513, 'state_dict_file': 'model_state_dict_3528755828343568927.pkl'}\n",
      "2019-03-16 18:16:21.999126, fold=7, rep=1, eta=0d 0h 10m 35s \n",
      "{'fold': 7, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.002027318114414811, 'train_time': 156.87744223501068, 'prior_train_nmll': -1.2286889553070068, 'train_nll': 8057.5126953125, 'test_nll': 134194.0625, 'train_mse': 0.000592307944316417, 'state_dict_file': 'model_state_dict_1172950915442010076.pkl'}\n",
      "2019-03-16 18:18:33.565870, fold=8, rep=0, eta=0d 0h 7m 51s \n",
      "{'fold': 8, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.0015262147644534707, 'train_time': 131.563447145978, 'prior_train_nmll': -1.2842004299163818, 'train_nll': -2871.181640625, 'test_nll': 37248468.0, 'train_mse': 0.0009905697079375386, 'state_dict_file': 'model_state_dict_9154283016988521729.pkl'}\n",
      "2019-03-16 18:21:06.471897, fold=8, rep=1, eta=0d 0h 5m 14s \n",
      "{'fold': 8, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0015433777589350939, 'train_time': 152.9057303860318, 'prior_train_nmll': -1.2035107612609863, 'train_nll': 356464.375, 'test_nll': -43051748.0, 'train_mse': 0.0008178309071809053, 'state_dict_file': 'model_state_dict_-7783410578885451285.pkl'}\n",
      "2019-03-16 18:24:00.276174, fold=9, rep=0, eta=0d 0h 2m 37s \n",
      "{'fold': 9, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.003244179068133235, 'train_time': 173.8003756949911, 'prior_train_nmll': -1.269322395324707, 'train_nll': 11680476.0, 'test_nll': 451252.3125, 'train_mse': 0.0007439854089170694, 'state_dict_file': 'model_state_dict_-4767226036006228417.pkl'}\n",
      "2019-03-16 18:26:20.866892, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.0022167020943015814, 'train_time': 140.590275676921, 'prior_train_nmll': -1.310279369354248, 'train_nll': -2047.35498046875, 'test_nll': -8439.921875, 'train_mse': 0.000806266616564244, 'state_dict_file': 'model_state_dict_7902976548146905248.pkl'}\n",
      "2019-03-16 18:28:35.548397, fold=0, rep=0, eta=0d 0h 42m 38s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.3932395577430725, 'train_time': 134.65228815400042, 'prior_train_nmll': 0.17284442484378815, 'train_nll': -519.6461791992188, 'test_nll': 141.74197387695312, 'train_mse': 0.020339662209153175, 'state_dict_file': 'model_state_dict_-8425967434992809637.pkl'}\n",
      "2019-03-16 18:30:52.930571, fold=0, rep=1, eta=0d 0h 40m 48s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.33278292417526245, 'train_time': 137.38182116800454, 'prior_train_nmll': 0.1725461632013321, 'train_nll': -997.8850708007812, 'test_nll': 106.31167602539062, 'train_mse': 0.020445289090275764, 'state_dict_file': 'model_state_dict_3109934469086347314.pkl'}\n",
      "2019-03-16 18:33:14.729448, fold=1, rep=0, eta=0d 0h 39m 5s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.5723753571510315, 'train_time': 141.7956429659389, 'prior_train_nmll': 0.11028752475976944, 'train_nll': -1145.58056640625, 'test_nll': 115.81900024414062, 'train_mse': 0.01734481193125248, 'state_dict_file': 'model_state_dict_-6348996147083705807.pkl'}\n",
      "2019-03-16 18:36:01.807085, fold=1, rep=1, eta=0d 0h 38m 43s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.2643355131149292, 'train_time': 167.0772715699859, 'prior_train_nmll': 0.1541914939880371, 'train_nll': -125298200.0, 'test_nll': 140.6761016845703, 'train_mse': 0.0156466793268919, 'state_dict_file': 'model_state_dict_2976115838609559137.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 18:39:48.471801, fold=2, rep=0, eta=0d 0h 40m 22s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.10179780423641205, 'train_time': 226.6616673640674, 'prior_train_nmll': 0.12809069454669952, 'train_nll': -930.4108276367188, 'test_nll': 24.195533752441406, 'train_mse': 0.017051054164767265, 'state_dict_file': 'model_state_dict_5836625077770178864.pkl'}\n",
      "2019-03-16 18:42:08.870085, fold=2, rep=1, eta=0d 0h 36m 51s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.1108885183930397, 'train_time': 140.39797323406674, 'prior_train_nmll': 0.1539066582918167, 'train_nll': -393.44647216796875, 'test_nll': 8.226348876953125, 'train_mse': 0.021703802049160004, 'state_dict_file': 'model_state_dict_7353871099130475635.pkl'}\n",
      "2019-03-16 18:48:15.878922, fold=3, rep=0, eta=0d 0h 40m 42s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.3360963761806488, 'train_time': 367.00559619499836, 'prior_train_nmll': 0.00011301092308713123, 'train_nll': -1972.036865234375, 'test_nll': 191.50204467773438, 'train_mse': 0.01511925458908081, 'state_dict_file': 'model_state_dict_4442025281527646356.pkl'}\n",
      "2019-03-16 18:50:35.353530, fold=3, rep=1, eta=0d 0h 36m 21s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.24836377799510956, 'train_time': 139.47428978898097, 'prior_train_nmll': 0.11819865554571152, 'train_nll': -924.8434448242188, 'test_nll': 96.21572875976562, 'train_mse': 0.021799085661768913, 'state_dict_file': 'model_state_dict_-3116335354061526603.pkl'}\n",
      "2019-03-16 18:58:33.629307, fold=4, rep=0, eta=0d 0h 39m 22s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.19599546492099762, 'train_time': 478.2725005430402, 'prior_train_nmll': 0.025783132761716843, 'train_nll': -1296.62841796875, 'test_nll': 95.88435363769531, 'train_mse': 0.011125875636935234, 'state_dict_file': 'model_state_dict_8923549467855894299.pkl'}\n",
      "2019-03-16 19:04:32.333215, fold=4, rep=1, eta=0d 0h 38m 11s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.2798973321914673, 'train_time': 358.703583098948, 'prior_train_nmll': 0.08473861217498779, 'train_nll': -10918.556640625, 'test_nll': 105.24153137207031, 'train_mse': 0.014307048171758652, 'state_dict_file': 'model_state_dict_-885036246475903843.pkl'}\n",
      "2019-03-16 19:09:57.339047, fold=5, rep=0, eta=0d 0h 35m 40s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.10908115655183792, 'train_time': 325.0026179510169, 'prior_train_nmll': 0.0622277706861496, 'train_nll': -1092.35693359375, 'test_nll': 1.073333740234375, 'train_mse': 0.016860779374837875, 'state_dict_file': 'model_state_dict_7839369310875611437.pkl'}\n",
      "2019-03-16 19:14:57.306402, fold=5, rep=1, eta=0d 0h 32m 24s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.2255885899066925, 'train_time': 299.9665438710945, 'prior_train_nmll': 0.09801635891199112, 'train_nll': 226.27227783203125, 'test_nll': 39.62053680419922, 'train_mse': 0.01789700612425804, 'state_dict_file': 'model_state_dict_728232986395348305.pkl'}\n",
      "2019-03-16 19:20:25.174006, fold=6, rep=0, eta=0d 0h 29m 6s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.08876141905784607, 'train_time': 327.86452589998953, 'prior_train_nmll': 0.09156356006860733, 'train_nll': -702.7617797851562, 'test_nll': 2.831756591796875, 'train_mse': 0.01764809340238571, 'state_dict_file': 'model_state_dict_-6666213098506117145.pkl'}\n",
      "2019-03-16 19:24:01.613847, fold=6, rep=1, eta=0d 0h 24m 43s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.2372412383556366, 'train_time': 216.43947414297145, 'prior_train_nmll': 0.1509907990694046, 'train_nll': -864.8777465820312, 'test_nll': 26.687286376953125, 'train_mse': 0.020310528576374054, 'state_dict_file': 'model_state_dict_-191941160472903670.pkl'}\n",
      "2019-03-16 19:29:15.992106, fold=7, rep=0, eta=0d 0h 20m 58s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.16865554451942444, 'train_time': 314.3750107520027, 'prior_train_nmll': 0.12057329714298248, 'train_nll': -1331.64404296875, 'test_nll': 1.8607406616210938, 'train_mse': 0.0172087624669075, 'state_dict_file': 'model_state_dict_1337353042759012659.pkl'}\n",
      "2019-03-16 19:32:57.419380, fold=7, rep=1, eta=0d 0h 16m 39s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.08936597406864166, 'train_time': 221.4268884359626, 'prior_train_nmll': 0.10890794545412064, 'train_nll': -702.3435668945312, 'test_nll': -3.7323150634765625, 'train_mse': 0.017730798572301865, 'state_dict_file': 'model_state_dict_-4406737665752490274.pkl'}\n",
      "2019-03-16 19:38:01.301039, fold=8, rep=0, eta=0d 0h 12m 38s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.03900445997714996, 'train_time': 303.87853194295894, 'prior_train_nmll': 0.0653231143951416, 'train_nll': -633.9083862304688, 'test_nll': -26.626976013183594, 'train_mse': 0.019322190433740616, 'state_dict_file': 'model_state_dict_-6728983664535263178.pkl'}\n",
      "2019-03-16 19:44:40.295038, fold=8, rep=1, eta=0d 0h 8m 42s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.03715788200497627, 'train_time': 398.9936829620274, 'prior_train_nmll': 0.033004242926836014, 'train_nll': -722.6067504882812, 'test_nll': -35.756195068359375, 'train_mse': 0.01795324683189392, 'state_dict_file': 'model_state_dict_-7305189109159148913.pkl'}\n",
      "2019-03-16 19:48:31.675307, fold=9, rep=0, eta=0d 0h 4m 19s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.042559873312711716, 'train_time': 231.3770448010182, 'prior_train_nmll': 0.07443032413721085, 'train_nll': -762.4440307617188, 'test_nll': -36.35923767089844, 'train_mse': 0.0182906836271286, 'state_dict_file': 'model_state_dict_207900400588105314.pkl'}\n",
      "2019-03-16 19:53:50.957405, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.03935829550027847, 'train_time': 319.2817978520179, 'prior_train_nmll': 0.04634098336100578, 'train_nll': -1218.927978515625, 'test_nll': -30.18268585205078, 'train_mse': 0.017830079421401024, 'state_dict_file': 'model_state_dict_-198615676271461792.pkl'}\n",
      "2019-03-16 19:58:44.905569, fold=0, rep=0, eta=0d 1h 33m 4s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.0545166730880737, 'train_time': 293.91574126505293, 'prior_train_nmll': 1.270485758781433, 'train_nll': 1073.8359375, 'test_nll': 157.25198364257812, 'train_mse': 0.5090353488922119, 'state_dict_file': 'model_state_dict_-6509637534597033100.pkl'}\n",
      "2019-03-16 20:01:47.829097, fold=0, rep=1, eta=0d 1h 11m 31s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.0891395807266235, 'train_time': 182.92316851904616, 'prior_train_nmll': 1.2806752920150757, 'train_nll': 1075.72265625, 'test_nll': 160.5229949951172, 'train_mse': 0.5130524039268494, 'state_dict_file': 'model_state_dict_2700101594247263020.pkl'}\n",
      "2019-03-16 20:04:27.811676, fold=1, rep=0, eta=0d 1h 0m 8s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.8268647193908691, 'train_time': 159.97931923007127, 'prior_train_nmll': 1.307553768157959, 'train_nll': 1113.770751953125, 'test_nll': 135.0001983642578, 'train_mse': 0.557832658290863, 'state_dict_file': 'model_state_dict_-2929372070351898934.pkl'}\n",
      "2019-03-16 20:11:12.508511, fold=1, rep=1, eta=0d 1h 9m 26s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.7783772349357605, 'train_time': 404.69633441395126, 'prior_train_nmll': 1.2972019910812378, 'train_nll': 1108.307373046875, 'test_nll': 130.86170959472656, 'train_mse': 0.5496776700019836, 'state_dict_file': 'model_state_dict_6445696905636352207.pkl'}\n",
      "2019-03-16 20:14:27.200137, fold=2, rep=0, eta=0d 1h 1m 48s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.522163987159729, 'train_time': 194.68668257200625, 'prior_train_nmll': 1.2658073902130127, 'train_nll': 1079.214111328125, 'test_nll': 183.65280151367188, 'train_mse': 0.5194718241691589, 'state_dict_file': 'model_state_dict_2438250296158067258.pkl'}\n",
      "2019-03-16 20:18:06.720513, fold=2, rep=1, eta=0d 0h 56m 36s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.3905775547027588, 'train_time': 219.51998347206973, 'prior_train_nmll': 1.2699989080429077, 'train_nll': 1076.9716796875, 'test_nll': 177.6867218017578, 'train_mse': 0.5148523449897766, 'state_dict_file': 'model_state_dict_-6239351319021552201.pkl'}\n",
      "2019-03-16 20:21:41.856476, fold=3, rep=0, eta=0d 0h 51m 43s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.8387793302536011, 'train_time': 215.13278413703665, 'prior_train_nmll': 1.2988377809524536, 'train_nll': 1093.83935546875, 'test_nll': 138.18844604492188, 'train_mse': 0.5313745141029358, 'state_dict_file': 'model_state_dict_7141667098977124233.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 20:25:08.580170, fold=3, rep=1, eta=0d 0h 46m 56s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.8082998394966125, 'train_time': 206.7233641789062, 'prior_train_nmll': 1.2874209880828857, 'train_nll': 1089.19775390625, 'test_nll': 135.52880859375, 'train_mse': 0.5285563468933105, 'state_dict_file': 'model_state_dict_7732197496245542916.pkl'}\n",
      "2019-03-16 20:29:03.242505, fold=4, rep=0, eta=0d 0h 43m 1s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.5192204117774963, 'train_time': 234.65909323003143, 'prior_train_nmll': 1.3100636005401611, 'train_nll': 1112.269287109375, 'test_nll': 114.26042938232422, 'train_mse': 0.5515456795692444, 'state_dict_file': 'model_state_dict_-628630097177456058.pkl'}\n",
      "2019-03-16 20:32:26.038980, fold=4, rep=1, eta=0d 0h 38m 35s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.5150270462036133, 'train_time': 202.79611020407174, 'prior_train_nmll': 1.3080652952194214, 'train_nll': 1111.36474609375, 'test_nll': 114.97632598876953, 'train_mse': 0.5498210191726685, 'state_dict_file': 'model_state_dict_6232569870750488982.pkl'}\n",
      "2019-03-16 20:34:40.763056, fold=5, rep=0, eta=0d 0h 33m 24s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.5929841995239258, 'train_time': 134.71790533897, 'prior_train_nmll': 1.2439550161361694, 'train_nll': 1052.2635498046875, 'test_nll': 201.07595825195312, 'train_mse': 0.48792439699172974, 'state_dict_file': 'model_state_dict_-6607854087019360775.pkl'}\n",
      "2019-03-16 20:37:02.075678, fold=5, rep=1, eta=0d 0h 28m 47s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.5898243188858032, 'train_time': 141.31229774095118, 'prior_train_nmll': 1.2456094026565552, 'train_nll': 1047.0244140625, 'test_nll': 199.40740966796875, 'train_mse': 0.480694442987442, 'state_dict_file': 'model_state_dict_6027970676218304905.pkl'}\n",
      "2019-03-16 20:41:02.079649, fold=6, rep=0, eta=0d 0h 25m 24s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.2362451553344727, 'train_time': 240.0007253329968, 'prior_train_nmll': 1.2719533443450928, 'train_nll': 1109.7559814453125, 'test_nll': 164.565673828125, 'train_mse': 0.5605046153068542, 'state_dict_file': 'model_state_dict_9177115580300549415.pkl'}\n",
      "2019-03-16 20:44:49.276974, fold=6, rep=1, eta=0d 0h 21m 50s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.5149507522583008, 'train_time': 227.1969768969575, 'prior_train_nmll': 1.2795816659927368, 'train_nll': 1116.1446533203125, 'test_nll': 177.94775390625, 'train_mse': 0.5669808387756348, 'state_dict_file': 'model_state_dict_4432382435842291537.pkl'}\n",
      "2019-03-16 20:47:46.093481, fold=7, rep=0, eta=0d 0h 17m 58s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.597987413406372, 'train_time': 176.8111330299871, 'prior_train_nmll': 1.249582052230835, 'train_nll': 1058.192138671875, 'test_nll': 184.38528442382812, 'train_mse': 0.49205222725868225, 'state_dict_file': 'model_state_dict_-2887531277619798373.pkl'}\n",
      "2019-03-16 20:51:01.835204, fold=7, rep=1, eta=0d 0h 14m 17s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.5141324996948242, 'train_time': 195.7414006969193, 'prior_train_nmll': 1.250575304031372, 'train_nll': 1058.664794921875, 'test_nll': 182.0358123779297, 'train_mse': 0.4961441159248352, 'state_dict_file': 'model_state_dict_6768224500000528918.pkl'}\n",
      "2019-03-16 20:54:31.302205, fold=8, rep=0, eta=0d 0h 10m 42s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.5409173369407654, 'train_time': 209.4620050659869, 'prior_train_nmll': 1.3095515966415405, 'train_nll': 1123.7730712890625, 'test_nll': 117.095703125, 'train_mse': 0.5659506320953369, 'state_dict_file': 'model_state_dict_1813635408411616231.pkl'}\n",
      "2019-03-16 20:57:31.551911, fold=8, rep=1, eta=0d 0h 7m 4s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.6433196067810059, 'train_time': 180.24919080198742, 'prior_train_nmll': 1.3057726621627808, 'train_nll': 1123.736572265625, 'test_nll': 123.25755310058594, 'train_mse': 0.5696474313735962, 'state_dict_file': 'model_state_dict_2954571903160382064.pkl'}\n",
      "2019-03-16 21:00:14.842859, fold=9, rep=0, eta=0d 0h 3m 29s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.2291048765182495, 'train_time': 163.28760698006954, 'prior_train_nmll': 1.2624658346176147, 'train_nll': 1058.8355712890625, 'test_nll': 164.18296813964844, 'train_mse': 0.4861425459384918, 'state_dict_file': 'model_state_dict_9022865572372386669.pkl'}\n",
      "2019-03-16 21:04:50.956877, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.3202711343765259, 'train_time': 276.11372263508383, 'prior_train_nmll': 1.2569892406463623, 'train_nll': 1059.398193359375, 'test_nll': 167.9287872314453, 'train_mse': 0.48804646730422974, 'state_dict_file': 'model_state_dict_-3281827874826445049.pkl'}\n",
      "2019-03-16 21:24:25.869209, fold=0, rep=0, eta=0d 6h 12m 2s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.046494629234075546, 'train_time': 1174.873012644006, 'prior_train_nmll': 0.04618096724152565, 'train_nll': -2745.3759765625, 'test_nll': -22.6875, 'train_mse': 0.016726404428482056, 'state_dict_file': 'model_state_dict_3041930715601302727.pkl'}\n",
      "2019-03-16 21:43:42.652696, fold=0, rep=1, eta=0d 5h 49m 44s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.04388735070824623, 'train_time': 1156.7831204429967, 'prior_train_nmll': 0.024647746235132217, 'train_nll': -3367.55615234375, 'test_nll': -13.448257446289062, 'train_mse': 0.020146263763308525, 'state_dict_file': 'model_state_dict_1023646031040224627.pkl'}\n",
      "2019-03-16 21:56:19.311423, fold=1, rep=0, eta=0d 4h 51m 40s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.054037775844335556, 'train_time': 756.6555300550535, 'prior_train_nmll': 0.050320252776145935, 'train_nll': 3567.23876953125, 'test_nll': -20.227706909179688, 'train_mse': 0.02088610827922821, 'state_dict_file': 'model_state_dict_-7514956394232736782.pkl'}\n",
      "2019-03-16 22:07:43.242342, fold=1, rep=1, eta=0d 4h 11m 28s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.06630677729845047, 'train_time': 683.9306093569612, 'prior_train_nmll': 0.08266996592283249, 'train_nll': 1492.1524658203125, 'test_nll': 11.786674499511719, 'train_mse': 0.02388131432235241, 'state_dict_file': 'model_state_dict_3529340652437905011.pkl'}\n",
      "2019-03-16 22:28:54.669635, fold=2, rep=0, eta=0d 4h 12m 11s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.05129827558994293, 'train_time': 1271.4240251170704, 'prior_train_nmll': 0.04776779189705849, 'train_nll': -1076.2208251953125, 'test_nll': -6.6652679443359375, 'train_mse': 0.018876302987337112, 'state_dict_file': 'model_state_dict_-6829790629016613931.pkl'}\n",
      "2019-03-16 22:38:18.354114, fold=2, rep=1, eta=0d 3h 38m 3s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.06625482439994812, 'train_time': 563.683750999975, 'prior_train_nmll': 0.09731028974056244, 'train_nll': 7501.13427734375, 'test_nll': 10.631179809570312, 'train_mse': 0.025607174262404442, 'state_dict_file': 'model_state_dict_6952228952744705488.pkl'}\n",
      "2019-03-16 22:50:02.667160, fold=3, rep=0, eta=0d 3h 15m 21s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.03762441128492355, 'train_time': 704.3098590599839, 'prior_train_nmll': 0.11320759356021881, 'train_nll': 35742.1328125, 'test_nll': -33.511932373046875, 'train_mse': 0.025408508256077766, 'state_dict_file': 'model_state_dict_483424908921672249.pkl'}\n",
      "2019-03-16 23:04:21.112323, fold=3, rep=1, eta=0d 2h 59m 15s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.0472995825111866, 'train_time': 858.4447440669173, 'prior_train_nmll': 0.13123780488967896, 'train_nll': 18407.763671875, 'test_nll': -19.188766479492188, 'train_mse': 0.02260175161063671, 'state_dict_file': 'model_state_dict_4262636960961645847.pkl'}\n",
      "2019-03-16 23:26:29.958392, fold=4, rep=0, eta=0d 2h 53m 7s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.0692625641822815, 'train_time': 1328.8422542209737, 'prior_train_nmll': 0.04814962297677994, 'train_nll': 13169.166015625, 'test_nll': 17.5670166015625, 'train_mse': 0.018495161086320877, 'state_dict_file': 'model_state_dict_-3737935813067514493.pkl'}\n",
      "2019-03-16 23:38:52.731401, fold=4, rep=1, eta=0d 2h 34m 1s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.05593959987163544, 'train_time': 742.7726665940136, 'prior_train_nmll': 0.038691699504852295, 'train_nll': -519.9422607421875, 'test_nll': 6.2329864501953125, 'train_mse': 0.02130872756242752, 'state_dict_file': 'model_state_dict_-8334273854518432579.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-16 23:58:23.441453, fold=5, rep=0, eta=0d 2h 21m 59s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.04358627647161484, 'train_time': 1170.7068703579716, 'prior_train_nmll': 0.08745770901441574, 'train_nll': 1733.0592041015625, 'test_nll': -25.13720703125, 'train_mse': 0.0214262455701828, 'state_dict_file': 'model_state_dict_6758133343800201166.pkl'}\n",
      "2019-03-17 00:13:10.581445, fold=5, rep=1, eta=0d 2h 5m 33s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.04177720099687576, 'train_time': 887.1395943480311, 'prior_train_nmll': 0.14173565804958344, 'train_nll': -1636.2767333984375, 'test_nll': -29.716812133789062, 'train_mse': 0.027276115491986275, 'state_dict_file': 'model_state_dict_8372214585885256749.pkl'}\n",
      "2019-03-17 00:23:48.712351, fold=6, rep=0, eta=0d 1h 47m 8s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.059028226882219315, 'train_time': 638.1277388989693, 'prior_train_nmll': 0.06730644404888153, 'train_nll': -1619.2830810546875, 'test_nll': -0.0647125244140625, 'train_mse': 0.024221930652856827, 'state_dict_file': 'model_state_dict_-3142926522730086806.pkl'}\n",
      "2019-03-17 00:42:23.439640, fold=6, rep=1, eta=0d 1h 33m 13s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.05081271007657051, 'train_time': 1114.7268693549559, 'prior_train_nmll': 0.08160528540611267, 'train_nll': -1203.0731201171875, 'test_nll': -12.952407836914062, 'train_mse': 0.021834127604961395, 'state_dict_file': 'model_state_dict_-117726306867741720.pkl'}\n",
      "2019-03-17 00:58:30.271894, fold=7, rep=0, eta=0d 1h 17m 53s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.060679689049720764, 'train_time': 966.8290007730247, 'prior_train_nmll': 0.031174715608358383, 'train_nll': 33016.73828125, 'test_nll': 4.1878204345703125, 'train_mse': 0.02029009908437729, 'state_dict_file': 'model_state_dict_-7898324877810677894.pkl'}\n",
      "2019-03-17 01:18:53.761663, fold=7, rep=1, eta=0d 1h 3m 30s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.06395066529512405, 'train_time': 1223.489382144995, 'prior_train_nmll': 0.056267570704221725, 'train_nll': -1903.4417724609375, 'test_nll': 6.6984405517578125, 'train_mse': 0.01945304311811924, 'state_dict_file': 'model_state_dict_222557329507860691.pkl'}\n",
      "2019-03-17 01:33:34.552527, fold=8, rep=0, eta=0d 0h 47m 25s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.03477467596530914, 'train_time': 880.7868062499911, 'prior_train_nmll': 0.0667576715350151, 'train_nll': -3022.287109375, 'test_nll': -46.94384765625, 'train_mse': 0.01933920383453369, 'state_dict_file': 'model_state_dict_5016723689222484606.pkl'}\n",
      "2019-03-17 01:50:11.921368, fold=8, rep=1, eta=0d 0h 31m 42s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.04221637547016144, 'train_time': 997.3685051430948, 'prior_train_nmll': 0.04247452691197395, 'train_nll': 10253.8310546875, 'test_nll': -30.235214233398438, 'train_mse': 0.01931542344391346, 'state_dict_file': 'model_state_dict_-74060502897456956.pkl'}\n",
      "2019-03-17 02:08:39.583266, fold=9, rep=0, eta=0d 0h 15m 59s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.03346605226397514, 'train_time': 1107.6585528010037, 'prior_train_nmll': 0.06344756484031677, 'train_nll': -2749.140625, 'test_nll': -44.68408203125, 'train_mse': 0.019100824370980263, 'state_dict_file': 'model_state_dict_-6620002810499880486.pkl'}\n",
      "2019-03-17 02:32:03.693043, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.03725513443350792, 'train_time': 1404.1094166529365, 'prior_train_nmll': 0.07899001985788345, 'train_nll': 3091.82373046875, 'test_nll': -33.514556884765625, 'train_mse': 0.017240729182958603, 'state_dict_file': 'model_state_dict_-1909482001753533919.pkl'}\n",
      "2019-03-17 02:40:47.104329, fold=0, rep=0, eta=0d 2h 45m 44s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.2153247892856598, 'train_time': 523.3808239339851, 'prior_train_nmll': 0.530367910861969, 'train_nll': 99.414794921875, 'test_nll': 119.73600769042969, 'train_mse': 0.05275202915072441, 'state_dict_file': 'model_state_dict_-7609031331282600923.pkl'}\n",
      "2019-03-17 02:52:35.980532, fold=0, rep=1, eta=0d 3h 4m 50s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.22690288722515106, 'train_time': 708.8758392080199, 'prior_train_nmll': 0.45438405871391296, 'train_nll': -26.44091796875, 'test_nll': 160.29928588867188, 'train_mse': 0.04445524513721466, 'state_dict_file': 'model_state_dict_-2566619435621158921.pkl'}\n",
      "2019-03-17 03:02:49.545127, fold=1, rep=0, eta=0d 2h 54m 19s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.3444783389568329, 'train_time': 613.561466280953, 'prior_train_nmll': 0.48867419362068176, 'train_nll': 31.3497314453125, 'test_nll': 176.4251708984375, 'train_mse': 0.04794849082827568, 'state_dict_file': 'model_state_dict_-8449148210040149760.pkl'}\n",
      "2019-03-17 03:08:01.489848, fold=1, rep=1, eta=0d 2h 23m 51s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.24019408226013184, 'train_time': 311.9441243340261, 'prior_train_nmll': 0.5938705205917358, 'train_nll': 376.0242919921875, 'test_nll': 108.05179595947266, 'train_mse': 0.08021409809589386, 'state_dict_file': 'model_state_dict_-6870817539687863119.pkl'}\n",
      "2019-03-17 03:18:01.065056, fold=2, rep=0, eta=0d 2h 17m 52s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.22658701241016388, 'train_time': 599.5718541160459, 'prior_train_nmll': 0.5090373158454895, 'train_nll': 59.741455078125, 'test_nll': 127.58946228027344, 'train_mse': 0.05019012466073036, 'state_dict_file': 'model_state_dict_2958298472793669724.pkl'}\n",
      "2019-03-17 03:37:45.652532, fold=2, rep=1, eta=0d 2h 33m 17s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2857530117034912, 'train_time': 1184.58714619698, 'prior_train_nmll': 0.508533239364624, 'train_nll': -252.111083984375, 'test_nll': 202.59078979492188, 'train_mse': 0.03165300190448761, 'state_dict_file': 'model_state_dict_5040195194206612216.pkl'}\n",
      "2019-03-17 03:51:36.665231, fold=3, rep=0, eta=0d 2h 27m 44s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.2750524878501892, 'train_time': 831.0093742309837, 'prior_train_nmll': 0.44422659277915955, 'train_nll': -110.7611083984375, 'test_nll': 204.06344604492188, 'train_mse': 0.03928288072347641, 'state_dict_file': 'model_state_dict_7194092492420008955.pkl'}\n",
      "2019-03-17 04:08:55.027633, fold=3, rep=1, eta=0d 2h 25m 16s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2910401523113251, 'train_time': 1038.3620703070192, 'prior_train_nmll': 0.4894682765007019, 'train_nll': -90.9290771484375, 'test_nll': 212.74395751953125, 'train_mse': 0.04048491269350052, 'state_dict_file': 'model_state_dict_-2104684984168198633.pkl'}\n",
      "2019-03-17 04:21:15.883222, fold=4, rep=0, eta=0d 2h 13m 28s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.27007588744163513, 'train_time': 740.8522486800794, 'prior_train_nmll': 0.4845280051231384, 'train_nll': -45.8582763671875, 'test_nll': 167.03646850585938, 'train_mse': 0.04420095682144165, 'state_dict_file': 'model_state_dict_-8130847377363174253.pkl'}\n",
      "2019-03-17 04:32:43.462291, fold=4, rep=1, eta=0d 2h 0m 39s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2648388743400574, 'train_time': 687.5786374050658, 'prior_train_nmll': 0.4870985448360443, 'train_nll': -10.2589111328125, 'test_nll': 157.12899780273438, 'train_mse': 0.04463343322277069, 'state_dict_file': 'model_state_dict_-242075918700283117.pkl'}\n",
      "2019-03-17 04:55:37.243715, fold=5, rep=0, eta=0d 1h 57m 27s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.4096619188785553, 'train_time': 1373.7772542750463, 'prior_train_nmll': 0.44235455989837646, 'train_nll': -330.8912353515625, 'test_nll': 284.91845703125, 'train_mse': 0.02701178565621376, 'state_dict_file': 'model_state_dict_2767322322882826.pkl'}\n",
      "2019-03-17 05:07:48.885592, fold=5, rep=1, eta=0d 1h 43m 50s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.3794260323047638, 'train_time': 731.6415248119738, 'prior_train_nmll': 0.43038246035575867, 'train_nll': -153.28271484375, 'test_nll': 244.1433868408203, 'train_mse': 0.036472298204898834, 'state_dict_file': 'model_state_dict_-8124618842634228314.pkl'}\n",
      "2019-03-17 05:19:00.786736, fold=6, rep=0, eta=0d 1h 29m 53s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.24976825714111328, 'train_time': 671.8978435979225, 'prior_train_nmll': 0.5190816521644592, 'train_nll': -15.98876953125, 'test_nll': 146.14303588867188, 'train_mse': 0.04563242569565773, 'state_dict_file': 'model_state_dict_5652208934352475848.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-17 05:32:45.864130, fold=6, rep=1, eta=0d 1h 17m 26s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.29112425446510315, 'train_time': 825.077078782022, 'prior_train_nmll': 0.46508094668388367, 'train_nll': -144.9638671875, 'test_nll': 193.7093505859375, 'train_mse': 0.037658654153347015, 'state_dict_file': 'model_state_dict_-3042194656190362366.pkl'}\n",
      "2019-03-17 05:50:04.055795, fold=7, rep=0, eta=0d 1h 6m 0s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.29162806272506714, 'train_time': 1038.1808161300141, 'prior_train_nmll': 0.44287392497062683, 'train_nll': -165.510986328125, 'test_nll': 167.47731018066406, 'train_mse': 0.03565804660320282, 'state_dict_file': 'model_state_dict_-876285943403650734.pkl'}\n",
      "2019-03-17 06:05:59.565592, fold=7, rep=1, eta=0d 0h 53m 28s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2606983780860901, 'train_time': 955.5094747979892, 'prior_train_nmll': 0.4788036346435547, 'train_nll': -115.119384765625, 'test_nll': 163.36483764648438, 'train_mse': 0.0391848050057888, 'state_dict_file': 'model_state_dict_-2305825013345228209.pkl'}\n",
      "2019-03-17 06:18:40.536192, fold=8, rep=0, eta=0d 0h 39m 59s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.31779202818870544, 'train_time': 760.9671591949882, 'prior_train_nmll': 0.5001834034919739, 'train_nll': -39.88232421875, 'test_nll': 204.34002685546875, 'train_mse': 0.04362396150827408, 'state_dict_file': 'model_state_dict_-5180184697674219611.pkl'}\n",
      "2019-03-17 06:37:12.449138, fold=8, rep=1, eta=0d 0h 27m 14s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.26559552550315857, 'train_time': 1111.9126205779612, 'prior_train_nmll': 0.4344623386859894, 'train_nll': -264.33251953125, 'test_nll': 191.72523498535156, 'train_mse': 0.030487272888422012, 'state_dict_file': 'model_state_dict_1247197165379892254.pkl'}\n",
      "2019-03-17 06:50:56.800342, fold=9, rep=0, eta=0d 0h 13m 37s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.2465350329875946, 'train_time': 824.3478834109847, 'prior_train_nmll': 0.4716823995113373, 'train_nll': -110.6832275390625, 'test_nll': 160.50843811035156, 'train_mse': 0.03932274132966995, 'state_dict_file': 'model_state_dict_2982344425001775080.pkl'}\n",
      "2019-03-17 07:07:46.372895, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2401200532913208, 'train_time': 1009.5722141109873, 'prior_train_nmll': 0.49148887395858765, 'train_nll': -124.615478515625, 'test_nll': 145.9842529296875, 'train_mse': 0.03804611414670944, 'state_dict_file': 'model_state_dict_-638053533133345781.pkl'}\n",
      "2019-03-17 07:55:38.458525, fold=0, rep=0, eta=0d 15h 9m 28s \n",
      "{'fold': 0, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.020586419850587845, 'train_time': 2872.0378410769626, 'prior_train_nmll': -0.2255227267742157, 'train_nll': -2989.19287109375, 'test_nll': -164.87142944335938, 'train_mse': 0.0018306989222764969, 'state_dict_file': 'model_state_dict_-2446471872400458474.pkl'}\n",
      "2019-03-17 08:54:15.939107, fold=0, rep=1, eta=0d 15h 58m 25s \n",
      "{'fold': 0, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.02532120980322361, 'train_time': 3517.4802061410155, 'prior_train_nmll': -0.34150752425193787, 'train_nll': 10112.404296875, 'test_nll': 74.21835327148438, 'train_mse': 0.0003580099728424102, 'state_dict_file': 'model_state_dict_5292581007092756666.pkl'}\n",
      "2019-03-17 09:42:42.785783, fold=1, rep=0, eta=0d 14h 37m 59s \n",
      "{'fold': 1, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.04545779153704643, 'train_time': 2906.841232025996, 'prior_train_nmll': -0.3094496726989746, 'train_nll': -3243.38427734375, 'test_nll': 34.01806640625, 'train_mse': 0.0013068990083411336, 'state_dict_file': 'model_state_dict_-2119406410749270582.pkl'}\n",
      "2019-03-17 10:49:43.491711, fold=1, rep=1, eta=0d 14h 47m 48s \n",
      "{'fold': 1, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.053849250078201294, 'train_time': 4020.7053859679727, 'prior_train_nmll': -0.41408592462539673, 'train_nll': 2053397248.0, 'test_nll': 978.8646240234375, 'train_mse': 0.00041789101669564843, 'state_dict_file': 'model_state_dict_6088061456114734270.pkl'}\n",
      "2019-03-17 11:56:45.631244, fold=2, rep=0, eta=0d 14h 26m 57s \n",
      "{'fold': 2, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.04734834283590317, 'train_time': 4022.1319154939847, 'prior_train_nmll': -0.3387273848056793, 'train_nll': 29045.490234375, 'test_nll': 923.255859375, 'train_mse': 0.0002447341976221651, 'state_dict_file': 'model_state_dict_-7263304326643703012.pkl'}\n",
      "2019-03-17 12:40:53.075294, fold=2, rep=1, eta=0d 12h 57m 15s \n",
      "{'fold': 2, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.04063461348414421, 'train_time': 2647.4436870969366, 'prior_train_nmll': -0.2250477820634842, 'train_nll': -3645.7177734375, 'test_nll': 276.85516357421875, 'train_mse': 0.000702439050655812, 'state_dict_file': 'model_state_dict_-6619036071570073128.pkl'}\n",
      "2019-03-17 13:37:15.706172, fold=3, rep=0, eta=0d 12h 3m 20s \n",
      "{'fold': 3, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.07725167274475098, 'train_time': 3382.62563279795, 'prior_train_nmll': -0.30047792196273804, 'train_nll': -4881.208984375, 'test_nll': 570.321533203125, 'train_mse': 0.0015714741311967373, 'state_dict_file': 'model_state_dict_-2962252920231460936.pkl'}\n",
      "2019-03-17 14:24:18.231790, fold=3, rep=1, eta=0d 10h 54m 47s \n",
      "{'fold': 3, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.08038511127233505, 'train_time': 2822.525262512965, 'prior_train_nmll': -0.3459133505821228, 'train_nll': -3577.41455078125, 'test_nll': 697.0670166015625, 'train_mse': 0.0007970031583681703, 'state_dict_file': 'model_state_dict_8838354834468179926.pkl'}\n",
      "2019-03-17 15:42:30.576952, fold=4, rep=0, eta=0d 10h 29m 7s \n",
      "{'fold': 4, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.038606029003858566, 'train_time': 4692.339256075094, 'prior_train_nmll': -0.24307124316692352, 'train_nll': 17359142.0, 'test_nll': 205.89932250976562, 'train_mse': 0.0007084375247359276, 'state_dict_file': 'model_state_dict_-959622480366835686.pkl'}\n",
      "2019-03-17 16:54:14.889124, fold=4, rep=1, eta=0d 9h 46m 28s \n",
      "{'fold': 4, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.03869284316897392, 'train_time': 4304.311839670991, 'prior_train_nmll': -0.2841443121433258, 'train_nll': 4144864.5, 'test_nll': 129.91220092773438, 'train_mse': 0.0003663790412247181, 'state_dict_file': 'model_state_dict_-6907845755968772654.pkl'}\n",
      "2019-03-17 17:34:22.677978, fold=5, rep=0, eta=0d 8h 32m 40s \n",
      "{'fold': 5, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.023774608969688416, 'train_time': 2407.7836689959513, 'prior_train_nmll': -0.2714884579181671, 'train_nll': -2624.3134765625, 'test_nll': -161.30606079101562, 'train_mse': 0.0026514935307204723, 'state_dict_file': 'model_state_dict_-2892601764377673090.pkl'}\n",
      "2019-03-17 18:29:16.934464, fold=5, rep=1, eta=0d 7h 34m 20s \n",
      "{'fold': 5, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.03314725309610367, 'train_time': 3294.256101410021, 'prior_train_nmll': -0.3017530143260956, 'train_nll': -3751.50048828125, 'test_nll': -128.69046020507812, 'train_mse': 0.0006337125087156892, 'state_dict_file': 'model_state_dict_-679308468912929177.pkl'}\n",
      "2019-03-17 19:41:20.427166, fold=6, rep=0, eta=0d 6h 45m 46s \n",
      "{'fold': 6, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.02278176322579384, 'train_time': 4323.487459921977, 'prior_train_nmll': -0.35856783390045166, 'train_nll': 4883502.5, 'test_nll': -8.55572509765625, 'train_mse': 0.0010587210999801755, 'state_dict_file': 'model_state_dict_9215689806627066507.pkl'}\n",
      "2019-03-17 20:48:41.541746, fold=6, rep=1, eta=0d 5h 51m 49s \n",
      "{'fold': 6, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.02283502370119095, 'train_time': 4041.114244194003, 'prior_train_nmll': -0.332590252161026, 'train_nll': -4862.4970703125, 'test_nll': -51.050933837890625, 'train_mse': 0.0002258984459331259, 'state_dict_file': 'model_state_dict_-4949901887659071017.pkl'}\n",
      "2019-03-17 21:54:42.571690, fold=7, rep=0, eta=0d 4h 55m 38s \n",
      "{'fold': 7, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.04064834862947464, 'train_time': 3961.0245485759806, 'prior_train_nmll': -0.19981729984283447, 'train_nll': 19843.24609375, 'test_nll': -98.81393432617188, 'train_mse': 0.00026631445507518947, 'state_dict_file': 'model_state_dict_-836049127677496450.pkl'}\n",
      "2019-03-17 22:34:35.822693, fold=7, rep=1, eta=0d 3h 51m 42s \n",
      "{'fold': 7, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.019999852403998375, 'train_time': 2393.250603144057, 'prior_train_nmll': -0.2052089124917984, 'train_nll': -2546.52490234375, 'test_nll': -170.84054565429688, 'train_mse': 0.0031697896774858236, 'state_dict_file': 'model_state_dict_-1866157382194768509.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-17 23:45:04.781789, fold=8, rep=0, eta=0d 2h 55m 59s \n",
      "{'fold': 8, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.025813965126872063, 'train_time': 4228.947037538979, 'prior_train_nmll': -0.2600017786026001, 'train_nll': -5229.9189453125, 'test_nll': -94.48910522460938, 'train_mse': 0.0011938211973756552, 'state_dict_file': 'model_state_dict_-655316556385848474.pkl'}\n",
      "2019-03-18 00:26:33.872666, fold=8, rep=1, eta=0d 1h 55m 25s \n",
      "{'fold': 8, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.016700752079486847, 'train_time': 2489.0905095019843, 'prior_train_nmll': -0.1977592408657074, 'train_nll': -2571.8017578125, 'test_nll': -177.2943115234375, 'train_mse': 0.0029408272821456194, 'state_dict_file': 'model_state_dict_-6923132928327695668.pkl'}\n",
      "2019-03-18 01:20:57.018035, fold=9, rep=0, eta=0d 0h 57m 32s \n",
      "{'fold': 9, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.051896993070840836, 'train_time': 3263.136807493982, 'prior_train_nmll': -0.24873818457126617, 'train_nll': -3993.47509765625, 'test_nll': 81.205078125, 'train_mse': 0.0005783208180218935, 'state_dict_file': 'model_state_dict_-8271784189816178677.pkl'}\n",
      "2019-03-18 02:30:30.153492, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.043836578726768494, 'train_time': 4173.135086619994, 'prior_train_nmll': -0.3523392975330353, 'train_nll': 29536.41015625, 'test_nll': 545.309814453125, 'train_mse': 0.0002976350369863212, 'state_dict_file': 'model_state_dict_6559056478429304278.pkl'}\n",
      "2019-03-18 04:03:30.215535, fold=0, rep=0, eta=1d 5h 26m 59s \n",
      "{'fold': 0, 'repeat': 0, 'n': 3338, 'd': 17, 'mse': 0.5871098041534424, 'train_time': 5579.964713861933, 'prior_train_nmll': 0.879579484462738, 'train_nll': 1796.0101318359375, 'test_nll': 426.23162841796875, 'train_mse': 0.16730615496635437, 'state_dict_file': 'model_state_dict_490602208582316812.pkl'}\n",
      "2019-03-18 05:49:10.266575, fold=0, rep=1, eta=1d 5h 48m 0s \n",
      "{'fold': 0, 'repeat': 1, 'n': 3338, 'd': 17, 'mse': 0.7436127662658691, 'train_time': 6340.050643499009, 'prior_train_nmll': 0.8842068910598755, 'train_nll': 1720.028564453125, 'test_nll': 522.05908203125, 'train_mse': 0.15804541110992432, 'state_dict_file': 'model_state_dict_3678717484877942084.pkl'}\n",
      "2019-03-18 08:07:30.449766, fold=1, rep=0, eta=1d 7h 49m 41s \n",
      "{'fold': 1, 'repeat': 0, 'n': 3338, 'd': 17, 'mse': 0.6359672546386719, 'train_time': 8300.177167467074, 'prior_train_nmll': 0.8704572319984436, 'train_nll': 1643.6578369140625, 'test_nll': 492.5637512207031, 'train_mse': 0.1488538235425949, 'state_dict_file': 'model_state_dict_-1364205099322925238.pkl'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-045badb2557c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcg_tolerance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         result = rp_experiments.run_experiment(training_routines.train_additive_rp_gp, options, \n\u001b[0;32m----> 8\u001b[0;31m                                      dataset=dataset, split=0.1, cv=True, repeats=2)\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/rp_experiments.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(training_routine, training_options, dataset, split, cv, addl_metrics, repeats, error_repeats)\u001b[0m\n\u001b[1;32m    136\u001b[0m                     ret = training_routine(trainX, trainY, testX,\n\u001b[1;32m    137\u001b[0m                                                             \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                                                             **training_options)\n\u001b[0m\u001b[1;32m    139\u001b[0m                     \u001b[0mmodel_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                     \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/training_routines.py\u001b[0m in \u001b[0;36mtrain_additive_rp_gp\u001b[0;34m(trainX, trainY, testX, testY, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_additive_rp_gp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_exact_gp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/training_routines.py\u001b[0m in \u001b[0;36mtrain_exact_gp\u001b[0;34m(trainX, trainY, testX, testY, rp, k, J, ard, activation, optimizer, n_epochs, lr, verbose, patience, smooth, noise_prior, ski, grid_ratio, grid_size, learn_weights, additive)\u001b[0m\n\u001b[1;32m    229\u001b[0m                          \u001b[0misloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                          smooth=smooth)\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mtrain_to_convergence\u001b[0;34m(model, xs, ys, optimizer, lr, objective, max_iter, verbose, patience, conv_tol, check_conv, smooth, isloss, batch_size)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch {}, iter {}, loss {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "options = dict(verbose=False, ard=False, activation=None, optimizer='adam',\n",
    "               n_epochs=1000, lr=0.1, patience=20, k=1, J=20, smooth=True, \n",
    "               noise_prior=True, learn_weights=True)\n",
    "datasets = rp_experiments.get_small_datasets() + rp_experiments.get_medium_datasets()\n",
    "for dataset in datasets:\n",
    "    with gpytorch.settings.cg_tolerance(0.01):\n",
    "        result = rp_experiments.run_experiment(training_routines.train_additive_rp_gp, options, \n",
    "                                     dataset=dataset, split=0.1, cv=True, repeats=2)\n",
    "    result['RP'] = True\n",
    "    result['k'] = 1\n",
    "    result['J'] = 20\n",
    "    result['dataset'] = dataset\n",
    "    result['options'] = json.dumps(options)\n",
    "    result['learn_weights'] = True\n",
    "    df = pd.concat([df, result])\n",
    "    df.to_csv('./learning_weights_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:15:33.566136, fold=0, rep=0, eta=0d 0h 2m 6s \n",
      "{'fold': 0, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.7450146079063416, 'train_time': 6.634824585053138, 'prior_train_nmll': 1.5308949947357178, 'train_nll': 28.302593231201172, 'test_nll': 3.8717808723449707, 'train_mse': 0.9912747144699097, 'state_dict_file': 'model_state_dict_2423306063310332892.pkl'}\n",
      "2019-03-18 09:15:39.539527, fold=0, rep=1, eta=0d 0h 1m 53s \n",
      "{'fold': 0, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.7481644153594971, 'train_time': 5.973213240969926, 'prior_train_nmll': 1.5309746265411377, 'train_nll': 28.302841186523438, 'test_nll': 3.8762526512145996, 'train_mse': 0.9911548495292664, 'state_dict_file': 'model_state_dict_542949109278125436.pkl'}\n",
      "2019-03-18 09:15:45.178605, fold=1, rep=0, eta=0d 0h 1m 43s \n",
      "{'fold': 1, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.46983885765075684, 'train_time': 5.636262011015788, 'prior_train_nmll': 1.553665041923523, 'train_nll': 28.738441467285156, 'test_nll': 3.4915037155151367, 'train_mse': 1.0356274843215942, 'state_dict_file': 'model_state_dict_1555281119161994884.pkl'}\n",
      "2019-03-18 09:15:51.380999, fold=1, rep=1, eta=0d 0h 1m 37s \n",
      "{'fold': 1, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.4698532521724701, 'train_time': 6.202207705006003, 'prior_train_nmll': 1.5536164045333862, 'train_nll': 28.740554809570312, 'test_nll': 3.492969512939453, 'train_mse': 1.035938024520874, 'state_dict_file': 'model_state_dict_6775299340391294637.pkl'}\n",
      "2019-03-18 09:15:57.356477, fold=2, rep=0, eta=0d 0h 1m 31s \n",
      "{'fold': 2, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.5309253334999084, 'train_time': 5.972311300924048, 'prior_train_nmll': 1.5451773405075073, 'train_nll': 28.538135528564453, 'test_nll': 3.5717380046844482, 'train_mse': 1.0151714086532593, 'state_dict_file': 'model_state_dict_1883459278433759298.pkl'}\n",
      "2019-03-18 09:16:03.627713, fold=2, rep=1, eta=0d 0h 1m 25s \n",
      "{'fold': 2, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.5309463143348694, 'train_time': 6.271049955044873, 'prior_train_nmll': 1.5452220439910889, 'train_nll': 28.53241539001465, 'test_nll': 3.5722053050994873, 'train_mse': 1.0144990682601929, 'state_dict_file': 'model_state_dict_-3740205190291829145.pkl'}\n",
      "2019-03-18 09:16:10.403051, fold=3, rep=0, eta=0d 0h 1m 20s \n",
      "{'fold': 3, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 2.2893741130828857, 'train_time': 6.771778531954624, 'prior_train_nmll': 1.3977385759353638, 'train_nll': 25.6298885345459, 'test_nll': 6.866178512573242, 'train_mse': 0.7586932182312012, 'state_dict_file': 'model_state_dict_8743557913308060075.pkl'}\n",
      "2019-03-18 09:16:17.342141, fold=3, rep=1, eta=0d 0h 1m 15s \n",
      "{'fold': 3, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 2.302112102508545, 'train_time': 6.938899619039148, 'prior_train_nmll': 1.3977482318878174, 'train_nll': 25.6314640045166, 'test_nll': 6.876904487609863, 'train_mse': 0.7588581442832947, 'state_dict_file': 'model_state_dict_-3679457568055323053.pkl'}\n",
      "2019-03-18 09:16:19.651993, fold=4, rep=0, eta=0d 0h 1m 4s \n",
      "{'fold': 4, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 2.9776837825775146, 'train_time': 2.307023114990443, 'prior_train_nmll': 1.3415464162826538, 'train_nll': 16.39617156982422, 'test_nll': 11.76130485534668, 'train_mse': 0.24155297875404358, 'state_dict_file': 'model_state_dict_-5924260493582590310.pkl'}\n",
      "2019-03-18 09:16:21.829002, fold=4, rep=1, eta=0d 0h 0m 54s \n",
      "{'fold': 4, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 2.983278512954712, 'train_time': 2.176818842999637, 'prior_train_nmll': 1.3416610956192017, 'train_nll': 16.21848487854004, 'test_nll': 11.90133285522461, 'train_mse': 0.2349368780851364, 'state_dict_file': 'model_state_dict_-425915501881039423.pkl'}\n",
      "2019-03-18 09:16:28.088694, fold=5, rep=0, eta=0d 0h 0m 50s \n",
      "{'fold': 5, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.4680953323841095, 'train_time': 6.256830419995822, 'prior_train_nmll': 1.5537104606628418, 'train_nll': 28.755191802978516, 'test_nll': 3.4891889095306396, 'train_mse': 1.0372668504714966, 'state_dict_file': 'model_state_dict_-5548159596058363009.pkl'}\n",
      "2019-03-18 09:16:34.272766, fold=5, rep=1, eta=0d 0h 0m 44s \n",
      "{'fold': 5, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.4681468904018402, 'train_time': 6.1838804699946195, 'prior_train_nmll': 1.5536963939666748, 'train_nll': 28.755287170410156, 'test_nll': 3.489330530166626, 'train_mse': 1.0373051166534424, 'state_dict_file': 'model_state_dict_1528149994897744978.pkl'}\n",
      "2019-03-18 09:16:40.264253, fold=6, rep=0, eta=0d 0h 0m 39s \n",
      "{'fold': 6, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.5254406332969666, 'train_time': 5.988487365073524, 'prior_train_nmll': 1.545160174369812, 'train_nll': 28.575695037841797, 'test_nll': 3.5623152256011963, 'train_mse': 1.0190120935440063, 'state_dict_file': 'model_state_dict_-5483896020727817365.pkl'}\n",
      "2019-03-18 09:16:45.874796, fold=6, rep=1, eta=0d 0h 0m 33s \n",
      "{'fold': 6, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.5254341959953308, 'train_time': 5.610356790013611, 'prior_train_nmll': 1.5451644659042358, 'train_nll': 28.57558822631836, 'test_nll': 3.5627942085266113, 'train_mse': 1.0189907550811768, 'state_dict_file': 'model_state_dict_-6191827001649453446.pkl'}\n",
      "2019-03-18 09:16:51.840028, fold=7, rep=0, eta=0d 0h 0m 28s \n",
      "{'fold': 7, 'repeat': 0, 'n': 23, 'd': 2, 'mse': 0.6130496859550476, 'train_time': 5.962184939999133, 'prior_train_nmll': 1.5238935947418213, 'train_nll': 29.66799545288086, 'test_nll': 2.448551654815674, 'train_mse': 0.9868091344833374, 'state_dict_file': 'model_state_dict_6674307560284668816.pkl'}\n",
      "2019-03-18 09:16:57.916752, fold=7, rep=1, eta=0d 0h 0m 22s \n",
      "{'fold': 7, 'repeat': 1, 'n': 23, 'd': 2, 'mse': 0.6130255460739136, 'train_time': 6.076550308964215, 'prior_train_nmll': 1.523985743522644, 'train_nll': 29.66415786743164, 'test_nll': 2.4485254287719727, 'train_mse': 0.9862755537033081, 'state_dict_file': 'model_state_dict_3775662321884807341.pkl'}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 8, 'n': 23, 'd': 2}\n",
      "errors:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\\n    test_outputs = model(testX)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\\n    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\\n  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\\n    res = test_train_covar.matmul(precomputed_cache)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\\n    func = Matmul(self.representation_tree())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\\n    return LazyTensorRepresentationTree(self.evaluate_kernel())\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\\n    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\\n    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\\n    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\\n    x1_projections = self._project(x1)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\\n    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\\nRuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\\n', 'fold': 9, 'n': 23, 'd': 2}\n",
      "errors:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 138, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/training_routines.py\", line 246, in train_exact_gp\n",
      "    test_outputs = model(testX)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_gp.py\", line 276, in __call__\n",
      "    predictive_mean = self.prediction_strategy.exact_predictive_mean(test_mean, test_train_covar)\n",
      "  File \"/home/ian/gpytorch/gpytorch/models/exact_prediction_strategies.py\", line 291, in exact_predictive_mean\n",
      "    res = test_train_covar.matmul(precomputed_cache)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 806, in matmul\n",
      "    func = Matmul(self.representation_tree())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 238, in representation_tree\n",
      "    return LazyTensorRepresentationTree(self.evaluate_kernel())\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 199, in evaluate_kernel\n",
      "    x1, x2, diag=False, batch_dims=self.batch_dims, **self.params\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/scale_kernel.py\", line 94, in forward\n",
      "    orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/kernels/kernel.py\", line 384, in __call__\n",
      "    res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 87, in forward\n",
      "    x1_projections = self._project(x1)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 70, in _project\n",
      "    linear_projection = x.matmul(self.Ws[j]) + self.bs[j].unsqueeze(0)\n",
      "RuntimeError: invalid argument 13: ldc should be at least max(1, m=0), but have 0 at /opt/conda/conda-bld/pytorch-nightly_1551935369666/work/aten/src/TH/generic/THBlas.cpp:334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:18:51.771028, fold=0, rep=0, eta=0d 0h 1m 37s \n",
      "{'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.8935586810112, 'train_time': 5.104100310942158, 'prior_train_nmll': 1.391670823097229, 'train_nll': 112.65694427490234, 'test_nll': 13.50079345703125, 'train_mse': 0.6764059066772461, 'state_dict_file': 'model_state_dict_3692748986302200016.pkl'}\n",
      "2019-03-18 09:18:54.748095, fold=0, rep=1, eta=0d 0h 1m 12s \n",
      "{'fold': 0, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.8909379839897156, 'train_time': 2.976879597059451, 'prior_train_nmll': 1.3903921842575073, 'train_nll': 112.53953552246094, 'test_nll': 13.489068984985352, 'train_mse': 0.6750819683074951, 'state_dict_file': 'model_state_dict_9187060687997577047.pkl'}\n",
      "2019-03-18 09:18:58.413039, fold=1, rep=0, eta=0d 0h 1m 6s \n",
      "{'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0247604846954346, 'train_time': 3.6616546559380367, 'prior_train_nmll': 1.3803362846374512, 'train_nll': 110.45942687988281, 'test_nll': 14.547926902770996, 'train_mse': 0.6400957703590393, 'state_dict_file': 'model_state_dict_-1499024530798744505.pkl'}\n",
      "2019-03-18 09:19:01.193591, fold=1, rep=1, eta=0d 0h 0m 58s \n",
      "{'fold': 1, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.028132677078247, 'train_time': 2.7803551739780232, 'prior_train_nmll': 1.381598949432373, 'train_nll': 110.69739532470703, 'test_nll': 14.537527084350586, 'train_mse': 0.6429944038391113, 'state_dict_file': 'model_state_dict_-3590692116839351712.pkl'}\n",
      "2019-03-18 09:19:04.084474, fold=2, rep=0, eta=0d 0h 0m 52s \n",
      "{'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.526877760887146, 'train_time': 2.8869426239980385, 'prior_train_nmll': 1.350809931755066, 'train_nll': 103.13055419921875, 'test_nll': 18.476089477539062, 'train_mse': 0.5253902673721313, 'state_dict_file': 'model_state_dict_6143754575612585683.pkl'}\n",
      "2019-03-18 09:19:07.147772, fold=2, rep=1, eta=0d 0h 0m 47s \n",
      "{'fold': 2, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.499345302581787, 'train_time': 3.0631181329954416, 'prior_train_nmll': 1.3457499742507935, 'train_nll': 103.5033950805664, 'test_nll': 18.377775192260742, 'train_mse': 0.5312299728393555, 'state_dict_file': 'model_state_dict_-1586537366010012459.pkl'}\n",
      "2019-03-18 09:19:10.795087, fold=3, rep=0, eta=0d 0h 0m 44s \n",
      "{'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.952713668346405, 'train_time': 3.644136199960485, 'prior_train_nmll': 1.3875694274902344, 'train_nll': 111.4474868774414, 'test_nll': 13.771360397338867, 'train_mse': 0.6568231582641602, 'state_dict_file': 'model_state_dict_7357732425591519423.pkl'}\n",
      "2019-03-18 09:19:13.946879, fold=3, rep=1, eta=0d 0h 0m 40s \n",
      "{'fold': 3, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.9558731317520142, 'train_time': 3.1516019969712943, 'prior_train_nmll': 1.3858307600021362, 'train_nll': 111.83456420898438, 'test_nll': 13.787601470947266, 'train_mse': 0.6611691117286682, 'state_dict_file': 'model_state_dict_2006437241760724802.pkl'}\n",
      "2019-03-18 09:19:17.841977, fold=4, rep=0, eta=0d 0h 0m 38s \n",
      "{'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.24951064586639404, 'train_time': 3.891675909049809, 'prior_train_nmll': 1.4287365674972534, 'train_nll': 115.29966735839844, 'test_nll': 10.038459777832031, 'train_mse': 0.7067891955375671, 'state_dict_file': 'model_state_dict_8606758339310236885.pkl'}\n",
      "2019-03-18 09:19:21.111411, fold=4, rep=1, eta=0d 0h 0m 34s \n",
      "{'fold': 4, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.2490706443786621, 'train_time': 3.2692187969805673, 'prior_train_nmll': 1.43008553981781, 'train_nll': 114.8673324584961, 'test_nll': 9.970601081848145, 'train_mse': 0.70827317237854, 'state_dict_file': 'model_state_dict_-336439955838393503.pkl'}\n",
      "2019-03-18 09:19:25.143384, fold=5, rep=0, eta=0d 0h 0m 31s \n",
      "{'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.6218646168708801, 'train_time': 4.028655098052695, 'prior_train_nmll': 1.407871961593628, 'train_nll': 114.05864715576172, 'test_nll': 12.005081176757812, 'train_mse': 0.6968430876731873, 'state_dict_file': 'model_state_dict_2457747377422161989.pkl'}\n",
      "2019-03-18 09:19:28.405165, fold=5, rep=1, eta=0d 0h 0m 27s \n",
      "{'fold': 5, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.6263669729232788, 'train_time': 3.261517592938617, 'prior_train_nmll': 1.407341480255127, 'train_nll': 114.2808609008789, 'test_nll': 12.046429634094238, 'train_mse': 0.6993473172187805, 'state_dict_file': 'model_state_dict_5664922034502428943.pkl'}\n",
      "2019-03-18 09:19:32.331928, fold=6, rep=0, eta=0d 0h 0m 24s \n",
      "{'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9620896577835083, 'train_time': 3.9227677430026233, 'prior_train_nmll': 1.3829371929168701, 'train_nll': 110.22630310058594, 'test_nll': 14.325834274291992, 'train_mse': 0.635238766670227, 'state_dict_file': 'model_state_dict_-8217572496010748050.pkl'}\n",
      "2019-03-18 09:19:36.506655, fold=6, rep=1, eta=0d 0h 0m 21s \n",
      "{'fold': 6, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.9765125513076782, 'train_time': 4.174529860028997, 'prior_train_nmll': 1.3806625604629517, 'train_nll': 110.87720489501953, 'test_nll': 14.367923736572266, 'train_mse': 0.6489965915679932, 'state_dict_file': 'model_state_dict_6209540744963818705.pkl'}\n",
      "2019-03-18 09:19:39.603196, fold=7, rep=0, eta=0d 0h 0m 17s \n",
      "{'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4728807508945465, 'train_time': 3.092306245933287, 'prior_train_nmll': 1.4152108430862427, 'train_nll': 112.5802001953125, 'test_nll': 11.184165954589844, 'train_mse': 0.6607361435890198, 'state_dict_file': 'model_state_dict_3728140296248089684.pkl'}\n",
      "2019-03-18 09:19:42.479077, fold=7, rep=1, eta=0d 0h 0m 13s \n",
      "{'fold': 7, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.4680701792240143, 'train_time': 2.875701952027157, 'prior_train_nmll': 1.4164743423461914, 'train_nll': 112.43513488769531, 'test_nll': 11.07065200805664, 'train_mse': 0.664053201675415, 'state_dict_file': 'model_state_dict_-1085335863676443518.pkl'}\n",
      "2019-03-18 09:19:45.652803, fold=8, rep=0, eta=0d 0h 0m 10s \n",
      "{'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.414595127105713, 'train_time': 3.169898091000505, 'prior_train_nmll': 1.356185793876648, 'train_nll': 107.3428955078125, 'test_nll': 16.51818084716797, 'train_mse': 0.5912541151046753, 'state_dict_file': 'model_state_dict_-2436515702710127679.pkl'}\n",
      "2019-03-18 09:19:48.581784, fold=8, rep=1, eta=0d 0h 0m 6s \n",
      "{'fold': 8, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 1.4135230779647827, 'train_time': 2.9287878999020904, 'prior_train_nmll': 1.358680009841919, 'train_nll': 107.70106506347656, 'test_nll': 16.495176315307617, 'train_mse': 0.5959476232528687, 'state_dict_file': 'model_state_dict_-7495986679779950560.pkl'}\n",
      "2019-03-18 09:19:52.372817, fold=9, rep=0, eta=0d 0h 0m 3s \n",
      "{'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.5154028534889221, 'train_time': 3.78677869704552, 'prior_train_nmll': 1.4140985012054443, 'train_nll': 113.24482727050781, 'test_nll': 11.355196952819824, 'train_mse': 0.6748090386390686, 'state_dict_file': 'model_state_dict_6852777966358257801.pkl'}\n",
      "2019-03-18 09:19:55.225432, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 100, 'd': 7, 'mse': 0.5255488157272339, 'train_time': 2.8524410489480942, 'prior_train_nmll': 1.4104853868484497, 'train_nll': 112.46784210205078, 'test_nll': 11.386734008789062, 'train_mse': 0.6621494889259338, 'state_dict_file': 'model_state_dict_4919278498328883647.pkl'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/ipykernel_launcher.py:13: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:20:02.942216, fold=0, rep=0, eta=0d 0h 2m 26s \n",
      "{'fold': 0, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.05136743560433388, 'train_time': 7.691906241932884, 'prior_train_nmll': 0.2000836282968521, 'train_nll': -344.28912353515625, 'test_nll': 816.6103515625, 'train_mse': 0.0007207089220173657, 'state_dict_file': 'model_state_dict_-6848597462796387893.pkl'}\n",
      "2019-03-18 09:20:11.440474, fold=0, rep=1, eta=0d 0h 2m 25s \n",
      "{'fold': 0, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.05078882351517677, 'train_time': 8.49806865490973, 'prior_train_nmll': 0.018357422202825546, 'train_nll': -276.9967041015625, 'test_nll': 491.1421203613281, 'train_mse': 0.000729564402718097, 'state_dict_file': 'model_state_dict_-4493286406156117566.pkl'}\n",
      "2019-03-18 09:20:18.941884, fold=1, rep=0, eta=0d 0h 2m 14s \n",
      "{'fold': 1, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.004347425419837236, 'train_time': 7.497752876020968, 'prior_train_nmll': 0.22717668116092682, 'train_nll': 2072058.5, 'test_nll': 1141308.5, 'train_mse': 0.0008780179778113961, 'state_dict_file': 'model_state_dict_6902244195632313536.pkl'}\n",
      "2019-03-18 09:20:25.973440, fold=1, rep=1, eta=0d 0h 2m 2s \n",
      "{'fold': 1, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.004791967570781708, 'train_time': 7.031324315001257, 'prior_train_nmll': 0.2671961784362793, 'train_nll': -198.3792266845703, 'test_nll': -52.341766357421875, 'train_mse': 0.000873226614203304, 'state_dict_file': 'model_state_dict_1461727818237159858.pkl'}\n",
      "2019-03-18 09:20:32.462624, fold=2, rep=0, eta=0d 0h 1m 51s \n",
      "{'fold': 2, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.0033230907283723354, 'train_time': 6.485858166939579, 'prior_train_nmll': 0.242799773812294, 'train_nll': -219.35621643066406, 'test_nll': 6943.291015625, 'train_mse': 0.0008794552995823324, 'state_dict_file': 'model_state_dict_-1640733244509004210.pkl'}\n",
      "2019-03-18 09:20:39.674328, fold=2, rep=1, eta=0d 0h 1m 43s \n",
      "{'fold': 2, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.002988280961290002, 'train_time': 7.211526133003645, 'prior_train_nmll': 0.2901068329811096, 'train_nll': 405424.25, 'test_nll': 103.80422973632812, 'train_mse': 0.0008679372258484364, 'state_dict_file': 'model_state_dict_-2814338303557259918.pkl'}\n",
      "2019-03-18 09:20:46.358030, fold=3, rep=0, eta=0d 0h 1m 34s \n",
      "{'fold': 3, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.0031786260660737753, 'train_time': 6.680439756019041, 'prior_train_nmll': 0.2963576316833496, 'train_nll': -434.78485107421875, 'test_nll': -20.50843048095703, 'train_mse': 0.0009321143734268844, 'state_dict_file': 'model_state_dict_-7810609476839142939.pkl'}\n",
      "2019-03-18 09:20:53.178385, fold=3, rep=1, eta=0d 0h 1m 26s \n",
      "{'fold': 3, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.0034727565944194794, 'train_time': 6.820168631966226, 'prior_train_nmll': 0.18942950665950775, 'train_nll': -277.244140625, 'test_nll': 34.556480407714844, 'train_mse': 0.0008970918715931475, 'state_dict_file': 'model_state_dict_-7211371234760130451.pkl'}\n",
      "2019-03-18 09:21:00.144228, fold=4, rep=0, eta=0d 0h 1m 19s \n",
      "{'fold': 4, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.01623060368001461, 'train_time': 6.962566895992495, 'prior_train_nmll': 0.14966975152492523, 'train_nll': 337.81365966796875, 'test_nll': 45.6011962890625, 'train_mse': 0.000958446238655597, 'state_dict_file': 'model_state_dict_-5134349764832491272.pkl'}\n",
      "2019-03-18 09:21:07.007370, fold=4, rep=1, eta=0d 0h 1m 11s \n",
      "{'fold': 4, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.01925855129957199, 'train_time': 6.862936130957678, 'prior_train_nmll': 0.29505184292793274, 'train_nll': -260.6634521484375, 'test_nll': -28.506013870239258, 'train_mse': 0.0009639417985454202, 'state_dict_file': 'model_state_dict_1616917841713428254.pkl'}\n",
      "2019-03-18 09:21:13.927579, fold=5, rep=0, eta=0d 0h 1m 4s \n",
      "{'fold': 5, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.009447571821510792, 'train_time': 6.916998522006907, 'prior_train_nmll': 0.32211965322494507, 'train_nll': -359.919921875, 'test_nll': -20.465761184692383, 'train_mse': 0.000874713936354965, 'state_dict_file': 'model_state_dict_4731876008555956330.pkl'}\n",
      "2019-03-18 09:21:21.354259, fold=5, rep=1, eta=0d 0h 0m 57s \n",
      "{'fold': 5, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.009899908676743507, 'train_time': 7.426502356072888, 'prior_train_nmll': 0.2893064618110657, 'train_nll': -63.86799621582031, 'test_nll': 67.15229034423828, 'train_mse': 0.0008942899294197559, 'state_dict_file': 'model_state_dict_-5450461997723410158.pkl'}\n",
      "2019-03-18 09:21:28.791410, fold=6, rep=0, eta=0d 0h 0m 50s \n",
      "{'fold': 6, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.01923227496445179, 'train_time': 7.4331854389747605, 'prior_train_nmll': 0.15737062692642212, 'train_nll': -226.9586944580078, 'test_nll': -10.290878295898438, 'train_mse': 0.000632475595921278, 'state_dict_file': 'model_state_dict_9056638454741036821.pkl'}\n",
      "2019-03-18 09:21:35.797416, fold=6, rep=1, eta=0d 0h 0m 43s \n",
      "{'fold': 6, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.018677756190299988, 'train_time': 7.005826011067256, 'prior_train_nmll': 0.2665204703807831, 'train_nll': -161.22714233398438, 'test_nll': 10.946352005004883, 'train_mse': 0.0006410711212083697, 'state_dict_file': 'model_state_dict_-4309629758112397642.pkl'}\n",
      "2019-03-18 09:21:44.012636, fold=7, rep=0, eta=0d 0h 0m 36s \n",
      "{'fold': 7, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.06638502329587936, 'train_time': 8.211748477071524, 'prior_train_nmll': -0.016711007803678513, 'train_nll': 1848.3590087890625, 'test_nll': 13.033727645874023, 'train_mse': 0.0071387700736522675, 'state_dict_file': 'model_state_dict_-1076686800572998361.pkl'}\n",
      "2019-03-18 09:21:52.047852, fold=7, rep=1, eta=0d 0h 0m 29s \n",
      "{'fold': 7, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.08815901726484299, 'train_time': 8.035023916047066, 'prior_train_nmll': 0.0859643891453743, 'train_nll': -145.14674377441406, 'test_nll': 2415886.75, 'train_mse': 0.00593525217846036, 'state_dict_file': 'model_state_dict_8679694167481580324.pkl'}\n",
      "2019-03-18 09:21:59.373435, fold=8, rep=0, eta=0d 0h 0m 21s \n",
      "{'fold': 8, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.037296686321496964, 'train_time': 7.321580818039365, 'prior_train_nmll': 0.11365784704685211, 'train_nll': -388.45867919921875, 'test_nll': -77.77198028564453, 'train_mse': 0.000882828957401216, 'state_dict_file': 'model_state_dict_8436627879407837022.pkl'}\n",
      "2019-03-18 09:22:06.023387, fold=8, rep=1, eta=0d 0h 0m 14s \n",
      "{'fold': 8, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.03737775236368179, 'train_time': 6.649736374965869, 'prior_train_nmll': 0.1428968459367752, 'train_nll': -269.14044189453125, 'test_nll': 26.857009887695312, 'train_mse': 0.0008506851154379547, 'state_dict_file': 'model_state_dict_-7349457652673433092.pkl'}\n",
      "2019-03-18 09:22:11.683840, fold=9, rep=0, eta=0d 0h 0m 7s \n",
      "{'fold': 9, 'repeat': 0, 'n': 103, 'd': 5, 'mse': 0.018588215112686157, 'train_time': 5.656307109049521, 'prior_train_nmll': 0.08243485540151596, 'train_nll': -445.50714111328125, 'test_nll': 314394.6875, 'train_mse': 0.0009023280581459403, 'state_dict_file': 'model_state_dict_4054429709077646793.pkl'}\n",
      "2019-03-18 09:22:19.249425, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 103, 'd': 5, 'mse': 0.020073147490620613, 'train_time': 7.56540260091424, 'prior_train_nmll': 0.18450798094272614, 'train_nll': 10311852.0, 'test_nll': 239349.6875, 'train_mse': 0.0008858034270815551, 'state_dict_file': 'model_state_dict_-5898062570755547219.pkl'}\n",
      "2019-03-18 09:22:28.284218, fold=0, rep=0, eta=0d 0h 2m 51s \n",
      "{'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.05339839309453964, 'train_time': 9.013824168010615, 'prior_train_nmll': 0.4798189699649811, 'train_nll': -18.997970581054688, 'test_nll': 0.4470376968383789, 'train_mse': 0.03204067796468735, 'state_dict_file': 'model_state_dict_-6445739109542314970.pkl'}\n",
      "2019-03-18 09:22:37.345778, fold=0, rep=1, eta=0d 0h 2m 42s \n",
      "{'fold': 0, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.052755046635866165, 'train_time': 9.061338590923697, 'prior_train_nmll': 0.4855406880378723, 'train_nll': -17.884140014648438, 'test_nll': 0.3609428405761719, 'train_mse': 0.03242020308971405, 'state_dict_file': 'model_state_dict_-871838665718634311.pkl'}\n",
      "2019-03-18 09:22:50.462771, fold=1, rep=0, eta=0d 0h 2m 56s \n",
      "{'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.12794151902198792, 'train_time': 13.113907455001026, 'prior_train_nmll': 0.4608578383922577, 'train_nll': -19.4954833984375, 'test_nll': 3.203852653503418, 'train_mse': 0.03252750635147095, 'state_dict_file': 'model_state_dict_-604881760518698646.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:23:03.793906, fold=1, rep=1, eta=0d 0h 2m 58s \n",
      "{'fold': 1, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.13037388026714325, 'train_time': 13.330934713943861, 'prior_train_nmll': 0.47175970673561096, 'train_nll': -19.578628540039062, 'test_nll': 3.395488739013672, 'train_mse': 0.033251501619815826, 'state_dict_file': 'model_state_dict_4518166015571320666.pkl'}\n",
      "2019-03-18 09:23:12.513463, fold=2, rep=0, eta=0d 0h 2m 39s \n",
      "{'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.086161307990551, 'train_time': 8.716199649963528, 'prior_train_nmll': 0.48973703384399414, 'train_nll': -19.758056640625, 'test_nll': 1.7799491882324219, 'train_mse': 0.031682614237070084, 'state_dict_file': 'model_state_dict_255722711828547415.pkl'}\n",
      "2019-03-18 09:23:20.955946, fold=2, rep=1, eta=0d 0h 2m 23s \n",
      "{'fold': 2, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.08619024604558945, 'train_time': 8.44177742197644, 'prior_train_nmll': 0.4633810520172119, 'train_nll': -20.608261108398438, 'test_nll': 1.7349042892456055, 'train_mse': 0.03145406022667885, 'state_dict_file': 'model_state_dict_-2930159302343228571.pkl'}\n",
      "2019-03-18 09:23:30.245835, fold=3, rep=0, eta=0d 0h 2m 11s \n",
      "{'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.08316391706466675, 'train_time': 9.286766250035726, 'prior_train_nmll': 0.4616009294986725, 'train_nll': -23.960494995117188, 'test_nll': 3.1591577529907227, 'train_mse': 0.029801592230796814, 'state_dict_file': 'model_state_dict_4500306779865626715.pkl'}\n",
      "2019-03-18 09:23:38.920443, fold=3, rep=1, eta=0d 0h 1m 59s \n",
      "{'fold': 3, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.08400129526853561, 'train_time': 8.674326892010868, 'prior_train_nmll': 0.4372474253177643, 'train_nll': -25.5953369140625, 'test_nll': 3.2424259185791016, 'train_mse': 0.029159633442759514, 'state_dict_file': 'model_state_dict_-7537342784198181896.pkl'}\n",
      "2019-03-18 09:23:46.613759, fold=4, rep=0, eta=0d 0h 1m 46s \n",
      "{'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.11428079009056091, 'train_time': 7.690048202057369, 'prior_train_nmll': 0.4522971212863922, 'train_nll': -37.20201110839844, 'test_nll': 9.135089874267578, 'train_mse': 0.02394106425344944, 'state_dict_file': 'model_state_dict_-1583235698056216534.pkl'}\n",
      "2019-03-18 09:23:56.124668, fold=4, rep=1, eta=0d 0h 1m 36s \n",
      "{'fold': 4, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.11457902938127518, 'train_time': 9.510655391030014, 'prior_train_nmll': 0.425262451171875, 'train_nll': -36.167327880859375, 'test_nll': 9.25457763671875, 'train_mse': 0.024726996198296547, 'state_dict_file': 'model_state_dict_6335921402227210988.pkl'}\n",
      "2019-03-18 09:24:05.458048, fold=5, rep=0, eta=0d 0h 1m 26s \n",
      "{'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.09550265222787857, 'train_time': 9.330276967957616, 'prior_train_nmll': 0.48793888092041016, 'train_nll': -20.616928100585938, 'test_nll': 3.1042680740356445, 'train_mse': 0.0315198078751564, 'state_dict_file': 'model_state_dict_768258590684987609.pkl'}\n",
      "2019-03-18 09:24:14.858062, fold=5, rep=1, eta=0d 0h 1m 17s \n",
      "{'fold': 5, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.0955175831913948, 'train_time': 9.399775400059298, 'prior_train_nmll': 0.4788217544555664, 'train_nll': -19.203353881835938, 'test_nll': 3.0617761611938477, 'train_mse': 0.031960759311914444, 'state_dict_file': 'model_state_dict_-3539401552831179863.pkl'}\n",
      "2019-03-18 09:24:24.287041, fold=6, rep=0, eta=0d 0h 1m 7s \n",
      "{'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.06821160018444061, 'train_time': 9.425806674058549, 'prior_train_nmll': 0.49142295122146606, 'train_nll': -19.0460205078125, 'test_nll': -0.08542251586914062, 'train_mse': 0.032082699239254, 'state_dict_file': 'model_state_dict_-6576862373625504891.pkl'}\n",
      "2019-03-18 09:24:34.909872, fold=6, rep=1, eta=0d 0h 0m 58s \n",
      "{'fold': 6, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.06865194439888, 'train_time': 10.622581767034717, 'prior_train_nmll': 0.4729750454425812, 'train_nll': -18.78424072265625, 'test_nll': -0.29856109619140625, 'train_mse': 0.03157782182097435, 'state_dict_file': 'model_state_dict_-4335562361354261577.pkl'}\n",
      "2019-03-18 09:24:43.126047, fold=7, rep=0, eta=0d 0h 0m 47s \n",
      "{'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.13275405764579773, 'train_time': 8.212993789929897, 'prior_train_nmll': 0.46087032556533813, 'train_nll': -24.1768798828125, 'test_nll': 6.557415008544922, 'train_mse': 0.030328137800097466, 'state_dict_file': 'model_state_dict_5552992774546188639.pkl'}\n",
      "2019-03-18 09:24:57.114919, fold=7, rep=1, eta=0d 0h 0m 39s \n",
      "{'fold': 7, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.1303509771823883, 'train_time': 13.988630487001501, 'prior_train_nmll': 0.4599059224128723, 'train_nll': -23.98846435546875, 'test_nll': 6.522014617919922, 'train_mse': 0.02978772111237049, 'state_dict_file': 'model_state_dict_879188899149918471.pkl'}\n",
      "2019-03-18 09:25:10.012900, fold=8, rep=0, eta=0d 0h 0m 30s \n",
      "{'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.31394872069358826, 'train_time': 12.8940589010017, 'prior_train_nmll': 0.43628063797950745, 'train_nll': -13.794967651367188, 'test_nll': 6.610401153564453, 'train_mse': 0.03655043989419937, 'state_dict_file': 'model_state_dict_2346444876504478089.pkl'}\n",
      "2019-03-18 09:25:23.767102, fold=8, rep=1, eta=0d 0h 0m 20s \n",
      "{'fold': 8, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.3126642107963562, 'train_time': 13.753917644964531, 'prior_train_nmll': 0.45764511823654175, 'train_nll': -14.061614990234375, 'test_nll': 6.535600662231445, 'train_mse': 0.03628530725836754, 'state_dict_file': 'model_state_dict_-1614076644338949804.pkl'}\n",
      "2019-03-18 09:25:33.039680, fold=9, rep=0, eta=0d 0h 0m 10s \n",
      "{'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.05772441625595093, 'train_time': 9.268883878947236, 'prior_train_nmll': 0.4681326448917389, 'train_nll': -15.631088256835938, 'test_nll': 0.4504890441894531, 'train_mse': 0.03320324420928955, 'state_dict_file': 'model_state_dict_-5720010668140461623.pkl'}\n",
      "2019-03-18 09:25:44.219934, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 159, 'd': 22, 'mse': 0.057853080332279205, 'train_time': 11.180031545925885, 'prior_train_nmll': 0.45136240124702454, 'train_nll': -18.998382568359375, 'test_nll': 0.23655223846435547, 'train_mse': 0.03321466222405434, 'state_dict_file': 'model_state_dict_-5131423830824592479.pkl'}\n",
      "2019-03-18 09:25:48.680218, fold=0, rep=0, eta=0d 0h 1m 24s \n",
      "{'fold': 0, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.1264042854309082, 'train_time': 4.4358962720725685, 'prior_train_nmll': 0.7152093052864075, 'train_nll': 73.14045715332031, 'test_nll': 6.821469306945801, 'train_mse': 0.144584521651268, 'state_dict_file': 'model_state_dict_-9032126881886879197.pkl'}\n",
      "2019-03-18 09:25:52.655014, fold=0, rep=1, eta=0d 0h 1m 15s \n",
      "{'fold': 0, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.12658293545246124, 'train_time': 3.974587087985128, 'prior_train_nmll': 0.7197538018226624, 'train_nll': 73.3531265258789, 'test_nll': 6.826662063598633, 'train_mse': 0.14462830126285553, 'state_dict_file': 'model_state_dict_1939589459481236387.pkl'}\n",
      "2019-03-18 09:25:57.364848, fold=1, rep=0, eta=0d 0h 1m 14s \n",
      "{'fold': 1, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.3835367262363434, 'train_time': 4.706580898026004, 'prior_train_nmll': 0.636910617351532, 'train_nll': 58.127418518066406, 'test_nll': 20.128555297851562, 'train_mse': 0.11808440834283829, 'state_dict_file': 'model_state_dict_2320800875680570847.pkl'}\n",
      "2019-03-18 09:26:01.128969, fold=1, rep=1, eta=0d 0h 1m 7s \n",
      "{'fold': 1, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.3816453218460083, 'train_time': 3.763934595976025, 'prior_train_nmll': 0.6382912993431091, 'train_nll': 58.141624450683594, 'test_nll': 19.924442291259766, 'train_mse': 0.11818601191043854, 'state_dict_file': 'model_state_dict_-2463881956702015933.pkl'}\n",
      "2019-03-18 09:26:05.578909, fold=2, rep=0, eta=0d 0h 1m 4s \n",
      "{'fold': 2, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.21161986887454987, 'train_time': 4.446794994990341, 'prior_train_nmll': 0.69020015001297, 'train_nll': 69.30290985107422, 'test_nll': 10.509278297424316, 'train_mse': 0.13730557262897491, 'state_dict_file': 'model_state_dict_2358161959540419819.pkl'}\n",
      "2019-03-18 09:26:09.443947, fold=2, rep=1, eta=0d 0h 0m 58s \n",
      "{'fold': 2, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.21193304657936096, 'train_time': 3.8648607700597495, 'prior_train_nmll': 0.6940714120864868, 'train_nll': 69.16776275634766, 'test_nll': 10.476583480834961, 'train_mse': 0.13709065318107605, 'state_dict_file': 'model_state_dict_3145936442663195039.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:26:13.406246, fold=3, rep=0, eta=0d 0h 0m 54s \n",
      "{'fold': 3, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.2015819251537323, 'train_time': 3.959163842955604, 'prior_train_nmll': 0.6923119425773621, 'train_nll': 69.32673645019531, 'test_nll': 10.27140998840332, 'train_mse': 0.13707730174064636, 'state_dict_file': 'model_state_dict_-6544886406151472305.pkl'}\n",
      "2019-03-18 09:26:17.305704, fold=3, rep=1, eta=0d 0h 0m 49s \n",
      "{'fold': 3, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.20151694118976593, 'train_time': 3.8992448491044343, 'prior_train_nmll': 0.6968884468078613, 'train_nll': 69.27288055419922, 'test_nll': 10.268391609191895, 'train_mse': 0.13704656064510345, 'state_dict_file': 'model_state_dict_-4304938581820740442.pkl'}\n",
      "2019-03-18 09:26:20.794915, fold=4, rep=0, eta=0d 0h 0m 44s \n",
      "{'fold': 4, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.12455129623413086, 'train_time': 3.4860777070280164, 'prior_train_nmll': 0.7178508639335632, 'train_nll': 73.35645294189453, 'test_nll': 6.782876014709473, 'train_mse': 0.14483188092708588, 'state_dict_file': 'model_state_dict_7929369200058880889.pkl'}\n",
      "2019-03-18 09:26:24.748171, fold=4, rep=1, eta=0d 0h 0m 40s \n",
      "{'fold': 4, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.12526284158229828, 'train_time': 3.9530202250462025, 'prior_train_nmll': 0.7180538177490234, 'train_nll': 73.31993865966797, 'test_nll': 6.884514808654785, 'train_mse': 0.1449524611234665, 'state_dict_file': 'model_state_dict_7584555472572870387.pkl'}\n",
      "2019-03-18 09:26:27.945169, fold=5, rep=0, eta=0d 0h 0m 35s \n",
      "{'fold': 5, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.16795402765274048, 'train_time': 3.1934040939668193, 'prior_train_nmll': 0.707126796245575, 'train_nll': 71.7867202758789, 'test_nll': 8.452974319458008, 'train_mse': 0.14108599722385406, 'state_dict_file': 'model_state_dict_1246978512404388325.pkl'}\n",
      "2019-03-18 09:26:30.721386, fold=5, rep=1, eta=0d 0h 0m 30s \n",
      "{'fold': 5, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.16825073957443237, 'train_time': 2.7760032329242676, 'prior_train_nmll': 0.7031847238540649, 'train_nll': 71.66761016845703, 'test_nll': 8.510497093200684, 'train_mse': 0.14113298058509827, 'state_dict_file': 'model_state_dict_-4768926314193155557.pkl'}\n",
      "2019-03-18 09:26:35.514443, fold=6, rep=0, eta=0d 0h 0m 27s \n",
      "{'fold': 6, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.08867429196834564, 'train_time': 4.789923483040184, 'prior_train_nmll': 0.7289161682128906, 'train_nll': 74.60079193115234, 'test_nll': 5.242175102233887, 'train_mse': 0.1473310887813568, 'state_dict_file': 'model_state_dict_7924799629018876042.pkl'}\n",
      "2019-03-18 09:26:39.131109, fold=6, rep=1, eta=0d 0h 0m 23s \n",
      "{'fold': 6, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.08853336423635483, 'train_time': 3.616487749037333, 'prior_train_nmll': 0.727264404296875, 'train_nll': 74.71761322021484, 'test_nll': 5.291323661804199, 'train_mse': 0.1474343240261078, 'state_dict_file': 'model_state_dict_6105525524572049334.pkl'}\n",
      "2019-03-18 09:26:42.946100, fold=7, rep=0, eta=0d 0h 0m 19s \n",
      "{'fold': 7, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.1304914653301239, 'train_time': 3.811796652036719, 'prior_train_nmll': 0.7132013440132141, 'train_nll': 72.0532455444336, 'test_nll': 7.275860786437988, 'train_mse': 0.14215096831321716, 'state_dict_file': 'model_state_dict_-3376787196715153828.pkl'}\n",
      "2019-03-18 09:26:46.576946, fold=7, rep=1, eta=0d 0h 0m 15s \n",
      "{'fold': 7, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.13074783980846405, 'train_time': 3.6306711529614404, 'prior_train_nmll': 0.7123491764068604, 'train_nll': 71.90689849853516, 'test_nll': 7.258096694946289, 'train_mse': 0.14208677411079407, 'state_dict_file': 'model_state_dict_909956943306339092.pkl'}\n",
      "2019-03-18 09:26:51.108202, fold=8, rep=0, eta=0d 0h 0m 11s \n",
      "{'fold': 8, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.1364283263683319, 'train_time': 4.528047191910446, 'prior_train_nmll': 0.7154162526130676, 'train_nll': 72.509765625, 'test_nll': 7.280692100524902, 'train_mse': 0.14294013381004333, 'state_dict_file': 'model_state_dict_-2510991736045298452.pkl'}\n",
      "2019-03-18 09:26:56.131744, fold=8, rep=1, eta=0d 0h 0m 7s \n",
      "{'fold': 8, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.1364237517118454, 'train_time': 5.023311477969401, 'prior_train_nmll': 0.7154595255851746, 'train_nll': 72.28913116455078, 'test_nll': 7.291693687438965, 'train_mse': 0.142930269241333, 'state_dict_file': 'model_state_dict_-5513445404974853537.pkl'}\n",
      "2019-03-18 09:27:00.599956, fold=9, rep=0, eta=0d 0h 0m 4s \n",
      "{'fold': 9, 'repeat': 0, 'n': 167, 'd': 2, 'mse': 0.28497838973999023, 'train_time': 4.464923026040196, 'prior_train_nmll': 0.665922224521637, 'train_nll': 66.75214385986328, 'test_nll': 12.331635475158691, 'train_mse': 0.12978075444698334, 'state_dict_file': 'model_state_dict_-8362708840241191332.pkl'}\n",
      "2019-03-18 09:27:05.932271, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 167, 'd': 2, 'mse': 0.2843576967716217, 'train_time': 5.332106837071478, 'prior_train_nmll': 0.6660709381103516, 'train_nll': 66.27752685546875, 'test_nll': 12.282082557678223, 'train_mse': 0.12984450161457062, 'state_dict_file': 'model_state_dict_5242141093393857035.pkl'}\n",
      "2019-03-18 09:27:14.527242, fold=0, rep=0, eta=0d 0h 2m 42s \n",
      "{'fold': 0, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.6464399099349976, 'train_time': 8.573756246012636, 'prior_train_nmll': 1.3892782926559448, 'train_nll': 214.41285705566406, 'test_nll': 24.30893325805664, 'train_mse': 0.643894374370575, 'state_dict_file': 'model_state_dict_-8673717140372487079.pkl'}\n",
      "2019-03-18 09:27:25.358103, fold=0, rep=1, eta=0d 0h 2m 54s \n",
      "{'fold': 0, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.6456320881843567, 'train_time': 10.830618905951269, 'prior_train_nmll': 1.3852958679199219, 'train_nll': 213.728515625, 'test_nll': 24.3116512298584, 'train_mse': 0.6418510675430298, 'state_dict_file': 'model_state_dict_-4207977575660627928.pkl'}\n",
      "2019-03-18 09:27:34.204574, fold=1, rep=0, eta=0d 0h 2m 40s \n",
      "{'fold': 1, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.786089301109314, 'train_time': 8.843333662953228, 'prior_train_nmll': 1.374773621559143, 'train_nll': 210.08322143554688, 'test_nll': 26.025928497314453, 'train_mse': 0.6018120050430298, 'state_dict_file': 'model_state_dict_-5811633382014328011.pkl'}\n",
      "2019-03-18 09:27:41.433800, fold=1, rep=1, eta=0d 0h 2m 21s \n",
      "{'fold': 1, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.7864206433296204, 'train_time': 7.228948030038737, 'prior_train_nmll': 1.38120436668396, 'train_nll': 210.11614990234375, 'test_nll': 26.01101303100586, 'train_mse': 0.6039406061172485, 'state_dict_file': 'model_state_dict_7114667013249266960.pkl'}\n",
      "2019-03-18 09:27:49.954748, fold=2, rep=0, eta=0d 0h 2m 12s \n",
      "{'fold': 2, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.3083194494247437, 'train_time': 8.517049388028681, 'prior_train_nmll': 1.3493571281433105, 'train_nll': 196.48751831054688, 'test_nll': 31.986696243286133, 'train_mse': 0.4968078136444092, 'state_dict_file': 'model_state_dict_6022665882727957469.pkl'}\n",
      "2019-03-18 09:27:58.466387, fold=2, rep=1, eta=0d 0h 2m 2s \n",
      "{'fold': 2, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.2731235027313232, 'train_time': 8.510988579015248, 'prior_train_nmll': 1.3428623676300049, 'train_nll': 199.13502502441406, 'test_nll': 31.56992530822754, 'train_mse': 0.5246541500091553, 'state_dict_file': 'model_state_dict_1424552467950788054.pkl'}\n",
      "2019-03-18 09:28:07.920750, fold=3, rep=0, eta=0d 0h 1m 55s \n",
      "{'fold': 3, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.6587462425231934, 'train_time': 9.451057987986133, 'prior_train_nmll': 1.3871458768844604, 'train_nll': 217.31094360351562, 'test_nll': 24.476089477539062, 'train_mse': 0.6723073720932007, 'state_dict_file': 'model_state_dict_-8207687601853740496.pkl'}\n",
      "2019-03-18 09:28:20.922975, fold=3, rep=1, eta=0d 0h 1m 52s \n",
      "{'fold': 3, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.6585478186607361, 'train_time': 13.001921177958138, 'prior_train_nmll': 1.3866386413574219, 'train_nll': 217.63804626464844, 'test_nll': 24.430973052978516, 'train_mse': 0.6714490056037903, 'state_dict_file': 'model_state_dict_6009854870344617664.pkl'}\n",
      "2019-03-18 09:28:30.973505, fold=4, rep=0, eta=0d 0h 1m 43s \n",
      "{'fold': 4, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.6769680976867676, 'train_time': 10.047294310992584, 'prior_train_nmll': 1.382656455039978, 'train_nll': 213.2915802001953, 'test_nll': 24.824731826782227, 'train_mse': 0.6355994343757629, 'state_dict_file': 'model_state_dict_169465227665418905.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:28:40.488146, fold=4, rep=1, eta=0d 0h 1m 34s \n",
      "{'fold': 4, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.6770986318588257, 'train_time': 9.514366739080288, 'prior_train_nmll': 1.3847126960754395, 'train_nll': 212.77223205566406, 'test_nll': 24.821760177612305, 'train_mse': 0.6357473731040955, 'state_dict_file': 'model_state_dict_-8918961977065603595.pkl'}\n",
      "2019-03-18 09:28:47.444838, fold=5, rep=0, eta=0d 0h 1m 23s \n",
      "{'fold': 5, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.8302459716796875, 'train_time': 6.953424299950711, 'prior_train_nmll': 1.3715314865112305, 'train_nll': 207.97181701660156, 'test_nll': 26.716846466064453, 'train_mse': 0.5976935029029846, 'state_dict_file': 'model_state_dict_7904795667819012178.pkl'}\n",
      "2019-03-18 09:28:56.141471, fold=5, rep=1, eta=0d 0h 1m 13s \n",
      "{'fold': 5, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.8305532336235046, 'train_time': 8.69639510801062, 'prior_train_nmll': 1.370328664779663, 'train_nll': 207.8468017578125, 'test_nll': 26.719053268432617, 'train_mse': 0.5926002264022827, 'state_dict_file': 'model_state_dict_6308514129451600389.pkl'}\n",
      "2019-03-18 09:29:03.038670, fold=6, rep=0, eta=0d 0h 1m 3s \n",
      "{'fold': 6, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.5512961149215698, 'train_time': 6.893540819990449, 'prior_train_nmll': 1.3889998197555542, 'train_nll': 212.5137481689453, 'test_nll': 23.358781814575195, 'train_mse': 0.6296207904815674, 'state_dict_file': 'model_state_dict_-8583514380686190081.pkl'}\n",
      "2019-03-18 09:29:11.754668, fold=6, rep=1, eta=0d 0h 0m 53s \n",
      "{'fold': 6, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.5524905920028687, 'train_time': 8.715285483049229, 'prior_train_nmll': 1.3909770250320435, 'train_nll': 211.20301818847656, 'test_nll': 23.389678955078125, 'train_mse': 0.6163881421089172, 'state_dict_file': 'model_state_dict_5956426855982432395.pkl'}\n",
      "2019-03-18 09:29:21.439333, fold=7, rep=0, eta=0d 0h 0m 45s \n",
      "{'fold': 7, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 1.1557165384292603, 'train_time': 9.68038207408972, 'prior_train_nmll': 1.349603533744812, 'train_nll': 203.37454223632812, 'test_nll': 30.67349624633789, 'train_mse': 0.5596374869346619, 'state_dict_file': 'model_state_dict_-7591150367747547730.pkl'}\n",
      "2019-03-18 09:29:30.010249, fold=7, rep=1, eta=0d 0h 0m 36s \n",
      "{'fold': 7, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 1.1550166606903076, 'train_time': 8.570392303983681, 'prior_train_nmll': 1.3575583696365356, 'train_nll': 202.9656982421875, 'test_nll': 30.674575805664062, 'train_mse': 0.5527452230453491, 'state_dict_file': 'model_state_dict_4423282486945999397.pkl'}\n",
      "2019-03-18 09:29:38.548979, fold=8, rep=0, eta=0d 0h 0m 26s \n",
      "{'fold': 8, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.9511683583259583, 'train_time': 8.53531569801271, 'prior_train_nmll': 1.3692976236343384, 'train_nll': 210.15213012695312, 'test_nll': 27.832170486450195, 'train_mse': 0.6040816307067871, 'state_dict_file': 'model_state_dict_5017598705234355001.pkl'}\n",
      "2019-03-18 09:29:50.828500, fold=8, rep=1, eta=0d 0h 0m 18s \n",
      "{'fold': 8, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.9538860321044922, 'train_time': 12.278806600021198, 'prior_train_nmll': 1.3639369010925293, 'train_nll': 206.0755157470703, 'test_nll': 27.84652328491211, 'train_mse': 0.5794257521629333, 'state_dict_file': 'model_state_dict_868870528913673599.pkl'}\n",
      "2019-03-18 09:29:59.452441, fold=9, rep=0, eta=0d 0h 0m 9s \n",
      "{'fold': 9, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.9829001426696777, 'train_time': 8.620049600955099, 'prior_train_nmll': 1.357844591140747, 'train_nll': 209.19485473632812, 'test_nll': 20.26852798461914, 'train_mse': 0.5490240454673767, 'state_dict_file': 'model_state_dict_8837646108565134206.pkl'}\n",
      "2019-03-18 09:30:09.261415, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 194, 'd': 31, 'mse': 0.9760004878044128, 'train_time': 9.80871869600378, 'prior_train_nmll': 1.369264841079712, 'train_nll': 212.0331268310547, 'test_nll': 20.179445266723633, 'train_mse': 0.5585628747940063, 'state_dict_file': 'model_state_dict_-3928162366028288090.pkl'}\n",
      "2019-03-18 09:30:14.919622, fold=0, rep=0, eta=0d 0h 1m 46s \n",
      "{'fold': 0, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.10767967253923416, 'train_time': 5.6276977120433, 'prior_train_nmll': 0.643223226070404, 'train_nll': 76.66565704345703, 'test_nll': 7.758721351623535, 'train_mse': 0.12154480069875717, 'state_dict_file': 'model_state_dict_-8187094322658523574.pkl'}\n",
      "2019-03-18 09:30:20.144890, fold=0, rep=1, eta=0d 0h 1m 37s \n",
      "{'fold': 0, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.10780950635671616, 'train_time': 5.2250869979616255, 'prior_train_nmll': 0.6312511563301086, 'train_nll': 77.27466583251953, 'test_nll': 7.682100296020508, 'train_mse': 0.12131432443857193, 'state_dict_file': 'model_state_dict_1515127887920658091.pkl'}\n",
      "2019-03-18 09:30:25.341607, fold=1, rep=0, eta=0d 0h 1m 30s \n",
      "{'fold': 1, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.23424772918224335, 'train_time': 5.193674386013299, 'prior_train_nmll': 0.6108067035675049, 'train_nll': 68.29963684082031, 'test_nll': 14.976730346679688, 'train_mse': 0.11064369231462479, 'state_dict_file': 'model_state_dict_-4515312179762507576.pkl'}\n",
      "2019-03-18 09:30:31.521379, fold=1, rep=1, eta=0d 0h 1m 28s \n",
      "{'fold': 1, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.23394975066184998, 'train_time': 6.179555022041313, 'prior_train_nmll': 0.5904436707496643, 'train_nll': 70.87533569335938, 'test_nll': 14.790583610534668, 'train_mse': 0.11074879765510559, 'state_dict_file': 'model_state_dict_-6175502069144604192.pkl'}\n",
      "2019-03-18 09:30:36.893111, fold=2, rep=0, eta=0d 0h 1m 22s \n",
      "{'fold': 2, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.10810483992099762, 'train_time': 5.368435951997526, 'prior_train_nmll': 0.6386719346046448, 'train_nll': 79.17034149169922, 'test_nll': 6.870326995849609, 'train_mse': 0.12423371523618698, 'state_dict_file': 'model_state_dict_5963057277404323773.pkl'}\n",
      "2019-03-18 09:30:41.989763, fold=2, rep=1, eta=0d 0h 1m 16s \n",
      "{'fold': 2, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.10760385543107986, 'train_time': 5.096461010049097, 'prior_train_nmll': 0.6361403465270996, 'train_nll': 79.43944549560547, 'test_nll': 6.780921936035156, 'train_mse': 0.12397724390029907, 'state_dict_file': 'model_state_dict_-8956917818248161611.pkl'}\n",
      "2019-03-18 09:30:47.122766, fold=3, rep=0, eta=0d 0h 1m 10s \n",
      "{'fold': 3, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.20401231944561005, 'train_time': 5.129706413950771, 'prior_train_nmll': 0.5932125449180603, 'train_nll': 70.35944366455078, 'test_nll': 14.24300479888916, 'train_mse': 0.11181021481752396, 'state_dict_file': 'model_state_dict_3891249907936297534.pkl'}\n",
      "2019-03-18 09:30:52.708289, fold=3, rep=1, eta=0d 0h 1m 5s \n",
      "{'fold': 3, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.20514102280139923, 'train_time': 5.585298341931775, 'prior_train_nmll': 0.5889561176300049, 'train_nll': 70.32798767089844, 'test_nll': 14.145578384399414, 'train_mse': 0.1124650090932846, 'state_dict_file': 'model_state_dict_6154989123567471277.pkl'}\n",
      "2019-03-18 09:31:00.282278, fold=4, rep=0, eta=0d 0h 1m 2s \n",
      "{'fold': 4, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.12786713242530823, 'train_time': 7.570341094979085, 'prior_train_nmll': 0.6270865797996521, 'train_nll': 76.02361297607422, 'test_nll': 9.309871673583984, 'train_mse': 0.12005604803562164, 'state_dict_file': 'model_state_dict_6268103524787329556.pkl'}\n",
      "2019-03-18 09:31:06.793214, fold=4, rep=1, eta=0d 0h 0m 57s \n",
      "{'fold': 4, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.12792007625102997, 'train_time': 6.510707966983318, 'prior_train_nmll': 0.6168044805526733, 'train_nll': 75.8775863647461, 'test_nll': 9.356045722961426, 'train_mse': 0.11961858719587326, 'state_dict_file': 'model_state_dict_4960678376706816422.pkl'}\n",
      "2019-03-18 09:31:11.906962, fold=5, rep=0, eta=0d 0h 0m 51s \n",
      "{'fold': 5, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.22628234326839447, 'train_time': 5.110762085998431, 'prior_train_nmll': 0.5895674824714661, 'train_nll': 68.70379638671875, 'test_nll': 16.100873947143555, 'train_mse': 0.10959707200527191, 'state_dict_file': 'model_state_dict_-2657880702712081810.pkl'}\n",
      "2019-03-18 09:31:17.291688, fold=5, rep=1, eta=0d 0h 0m 45s \n",
      "{'fold': 5, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.2262139618396759, 'train_time': 5.384527185931802, 'prior_train_nmll': 0.5823103189468384, 'train_nll': 68.07511901855469, 'test_nll': 15.983814239501953, 'train_mse': 0.10951639711856842, 'state_dict_file': 'model_state_dict_1443120191366979771.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:31:22.581937, fold=6, rep=0, eta=0d 0h 0m 39s \n",
      "{'fold': 6, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.13107836246490479, 'train_time': 5.287351079052314, 'prior_train_nmll': 0.626961350440979, 'train_nll': 75.44969940185547, 'test_nll': 8.4273042678833, 'train_mse': 0.11822577565908432, 'state_dict_file': 'model_state_dict_-7438079619663287030.pkl'}\n",
      "2019-03-18 09:31:28.221437, fold=6, rep=1, eta=0d 0h 0m 33s \n",
      "{'fold': 6, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.13064992427825928, 'train_time': 5.639316670014523, 'prior_train_nmll': 0.6255247592926025, 'train_nll': 76.62989044189453, 'test_nll': 8.44007682800293, 'train_mse': 0.11888232827186584, 'state_dict_file': 'model_state_dict_4857921031545214737.pkl'}\n",
      "2019-03-18 09:31:34.639288, fold=7, rep=0, eta=0d 0h 0m 28s \n",
      "{'fold': 7, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.10899664461612701, 'train_time': 6.414837651071139, 'prior_train_nmll': 0.6488540172576904, 'train_nll': 76.9390640258789, 'test_nll': 7.329461097717285, 'train_mse': 0.1204151064157486, 'state_dict_file': 'model_state_dict_3617932538768425482.pkl'}\n",
      "2019-03-18 09:31:40.013750, fold=7, rep=1, eta=0d 0h 0m 22s \n",
      "{'fold': 7, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.10908124595880508, 'train_time': 5.374278285074979, 'prior_train_nmll': 0.629879891872406, 'train_nll': 77.17916107177734, 'test_nll': 7.347057342529297, 'train_mse': 0.12085384130477905, 'state_dict_file': 'model_state_dict_-3016760470203078153.pkl'}\n",
      "2019-03-18 09:31:46.840593, fold=8, rep=0, eta=0d 0h 0m 17s \n",
      "{'fold': 8, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.19664976000785828, 'train_time': 6.82389521796722, 'prior_train_nmll': 0.5938233733177185, 'train_nll': 69.69181823730469, 'test_nll': 13.403848648071289, 'train_mse': 0.11063572019338608, 'state_dict_file': 'model_state_dict_-6699727041318760450.pkl'}\n",
      "2019-03-18 09:31:52.252920, fold=8, rep=1, eta=0d 0h 0m 11s \n",
      "{'fold': 8, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.19638578593730927, 'train_time': 5.41214657202363, 'prior_train_nmll': 0.60728919506073, 'train_nll': 69.06858825683594, 'test_nll': 13.402864456176758, 'train_mse': 0.11089050769805908, 'state_dict_file': 'model_state_dict_-7497033799806272868.pkl'}\n",
      "2019-03-18 09:31:59.227172, fold=9, rep=0, eta=0d 0h 0m 5s \n",
      "{'fold': 9, 'repeat': 0, 'n': 209, 'd': 5, 'mse': 0.11332473903894424, 'train_time': 6.970833666971885, 'prior_train_nmll': 0.6428617238998413, 'train_nll': 77.65068817138672, 'test_nll': 7.209229469299316, 'train_mse': 0.1213778555393219, 'state_dict_file': 'model_state_dict_-6003656565953635169.pkl'}\n",
      "2019-03-18 09:32:04.234110, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 209, 'd': 5, 'mse': 0.11371657997369766, 'train_time': 5.006723391939886, 'prior_train_nmll': 0.6283143758773804, 'train_nll': 77.5273208618164, 'test_nll': 7.17939567565918, 'train_mse': 0.12068252265453339, 'state_dict_file': 'model_state_dict_1954781754004105824.pkl'}\n",
      "2019-03-18 09:32:10.713189, fold=0, rep=0, eta=0d 0h 2m 2s \n",
      "{'fold': 0, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.006727125961333513, 'train_time': 6.455474685993977, 'prior_train_nmll': -0.3864014148712158, 'train_nll': 1757.9949951171875, 'test_nll': -29.801864624023438, 'train_mse': 0.014414112083613873, 'state_dict_file': 'model_state_dict_-8502672808610962047.pkl'}\n",
      "2019-03-18 09:32:18.274827, fold=0, rep=1, eta=0d 0h 2m 6s \n",
      "{'fold': 0, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.00676859961822629, 'train_time': 7.561457121977583, 'prior_train_nmll': -0.377441942691803, 'train_nll': -195.9440155029297, 'test_nll': -29.405010223388672, 'train_mse': 0.014414727687835693, 'state_dict_file': 'model_state_dict_2812745156082577062.pkl'}\n",
      "2019-03-18 09:32:26.427441, fold=1, rep=0, eta=0d 0h 2m 5s \n",
      "{'fold': 1, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.013104057870805264, 'train_time': 8.14961751003284, 'prior_train_nmll': -0.39993536472320557, 'train_nll': -195.35646057128906, 'test_nll': -22.855022430419922, 'train_mse': 0.01377339567989111, 'state_dict_file': 'model_state_dict_-511168780889512971.pkl'}\n",
      "2019-03-18 09:32:36.111788, fold=1, rep=1, eta=0d 0h 2m 7s \n",
      "{'fold': 1, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.013080425560474396, 'train_time': 9.684167155995965, 'prior_train_nmll': -0.4145287275314331, 'train_nll': -16.058547973632812, 'test_nll': -22.511886596679688, 'train_mse': 0.013769376091659069, 'state_dict_file': 'model_state_dict_4997900947053297790.pkl'}\n",
      "2019-03-18 09:32:44.980332, fold=2, rep=0, eta=0d 0h 2m 2s \n",
      "{'fold': 2, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.009617314673960209, 'train_time': 8.865598571952432, 'prior_train_nmll': -0.4116031229496002, 'train_nll': 11240.541015625, 'test_nll': -25.890125274658203, 'train_mse': 0.014057212509214878, 'state_dict_file': 'model_state_dict_5725725722570999331.pkl'}\n",
      "2019-03-18 09:32:52.678905, fold=2, rep=1, eta=0d 0h 1m 52s \n",
      "{'fold': 2, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.009619158692657948, 'train_time': 7.698350079939701, 'prior_train_nmll': -0.40386441349983215, 'train_nll': -257.99310302734375, 'test_nll': -26.00605010986328, 'train_mse': 0.014051591046154499, 'state_dict_file': 'model_state_dict_7257602010651567480.pkl'}\n",
      "2019-03-18 09:32:58.570031, fold=3, rep=0, eta=0d 0h 1m 40s \n",
      "{'fold': 3, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.00591454841196537, 'train_time': 5.887069324031472, 'prior_train_nmll': -0.3862091898918152, 'train_nll': -179.2012176513672, 'test_nll': -34.64551544189453, 'train_mse': 0.014446595683693886, 'state_dict_file': 'model_state_dict_-192964165266971433.pkl'}\n",
      "2019-03-18 09:33:06.061816, fold=3, rep=1, eta=0d 0h 1m 32s \n",
      "{'fold': 3, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.005935682915151119, 'train_time': 7.491570558981039, 'prior_train_nmll': -0.3947959542274475, 'train_nll': -344.38134765625, 'test_nll': -29.217220306396484, 'train_mse': 0.014442791230976582, 'state_dict_file': 'model_state_dict_-3400554926702569883.pkl'}\n",
      "2019-03-18 09:33:13.506547, fold=4, rep=0, eta=0d 0h 1m 24s \n",
      "{'fold': 4, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.006335935089737177, 'train_time': 7.441882175975479, 'prior_train_nmll': -0.3909044861793518, 'train_nll': -184.5855255126953, 'test_nll': -28.874584197998047, 'train_mse': 0.014411509968340397, 'state_dict_file': 'model_state_dict_8253059663317726368.pkl'}\n",
      "2019-03-18 09:33:20.811948, fold=4, rep=1, eta=0d 0h 1m 16s \n",
      "{'fold': 4, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.006260713096708059, 'train_time': 7.3052251420449466, 'prior_train_nmll': -0.40455320477485657, 'train_nll': -3837.5693359375, 'test_nll': -29.32904815673828, 'train_mse': 0.0144118033349514, 'state_dict_file': 'model_state_dict_1864357177608787903.pkl'}\n",
      "2019-03-18 09:33:28.852334, fold=5, rep=0, eta=0d 0h 1m 9s \n",
      "{'fold': 5, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.012028934434056282, 'train_time': 8.037468711961992, 'prior_train_nmll': -0.4168876111507416, 'train_nll': -696741.375, 'test_nll': -25.304702758789062, 'train_mse': 0.013754052110016346, 'state_dict_file': 'model_state_dict_-234002608894982859.pkl'}\n",
      "2019-03-18 09:33:38.438330, fold=5, rep=1, eta=0d 0h 1m 2s \n",
      "{'fold': 5, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.011919467709958553, 'train_time': 9.585793914971873, 'prior_train_nmll': -0.41874346137046814, 'train_nll': 40.06343078613281, 'test_nll': -23.56964874267578, 'train_mse': 0.013764146715402603, 'state_dict_file': 'model_state_dict_-368708959552640774.pkl'}\n",
      "2019-03-18 09:33:46.537427, fold=6, rep=0, eta=0d 0h 0m 55s \n",
      "{'fold': 6, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.0832565501332283, 'train_time': 8.09618936793413, 'prior_train_nmll': -0.7615667581558228, 'train_nll': -657.479736328125, 'test_nll': 2270.47021484375, 'train_mse': 0.006680860184133053, 'state_dict_file': 'model_state_dict_6179065902166236700.pkl'}\n",
      "2019-03-18 09:33:55.204243, fold=6, rep=1, eta=0d 0h 0m 47s \n",
      "{'fold': 6, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.08430246263742447, 'train_time': 8.666614569025114, 'prior_train_nmll': -0.7483349442481995, 'train_nll': -630.1721801757812, 'test_nll': 134.6885528564453, 'train_mse': 0.006715681869536638, 'state_dict_file': 'model_state_dict_-1938089462497555139.pkl'}\n",
      "2019-03-18 09:34:02.322507, fold=7, rep=0, eta=0d 0h 0m 39s \n",
      "{'fold': 7, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.007385800126940012, 'train_time': 7.1151775440666825, 'prior_train_nmll': -0.39148926734924316, 'train_nll': -182.9648895263672, 'test_nll': -28.282806396484375, 'train_mse': 0.014318526722490788, 'state_dict_file': 'model_state_dict_-7680198480627790125.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:34:09.228673, fold=7, rep=1, eta=0d 0h 0m 31s \n",
      "{'fold': 7, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.007356285583227873, 'train_time': 6.905979070928879, 'prior_train_nmll': -0.39954492449760437, 'train_nll': -1966.9261474609375, 'test_nll': -27.928295135498047, 'train_mse': 0.01431970950216055, 'state_dict_file': 'model_state_dict_5216366829639864809.pkl'}\n",
      "2019-03-18 09:34:16.593421, fold=8, rep=0, eta=0d 0h 0m 23s \n",
      "{'fold': 8, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.010668231174349785, 'train_time': 7.361811151960865, 'prior_train_nmll': -0.4162733852863312, 'train_nll': 80078.765625, 'test_nll': -25.671611785888672, 'train_mse': 0.013851114548742771, 'state_dict_file': 'model_state_dict_-2938227351388400947.pkl'}\n",
      "2019-03-18 09:34:25.454465, fold=8, rep=1, eta=0d 0h 0m 15s \n",
      "{'fold': 8, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.01069683488458395, 'train_time': 8.860866083065048, 'prior_train_nmll': -0.4103619158267975, 'train_nll': -192.74803161621094, 'test_nll': -24.734851837158203, 'train_mse': 0.013868161477148533, 'state_dict_file': 'model_state_dict_-2680067323122995144.pkl'}\n",
      "2019-03-18 09:34:33.095422, fold=9, rep=0, eta=0d 0h 0m 7s \n",
      "{'fold': 9, 'repeat': 0, 'n': 308, 'd': 4, 'mse': 0.0050186109729111195, 'train_time': 7.638017802964896, 'prior_train_nmll': -0.3704030215740204, 'train_nll': -192.29507446289062, 'test_nll': -28.79991340637207, 'train_mse': 0.014474214054644108, 'state_dict_file': 'model_state_dict_1233918702055232903.pkl'}\n",
      "2019-03-18 09:34:38.972089, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 308, 'd': 4, 'mse': 0.005069484002888203, 'train_time': 5.876477568992414, 'prior_train_nmll': -0.39003074169158936, 'train_nll': -189.61557006835938, 'test_nll': -28.597387313842773, 'train_mse': 0.014478148892521858, 'state_dict_file': 'model_state_dict_-6755304230023622864.pkl'}\n",
      "2019-03-18 09:34:47.607920, fold=0, rep=0, eta=0d 0h 2m 43s \n",
      "{'fold': 0, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.14150230586528778, 'train_time': 8.61220404598862, 'prior_train_nmll': 0.46149536967277527, 'train_nll': 105.37728881835938, 'test_nll': 15.649524688720703, 'train_mse': 0.10012754797935486, 'state_dict_file': 'model_state_dict_-7695992415292324971.pkl'}\n",
      "2019-03-18 09:34:54.678754, fold=0, rep=1, eta=0d 0h 2m 21s \n",
      "{'fold': 0, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.1414966583251953, 'train_time': 7.07065687305294, 'prior_train_nmll': 0.4528859853744507, 'train_nll': 104.822265625, 'test_nll': 15.753599166870117, 'train_mse': 0.10013867169618607, 'state_dict_file': 'model_state_dict_-4792894867256447301.pkl'}\n",
      "2019-03-18 09:35:02.757837, fold=1, rep=0, eta=0d 0h 2m 14s \n",
      "{'fold': 1, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.12940871715545654, 'train_time': 8.076178863062523, 'prior_train_nmll': 0.4540671110153198, 'train_nll': 103.0616455078125, 'test_nll': 15.444595336914062, 'train_mse': 0.09999439120292664, 'state_dict_file': 'model_state_dict_-8157571365012074367.pkl'}\n",
      "2019-03-18 09:35:09.724730, fold=1, rep=1, eta=0d 0h 2m 2s \n",
      "{'fold': 1, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.12959407269954681, 'train_time': 6.966708664083853, 'prior_train_nmll': 0.4495968520641327, 'train_nll': 104.18562316894531, 'test_nll': 15.494932174682617, 'train_mse': 0.09981673210859299, 'state_dict_file': 'model_state_dict_694945123093696043.pkl'}\n",
      "2019-03-18 09:35:18.370323, fold=2, rep=0, eta=0d 0h 1m 58s \n",
      "{'fold': 2, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.09007371217012405, 'train_time': 8.64272918808274, 'prior_train_nmll': 0.4823663830757141, 'train_nll': 111.35838317871094, 'test_nll': 9.277006149291992, 'train_mse': 0.1038295328617096, 'state_dict_file': 'model_state_dict_-8422095049175166078.pkl'}\n",
      "2019-03-18 09:35:24.254530, fold=2, rep=1, eta=0d 0h 1m 45s \n",
      "{'fold': 2, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.09004393965005875, 'train_time': 5.884003391023725, 'prior_train_nmll': 0.46869659423828125, 'train_nll': 111.05911254882812, 'test_nll': 9.141054153442383, 'train_mse': 0.10387944430112839, 'state_dict_file': 'model_state_dict_1193418628665744421.pkl'}\n",
      "2019-03-18 09:35:33.813879, fold=3, rep=0, eta=0d 0h 1m 41s \n",
      "{'fold': 3, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.1069505587220192, 'train_time': 9.55622573394794, 'prior_train_nmll': 0.46643802523612976, 'train_nll': 109.53289794921875, 'test_nll': 12.54307746887207, 'train_mse': 0.10237403213977814, 'state_dict_file': 'model_state_dict_-6948044800097176488.pkl'}\n",
      "2019-03-18 09:35:43.038763, fold=3, rep=1, eta=0d 0h 1m 36s \n",
      "{'fold': 3, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.10705868154764175, 'train_time': 9.224681153078564, 'prior_train_nmll': 0.4721900224685669, 'train_nll': 107.78575134277344, 'test_nll': 12.527053833007812, 'train_mse': 0.10203118622303009, 'state_dict_file': 'model_state_dict_-7461220317446014754.pkl'}\n",
      "2019-03-18 09:35:48.679622, fold=4, rep=0, eta=0d 0h 1m 25s \n",
      "{'fold': 4, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.12150053679943085, 'train_time': 5.637913431972265, 'prior_train_nmll': 0.46761372685432434, 'train_nll': 104.68209838867188, 'test_nll': 14.073429107666016, 'train_mse': 0.1011807844042778, 'state_dict_file': 'model_state_dict_-963388316737469182.pkl'}\n",
      "2019-03-18 09:35:56.073375, fold=4, rep=1, eta=0d 0h 1m 17s \n",
      "{'fold': 4, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.12148462235927582, 'train_time': 7.393571696942672, 'prior_train_nmll': 0.4540729522705078, 'train_nll': 105.79684448242188, 'test_nll': 14.07009506225586, 'train_mse': 0.10140512138605118, 'state_dict_file': 'model_state_dict_4954363994734855622.pkl'}\n",
      "2019-03-18 09:36:05.409151, fold=5, rep=0, eta=0d 0h 1m 10s \n",
      "{'fold': 5, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.11712564527988434, 'train_time': 9.33286147005856, 'prior_train_nmll': 0.45502376556396484, 'train_nll': 106.25634765625, 'test_nll': 13.779905319213867, 'train_mse': 0.10190456360578537, 'state_dict_file': 'model_state_dict_1773577460141418520.pkl'}\n",
      "2019-03-18 09:36:12.843578, fold=5, rep=1, eta=0d 0h 1m 2s \n",
      "{'fold': 5, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.11691870540380478, 'train_time': 7.4342497469624504, 'prior_train_nmll': 0.46027520298957825, 'train_nll': 107.06825256347656, 'test_nll': 13.779193878173828, 'train_mse': 0.10145659744739532, 'state_dict_file': 'model_state_dict_4049028672860980694.pkl'}\n",
      "2019-03-18 09:36:21.307155, fold=6, rep=0, eta=0d 0h 0m 55s \n",
      "{'fold': 6, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.09743966162204742, 'train_time': 8.460602173930965, 'prior_train_nmll': 0.47088220715522766, 'train_nll': 112.8544921875, 'test_nll': 10.623001098632812, 'train_mse': 0.10419264435768127, 'state_dict_file': 'model_state_dict_3436782610369522928.pkl'}\n",
      "2019-03-18 09:36:29.554904, fold=6, rep=1, eta=0d 0h 0m 47s \n",
      "{'fold': 6, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.09733600914478302, 'train_time': 8.247543673962355, 'prior_train_nmll': 0.4712779223918915, 'train_nll': 111.05775451660156, 'test_nll': 10.656513214111328, 'train_mse': 0.10409223288297653, 'state_dict_file': 'model_state_dict_-7786422614688282265.pkl'}\n",
      "2019-03-18 09:36:37.643557, fold=7, rep=0, eta=0d 0h 0m 39s \n",
      "{'fold': 7, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.23648910224437714, 'train_time': 8.085633256006986, 'prior_train_nmll': 0.3985166549682617, 'train_nll': 85.66769409179688, 'test_nll': 33.946876525878906, 'train_mse': 0.08970365673303604, 'state_dict_file': 'model_state_dict_-5166786029143139864.pkl'}\n",
      "2019-03-18 09:36:46.652990, fold=7, rep=1, eta=0d 0h 0m 31s \n",
      "{'fold': 7, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.23707155883312225, 'train_time': 9.009246956091374, 'prior_train_nmll': 0.41100624203681946, 'train_nll': 86.63154602050781, 'test_nll': 33.8538818359375, 'train_mse': 0.09039101749658585, 'state_dict_file': 'model_state_dict_-1681184593751574878.pkl'}\n",
      "2019-03-18 09:36:56.117968, fold=8, rep=0, eta=0d 0h 0m 24s \n",
      "{'fold': 8, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.11853112280368805, 'train_time': 9.46195857890416, 'prior_train_nmll': 0.4580065608024597, 'train_nll': 107.70672607421875, 'test_nll': 14.090225219726562, 'train_mse': 0.10179927200078964, 'state_dict_file': 'model_state_dict_5019115008061582511.pkl'}\n",
      "2019-03-18 09:37:01.939368, fold=8, rep=1, eta=0d 0h 0m 15s \n",
      "{'fold': 8, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.11854173988103867, 'train_time': 5.821181375999004, 'prior_train_nmll': 0.4603461027145386, 'train_nll': 107.73124694824219, 'test_nll': 13.97163200378418, 'train_mse': 0.10180902481079102, 'state_dict_file': 'model_state_dict_6259165568679831629.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:37:10.753566, fold=9, rep=0, eta=0d 0h 0m 7s \n",
      "{'fold': 9, 'repeat': 0, 'n': 392, 'd': 5, 'mse': 0.12706312537193298, 'train_time': 8.81040406296961, 'prior_train_nmll': 0.47092217206954956, 'train_nll': 108.43069458007812, 'test_nll': 11.0948486328125, 'train_mse': 0.1011708676815033, 'state_dict_file': 'model_state_dict_-6183565825878766297.pkl'}\n",
      "2019-03-18 09:37:19.136950, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 392, 'd': 5, 'mse': 0.1275349259376526, 'train_time': 8.38272697397042, 'prior_train_nmll': 0.4555804431438446, 'train_nll': 107.33316040039062, 'test_nll': 11.098493576049805, 'train_mse': 0.10068178921937943, 'state_dict_file': 'model_state_dict_1063038631932153407.pkl'}\n",
      "2019-03-18 09:37:34.466937, fold=0, rep=0, eta=0d 0h 4m 50s \n",
      "{'fold': 0, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.11840296536684036, 'train_time': 15.307228822028264, 'prior_train_nmll': 0.6618500351905823, 'train_nll': -217.18734741210938, 'test_nll': 17.247421264648438, 'train_mse': 0.11354318261146545, 'state_dict_file': 'model_state_dict_1054035716219336373.pkl'}\n",
      "2019-03-18 09:37:51.702246, fold=0, rep=1, eta=0d 0h 4m 52s \n",
      "{'fold': 0, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.11897441744804382, 'train_time': 17.235078540048562, 'prior_train_nmll': 0.6567511558532715, 'train_nll': -125.58621215820312, 'test_nll': 17.65892219543457, 'train_mse': 0.1134941428899765, 'state_dict_file': 'model_state_dict_1850578512600095716.pkl'}\n",
      "2019-03-18 09:38:08.102340, fold=1, rep=0, eta=0d 0h 4m 37s \n",
      "{'fold': 1, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.09720684587955475, 'train_time': 16.396876081009395, 'prior_train_nmll': 0.651199996471405, 'train_nll': 68.44515991210938, 'test_nll': 14.869869232177734, 'train_mse': 0.12016946077346802, 'state_dict_file': 'model_state_dict_-4656208279569485562.pkl'}\n",
      "2019-03-18 09:38:21.259759, fold=1, rep=1, eta=0d 0h 4m 8s \n",
      "{'fold': 1, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.09594622999429703, 'train_time': 13.157181872986257, 'prior_train_nmll': 0.6714425086975098, 'train_nll': 165.545166015625, 'test_nll': 14.16823959350586, 'train_mse': 0.1181870624423027, 'state_dict_file': 'model_state_dict_8427025106717282061.pkl'}\n",
      "2019-03-18 09:38:32.530795, fold=2, rep=0, eta=0d 0h 3m 40s \n",
      "{'fold': 2, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.286789208650589, 'train_time': 11.26781468000263, 'prior_train_nmll': 0.5937543511390686, 'train_nll': -27.0010986328125, 'test_nll': 46.37066650390625, 'train_mse': 0.1000242605805397, 'state_dict_file': 'model_state_dict_-7446359009820133350.pkl'}\n",
      "2019-03-18 09:38:47.991320, fold=2, rep=1, eta=0d 0h 3m 27s \n",
      "{'fold': 2, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.2874525189399719, 'train_time': 15.46032402291894, 'prior_train_nmll': 0.5931833386421204, 'train_nll': -180.84475708007812, 'test_nll': 47.2630729675293, 'train_mse': 0.10045870393514633, 'state_dict_file': 'model_state_dict_6704096209555641761.pkl'}\n",
      "2019-03-18 09:39:04.245838, fold=3, rep=0, eta=0d 0h 3m 15s \n",
      "{'fold': 3, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.2509516179561615, 'train_time': 16.251261417986825, 'prior_train_nmll': 0.6076493859291077, 'train_nll': -230945200.0, 'test_nll': 42.70270538330078, 'train_mse': 0.10637596994638443, 'state_dict_file': 'model_state_dict_-5689191678574633085.pkl'}\n",
      "2019-03-18 09:39:20.278169, fold=3, rep=1, eta=0d 0h 3m 1s \n",
      "{'fold': 3, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.24895133078098297, 'train_time': 16.032120229909196, 'prior_train_nmll': 0.6084486246109009, 'train_nll': 234.88238525390625, 'test_nll': 42.44073486328125, 'train_mse': 0.10611452907323837, 'state_dict_file': 'model_state_dict_270721095990166787.pkl'}\n",
      "2019-03-18 09:39:34.657462, fold=4, rep=0, eta=0d 0h 2m 45s \n",
      "{'fold': 4, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.1301366239786148, 'train_time': 14.37558080907911, 'prior_train_nmll': 0.6438323855400085, 'train_nll': 100.9234619140625, 'test_nll': 20.959001541137695, 'train_mse': 0.11488872766494751, 'state_dict_file': 'model_state_dict_-2655761604905292369.pkl'}\n",
      "2019-03-18 09:39:48.591180, fold=4, rep=1, eta=0d 0h 2m 29s \n",
      "{'fold': 4, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.13032618165016174, 'train_time': 13.933508011046797, 'prior_train_nmll': 0.641627848148346, 'train_nll': 501.7301025390625, 'test_nll': 21.274898529052734, 'train_mse': 0.11521155387163162, 'state_dict_file': 'model_state_dict_-4972314287241333024.pkl'}\n",
      "2019-03-18 09:40:03.637185, fold=5, rep=0, eta=0d 0h 2m 14s \n",
      "{'fold': 5, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.12770205736160278, 'train_time': 15.042600902030244, 'prior_train_nmll': 0.6368802189826965, 'train_nll': 20.760772705078125, 'test_nll': 20.12833595275879, 'train_mse': 0.11210788786411285, 'state_dict_file': 'model_state_dict_6249386190548117811.pkl'}\n",
      "2019-03-18 09:40:17.980313, fold=5, rep=1, eta=0d 0h 1m 59s \n",
      "{'fold': 5, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.12748250365257263, 'train_time': 14.3429179150844, 'prior_train_nmll': 0.6562351584434509, 'train_nll': -124.17190551757812, 'test_nll': 19.53235626220703, 'train_mse': 0.11099445819854736, 'state_dict_file': 'model_state_dict_-317494362304340364.pkl'}\n",
      "2019-03-18 09:40:33.983928, fold=6, rep=0, eta=0d 0h 1m 44s \n",
      "{'fold': 6, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.10541123151779175, 'train_time': 16.000312254065648, 'prior_train_nmll': 0.6557765603065491, 'train_nll': 133.70126342773438, 'test_nll': 16.5782413482666, 'train_mse': 0.11749168485403061, 'state_dict_file': 'model_state_dict_5356853478116762027.pkl'}\n",
      "2019-03-18 09:40:49.444245, fold=6, rep=1, eta=0d 0h 1m 30s \n",
      "{'fold': 6, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.1055009514093399, 'train_time': 15.460083481972106, 'prior_train_nmll': 0.6793281435966492, 'train_nll': 107.87722778320312, 'test_nll': 17.138864517211914, 'train_mse': 0.11773474514484406, 'state_dict_file': 'model_state_dict_5179927010261479925.pkl'}\n",
      "2019-03-18 09:41:03.588660, fold=7, rep=0, eta=0d 0h 1m 14s \n",
      "{'fold': 7, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.19781415164470673, 'train_time': 14.141047565964982, 'prior_train_nmll': 0.6278031468391418, 'train_nll': -281.9795227050781, 'test_nll': 29.868228912353516, 'train_mse': 0.11024519801139832, 'state_dict_file': 'model_state_dict_-7046267605661457185.pkl'}\n",
      "2019-03-18 09:41:16.302323, fold=7, rep=1, eta=0d 0h 0m 59s \n",
      "{'fold': 7, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.1974640041589737, 'train_time': 12.713427637936547, 'prior_train_nmll': 0.6219649910926819, 'train_nll': 175.61587524414062, 'test_nll': 30.653377532958984, 'train_mse': 0.110249362885952, 'state_dict_file': 'model_state_dict_117125819168139090.pkl'}\n",
      "2019-03-18 09:41:30.730582, fold=8, rep=0, eta=0d 0h 0m 44s \n",
      "{'fold': 8, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.0925530344247818, 'train_time': 14.424883869942278, 'prior_train_nmll': 0.6603538990020752, 'train_nll': 243.04302978515625, 'test_nll': 13.992435455322266, 'train_mse': 0.11733445525169373, 'state_dict_file': 'model_state_dict_6954739179026857453.pkl'}\n",
      "2019-03-18 09:41:47.470198, fold=8, rep=1, eta=0d 0h 0m 29s \n",
      "{'fold': 8, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.09270083904266357, 'train_time': 16.739418028038926, 'prior_train_nmll': 0.6555647253990173, 'train_nll': 81.55148315429688, 'test_nll': 14.422508239746094, 'train_mse': 0.11803357303142548, 'state_dict_file': 'model_state_dict_2729013574775776257.pkl'}\n",
      "2019-03-18 09:41:59.664958, fold=9, rep=0, eta=0d 0h 0m 14s \n",
      "{'fold': 9, 'repeat': 0, 'n': 506, 'd': 11, 'mse': 0.27366840839385986, 'train_time': 12.191288658068515, 'prior_train_nmll': 0.6205942034721375, 'train_nll': -79.91140747070312, 'test_nll': 36.98507308959961, 'train_mse': 0.09853608906269073, 'state_dict_file': 'model_state_dict_-3392000709793258335.pkl'}\n",
      "2019-03-18 09:42:17.300127, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 506, 'd': 11, 'mse': 0.27175629138946533, 'train_time': 17.634964248980395, 'prior_train_nmll': 0.5802828669548035, 'train_nll': -250.26727294921875, 'test_nll': 36.56142044067383, 'train_mse': 0.09968283027410507, 'state_dict_file': 'model_state_dict_-7377940111031628063.pkl'}\n",
      "2019-03-18 09:42:50.376955, fold=0, rep=0, eta=0d 0h 10m 28s \n",
      "{'fold': 0, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 0.7765188813209534, 'train_time': 33.0522524160333, 'prior_train_nmll': 1.4358190298080444, 'train_nll': 664.858154296875, 'test_nll': 68.09758758544922, 'train_mse': 1.0212643146514893, 'state_dict_file': 'model_state_dict_-365020334976156502.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:43:27.068830, fold=0, rep=1, eta=0d 0h 10m 27s \n",
      "{'fold': 0, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 0.7772942185401917, 'train_time': 36.69165527599398, 'prior_train_nmll': 1.4358272552490234, 'train_nll': 664.8236083984375, 'test_nll': 68.11229705810547, 'train_mse': 1.021009922027588, 'state_dict_file': 'model_state_dict_4883013045716338156.pkl'}\n",
      "2019-03-18 09:43:41.518812, fold=1, rep=0, eta=0d 0h 7m 57s \n",
      "{'fold': 1, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.0423712730407715, 'train_time': 14.446155652985908, 'prior_train_nmll': 1.4229164123535156, 'train_nll': 653.4039916992188, 'test_nll': 74.9128646850586, 'train_mse': 0.9705911874771118, 'state_dict_file': 'model_state_dict_-502562765485542441.pkl'}\n",
      "2019-03-18 09:43:56.533740, fold=1, rep=1, eta=0d 0h 6m 36s \n",
      "{'fold': 1, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.0430717468261719, 'train_time': 15.01469530898612, 'prior_train_nmll': 1.4227421283721924, 'train_nll': 653.4459228515625, 'test_nll': 74.9353256225586, 'train_mse': 0.9698141813278198, 'state_dict_file': 'model_state_dict_-3396121801998205666.pkl'}\n",
      "2019-03-18 09:44:16.434011, fold=2, rep=0, eta=0d 0h 5m 57s \n",
      "{'fold': 2, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 0.735260009765625, 'train_time': 19.89613202691544, 'prior_train_nmll': 1.4388326406478882, 'train_nll': 664.65478515625, 'test_nll': 67.06289672851562, 'train_mse': 1.019576907157898, 'state_dict_file': 'model_state_dict_1820296558376854783.pkl'}\n",
      "2019-03-18 09:44:38.267049, fold=2, rep=1, eta=0d 0h 5m 28s \n",
      "{'fold': 2, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 0.7342832088470459, 'train_time': 21.832827656064183, 'prior_train_nmll': 1.4387621879577637, 'train_nll': 664.4573364257812, 'test_nll': 67.05398559570312, 'train_mse': 1.018758773803711, 'state_dict_file': 'model_state_dict_6573479380995715152.pkl'}\n",
      "2019-03-18 09:45:12.287204, fold=3, rep=0, eta=0d 0h 5m 24s \n",
      "{'fold': 3, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.430138111114502, 'train_time': 34.01683483098168, 'prior_train_nmll': 1.3989894390106201, 'train_nll': 647.8034057617188, 'test_nll': 85.49749755859375, 'train_mse': 0.9490931630134583, 'state_dict_file': 'model_state_dict_3266569844268872602.pkl'}\n",
      "2019-03-18 09:45:43.065676, fold=3, rep=1, eta=0d 0h 5m 8s \n",
      "{'fold': 3, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.4372817277908325, 'train_time': 30.778258297010325, 'prior_train_nmll': 1.3990974426269531, 'train_nll': 647.789794921875, 'test_nll': 85.68000793457031, 'train_mse': 0.9489477872848511, 'state_dict_file': 'model_state_dict_8122566622553191251.pkl'}\n",
      "2019-03-18 09:46:20.635444, fold=4, rep=0, eta=0d 0h 4m 57s \n",
      "{'fold': 4, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 0.9766000509262085, 'train_time': 37.56650953297503, 'prior_train_nmll': 1.424597978591919, 'train_nll': 659.606201171875, 'test_nll': 73.19425201416016, 'train_mse': 0.9984941482543945, 'state_dict_file': 'model_state_dict_9179338762543754974.pkl'}\n",
      "2019-03-18 09:46:57.168717, fold=4, rep=1, eta=0d 0h 4m 39s \n",
      "{'fold': 4, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 0.9754611253738403, 'train_time': 36.53308136295527, 'prior_train_nmll': 1.4245730638504028, 'train_nll': 659.5919799804688, 'test_nll': 73.16665649414062, 'train_mse': 0.9984772205352783, 'state_dict_file': 'model_state_dict_-4394443636471215507.pkl'}\n",
      "2019-03-18 09:47:29.290701, fold=5, rep=0, eta=0d 0h 4m 15s \n",
      "{'fold': 5, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 0.916938066482544, 'train_time': 32.11864620598499, 'prior_train_nmll': 1.428157925605774, 'train_nll': 660.506103515625, 'test_nll': 71.685302734375, 'train_mse': 1.002215027809143, 'state_dict_file': 'model_state_dict_-8863009667512237209.pkl'}\n",
      "2019-03-18 09:47:49.888956, fold=5, rep=1, eta=0d 0h 3m 41s \n",
      "{'fold': 5, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 0.918401837348938, 'train_time': 20.598045808030292, 'prior_train_nmll': 1.4284478425979614, 'train_nll': 659.31689453125, 'test_nll': 71.74893188476562, 'train_mse': 0.9967426657676697, 'state_dict_file': 'model_state_dict_-8123906319034146180.pkl'}\n",
      "2019-03-18 09:48:17.628018, fold=6, rep=0, eta=0d 0h 3m 14s \n",
      "{'fold': 6, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 0.9218021631240845, 'train_time': 27.73562951304484, 'prior_train_nmll': 1.427963376045227, 'train_nll': 660.272705078125, 'test_nll': 71.81269836425781, 'train_mse': 1.0010329484939575, 'state_dict_file': 'model_state_dict_6010311559685013744.pkl'}\n",
      "2019-03-18 09:48:45.399647, fold=6, rep=1, eta=0d 0h 2m 46s \n",
      "{'fold': 6, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 0.9216387867927551, 'train_time': 27.771411164896563, 'prior_train_nmll': 1.427917242050171, 'train_nll': 660.244873046875, 'test_nll': 71.80706024169922, 'train_mse': 1.0010385513305664, 'state_dict_file': 'model_state_dict_-5780384466101119735.pkl'}\n",
      "2019-03-18 09:49:24.631187, fold=7, rep=0, eta=0d 0h 2m 22s \n",
      "{'fold': 7, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 0.7128173112869263, 'train_time': 39.22800060897134, 'prior_train_nmll': 1.4390358924865723, 'train_nll': 666.3883056640625, 'test_nll': 66.55964660644531, 'train_mse': 1.0281622409820557, 'state_dict_file': 'model_state_dict_-267225571586660830.pkl'}\n",
      "2019-03-18 09:49:59.511525, fold=7, rep=1, eta=0d 0h 1m 55s \n",
      "{'fold': 7, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 0.7130588293075562, 'train_time': 34.87970766192302, 'prior_train_nmll': 1.4389735460281372, 'train_nll': 666.4139404296875, 'test_nll': 66.56356811523438, 'train_mse': 1.028146505355835, 'state_dict_file': 'model_state_dict_3393920027156164592.pkl'}\n",
      "2019-03-18 09:50:39.312487, fold=8, rep=0, eta=0d 0h 1m 28s \n",
      "{'fold': 8, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 1.8495157957077026, 'train_time': 39.79754552303348, 'prior_train_nmll': 1.3737691640853882, 'train_nll': 636.1986694335938, 'test_nll': 98.24329376220703, 'train_mse': 0.9028993248939514, 'state_dict_file': 'model_state_dict_5482060342103598493.pkl'}\n",
      "2019-03-18 09:51:11.411982, fold=8, rep=1, eta=0d 0h 0m 59s \n",
      "{'fold': 8, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 1.8462376594543457, 'train_time': 32.09928188798949, 'prior_train_nmll': 1.3738549947738647, 'train_nll': 636.16650390625, 'test_nll': 98.13250732421875, 'train_mse': 0.9026612639427185, 'state_dict_file': 'model_state_dict_-2185744556085306673.pkl'}\n",
      "2019-03-18 09:51:28.842672, fold=9, rep=0, eta=0d 0h 0m 29s \n",
      "{'fold': 9, 'repeat': 0, 'n': 517, 'd': 10, 'mse': 0.7015047669410706, 'train_time': 17.427048493991606, 'prior_train_nmll': 1.4398771524429321, 'train_nll': 667.6239013671875, 'test_nll': 62.44655990600586, 'train_mse': 1.013534426689148, 'state_dict_file': 'model_state_dict_6376836394988230693.pkl'}\n",
      "2019-03-18 09:51:47.392229, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 517, 'd': 10, 'mse': 0.7004292607307434, 'train_time': 18.549366767983884, 'prior_train_nmll': 1.4398030042648315, 'train_nll': 667.9110717773438, 'test_nll': 62.404823303222656, 'train_mse': 1.0147336721420288, 'state_dict_file': 'model_state_dict_6027477817179584788.pkl'}\n",
      "2019-03-18 09:52:06.724600, fold=0, rep=0, eta=0d 0h 6m 6s \n",
      "{'fold': 0, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.10951399058103561, 'train_time': 19.3073003370082, 'prior_train_nmll': 0.5726747512817383, 'train_nll': -1442.5054931640625, 'test_nll': -2482.95947265625, 'train_mse': 0.0677771046757698, 'state_dict_file': 'model_state_dict_7994006879757389574.pkl'}\n",
      "2019-03-18 09:52:23.995054, fold=0, rep=1, eta=0d 0h 5m 29s \n",
      "{'fold': 0, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.11170945316553116, 'train_time': 17.27024154807441, 'prior_train_nmll': 0.562916100025177, 'train_nll': 30885030.0, 'test_nll': 2942.771484375, 'train_mse': 0.06772544234991074, 'state_dict_file': 'model_state_dict_-364241001769897450.pkl'}\n",
      "2019-03-18 09:52:42.398414, fold=1, rep=0, eta=0d 0h 5m 11s \n",
      "{'fold': 1, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.1990036964416504, 'train_time': 18.399933487991802, 'prior_train_nmll': 0.5299111604690552, 'train_nll': -936.4288330078125, 'test_nll': 58.25951385498047, 'train_mse': 0.0655299499630928, 'state_dict_file': 'model_state_dict_-5481126294397474716.pkl'}\n",
      "2019-03-18 09:53:10.035993, fold=1, rep=1, eta=0d 0h 5m 30s \n",
      "{'fold': 1, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.19895042479038239, 'train_time': 27.637363071087748, 'prior_train_nmll': 0.5385738015174866, 'train_nll': 1065.4276123046875, 'test_nll': 7081.9755859375, 'train_mse': 0.06562165915966034, 'state_dict_file': 'model_state_dict_-7531528321750853578.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:53:30.498828, fold=2, rep=0, eta=0d 0h 5m 9s \n",
      "{'fold': 2, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.20447736978530884, 'train_time': 20.459447656059638, 'prior_train_nmll': 0.5299175977706909, 'train_nll': 705.6529541015625, 'test_nll': 4218180.5, 'train_mse': 0.06835637986660004, 'state_dict_file': 'model_state_dict_-312969517526312364.pkl'}\n",
      "2019-03-18 09:53:46.459786, fold=2, rep=1, eta=0d 0h 4m 37s \n",
      "{'fold': 2, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.20212213695049286, 'train_time': 15.9607518640114, 'prior_train_nmll': 0.5557267069816589, 'train_nll': -954.7108154296875, 'test_nll': 60.956783294677734, 'train_mse': 0.06812433898448944, 'state_dict_file': 'model_state_dict_5460829305182574147.pkl'}\n",
      "2019-03-18 09:54:09.014903, fold=3, rep=0, eta=0d 0h 4m 22s \n",
      "{'fold': 3, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.10149616748094559, 'train_time': 22.55173236306291, 'prior_train_nmll': 0.5793408155441284, 'train_nll': -550.5029296875, 'test_nll': 1590.058349609375, 'train_mse': 0.06934203207492828, 'state_dict_file': 'model_state_dict_6242905289152609862.pkl'}\n",
      "2019-03-18 09:54:30.410850, fold=3, rep=1, eta=0d 0h 4m 4s \n",
      "{'fold': 3, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.10139872133731842, 'train_time': 21.395727545022964, 'prior_train_nmll': 0.5510220527648926, 'train_nll': -1120.483154296875, 'test_nll': 251.4036407470703, 'train_mse': 0.06924666464328766, 'state_dict_file': 'model_state_dict_1201461294393058052.pkl'}\n",
      "2019-03-18 09:54:47.528014, fold=4, rep=0, eta=0d 0h 3m 40s \n",
      "{'fold': 4, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.10088035464286804, 'train_time': 17.113760947017, 'prior_train_nmll': 0.5413981080055237, 'train_nll': -341.4006042480469, 'test_nll': 96.00921630859375, 'train_mse': 0.06876051425933838, 'state_dict_file': 'model_state_dict_-6934289804463279753.pkl'}\n",
      "2019-03-18 09:55:11.164951, fold=4, rep=1, eta=0d 0h 3m 23s \n",
      "{'fold': 4, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.0998072475194931, 'train_time': 23.63670316990465, 'prior_train_nmll': 0.5650694370269775, 'train_nll': -1064.7894287109375, 'test_nll': -703.7154541015625, 'train_mse': 0.06884349137544632, 'state_dict_file': 'model_state_dict_-8944090226089206513.pkl'}\n",
      "2019-03-18 09:55:28.121046, fold=5, rep=0, eta=0d 0h 3m 0s \n",
      "{'fold': 5, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.0794171392917633, 'train_time': 16.95229785796255, 'prior_train_nmll': 0.6094346046447754, 'train_nll': 1419.3409423828125, 'test_nll': 4793.12939453125, 'train_mse': 0.07067466527223587, 'state_dict_file': 'model_state_dict_-655587995661442746.pkl'}\n",
      "2019-03-18 09:55:45.999206, fold=5, rep=1, eta=0d 0h 2m 39s \n",
      "{'fold': 5, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.0803029015660286, 'train_time': 17.87794900604058, 'prior_train_nmll': 0.594843864440918, 'train_nll': -610.53369140625, 'test_nll': 7476.3828125, 'train_mse': 0.07067373394966125, 'state_dict_file': 'model_state_dict_4497473672827964950.pkl'}\n",
      "2019-03-18 09:56:03.904630, fold=6, rep=0, eta=0d 0h 2m 18s \n",
      "{'fold': 6, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.4410068690776825, 'train_time': 17.90205807692837, 'prior_train_nmll': 0.5542964339256287, 'train_nll': 11634.85546875, 'test_nll': -2.59735107421875, 'train_mse': 0.06880158931016922, 'state_dict_file': 'model_state_dict_6536881070977151289.pkl'}\n",
      "2019-03-18 09:56:23.680659, fold=6, rep=1, eta=0d 0h 1m 58s \n",
      "{'fold': 6, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.44123539328575134, 'train_time': 19.775794220040552, 'prior_train_nmll': 0.5231490731239319, 'train_nll': -1022.662109375, 'test_nll': 26.708484649658203, 'train_mse': 0.06896458566188812, 'state_dict_file': 'model_state_dict_2857096041112309645.pkl'}\n",
      "2019-03-18 09:56:41.204554, fold=7, rep=0, eta=0d 0h 1m 37s \n",
      "{'fold': 7, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.22058700025081635, 'train_time': 17.520483438973315, 'prior_train_nmll': 0.5658678412437439, 'train_nll': -783.1019287109375, 'test_nll': -6.9604034423828125, 'train_mse': 0.07055484503507614, 'state_dict_file': 'model_state_dict_-2379189077437447110.pkl'}\n",
      "2019-03-18 09:56:56.566806, fold=7, rep=1, eta=0d 0h 1m 17s \n",
      "{'fold': 7, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.21829058229923248, 'train_time': 15.362064516055398, 'prior_train_nmll': 0.5555252432823181, 'train_nll': 34194.74609375, 'test_nll': 28.119571685791016, 'train_mse': 0.07055230438709259, 'state_dict_file': 'model_state_dict_7437769883555825821.pkl'}\n",
      "2019-03-18 09:57:18.221574, fold=8, rep=0, eta=0d 0h 0m 58s \n",
      "{'fold': 8, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.13368473947048187, 'train_time': 21.651259525911883, 'prior_train_nmll': 0.5762789249420166, 'train_nll': -805.7247314453125, 'test_nll': 45.54540252685547, 'train_mse': 0.0684225931763649, 'state_dict_file': 'model_state_dict_371109603471484057.pkl'}\n",
      "2019-03-18 09:57:34.635626, fold=8, rep=1, eta=0d 0h 0m 38s \n",
      "{'fold': 8, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.13362213969230652, 'train_time': 16.413837749045342, 'prior_train_nmll': 0.5577378869056702, 'train_nll': -846.242431640625, 'test_nll': 44.94160461425781, 'train_mse': 0.0684041827917099, 'state_dict_file': 'model_state_dict_1434894149523939303.pkl'}\n",
      "2019-03-18 09:57:53.537125, fold=9, rep=0, eta=0d 0h 0m 19s \n",
      "{'fold': 9, 'repeat': 0, 'n': 536, 'd': 9, 'mse': 0.1598721593618393, 'train_time': 18.89794184907805, 'prior_train_nmll': 0.5357213020324707, 'train_nll': 13634.9267578125, 'test_nll': 135946240.0, 'train_mse': 0.06513103097677231, 'state_dict_file': 'model_state_dict_-4800069452256654762.pkl'}\n",
      "2019-03-18 09:58:18.484804, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 536, 'd': 9, 'mse': 0.16207261383533478, 'train_time': 24.947340304031968, 'prior_train_nmll': 0.5340083241462708, 'train_nll': -1344.1461181640625, 'test_nll': -25.579448699951172, 'train_mse': 0.06505566835403442, 'state_dict_file': 'model_state_dict_-8929266197553428083.pkl'}\n",
      "2019-03-18 09:58:33.852530, fold=0, rep=0, eta=0d 0h 4m 51s \n",
      "{'fold': 0, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 1.0587058067321777, 'train_time': 15.342387882992625, 'prior_train_nmll': 1.2498855590820312, 'train_nll': -45.51715087890625, 'test_nll': 91.5259780883789, 'train_mse': 0.46759361028671265, 'state_dict_file': 'model_state_dict_7926051464537880796.pkl'}\n",
      "2019-03-18 09:58:49.274663, fold=0, rep=1, eta=0d 0h 4m 36s \n",
      "{'fold': 0, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 1.0630563497543335, 'train_time': 15.421941704931669, 'prior_train_nmll': 1.2490417957305908, 'train_nll': -24.85479736328125, 'test_nll': 92.41609191894531, 'train_mse': 0.46600550413131714, 'state_dict_file': 'model_state_dict_-8100614763230753987.pkl'}\n",
      "2019-03-18 09:58:59.786444, fold=1, rep=0, eta=0d 0h 3m 53s \n",
      "{'fold': 1, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.5823898911476135, 'train_time': 10.508394682896324, 'prior_train_nmll': 1.296559453010559, 'train_nll': 644.1783447265625, 'test_nll': 69.61270141601562, 'train_mse': 0.5461844801902771, 'state_dict_file': 'model_state_dict_-4132993913588402417.pkl'}\n",
      "2019-03-18 09:59:09.510067, fold=1, rep=1, eta=0d 0h 3m 24s \n",
      "{'fold': 1, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.5844163298606873, 'train_time': 9.723425706033595, 'prior_train_nmll': 1.2867364883422852, 'train_nll': 644.9615478515625, 'test_nll': 69.56168365478516, 'train_mse': 0.5480960011482239, 'state_dict_file': 'model_state_dict_-5804796237567648086.pkl'}\n",
      "2019-03-18 09:59:21.101612, fold=2, rep=0, eta=0d 0h 3m 7s \n",
      "{'fold': 2, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.6304532289505005, 'train_time': 11.588298704940826, 'prior_train_nmll': 1.2953970432281494, 'train_nll': 675.997314453125, 'test_nll': 69.99705505371094, 'train_mse': 0.5230411291122437, 'state_dict_file': 'model_state_dict_6973375187907196329.pkl'}\n",
      "2019-03-18 09:59:35.306616, fold=2, rep=1, eta=0d 0h 2m 59s \n",
      "{'fold': 2, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.6349143981933594, 'train_time': 14.204800095059909, 'prior_train_nmll': 1.2902038097381592, 'train_nll': 652.2150268554688, 'test_nll': 70.28870391845703, 'train_mse': 0.521350085735321, 'state_dict_file': 'model_state_dict_6693545756217747774.pkl'}\n",
      "2019-03-18 09:59:48.160857, fold=3, rep=0, eta=0d 0h 2m 46s \n",
      "{'fold': 3, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.5747998356819153, 'train_time': 12.850276274024509, 'prior_train_nmll': 1.28871750831604, 'train_nll': 632.1688232421875, 'test_nll': 70.65766143798828, 'train_mse': 0.5246129631996155, 'state_dict_file': 'model_state_dict_3364908521942546228.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 09:59:59.390109, fold=3, rep=1, eta=0d 0h 2m 31s \n",
      "{'fold': 3, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.5747449994087219, 'train_time': 11.228999131941237, 'prior_train_nmll': 1.2803229093551636, 'train_nll': 632.8082275390625, 'test_nll': 70.69336700439453, 'train_mse': 0.5238295197486877, 'state_dict_file': 'model_state_dict_-4182907633399847903.pkl'}\n",
      "2019-03-18 10:00:11.713958, fold=4, rep=0, eta=0d 0h 2m 18s \n",
      "{'fold': 4, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 1.0452897548675537, 'train_time': 12.320142924087122, 'prior_train_nmll': 1.252483606338501, 'train_nll': 612.4981689453125, 'test_nll': 92.22642517089844, 'train_mse': 0.48930808901786804, 'state_dict_file': 'model_state_dict_3835026928224758803.pkl'}\n",
      "2019-03-18 10:00:21.946676, fold=4, rep=1, eta=0d 0h 2m 3s \n",
      "{'fold': 4, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 1.0456587076187134, 'train_time': 10.232514630071819, 'prior_train_nmll': 1.257266640663147, 'train_nll': 615.347412109375, 'test_nll': 91.94681549072266, 'train_mse': 0.4902186095714569, 'state_dict_file': 'model_state_dict_-3798468290146976260.pkl'}\n",
      "2019-03-18 10:00:36.300111, fold=5, rep=0, eta=0d 0h 1m 52s \n",
      "{'fold': 5, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.585999071598053, 'train_time': 14.350193849066272, 'prior_train_nmll': 1.2845748662948608, 'train_nll': 665.3302612304688, 'test_nll': 71.6017837524414, 'train_mse': 0.5156102180480957, 'state_dict_file': 'model_state_dict_3385258783861456794.pkl'}\n",
      "2019-03-18 10:00:48.961412, fold=5, rep=1, eta=0d 0h 1m 40s \n",
      "{'fold': 5, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.5861114263534546, 'train_time': 12.661098508047871, 'prior_train_nmll': 1.285695195198059, 'train_nll': 623.48291015625, 'test_nll': 72.08039855957031, 'train_mse': 0.51945561170578, 'state_dict_file': 'model_state_dict_-8297335223830286877.pkl'}\n",
      "2019-03-18 10:00:57.231251, fold=6, rep=0, eta=0d 0h 1m 25s \n",
      "{'fold': 6, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.8378850817680359, 'train_time': 8.266592692933045, 'prior_train_nmll': 1.2626769542694092, 'train_nll': 635.7675170898438, 'test_nll': 81.19429016113281, 'train_mse': 0.5305231809616089, 'state_dict_file': 'model_state_dict_6795318651583842945.pkl'}\n",
      "2019-03-18 10:01:05.246685, fold=6, rep=1, eta=0d 0h 1m 11s \n",
      "{'fold': 6, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.8402398228645325, 'train_time': 8.015237363986671, 'prior_train_nmll': 1.2651731967926025, 'train_nll': 636.3592529296875, 'test_nll': 81.31936645507812, 'train_mse': 0.5319481492042542, 'state_dict_file': 'model_state_dict_-5222655919536350689.pkl'}\n",
      "2019-03-18 10:01:14.629568, fold=7, rep=0, eta=0d 0h 0m 58s \n",
      "{'fold': 7, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.9018601775169373, 'train_time': 9.379632359952666, 'prior_train_nmll': 1.2689841985702515, 'train_nll': 636.8683471679688, 'test_nll': 82.69920349121094, 'train_mse': 0.5293565392494202, 'state_dict_file': 'model_state_dict_6890757278280030356.pkl'}\n",
      "2019-03-18 10:01:23.033925, fold=7, rep=1, eta=0d 0h 0m 46s \n",
      "{'fold': 7, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.903730571269989, 'train_time': 8.404152605915442, 'prior_train_nmll': 1.264748454093933, 'train_nll': 637.3778686523438, 'test_nll': 82.68177795410156, 'train_mse': 0.5304521322250366, 'state_dict_file': 'model_state_dict_-318121608134850036.pkl'}\n",
      "2019-03-18 10:01:32.617639, fold=8, rep=0, eta=0d 0h 0m 34s \n",
      "{'fold': 8, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.7309442162513733, 'train_time': 9.580369036993943, 'prior_train_nmll': 1.279163122177124, 'train_nll': 616.969970703125, 'test_nll': 77.66484069824219, 'train_mse': 0.5056763887405396, 'state_dict_file': 'model_state_dict_-1711061530862476996.pkl'}\n",
      "2019-03-18 10:01:45.624802, fold=8, rep=1, eta=0d 0h 0m 23s \n",
      "{'fold': 8, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.7370921969413757, 'train_time': 13.00694097392261, 'prior_train_nmll': 1.2827869653701782, 'train_nll': 1004.2607421875, 'test_nll': 77.2896728515625, 'train_mse': 0.5004479885101318, 'state_dict_file': 'model_state_dict_5227163137805749183.pkl'}\n",
      "2019-03-18 10:01:57.311422, fold=9, rep=0, eta=0d 0h 0m 11s \n",
      "{'fold': 9, 'repeat': 0, 'n': 630, 'd': 7, 'mse': 0.7074057459831238, 'train_time': 11.683290380053222, 'prior_train_nmll': 1.274959683418274, 'train_nll': 633.7503051757812, 'test_nll': 75.88270568847656, 'train_mse': 0.5244824290275574, 'state_dict_file': 'model_state_dict_2820571265025919862.pkl'}\n",
      "2019-03-18 10:02:06.283372, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 630, 'd': 7, 'mse': 0.7074930667877197, 'train_time': 8.971746021066792, 'prior_train_nmll': 1.2761027812957764, 'train_nll': 632.663818359375, 'test_nll': 76.003662109375, 'train_mse': 0.5250374674797058, 'state_dict_file': 'model_state_dict_-8299449213489652457.pkl'}\n",
      "2019-03-18 10:02:29.751189, fold=0, rep=0, eta=0d 0h 7m 25s \n",
      "{'fold': 0, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.00793090183287859, 'train_time': 23.44148771406617, 'prior_train_nmll': -0.7060234546661377, 'train_nll': -3511.456787109375, 'test_nll': -98.78671264648438, 'train_mse': 0.010262222029268742, 'state_dict_file': 'model_state_dict_-7882377480235775543.pkl'}\n",
      "2019-03-18 10:02:52.682640, fold=0, rep=1, eta=0d 0h 6m 57s \n",
      "{'fold': 0, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.007929202169179916, 'train_time': 22.93122426199261, 'prior_train_nmll': -0.7310975193977356, 'train_nll': -2684.103515625, 'test_nll': -249.97637939453125, 'train_mse': 0.010308865457773209, 'state_dict_file': 'model_state_dict_6279366537424247656.pkl'}\n",
      "2019-03-18 10:03:18.701595, fold=1, rep=0, eta=0d 0h 6m 50s \n",
      "{'fold': 1, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.016720430925488472, 'train_time': 26.015994466957636, 'prior_train_nmll': -0.7647963762283325, 'train_nll': 162927792.0, 'test_nll': -200.0325927734375, 'train_mse': 0.00933155883103609, 'state_dict_file': 'model_state_dict_-5983917683510514889.pkl'}\n",
      "2019-03-18 10:03:41.507875, fold=1, rep=1, eta=0d 0h 6m 20s \n",
      "{'fold': 1, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.016649506986141205, 'train_time': 22.806097461027093, 'prior_train_nmll': -0.7555626630783081, 'train_nll': 369.55242919921875, 'test_nll': -229.333740234375, 'train_mse': 0.009392489679157734, 'state_dict_file': 'model_state_dict_-5553213818264843267.pkl'}\n",
      "2019-03-18 10:04:07.192046, fold=2, rep=0, eta=0d 0h 6m 2s \n",
      "{'fold': 2, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.013164060190320015, 'train_time': 25.6807981840102, 'prior_train_nmll': -0.758584201335907, 'train_nll': -2958.03271484375, 'test_nll': -232.42547607421875, 'train_mse': 0.00973305944353342, 'state_dict_file': 'model_state_dict_7737287088832181869.pkl'}\n",
      "2019-03-18 10:04:35.766945, fold=2, rep=1, eta=0d 0h 5m 48s \n",
      "{'fold': 2, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.013494337908923626, 'train_time': 28.574651069007814, 'prior_train_nmll': -0.7404109239578247, 'train_nll': -2608.2802734375, 'test_nll': -144.79867553710938, 'train_mse': 0.009739711880683899, 'state_dict_file': 'model_state_dict_-4604331775050234077.pkl'}\n",
      "2019-03-18 10:05:01.924248, fold=3, rep=0, eta=0d 0h 5m 26s \n",
      "{'fold': 3, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.010396203026175499, 'train_time': 26.154177512973547, 'prior_train_nmll': -0.7319583892822266, 'train_nll': -2888.6298828125, 'test_nll': -224.69308471679688, 'train_mse': 0.010035848245024681, 'state_dict_file': 'model_state_dict_-1794079191885325666.pkl'}\n",
      "2019-03-18 10:05:28.124093, fold=3, rep=1, eta=0d 0h 5m 2s \n",
      "{'fold': 3, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.010368241928517818, 'train_time': 26.199649180052802, 'prior_train_nmll': -0.7409833073616028, 'train_nll': 4509888512.0, 'test_nll': -188.70046997070312, 'train_mse': 0.010039118118584156, 'state_dict_file': 'model_state_dict_-4100744448311223737.pkl'}\n",
      "2019-03-18 10:05:52.017122, fold=4, rep=0, eta=0d 0h 4m 35s \n",
      "{'fold': 4, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.00846836343407631, 'train_time': 23.889888686011545, 'prior_train_nmll': -0.7239390015602112, 'train_nll': -3469.848876953125, 'test_nll': -132.9818115234375, 'train_mse': 0.010241160169243813, 'state_dict_file': 'model_state_dict_-1163656125712248393.pkl'}\n",
      "2019-03-18 10:06:15.972085, fold=4, rep=1, eta=0d 0h 4m 9s \n",
      "{'fold': 4, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.008469781838357449, 'train_time': 23.954762822017074, 'prior_train_nmll': -0.7271324396133423, 'train_nll': -4457.84619140625, 'test_nll': 2430898.5, 'train_mse': 0.010250854305922985, 'state_dict_file': 'model_state_dict_-4456250031492137086.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 10:06:41.497291, fold=5, rep=0, eta=0d 0h 3m 45s \n",
      "{'fold': 5, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.013416287489235401, 'train_time': 25.521839366992936, 'prior_train_nmll': -0.7511962652206421, 'train_nll': 72656824.0, 'test_nll': -154.0302734375, 'train_mse': 0.009767429903149605, 'state_dict_file': 'model_state_dict_3563802791509689163.pkl'}\n",
      "2019-03-18 10:07:05.680005, fold=5, rep=1, eta=0d 0h 3m 19s \n",
      "{'fold': 5, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.013704989105463028, 'train_time': 24.182503337040544, 'prior_train_nmll': -0.7386513948440552, 'train_nll': 8802687.0, 'test_nll': -198.313232421875, 'train_mse': 0.009717265143990517, 'state_dict_file': 'model_state_dict_-8904366644127289458.pkl'}\n",
      "2019-03-18 10:07:32.285203, fold=6, rep=0, eta=0d 0h 2m 55s \n",
      "{'fold': 6, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.008258320391178131, 'train_time': 26.6021124930121, 'prior_train_nmll': -0.7194504737854004, 'train_nll': 5626819.0, 'test_nll': -137.468994140625, 'train_mse': 0.010234346613287926, 'state_dict_file': 'model_state_dict_8077351270255002706.pkl'}\n",
      "2019-03-18 10:07:55.910999, fold=6, rep=1, eta=0d 0h 2m 29s \n",
      "{'fold': 6, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.00836392492055893, 'train_time': 23.625609647948295, 'prior_train_nmll': -0.7198965549468994, 'train_nll': 9505756.0, 'test_nll': -77.6226806640625, 'train_mse': 0.010251176543533802, 'state_dict_file': 'model_state_dict_5665791499556960152.pkl'}\n",
      "2019-03-18 10:08:21.190361, fold=7, rep=0, eta=0d 0h 2m 4s \n",
      "{'fold': 7, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.01314457319676876, 'train_time': 25.275884383008815, 'prior_train_nmll': -0.7402914762496948, 'train_nll': -2918.435302734375, 'test_nll': 23750900.0, 'train_mse': 0.009739221073687077, 'state_dict_file': 'model_state_dict_-5142975272553339812.pkl'}\n",
      "2019-03-18 10:08:44.375345, fold=7, rep=1, eta=0d 0h 1m 39s \n",
      "{'fold': 7, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.012810742482542992, 'train_time': 23.184722116100602, 'prior_train_nmll': -0.7550244331359863, 'train_nll': -3505.602294921875, 'test_nll': -304.66754150390625, 'train_mse': 0.009728585369884968, 'state_dict_file': 'model_state_dict_7853060985504193468.pkl'}\n",
      "2019-03-18 10:09:06.832636, fold=8, rep=0, eta=0d 0h 1m 14s \n",
      "{'fold': 8, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.007223609369248152, 'train_time': 22.454056379036047, 'prior_train_nmll': -0.7193732857704163, 'train_nll': -3506.514404296875, 'test_nll': 252704336.0, 'train_mse': 0.010321629233658314, 'state_dict_file': 'model_state_dict_-6048841327869518552.pkl'}\n",
      "2019-03-18 10:09:30.326197, fold=8, rep=1, eta=0d 0h 0m 49s \n",
      "{'fold': 8, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.007388214115053415, 'train_time': 23.493378815008327, 'prior_train_nmll': -0.7196773290634155, 'train_nll': 23064384.0, 'test_nll': -247.59768676757812, 'train_mse': 0.010332666337490082, 'state_dict_file': 'model_state_dict_-7407194679652229306.pkl'}\n",
      "2019-03-18 10:09:55.693907, fold=9, rep=0, eta=0d 0h 0m 24s \n",
      "{'fold': 9, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.009720366448163986, 'train_time': 25.36430911102798, 'prior_train_nmll': -0.7338261604309082, 'train_nll': 328.37274169921875, 'test_nll': -193.03878784179688, 'train_mse': 0.010047078132629395, 'state_dict_file': 'model_state_dict_211020666766323257.pkl'}\n",
      "2019-03-18 10:10:19.703067, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 768, 'd': 6, 'mse': 0.009756423532962799, 'train_time': 24.00894677999895, 'prior_train_nmll': -0.7274137139320374, 'train_nll': -3066.457275390625, 'test_nll': 53645740.0, 'train_mse': 0.010073637589812279, 'state_dict_file': 'model_state_dict_-722610201581742034.pkl'}\n",
      "2019-03-18 10:10:55.554688, fold=0, rep=0, eta=0d 0h 11m 20s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.2943589389324188, 'train_time': 35.826099135912955, 'prior_train_nmll': 0.4248936176300049, 'train_nll': -1630.73291015625, 'test_nll': -16.2774658203125, 'train_mse': 0.09339378029108047, 'state_dict_file': 'model_state_dict_5666863022075130067.pkl'}\n",
      "2019-03-18 10:11:28.963644, fold=0, rep=1, eta=0d 0h 10m 23s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.2896733582019806, 'train_time': 33.40866598195862, 'prior_train_nmll': 0.40776002407073975, 'train_nll': 8898.0126953125, 'test_nll': -4042.068115234375, 'train_mse': 0.09342686086893082, 'state_dict_file': 'model_state_dict_36719831047607345.pkl'}\n",
      "2019-03-18 10:12:05.775793, fold=1, rep=0, eta=0d 0h 10m 0s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.20934945344924927, 'train_time': 36.80905890907161, 'prior_train_nmll': 0.4391075372695923, 'train_nll': -1246.91552734375, 'test_nll': -631.7608642578125, 'train_mse': 0.09539537876844406, 'state_dict_file': 'model_state_dict_-3700335380884049702.pkl'}\n",
      "2019-03-18 10:12:42.783673, fold=1, rep=1, eta=0d 0h 9m 32s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.2090538889169693, 'train_time': 37.00764849409461, 'prior_train_nmll': 0.43455708026885986, 'train_nll': 53873.40625, 'test_nll': 785.239501953125, 'train_mse': 0.0953940600156784, 'state_dict_file': 'model_state_dict_9004356790767453529.pkl'}\n",
      "2019-03-18 10:13:16.017409, fold=2, rep=0, eta=0d 0h 8m 48s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.09111332148313522, 'train_time': 33.23063758306671, 'prior_train_nmll': 0.4762555658817291, 'train_nll': -1596.70166015625, 'test_nll': 6201.36474609375, 'train_mse': 0.10398086905479431, 'state_dict_file': 'model_state_dict_-6722049668589417970.pkl'}\n",
      "2019-03-18 10:13:50.059955, fold=2, rep=1, eta=0d 0h 8m 10s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.0912582278251648, 'train_time': 34.04225887695793, 'prior_train_nmll': 0.4656372368335724, 'train_nll': -1085.59130859375, 'test_nll': 263.6630859375, 'train_mse': 0.10398636013269424, 'state_dict_file': 'model_state_dict_-940325433608176204.pkl'}\n",
      "2019-03-18 10:14:24.223377, fold=3, rep=0, eta=0d 0h 7m 34s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.25853419303894043, 'train_time': 34.160443259053864, 'prior_train_nmll': 0.39690545201301575, 'train_nll': 1735.431884765625, 'test_nll': 794.8927001953125, 'train_mse': 0.09258652478456497, 'state_dict_file': 'model_state_dict_240602054810785967.pkl'}\n",
      "2019-03-18 10:15:09.193072, fold=3, rep=1, eta=0d 0h 7m 14s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.25762999057769775, 'train_time': 44.96921738702804, 'prior_train_nmll': 0.40936172008514404, 'train_nll': 7230.52001953125, 'test_nll': 518021.96875, 'train_mse': 0.09263474494218826, 'state_dict_file': 'model_state_dict_1820389813865159438.pkl'}\n",
      "2019-03-18 10:15:45.827668, fold=4, rep=0, eta=0d 0h 6m 38s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.18325284123420715, 'train_time': 36.631536122993566, 'prior_train_nmll': 0.4431324601173401, 'train_nll': 9232.111328125, 'test_nll': 20519.5546875, 'train_mse': 0.0987226590514183, 'state_dict_file': 'model_state_dict_-5631082816838660881.pkl'}\n",
      "2019-03-18 10:16:23.000365, fold=4, rep=1, eta=0d 0h 6m 3s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.18376551568508148, 'train_time': 37.172443440998904, 'prior_train_nmll': 0.45398053526878357, 'train_nll': -1457.23974609375, 'test_nll': 1015.8880615234375, 'train_mse': 0.09871915727853775, 'state_dict_file': 'model_state_dict_6848416868658728339.pkl'}\n",
      "2019-03-18 10:16:56.645258, fold=5, rep=0, eta=0d 0h 5m 24s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.18083380162715912, 'train_time': 33.64112236700021, 'prior_train_nmll': 0.430547833442688, 'train_nll': 10258.779296875, 'test_nll': -344.3722229003906, 'train_mse': 0.09718821197748184, 'state_dict_file': 'model_state_dict_2031011765618676105.pkl'}\n",
      "2019-03-18 10:17:38.611195, fold=5, rep=1, eta=0d 0h 4m 52s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.1806260049343109, 'train_time': 41.96568725502584, 'prior_train_nmll': 0.4484502375125885, 'train_nll': 29216.779296875, 'test_nll': -2251.689208984375, 'train_mse': 0.09702571481466293, 'state_dict_file': 'model_state_dict_7782541482818740114.pkl'}\n",
      "2019-03-18 10:18:14.793198, fold=6, rep=0, eta=0d 0h 4m 15s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.08822093904018402, 'train_time': 36.17896450590342, 'prior_train_nmll': 0.47477078437805176, 'train_nll': -2097.434326171875, 'test_nll': -41.21623229980469, 'train_mse': 0.10594957321882248, 'state_dict_file': 'model_state_dict_8355194342663172166.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 10:18:58.453856, fold=6, rep=1, eta=0d 0h 3m 42s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.08999788761138916, 'train_time': 43.66024388000369, 'prior_train_nmll': 0.4747215807437897, 'train_nll': 9671.7021484375, 'test_nll': -32.370208740234375, 'train_mse': 0.10578848421573639, 'state_dict_file': 'model_state_dict_-1984113298785727397.pkl'}\n",
      "2019-03-18 10:19:38.000333, fold=7, rep=0, eta=0d 0h 3m 6s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.1455375850200653, 'train_time': 39.5434177609859, 'prior_train_nmll': 0.4662531018257141, 'train_nll': 10152.7236328125, 'test_nll': 327.6105041503906, 'train_mse': 0.10386054217815399, 'state_dict_file': 'model_state_dict_8018127297481181263.pkl'}\n",
      "2019-03-18 10:20:17.498857, fold=7, rep=1, eta=0d 0h 2m 29s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.14539887011051178, 'train_time': 39.49820475594606, 'prior_train_nmll': 0.4703485667705536, 'train_nll': -871.0419311523438, 'test_nll': 374.5824279785156, 'train_mse': 0.1038062646985054, 'state_dict_file': 'model_state_dict_-4858077505934856651.pkl'}\n",
      "2019-03-18 10:20:50.744136, fold=8, rep=0, eta=0d 0h 1m 51s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.12104283273220062, 'train_time': 33.24240169895347, 'prior_train_nmll': 0.45773011445999146, 'train_nll': -838.8012084960938, 'test_nll': -108.44358825683594, 'train_mse': 0.10058847069740295, 'state_dict_file': 'model_state_dict_5852172074510616790.pkl'}\n",
      "2019-03-18 10:21:31.067404, fold=8, rep=1, eta=0d 0h 1m 14s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.12017574906349182, 'train_time': 40.32289370696526, 'prior_train_nmll': 0.4576641917228699, 'train_nll': 10164.8076171875, 'test_nll': 1569.429931640625, 'train_mse': 0.10051783174276352, 'state_dict_file': 'model_state_dict_-9216826577303925182.pkl'}\n",
      "2019-03-18 10:22:05.011043, fold=9, rep=0, eta=0d 0h 0m 37s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1030, 'd': 6, 'mse': 0.11791686713695526, 'train_time': 33.94070928101428, 'prior_train_nmll': 0.4629298150539398, 'train_nll': -1600.07958984375, 'test_nll': 948.4515380859375, 'train_mse': 0.10122096538543701, 'state_dict_file': 'model_state_dict_-5696627344126914013.pkl'}\n",
      "2019-03-18 10:22:39.885784, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1030, 'd': 6, 'mse': 0.11951915919780731, 'train_time': 34.87450759310741, 'prior_train_nmll': 0.4558018743991852, 'train_nll': -1177.884521484375, 'test_nll': -3.3228836059570312, 'train_mse': 0.10127117484807968, 'state_dict_file': 'model_state_dict_6653323807762401982.pkl'}\n",
      "2019-03-18 10:23:03.200851, fold=0, rep=0, eta=0d 0h 7m 22s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.7912361025810242, 'train_time': 23.28645236196462, 'prior_train_nmll': 1.3551827669143677, 'train_nll': 1273.6143798828125, 'test_nll': 139.56027221679688, 'train_mse': 0.8282080292701721, 'state_dict_file': 'model_state_dict_7672958819107161904.pkl'}\n",
      "2019-03-18 10:23:35.224995, fold=0, rep=1, eta=0d 0h 8m 17s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.7904981374740601, 'train_time': 32.02388906804845, 'prior_train_nmll': 1.3549543619155884, 'train_nll': 1274.04736328125, 'test_nll': 139.5196533203125, 'train_mse': 0.8286258578300476, 'state_dict_file': 'model_state_dict_-6044803835778871925.pkl'}\n",
      "2019-03-18 10:24:03.754302, fold=1, rep=0, eta=0d 0h 7m 55s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.6615018844604492, 'train_time': 28.526158155989833, 'prior_train_nmll': 1.3640391826629639, 'train_nll': 1283.0533447265625, 'test_nll': 130.9544219970703, 'train_mse': 0.8439452648162842, 'state_dict_file': 'model_state_dict_2566223177733592670.pkl'}\n",
      "2019-03-18 10:24:30.972189, fold=1, rep=1, eta=0d 0h 7m 24s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.6607114672660828, 'train_time': 27.217540540965274, 'prior_train_nmll': 1.3640393018722534, 'train_nll': 1283.2744140625, 'test_nll': 130.95748901367188, 'train_mse': 0.84449303150177, 'state_dict_file': 'model_state_dict_624265980935606891.pkl'}\n",
      "2019-03-18 10:24:55.994288, fold=2, rep=0, eta=0d 0h 6m 48s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.0162521600723267, 'train_time': 25.018935612984933, 'prior_train_nmll': 1.3408743143081665, 'train_nll': 1263.10888671875, 'test_nll': 153.40623474121094, 'train_mse': 0.8098695874214172, 'state_dict_file': 'model_state_dict_-508825371004056414.pkl'}\n",
      "2019-03-18 10:25:23.049245, fold=2, rep=1, eta=0d 0h 6m 20s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.0143712759017944, 'train_time': 27.05419078806881, 'prior_train_nmll': 1.340987205505371, 'train_nll': 1262.9017333984375, 'test_nll': 153.32861328125, 'train_mse': 0.8089299201965332, 'state_dict_file': 'model_state_dict_1424256101320524046.pkl'}\n",
      "2019-03-18 10:25:54.800714, fold=3, rep=0, eta=0d 0h 6m 1s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.5986762642860413, 'train_time': 31.748443426098675, 'prior_train_nmll': 1.367774248123169, 'train_nll': 1288.756591796875, 'test_nll': 127.19498443603516, 'train_mse': 0.8537840843200684, 'state_dict_file': 'model_state_dict_-9187742090793777424.pkl'}\n",
      "2019-03-18 10:26:29.605742, fold=3, rep=1, eta=0d 0h 5m 44s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.5988895893096924, 'train_time': 34.804732772987336, 'prior_train_nmll': 1.3681210279464722, 'train_nll': 1289.0009765625, 'test_nll': 127.20805358886719, 'train_mse': 0.8542295694351196, 'state_dict_file': 'model_state_dict_-5743045596339771865.pkl'}\n",
      "2019-03-18 10:27:01.581423, fold=4, rep=0, eta=0d 0h 5m 19s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.35764554142951965, 'train_time': 31.972624965943396, 'prior_train_nmll': 1.3833616971969604, 'train_nll': 1302.05126953125, 'test_nll': 113.57418060302734, 'train_mse': 0.8769720196723938, 'state_dict_file': 'model_state_dict_-6286884458060335699.pkl'}\n",
      "2019-03-18 10:27:31.560043, fold=4, rep=1, eta=0d 0h 4m 51s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.35924097895622253, 'train_time': 29.97791541693732, 'prior_train_nmll': 1.383583664894104, 'train_nll': 1301.552734375, 'test_nll': 113.24674987792969, 'train_mse': 0.8761312961578369, 'state_dict_file': 'model_state_dict_568611628431581716.pkl'}\n",
      "2019-03-18 10:27:52.195758, fold=5, rep=0, eta=0d 0h 4m 15s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.443103313446045, 'train_time': 20.632643452030607, 'prior_train_nmll': 1.3114420175552368, 'train_nll': 1231.527587890625, 'test_nll': 182.6605224609375, 'train_mse': 0.7577691674232483, 'state_dict_file': 'model_state_dict_-3753272366358820514.pkl'}\n",
      "2019-03-18 10:28:13.931619, fold=5, rep=1, eta=0d 0h 3m 42s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.4428824186325073, 'train_time': 21.73557755094953, 'prior_train_nmll': 1.311132550239563, 'train_nll': 1231.2554931640625, 'test_nll': 182.89511108398438, 'train_mse': 0.7578216791152954, 'state_dict_file': 'model_state_dict_-3970541343200774073.pkl'}\n",
      "2019-03-18 10:28:36.612612, fold=6, rep=0, eta=0d 0h 3m 12s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.3161249160766602, 'train_time': 22.67782757396344, 'prior_train_nmll': 1.3191746473312378, 'train_nll': 1241.00732421875, 'test_nll': 174.62606811523438, 'train_mse': 0.7732707262039185, 'state_dict_file': 'model_state_dict_7208880552750304890.pkl'}\n",
      "2019-03-18 10:28:57.210275, fold=6, rep=1, eta=0d 0h 2m 41s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.3159091472625732, 'train_time': 20.596908280975185, 'prior_train_nmll': 1.3190170526504517, 'train_nll': 1241.143798828125, 'test_nll': 174.88565063476562, 'train_mse': 0.7727273106575012, 'state_dict_file': 'model_state_dict_-3487937538137235830.pkl'}\n",
      "2019-03-18 10:29:19.769560, fold=7, rep=0, eta=0d 0h 2m 13s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 1.1265075206756592, 'train_time': 22.555562902009115, 'prior_train_nmll': 1.3346079587936401, 'train_nll': 1256.463623046875, 'test_nll': 159.02188110351562, 'train_mse': 0.7990365624427795, 'state_dict_file': 'model_state_dict_-3794257233508999524.pkl'}\n",
      "2019-03-18 10:29:44.879498, fold=7, rep=1, eta=0d 0h 1m 46s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 1.126706600189209, 'train_time': 25.10959740006365, 'prior_train_nmll': 1.3347198963165283, 'train_nll': 1256.183349609375, 'test_nll': 158.76779174804688, 'train_mse': 0.798462450504303, 'state_dict_file': 'model_state_dict_-8714831924689816758.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 10:30:13.762660, fold=8, rep=0, eta=0d 0h 1m 20s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.46332234144210815, 'train_time': 28.880000848905183, 'prior_train_nmll': 1.376152753829956, 'train_nll': 1297.1259765625, 'test_nll': 119.43507385253906, 'train_mse': 0.8692604899406433, 'state_dict_file': 'model_state_dict_2604352524265896101.pkl'}\n",
      "2019-03-18 10:30:41.062339, fold=8, rep=1, eta=0d 0h 0m 53s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.4630216658115387, 'train_time': 27.29929180792533, 'prior_train_nmll': 1.3765826225280762, 'train_nll': 1296.8875732421875, 'test_nll': 119.38835906982422, 'train_mse': 0.8688446283340454, 'state_dict_file': 'model_state_dict_7473946519165477459.pkl'}\n",
      "2019-03-18 10:31:09.646444, fold=9, rep=0, eta=0d 0h 0m 26s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1066, 'd': 8, 'mse': 0.8773569464683533, 'train_time': 28.58095117798075, 'prior_train_nmll': 1.34923255443573, 'train_nll': 1276.26953125, 'test_nll': 139.42672729492188, 'train_mse': 0.8220434188842773, 'state_dict_file': 'model_state_dict_6655168086877659740.pkl'}\n",
      "2019-03-18 10:31:39.789470, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1066, 'd': 8, 'mse': 0.8775131702423096, 'train_time': 30.14276649605017, 'prior_train_nmll': 1.3490657806396484, 'train_nll': 1275.8212890625, 'test_nll': 139.44717407226562, 'train_mse': 0.8221511840820312, 'state_dict_file': 'model_state_dict_47096696348964559.pkl'}\n",
      "2019-03-18 10:32:08.441739, fold=0, rep=0, eta=0d 0h 9m 3s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.4679909646511078, 'train_time': 28.622357108048163, 'prior_train_nmll': 1.0292949676513672, 'train_nll': 1327.8592529296875, 'test_nll': 157.14407348632812, 'train_mse': 0.4128008484840393, 'state_dict_file': 'model_state_dict_-8202177851228078472.pkl'}\n",
      "2019-03-18 10:32:45.178905, fold=0, rep=1, eta=0d 0h 9m 48s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.4685434103012085, 'train_time': 36.73690588294994, 'prior_train_nmll': 1.032030701637268, 'train_nll': 1327.826416015625, 'test_nll': 156.59547424316406, 'train_mse': 0.41288483142852783, 'state_dict_file': 'model_state_dict_7622954593970958138.pkl'}\n",
      "2019-03-18 10:33:09.406921, fold=1, rep=0, eta=0d 0h 8m 27s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.47490090131759644, 'train_time': 24.224851339007728, 'prior_train_nmll': 1.0309733152389526, 'train_nll': 1325.191162109375, 'test_nll': 158.95343017578125, 'train_mse': 0.41149190068244934, 'state_dict_file': 'model_state_dict_3015082037994898084.pkl'}\n",
      "2019-03-18 10:33:33.467608, fold=1, rep=1, eta=0d 0h 7m 34s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.47490501403808594, 'train_time': 24.060304023092613, 'prior_train_nmll': 1.030820369720459, 'train_nll': 1326.23486328125, 'test_nll': 158.7340087890625, 'train_mse': 0.41144484281539917, 'state_dict_file': 'model_state_dict_-228033617568782928.pkl'}\n",
      "2019-03-18 10:34:05.103340, fold=2, rep=0, eta=0d 0h 7m 15s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.4669587016105652, 'train_time': 31.632064376957715, 'prior_train_nmll': 1.0354431867599487, 'train_nll': 1326.00927734375, 'test_nll': 157.1844940185547, 'train_mse': 0.41291460394859314, 'state_dict_file': 'model_state_dict_8151872761885047588.pkl'}\n",
      "2019-03-18 10:34:35.740592, fold=2, rep=1, eta=0d 0h 6m 50s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.4671877920627594, 'train_time': 30.636969759012572, 'prior_train_nmll': 1.034080147743225, 'train_nll': 1327.029541015625, 'test_nll': 157.01290893554688, 'train_mse': 0.4129195213317871, 'state_dict_file': 'model_state_dict_-289718852685038272.pkl'}\n",
      "2019-03-18 10:35:04.752910, fold=3, rep=0, eta=0d 0h 6m 20s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.39563602209091187, 'train_time': 29.00917769689113, 'prior_train_nmll': 1.0412027835845947, 'train_nll': 1340.09912109375, 'test_nll': 144.80589294433594, 'train_mse': 0.42095252871513367, 'state_dict_file': 'model_state_dict_-4097769364374353764.pkl'}\n",
      "2019-03-18 10:35:30.793911, fold=3, rep=1, eta=0d 0h 5m 46s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.3955334722995758, 'train_time': 26.040708717075177, 'prior_train_nmll': 1.0422239303588867, 'train_nll': 1338.52294921875, 'test_nll': 144.89212036132812, 'train_mse': 0.42096254229545593, 'state_dict_file': 'model_state_dict_2570528705594858264.pkl'}\n",
      "2019-03-18 10:35:56.170562, fold=4, rep=0, eta=0d 0h 5m 13s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.5107420086860657, 'train_time': 25.373558365972713, 'prior_train_nmll': 1.032908320426941, 'train_nll': 1320.747314453125, 'test_nll': 164.31021118164062, 'train_mse': 0.40856462717056274, 'state_dict_file': 'model_state_dict_2427177640194821171.pkl'}\n",
      "2019-03-18 10:36:28.505977, fold=4, rep=1, eta=0d 0h 4m 48s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.5128888487815857, 'train_time': 32.33480514807161, 'prior_train_nmll': 1.028516411781311, 'train_nll': 1319.2957763671875, 'test_nll': 165.22991943359375, 'train_mse': 0.40842944383621216, 'state_dict_file': 'model_state_dict_-3368431434613729502.pkl'}\n",
      "2019-03-18 10:36:55.958422, fold=5, rep=0, eta=0d 0h 4m 18s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.4037492275238037, 'train_time': 27.449437720002607, 'prior_train_nmll': 1.0441524982452393, 'train_nll': 1339.680908203125, 'test_nll': 146.27667236328125, 'train_mse': 0.4204731285572052, 'state_dict_file': 'model_state_dict_4825934406670368026.pkl'}\n",
      "2019-03-18 10:37:24.054153, fold=5, rep=1, eta=0d 0h 3m 49s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.40412336587905884, 'train_time': 28.095456621958874, 'prior_train_nmll': 1.0420217514038086, 'train_nll': 1341.75244140625, 'test_nll': 146.52516174316406, 'train_mse': 0.4206158220767975, 'state_dict_file': 'model_state_dict_-5851262694263775002.pkl'}\n",
      "2019-03-18 10:37:56.446130, fold=6, rep=0, eta=0d 0h 3m 22s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.4483359158039093, 'train_time': 32.3888505109353, 'prior_train_nmll': 1.0350539684295654, 'train_nll': 1330.400146484375, 'test_nll': 154.46519470214844, 'train_mse': 0.4148317575454712, 'state_dict_file': 'model_state_dict_3188416311144975685.pkl'}\n",
      "2019-03-18 10:38:23.418554, fold=6, rep=1, eta=0d 0h 2m 52s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.4481872022151947, 'train_time': 26.971611337037757, 'prior_train_nmll': 1.0343672037124634, 'train_nll': 1330.24658203125, 'test_nll': 154.463623046875, 'train_mse': 0.4147703945636749, 'state_dict_file': 'model_state_dict_-423502903388188393.pkl'}\n",
      "2019-03-18 10:38:52.631090, fold=7, rep=0, eta=0d 0h 2m 24s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.3531140685081482, 'train_time': 29.20952180889435, 'prior_train_nmll': 1.0486174821853638, 'train_nll': 1349.9788818359375, 'test_nll': 137.5473175048828, 'train_mse': 0.42614537477493286, 'state_dict_file': 'model_state_dict_-1188731205439290757.pkl'}\n",
      "2019-03-18 10:39:19.553543, fold=7, rep=1, eta=0d 0h 1m 54s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.3530924618244171, 'train_time': 26.921630374970846, 'prior_train_nmll': 1.04878830909729, 'train_nll': 1349.6376953125, 'test_nll': 137.47421264648438, 'train_mse': 0.4261213541030884, 'state_dict_file': 'model_state_dict_78656442187915738.pkl'}\n",
      "2019-03-18 10:39:54.888833, fold=8, rep=0, eta=0d 0h 1m 27s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.4159112572669983, 'train_time': 35.332245965022594, 'prior_train_nmll': 1.0395638942718506, 'train_nll': 1329.692138671875, 'test_nll': 148.33865356445312, 'train_mse': 0.4178641438484192, 'state_dict_file': 'model_state_dict_-3567658654800988085.pkl'}\n",
      "2019-03-18 10:40:26.980504, fold=8, rep=1, eta=0d 0h 0m 58s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.41586634516716003, 'train_time': 32.09130480105523, 'prior_train_nmll': 1.0418559312820435, 'train_nll': 1336.4593505859375, 'test_nll': 147.92904663085938, 'train_mse': 0.4179173707962036, 'state_dict_file': 'model_state_dict_7984401536092238601.pkl'}\n",
      "2019-03-18 10:40:56.402020, fold=9, rep=0, eta=0d 0h 0m 29s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1503, 'd': 3, 'mse': 0.37929007411003113, 'train_time': 29.41846807510592, 'prior_train_nmll': 1.0421068668365479, 'train_nll': 1359.296630859375, 'test_nll': 135.8194580078125, 'train_mse': 0.42183631658554077, 'state_dict_file': 'model_state_dict_5754377839509796391.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 10:41:28.228322, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1503, 'd': 3, 'mse': 0.3793793022632599, 'train_time': 31.826025624992326, 'prior_train_nmll': 1.0453119277954102, 'train_nll': 1348.7489013671875, 'test_nll': 135.62794494628906, 'train_mse': 0.4218611717224121, 'state_dict_file': 'model_state_dict_-7409565454793008065.pkl'}\n",
      "2019-03-18 10:42:57.927416, fold=0, rep=0, eta=0d 0h 28m 23s \n",
      "{'fold': 0, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.23607632517814636, 'train_time': 89.67286666401196, 'prior_train_nmll': 0.7958605289459229, 'train_nll': -50.962646484375, 'test_nll': 8107252.5, 'train_mse': 0.20651544630527496, 'state_dict_file': 'model_state_dict_2131762486211839977.pkl'}\n",
      "2019-03-18 10:44:24.032709, fold=0, rep=1, eta=0d 0h 26m 22s \n",
      "{'fold': 0, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.23474960029125214, 'train_time': 86.10489684296772, 'prior_train_nmll': 0.7979878783226013, 'train_nll': 265.9007568359375, 'test_nll': 277.88543701171875, 'train_mse': 0.20667025446891785, 'state_dict_file': 'model_state_dict_3261790070621252187.pkl'}\n",
      "2019-03-18 10:45:50.123896, fold=1, rep=0, eta=0d 0h 24m 43s \n",
      "{'fold': 1, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.20356972515583038, 'train_time': 86.08793385303579, 'prior_train_nmll': 0.7984981536865234, 'train_nll': -1446.07421875, 'test_nll': 4677.39599609375, 'train_mse': 0.20875044167041779, 'state_dict_file': 'model_state_dict_8517266886023574904.pkl'}\n",
      "2019-03-18 10:47:30.914212, fold=1, rep=1, eta=0d 0h 24m 10s \n",
      "{'fold': 1, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.20322617888450623, 'train_time': 100.79001660703216, 'prior_train_nmll': 0.7873465418815613, 'train_nll': -711.3243408203125, 'test_nll': 248.6485137939453, 'train_mse': 0.20836815237998962, 'state_dict_file': 'model_state_dict_788393464022218754.pkl'}\n",
      "2019-03-18 10:48:53.410390, fold=2, rep=0, eta=0d 0h 22m 15s \n",
      "{'fold': 2, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.22344079613685608, 'train_time': 82.49295427999459, 'prior_train_nmll': 0.7846978902816772, 'train_nll': -519.36962890625, 'test_nll': 274.47174072265625, 'train_mse': 0.20697946846485138, 'state_dict_file': 'model_state_dict_-1135288121510003444.pkl'}\n",
      "2019-03-18 10:50:26.100108, fold=2, rep=1, eta=0d 0h 20m 54s \n",
      "{'fold': 2, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2246919572353363, 'train_time': 92.68941841099877, 'prior_train_nmll': 0.7945154309272766, 'train_nll': -120.853271484375, 'test_nll': 115.0425033569336, 'train_mse': 0.2067417949438095, 'state_dict_file': 'model_state_dict_4719054424500144561.pkl'}\n",
      "2019-03-18 10:52:07.549428, fold=3, rep=0, eta=0d 0h 19m 47s \n",
      "{'fold': 3, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.24631249904632568, 'train_time': 101.44619310798589, 'prior_train_nmll': 0.7869650721549988, 'train_nll': -945.630126953125, 'test_nll': 665.4522705078125, 'train_mse': 0.20476482808589935, 'state_dict_file': 'model_state_dict_7514743833844452732.pkl'}\n",
      "2019-03-18 10:53:39.126603, fold=3, rep=1, eta=0d 0h 18m 16s \n",
      "{'fold': 3, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.24621324241161346, 'train_time': 91.57682575308718, 'prior_train_nmll': 0.7852976322174072, 'train_nll': -649.612060546875, 'test_nll': 1205.8768310546875, 'train_mse': 0.20493420958518982, 'state_dict_file': 'model_state_dict_8372269763853609006.pkl'}\n",
      "2019-03-18 10:55:04.290408, fold=4, rep=0, eta=0d 0h 16m 37s \n",
      "{'fold': 4, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.2592950761318207, 'train_time': 85.16047678899486, 'prior_train_nmll': 0.7919126749038696, 'train_nll': 20.325439453125, 'test_nll': 325.41400146484375, 'train_mse': 0.20275963842868805, 'state_dict_file': 'model_state_dict_4070801064598053052.pkl'}\n",
      "2019-03-18 10:56:27.129797, fold=4, rep=1, eta=0d 0h 14m 58s \n",
      "{'fold': 4, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2588084042072296, 'train_time': 82.8390871809097, 'prior_train_nmll': 0.7817029356956482, 'train_nll': -129.90625, 'test_nll': 1189.245361328125, 'train_mse': 0.20281551778316498, 'state_dict_file': 'model_state_dict_-5252717478177201527.pkl'}\n",
      "2019-03-18 10:57:55.564431, fold=5, rep=0, eta=0d 0h 13m 27s \n",
      "{'fold': 5, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.31669947504997253, 'train_time': 88.4270044399891, 'prior_train_nmll': 0.7790209054946899, 'train_nll': 1247.09912109375, 'test_nll': 51.014678955078125, 'train_mse': 0.20383596420288086, 'state_dict_file': 'model_state_dict_-6644183341775755429.pkl'}\n",
      "2019-03-18 10:59:29.513850, fold=5, rep=1, eta=0d 0h 12m 0s \n",
      "{'fold': 5, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.3164379894733429, 'train_time': 93.94910021708347, 'prior_train_nmll': 0.7750490307807922, 'train_nll': -39.594482421875, 'test_nll': -5.1754913330078125, 'train_mse': 0.20378832519054413, 'state_dict_file': 'model_state_dict_-7540646082632203292.pkl'}\n",
      "2019-03-18 11:00:54.304144, fold=6, rep=0, eta=0d 0h 10m 27s \n",
      "{'fold': 6, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.2303193360567093, 'train_time': 84.78701205202378, 'prior_train_nmll': 0.7912403345108032, 'train_nll': 10.17919921875, 'test_nll': 139.41949462890625, 'train_mse': 0.20771758258342743, 'state_dict_file': 'model_state_dict_-5630482186693906010.pkl'}\n",
      "2019-03-18 11:02:25.359379, fold=6, rep=1, eta=0d 0h 8m 58s \n",
      "{'fold': 6, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.23095519840717316, 'train_time': 91.05489274591673, 'prior_train_nmll': 0.8015088438987732, 'train_nll': -742.04736328125, 'test_nll': 104.3461685180664, 'train_mse': 0.20767076313495636, 'state_dict_file': 'model_state_dict_5440812982127323330.pkl'}\n",
      "2019-03-18 11:03:58.603332, fold=7, rep=0, eta=0d 0h 7m 30s \n",
      "{'fold': 7, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.25243228673934937, 'train_time': 93.23719570599496, 'prior_train_nmll': 0.7985109686851501, 'train_nll': -77.256103515625, 'test_nll': 140.948486328125, 'train_mse': 0.20490194857120514, 'state_dict_file': 'model_state_dict_2627307492821709945.pkl'}\n",
      "2019-03-18 11:05:44.700785, fold=7, rep=1, eta=0d 0h 6m 4s \n",
      "{'fold': 7, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.25138723850250244, 'train_time': 106.09687845106237, 'prior_train_nmll': 0.7903088331222534, 'train_nll': -7506.57421875, 'test_nll': 176.79837036132812, 'train_mse': 0.20482110977172852, 'state_dict_file': 'model_state_dict_-4409717898516945615.pkl'}\n",
      "2019-03-18 11:07:03.079100, fold=8, rep=0, eta=0d 0h 4m 30s \n",
      "{'fold': 8, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.24931064248085022, 'train_time': 78.37499972793739, 'prior_train_nmll': 0.7896565794944763, 'train_nll': -589.12939453125, 'test_nll': 214.43951416015625, 'train_mse': 0.20420889556407928, 'state_dict_file': 'model_state_dict_2356687610877049901.pkl'}\n",
      "2019-03-18 11:08:26.865325, fold=8, rep=1, eta=0d 0h 2m 59s \n",
      "{'fold': 8, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2495245635509491, 'train_time': 83.78573998890352, 'prior_train_nmll': 0.7829902768135071, 'train_nll': -593.01123046875, 'test_nll': 413.62725830078125, 'train_mse': 0.20413607358932495, 'state_dict_file': 'model_state_dict_-5269473219637570462.pkl'}\n",
      "2019-03-18 11:09:48.893979, fold=9, rep=0, eta=0d 0h 1m 29s \n",
      "{'fold': 9, 'repeat': 0, 'n': 1599, 'd': 9, 'mse': 0.27480578422546387, 'train_time': 82.02550891099963, 'prior_train_nmll': 0.7891380190849304, 'train_nll': -637.446044921875, 'test_nll': 169.0540771484375, 'train_mse': 0.20382189750671387, 'state_dict_file': 'model_state_dict_6856332975076199851.pkl'}\n",
      "2019-03-18 11:11:31.146082, fold=9, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 9, 'repeat': 1, 'n': 1599, 'd': 9, 'mse': 0.2744188904762268, 'train_time': 102.25159726699349, 'prior_train_nmll': 0.7861227989196777, 'train_nll': -932.42578125, 'test_nll': 544.00732421875, 'train_mse': 0.2034567892551422, 'state_dict_file': 'model_state_dict_3067782811472379579.pkl'}\n"
     ]
    }
   ],
   "source": [
    "options = dict(verbose=False, rp=False, ard=False, activation=None, optimizer='adam',\n",
    "               n_epochs=1000, lr=0.1, patience=20, smooth=True, \n",
    "               noise_prior=True, additive=True)\n",
    "datasets = rp_experiments.get_small_datasets() + rp_experiments.get_medium_datasets()\n",
    "for dataset in datasets:\n",
    "    with gpytorch.settings.cg_tolerance(0.01):\n",
    "        result = rp_experiments.run_experiment(training_routines.train_exact_gp, options, \n",
    "                                     dataset=dataset, split=0.1, cv=True, repeats=2)\n",
    "    result['RP'] = False\n",
    "    result['additive'] = True\n",
    "    result['dataset'] = dataset\n",
    "    result['options'] = json.dumps(options)\n",
    "    df = pd.concat([df, result])\n",
    "    df.to_csv('./additive_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski_options = dict(verbose=True, rp=True, ard=False, activation=None, optimizer='adam',\n",
    "                   n_epochs=1000, lr=0.1, patience=5, k=1, J=20, smooth=True, \n",
    "                   noise_prior=True, learn_proj=False, ski=True, grid_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_options = dict(verbose=2, rp=True, ard=False, activation=None, optimizer='adam',\n",
    "                   n_epochs=1000, lr=0.1, patience=5, k=1, J=20, smooth=True, \n",
    "                   noise_prior=True, learn_proj=False, ski=False, grid_size=100,\n",
    "                   batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_rbf_options = dict(verbose=2, rp=False, ard=False, activation=None, optimizer='adam',\n",
    "                   n_epochs=1000, lr=0.1, patience=5, smooth=True, \n",
    "                   noise_prior=True, ski=False, grid_size=100,\n",
    "                   batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['gas', 'skillcraft', 'sml', 'parkinsons', 'pumadyn32nm',   # \"medium\"\n",
    "            'pol', 'elevators', 'bike', 'kin40k', 'protein', ]  # small ones of the big ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['keggdirected', 'slice', 'keggundirected', '3droad', 'song',\n",
    "     'buzz', 'houseelectric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin dataset  keggdirected\n",
      "Begin with SKI\n",
      "epoch 0, loss 0.7631620168685913\n",
      "epoch 1, loss 0.7262678742408752\n",
      "epoch 2, loss 0.689482569694519\n",
      "epoch 3, loss 0.6522213816642761\n",
      "epoch 4, loss 0.6134589910507202\n",
      "epoch 5, loss 0.5744760036468506\n",
      "epoch 6, loss 0.5347909927368164\n",
      "epoch 7, loss 0.4943849444389343\n",
      "epoch 8, loss 0.45330819487571716\n",
      "epoch 9, loss 0.41180941462516785\n",
      "epoch 10, loss 0.36999624967575073\n",
      "epoch 11, loss 0.32837581634521484\n",
      "epoch 12, loss 0.28506121039390564\n",
      "epoch 13, loss 0.24183648824691772\n",
      "epoch 14, loss 0.19837762415409088\n",
      "epoch 15, loss 0.15482211112976074\n",
      "epoch 16, loss 0.11090102046728134\n",
      "epoch 17, loss 0.06733899563550949\n",
      "epoch 18, loss 0.022920290008187294\n",
      "epoch 19, loss -0.02143421396613121\n",
      "epoch 20, loss -0.06546089798212051\n",
      "epoch 21, loss -0.10969710350036621\n",
      "epoch 22, loss -0.15323947370052338\n",
      "epoch 23, loss -0.19728626310825348\n",
      "epoch 24, loss -0.24016672372817993\n",
      "epoch 25, loss -0.2832379639148712\n",
      "epoch 26, loss -0.3251235783100128\n",
      "epoch 27, loss -0.36671319603919983\n",
      "epoch 28, loss -0.40736743807792664\n",
      "epoch 29, loss -0.44666585326194763\n",
      "epoch 30, loss -0.48559343814849854\n",
      "epoch 31, loss -0.5247125625610352\n",
      "epoch 32, loss -0.5593088865280151\n",
      "epoch 33, loss -0.5934612154960632\n",
      "epoch 34, loss -0.6259180307388306\n",
      "epoch 35, loss -0.657808244228363\n",
      "epoch 36, loss -0.686570942401886\n",
      "epoch 37, loss -0.7130815982818604\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3a5f2df7b264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             result = rp_experiments.run_experiment(\n\u001b[1;32m      8\u001b[0m                 \u001b[0mtraining_routines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_exact_gp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mski_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 dataset=dataset, split=0.2, cv=True, repeats=1)\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/rp_experiments.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(training_routine, training_options, dataset, split, cv, addl_metrics, repeats, error_repeats)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrepeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                     result_dict = {'fold': fold,\n\u001b[0m\u001b[1;32m    140\u001b[0m                                    \u001b[0;34m'repeat'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                                    \u001b[0;34m'n'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/training_routines.py\u001b[0m in \u001b[0;36mtrain_exact_gp\u001b[0;34m(trainX, trainY, testX, testY, rp, k, J, ard, activation, optimizer, n_epochs, lr, verbose, patience, smooth, noise_prior, ski, grid_ratio, grid_size, learn_proj, additive, weighted, kernel_type)\u001b[0m\n\u001b[1;32m    289\u001b[0m                          \u001b[0misloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                          smooth=smooth)\n\u001b[0m\u001b[1;32m    292\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mtrain_to_convergence\u001b[0;34m(model, xs, ys, optimizer, lr, objective, max_iter, verbose, patience, conv_tol, check_conv, smooth, isloss, batch_size)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch {}, iter {}, loss {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_validate_module_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, output, target, *params)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Get the log prob of the marginal distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Add terms for SGPR / when inducing points are learned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Get log determininat and first part of quadratic form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0minv_quad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_quad_logdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minv_quad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36minv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mprobe_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobe_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mprobe_vector_norms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobe_vector_norms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         )(*args)\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minv_quad_term\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce_inv_quad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/functions/_inv_quad_log_det.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mt_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdet\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_logdet_forward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0msolves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreconditioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tridiag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_random_probes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, rhs, preconditioner, num_tridiag)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cg_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mmax_tridiag_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_lanczos_quadrature_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mpreconditioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreconditioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/utils/linear_cg.py\u001b[0m in \u001b[0;36mlinear_cg\u001b[0;34m(matmul_closure, rhs, n_tridiag, tolerance, eps, stop_updating_after, max_iter, max_tridiag_iter, initial_guess, preconditioner)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# Get next alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# alpha_{k} = (residual_{k-1}^T precon_residual{k-1}) / (p_vec_{k-1}^T mat p_vec_{k-1})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mmvms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprecond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmul_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         return torch.addcmul(\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diag_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mrhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/constant_mul_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanded_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_quad_form_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_quad_form_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/interpolated_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# right_interp^T * rhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mright_interp_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbdsmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_interp_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# base_lazy_tensor * right_interp^T * rhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/utils/sparse.py\u001b[0m in \u001b[0;36mbdsmm\u001b[0;34m(sparse, dense)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdsmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with gpytorch.settings.cg_tolerance(0.01), gpytorch.settings.fast_pred_var(True):\n",
    "    for dataset in datasets:\n",
    "        print(\"Begin dataset \", dataset)\n",
    "        # with SKI\n",
    "        print(\"Begin with SKI\")\n",
    "        if dataset != 'gas' and df.iloc[0]['dataset'] == 'gas':\n",
    "            result = rp_experiments.run_experiment(\n",
    "                training_routines.train_exact_gp, ski_options, \n",
    "                dataset=dataset, split=0.2, cv=True, repeats=1)\n",
    "            result['RP'] = True\n",
    "            result['k'] = 1\n",
    "            result['J'] = 20\n",
    "            result['dataset'] = dataset\n",
    "            result['scalable_method'] = 'ski'\n",
    "            result['options'] = json.dumps(ski_options)\n",
    "\n",
    "            df = pd.concat([df, result])\n",
    "            df.to_csv('./scalable_methods_results.csv')\n",
    "        else:\n",
    "            print(\"Skipping gas for SKI since it's already done\")\n",
    "        \n",
    "        # with SVI\n",
    "        print(\"Begin with SVI\")\n",
    "        result = rp_experiments.run_experiment(\n",
    "            training_routines.train_svi_gp, svi_options, \n",
    "            dataset=dataset, split=0.2, cv=True, repeats=1)\n",
    "        result['RP'] = True\n",
    "        result['k'] = 1\n",
    "        result['J'] = 20\n",
    "        result['dataset'] = dataset\n",
    "        result['scalable_method'] = 'SVI'\n",
    "        result['options'] = json.dumps(svi_options)\n",
    "        \n",
    "        \n",
    "        df = pd.concat([df, result])\n",
    "        df.to_csv('./scalable_methods_results.csv')\n",
    "#         clear_output()\n",
    "        \n",
    "        # with SVI + RBF Kernel\n",
    "#         print(\"Begin with RBF SVI\")\n",
    "        result = rp_experiments.run_experiment(\n",
    "            training_routines.train_svi_gp, svi_rbf_options, \n",
    "            dataset=dataset, split=0.2, cv=True, repeats=1)\n",
    "        result['RP'] = False\n",
    "        result['k'] = 1\n",
    "        result['J'] = 20\n",
    "        result['dataset'] = dataset\n",
    "        result['scalable_method'] = 'SVI'\n",
    "        result['options'] = json.dumps(svi_rbf_options)\n",
    "        \n",
    "        df = pd.concat([df, result])\n",
    "#         df.to_csv('./scalable_methods_resultsv2.csv')\n",
    "#         clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests Matching the Multi-GPU paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski_options = dict(verbose=True, \n",
    "                  rp=True, k=1, J=20, ard=False, activation=None,\n",
    "                  weighted=True, learn_proj=False,\n",
    "                  kernel_type='Matern',\n",
    "                  optimizer='adam', n_epochs=100, lr=0.1, \n",
    "                  conv_tol=-20, smooth=False, \n",
    "                  noise_prior=False, \n",
    "                  ski=True, grid_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_rbf_options = dict(verbose=2, \n",
    "                       rp=False, ard=False, activation=None, \n",
    "                       kernel_type='Matern',\n",
    "                       optimizer='adam', n_epochs=100, lr=0.01, \n",
    "                       conv_tol=-20, smooth=False, \n",
    "                       noise_prior=False, \n",
    "                       batch_size=1024, grid_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['gas', 'skillcraft', 'sml', 'parkinsons', 'pumadyn32nm',   # \"medium\"\n",
    "            'pol', 'elevators', 'bike', 'kin40k', 'protein', # small ones of the big ones.\n",
    "            'keggdirected', 'slice', 'keggundirected', '3droad', 'song', # bigger\n",
    "            'buzz', 'houseelectric']   # big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d</th>\n",
       "      <th>fold</th>\n",
       "      <th>mse</th>\n",
       "      <th>n</th>\n",
       "      <th>prior_train_nmll</th>\n",
       "      <th>repeat</th>\n",
       "      <th>state_dict_file</th>\n",
       "      <th>test_nll</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>train_nll</th>\n",
       "      <th>train_time</th>\n",
       "      <th>trained_epochs</th>\n",
       "      <th>RP</th>\n",
       "      <th>k</th>\n",
       "      <th>J</th>\n",
       "      <th>dataset</th>\n",
       "      <th>scalable_method</th>\n",
       "      <th>options</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029951</td>\n",
       "      <td>2565</td>\n",
       "      <td>-0.443555</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_-2520936991060136728.pkl</td>\n",
       "      <td>-354.911072</td>\n",
       "      <td>0.011757</td>\n",
       "      <td>-1211.147583</td>\n",
       "      <td>626.541324</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027426</td>\n",
       "      <td>2565</td>\n",
       "      <td>-0.476100</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_7056651464425353781.pkl</td>\n",
       "      <td>20992.550781</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>-1160.805786</td>\n",
       "      <td>620.537275</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0.757075</td>\n",
       "      <td>2565</td>\n",
       "      <td>-0.397264</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_6414093292958064104.pkl</td>\n",
       "      <td>-2453.268799</td>\n",
       "      <td>0.015001</td>\n",
       "      <td>-1050.123901</td>\n",
       "      <td>611.133688</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092935</td>\n",
       "      <td>2565</td>\n",
       "      <td>-0.447060</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_-7254933470806942653.pkl</td>\n",
       "      <td>-3911.713867</td>\n",
       "      <td>0.014842</td>\n",
       "      <td>-1089.864624</td>\n",
       "      <td>622.477382</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030666</td>\n",
       "      <td>2565</td>\n",
       "      <td>-0.316160</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_2653933931941648032.pkl</td>\n",
       "      <td>-541.617615</td>\n",
       "      <td>0.019383</td>\n",
       "      <td>-850.208130</td>\n",
       "      <td>568.191759</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027722</td>\n",
       "      <td>2565</td>\n",
       "      <td>-0.284025</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_7397221207090968104.pkl</td>\n",
       "      <td>-509.705994</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>-768.309692</td>\n",
       "      <td>531.837809</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028286</td>\n",
       "      <td>2565</td>\n",
       "      <td>-0.052499</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_-1026243392941396606.pkl</td>\n",
       "      <td>-291.849548</td>\n",
       "      <td>0.019887</td>\n",
       "      <td>-728.667358</td>\n",
       "      <td>369.419794</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029651</td>\n",
       "      <td>2565</td>\n",
       "      <td>-0.057466</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_5407135502721811643.pkl</td>\n",
       "      <td>-267.471130</td>\n",
       "      <td>0.019717</td>\n",
       "      <td>-682.982056</td>\n",
       "      <td>364.475306</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>2565</td>\n",
       "      <td>-0.069248</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_-7316238441653972377.pkl</td>\n",
       "      <td>-289.513855</td>\n",
       "      <td>0.019389</td>\n",
       "      <td>-717.115356</td>\n",
       "      <td>348.241603</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028061</td>\n",
       "      <td>2565</td>\n",
       "      <td>-0.069926</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_-8955940526226463883.pkl</td>\n",
       "      <td>-316.822693</td>\n",
       "      <td>0.017092</td>\n",
       "      <td>-815.748169</td>\n",
       "      <td>377.487462</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012631</td>\n",
       "      <td>2565</td>\n",
       "      <td>0.021786</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_-380724250516844596.pkl</td>\n",
       "      <td>-353.190247</td>\n",
       "      <td>0.025261</td>\n",
       "      <td>-525.026733</td>\n",
       "      <td>360.112402</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>0.014029</td>\n",
       "      <td>2565</td>\n",
       "      <td>0.025167</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_-1818998069600200262.pkl</td>\n",
       "      <td>-341.682312</td>\n",
       "      <td>0.027850</td>\n",
       "      <td>-484.268188</td>\n",
       "      <td>334.697540</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>gas</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.447792</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.002191</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_-7584317644441931320.pkl</td>\n",
       "      <td>1132.283447</td>\n",
       "      <td>0.378167</td>\n",
       "      <td>2103.402832</td>\n",
       "      <td>353.327200</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.452391</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.026370</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_-3450579545679087754.pkl</td>\n",
       "      <td>1132.607666</td>\n",
       "      <td>0.383787</td>\n",
       "      <td>2127.938477</td>\n",
       "      <td>352.898367</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.452350</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.061412</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_-2761574158008091816.pkl</td>\n",
       "      <td>1131.770752</td>\n",
       "      <td>0.422747</td>\n",
       "      <td>2227.961914</td>\n",
       "      <td>353.426474</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.472292</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.061301</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_4662020800971597037.pkl</td>\n",
       "      <td>1156.297852</td>\n",
       "      <td>0.417966</td>\n",
       "      <td>2219.435791</td>\n",
       "      <td>354.114848</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.448703</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.053020</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_-3523820382663713223.pkl</td>\n",
       "      <td>1132.248535</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>2193.234375</td>\n",
       "      <td>354.655548</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.491560</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.058114</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_-7590313402037791437.pkl</td>\n",
       "      <td>1222.988037</td>\n",
       "      <td>0.403185</td>\n",
       "      <td>2182.492188</td>\n",
       "      <td>353.807698</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.444739</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.080020</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_-3920022020916148171.pkl</td>\n",
       "      <td>1118.586182</td>\n",
       "      <td>0.381712</td>\n",
       "      <td>2122.154297</td>\n",
       "      <td>526.385234</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.439224</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.075265</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_8662440023004582241.pkl</td>\n",
       "      <td>1113.726318</td>\n",
       "      <td>0.382149</td>\n",
       "      <td>2124.164307</td>\n",
       "      <td>510.148637</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.440828</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.120587</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_-3227707727073321107.pkl</td>\n",
       "      <td>1130.997192</td>\n",
       "      <td>0.403975</td>\n",
       "      <td>2206.554688</td>\n",
       "      <td>493.992487</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.428850</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.106416</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_-4193248160116633952.pkl</td>\n",
       "      <td>1113.614014</td>\n",
       "      <td>0.405702</td>\n",
       "      <td>2186.743652</td>\n",
       "      <td>503.391664</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.427553</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.103559</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_5888244112797075632.pkl</td>\n",
       "      <td>1110.486084</td>\n",
       "      <td>0.406029</td>\n",
       "      <td>2188.837891</td>\n",
       "      <td>506.896609</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.427689</td>\n",
       "      <td>3338</td>\n",
       "      <td>1.104072</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_8226955620288969464.pkl</td>\n",
       "      <td>1113.075928</td>\n",
       "      <td>0.403276</td>\n",
       "      <td>2189.624512</td>\n",
       "      <td>504.511756</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>skillcraft</td>\n",
       "      <td>SVI</td>\n",
       "      <td>{\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057325</td>\n",
       "      <td>4137</td>\n",
       "      <td>-0.105462</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_8868804238963868812.pkl</td>\n",
       "      <td>-178.464478</td>\n",
       "      <td>0.035454</td>\n",
       "      <td>-627.455566</td>\n",
       "      <td>797.965866</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>sml</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041669</td>\n",
       "      <td>4137</td>\n",
       "      <td>-0.084694</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_-3550325054912252185.pkl</td>\n",
       "      <td>-286.700073</td>\n",
       "      <td>0.026705</td>\n",
       "      <td>-838.258057</td>\n",
       "      <td>755.151880</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>sml</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.041539</td>\n",
       "      <td>4137</td>\n",
       "      <td>-0.155225</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_-2141744679382788571.pkl</td>\n",
       "      <td>-362.513550</td>\n",
       "      <td>0.024305</td>\n",
       "      <td>-969.880127</td>\n",
       "      <td>734.074241</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>sml</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046804</td>\n",
       "      <td>4137</td>\n",
       "      <td>-0.178754</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_-2089190461728636671.pkl</td>\n",
       "      <td>-281.921997</td>\n",
       "      <td>0.026830</td>\n",
       "      <td>-918.857666</td>\n",
       "      <td>751.053630</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>sml</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.077364</td>\n",
       "      <td>4137</td>\n",
       "      <td>0.084412</td>\n",
       "      <td>0</td>\n",
       "      <td>model_state_dict_2255435225393454837.pkl</td>\n",
       "      <td>139.446899</td>\n",
       "      <td>0.052705</td>\n",
       "      <td>-80.530029</td>\n",
       "      <td>620.933807</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>sml</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.040343</td>\n",
       "      <td>4137</td>\n",
       "      <td>-0.237814</td>\n",
       "      <td>1</td>\n",
       "      <td>model_state_dict_6673658152973209174.pkl</td>\n",
       "      <td>-308.614868</td>\n",
       "      <td>0.022544</td>\n",
       "      <td>-1120.413818</td>\n",
       "      <td>751.793511</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>sml</td>\n",
       "      <td>ski</td>\n",
       "      <td>{\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     d  fold       mse     n  prior_train_nmll  repeat  \\\n",
       "0  126     0  0.029951  2565         -0.443555       0   \n",
       "1  126     0  0.027426  2565         -0.476100       1   \n",
       "2  126     1  0.757075  2565         -0.397264       0   \n",
       "3  126     1  0.092935  2565         -0.447060       1   \n",
       "4  126     2  0.030666  2565         -0.316160       0   \n",
       "5  126     2  0.027722  2565         -0.284025       1   \n",
       "0  126     0  0.028286  2565         -0.052499       0   \n",
       "1  126     0  0.029651  2565         -0.057466       1   \n",
       "2  126     1  0.028997  2565         -0.069248       0   \n",
       "3  126     1  0.028061  2565         -0.069926       1   \n",
       "4  126     2  0.012631  2565          0.021786       0   \n",
       "5  126     2  0.014029  2565          0.025167       1   \n",
       "0   17     0  0.447792  3338          1.002191       0   \n",
       "1   17     0  0.452391  3338          1.026370       1   \n",
       "2   17     1  0.452350  3338          1.061412       0   \n",
       "3   17     1  0.472292  3338          1.061301       1   \n",
       "4   17     2  0.448703  3338          1.053020       0   \n",
       "5   17     2  0.491560  3338          1.058114       1   \n",
       "0   17     0  0.444739  3338          1.080020       0   \n",
       "1   17     0  0.439224  3338          1.075265       1   \n",
       "2   17     1  0.440828  3338          1.120587       0   \n",
       "3   17     1  0.428850  3338          1.106416       1   \n",
       "4   17     2  0.427553  3338          1.103559       0   \n",
       "5   17     2  0.427689  3338          1.104072       1   \n",
       "0   20     0  0.057325  4137         -0.105462       0   \n",
       "1   20     0  0.041669  4137         -0.084694       1   \n",
       "2   20     1  0.041539  4137         -0.155225       0   \n",
       "3   20     1  0.046804  4137         -0.178754       1   \n",
       "4   20     2  0.077364  4137          0.084412       0   \n",
       "5   20     2  0.040343  4137         -0.237814       1   \n",
       "\n",
       "                             state_dict_file      test_nll  train_mse  \\\n",
       "0  model_state_dict_-2520936991060136728.pkl   -354.911072   0.011757   \n",
       "1   model_state_dict_7056651464425353781.pkl  20992.550781   0.013425   \n",
       "2   model_state_dict_6414093292958064104.pkl  -2453.268799   0.015001   \n",
       "3  model_state_dict_-7254933470806942653.pkl  -3911.713867   0.014842   \n",
       "4   model_state_dict_2653933931941648032.pkl   -541.617615   0.019383   \n",
       "5   model_state_dict_7397221207090968104.pkl   -509.705994   0.021779   \n",
       "0  model_state_dict_-1026243392941396606.pkl   -291.849548   0.019887   \n",
       "1   model_state_dict_5407135502721811643.pkl   -267.471130   0.019717   \n",
       "2  model_state_dict_-7316238441653972377.pkl   -289.513855   0.019389   \n",
       "3  model_state_dict_-8955940526226463883.pkl   -316.822693   0.017092   \n",
       "4   model_state_dict_-380724250516844596.pkl   -353.190247   0.025261   \n",
       "5  model_state_dict_-1818998069600200262.pkl   -341.682312   0.027850   \n",
       "0  model_state_dict_-7584317644441931320.pkl   1132.283447   0.378167   \n",
       "1  model_state_dict_-3450579545679087754.pkl   1132.607666   0.383787   \n",
       "2  model_state_dict_-2761574158008091816.pkl   1131.770752   0.422747   \n",
       "3   model_state_dict_4662020800971597037.pkl   1156.297852   0.417966   \n",
       "4  model_state_dict_-3523820382663713223.pkl   1132.248535   0.407407   \n",
       "5  model_state_dict_-7590313402037791437.pkl   1222.988037   0.403185   \n",
       "0  model_state_dict_-3920022020916148171.pkl   1118.586182   0.381712   \n",
       "1   model_state_dict_8662440023004582241.pkl   1113.726318   0.382149   \n",
       "2  model_state_dict_-3227707727073321107.pkl   1130.997192   0.403975   \n",
       "3  model_state_dict_-4193248160116633952.pkl   1113.614014   0.405702   \n",
       "4   model_state_dict_5888244112797075632.pkl   1110.486084   0.406029   \n",
       "5   model_state_dict_8226955620288969464.pkl   1113.075928   0.403276   \n",
       "0   model_state_dict_8868804238963868812.pkl   -178.464478   0.035454   \n",
       "1  model_state_dict_-3550325054912252185.pkl   -286.700073   0.026705   \n",
       "2  model_state_dict_-2141744679382788571.pkl   -362.513550   0.024305   \n",
       "3  model_state_dict_-2089190461728636671.pkl   -281.921997   0.026830   \n",
       "4   model_state_dict_2255435225393454837.pkl    139.446899   0.052705   \n",
       "5   model_state_dict_6673658152973209174.pkl   -308.614868   0.022544   \n",
       "\n",
       "     train_nll  train_time  trained_epochs     RP  k   J     dataset  \\\n",
       "0 -1211.147583  626.541324             100   True  1  20         gas   \n",
       "1 -1160.805786  620.537275             100   True  1  20         gas   \n",
       "2 -1050.123901  611.133688             100   True  1  20         gas   \n",
       "3 -1089.864624  622.477382             100   True  1  20         gas   \n",
       "4  -850.208130  568.191759             100   True  1  20         gas   \n",
       "5  -768.309692  531.837809             100   True  1  20         gas   \n",
       "0  -728.667358  369.419794             100  False  1  20         gas   \n",
       "1  -682.982056  364.475306             100  False  1  20         gas   \n",
       "2  -717.115356  348.241603             100  False  1  20         gas   \n",
       "3  -815.748169  377.487462             100  False  1  20         gas   \n",
       "4  -525.026733  360.112402             100  False  1  20         gas   \n",
       "5  -484.268188  334.697540             100  False  1  20         gas   \n",
       "0  2103.402832  353.327200             100   True  1  20  skillcraft   \n",
       "1  2127.938477  352.898367             100   True  1  20  skillcraft   \n",
       "2  2227.961914  353.426474             100   True  1  20  skillcraft   \n",
       "3  2219.435791  354.114848             100   True  1  20  skillcraft   \n",
       "4  2193.234375  354.655548             100   True  1  20  skillcraft   \n",
       "5  2182.492188  353.807698             100   True  1  20  skillcraft   \n",
       "0  2122.154297  526.385234             100  False  1  20  skillcraft   \n",
       "1  2124.164307  510.148637             100  False  1  20  skillcraft   \n",
       "2  2206.554688  493.992487             100  False  1  20  skillcraft   \n",
       "3  2186.743652  503.391664             100  False  1  20  skillcraft   \n",
       "4  2188.837891  506.896609             100  False  1  20  skillcraft   \n",
       "5  2189.624512  504.511756             100  False  1  20  skillcraft   \n",
       "0  -627.455566  797.965866             100   True  1  20         sml   \n",
       "1  -838.258057  755.151880             100   True  1  20         sml   \n",
       "2  -969.880127  734.074241             100   True  1  20         sml   \n",
       "3  -918.857666  751.053630             100   True  1  20         sml   \n",
       "4   -80.530029  620.933807             100   True  1  20         sml   \n",
       "5 -1120.413818  751.793511             100   True  1  20         sml   \n",
       "\n",
       "  scalable_method                                            options  \n",
       "0             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "1             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "2             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "3             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "4             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "5             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "0             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "1             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "2             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "3             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "4             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "5             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "0             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "1             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "2             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "3             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "4             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "5             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "0             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "1             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "2             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "3             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "4             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "5             SVI  {\"verbose\": 2, \"rp\": false, \"ard\": false, \"act...  \n",
       "0             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "1             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "2             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "3             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "4             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  \n",
       "5             ski  {\"verbose\": true, \"rp\": true, \"k\": 1, \"J\": 20,...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df['dataset'] == dataset) & (df['scalable_method'] == 'ski')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin dataset  gas\n",
      "Begin with SVI\n",
      "epoch 0, iter 0, loss 2.04154896736145\n",
      "epoch 0, iter 1, loss 2.385745048522949\n",
      "epoch 0, loss 4.427294015884399\n",
      "epoch 1, iter 0, loss 2.308601140975952\n",
      "epoch 1, iter 1, loss 2.0707671642303467\n",
      "epoch 1, loss 4.379368305206299\n",
      "epoch 2, iter 0, loss 1.965134859085083\n",
      "epoch 2, iter 1, loss 1.975225806236267\n",
      "epoch 2, loss 3.94036066532135\n",
      "epoch 3, iter 0, loss 1.8031396865844727\n",
      "epoch 3, iter 1, loss 1.6615561246871948\n",
      "epoch 3, loss 3.4646958112716675\n",
      "epoch 4, iter 0, loss 1.6349296569824219\n",
      "epoch 4, iter 1, loss 1.6174741983413696\n",
      "epoch 4, loss 3.2524038553237915\n",
      "epoch 5, iter 0, loss 1.549227237701416\n",
      "epoch 5, iter 1, loss 1.472577452659607\n",
      "epoch 5, loss 3.021804690361023\n",
      "epoch 6, iter 0, loss 1.4409780502319336\n",
      "epoch 6, iter 1, loss 1.4461982250213623\n",
      "epoch 6, loss 2.887176275253296\n",
      "epoch 7, iter 0, loss 1.4053170680999756\n",
      "epoch 7, iter 1, loss 1.373504638671875\n",
      "epoch 7, loss 2.7788217067718506\n",
      "epoch 8, iter 0, loss 1.3211716413497925\n",
      "epoch 8, iter 1, loss 1.3245341777801514\n",
      "epoch 8, loss 2.645705819129944\n",
      "epoch 9, iter 0, loss 1.2975599765777588\n",
      "epoch 9, iter 1, loss 1.2608667612075806\n",
      "epoch 9, loss 2.5584267377853394\n",
      "epoch 10, iter 0, loss 1.2305418252944946\n",
      "epoch 10, iter 1, loss 1.1996686458587646\n",
      "epoch 10, loss 2.4302104711532593\n",
      "epoch 11, iter 0, loss 1.1825467348098755\n",
      "epoch 11, iter 1, loss 1.1498355865478516\n",
      "epoch 11, loss 2.332382321357727\n",
      "epoch 12, iter 0, loss 1.113102912902832\n",
      "epoch 12, iter 1, loss 1.104066014289856\n",
      "epoch 12, loss 2.217168927192688\n",
      "epoch 13, iter 0, loss 1.0666930675506592\n",
      "epoch 13, iter 1, loss 1.0320154428482056\n",
      "epoch 13, loss 2.0987085103988647\n",
      "epoch 14, iter 0, loss 1.0058115720748901\n",
      "epoch 14, iter 1, loss 0.9781684875488281\n",
      "epoch 14, loss 1.9839800596237183\n",
      "epoch 15, iter 0, loss 0.9486705660820007\n",
      "epoch 15, iter 1, loss 0.9127191305160522\n",
      "epoch 15, loss 1.861389696598053\n",
      "epoch 16, iter 0, loss 0.886755108833313\n",
      "epoch 16, iter 1, loss 0.8535752892494202\n",
      "epoch 16, loss 1.7403303980827332\n",
      "epoch 17, iter 0, loss 0.8271172046661377\n",
      "epoch 17, iter 1, loss 0.7801188230514526\n",
      "epoch 17, loss 1.6072360277175903\n",
      "epoch 18, iter 0, loss 0.7525047659873962\n",
      "epoch 18, iter 1, loss 0.7377526760101318\n",
      "epoch 18, loss 1.490257441997528\n",
      "epoch 19, iter 0, loss 0.6897531151771545\n",
      "epoch 19, iter 1, loss 0.6845410466194153\n",
      "epoch 19, loss 1.3742941617965698\n",
      "epoch 20, iter 0, loss 0.6408432722091675\n",
      "epoch 20, iter 1, loss 0.6150757670402527\n",
      "epoch 20, loss 1.2559190392494202\n",
      "epoch 21, iter 0, loss 0.6138076782226562\n",
      "epoch 21, iter 1, loss 0.5436119437217712\n",
      "epoch 21, loss 1.1574196219444275\n",
      "epoch 22, iter 0, loss 0.5609093904495239\n",
      "epoch 22, iter 1, loss 0.5052893161773682\n",
      "epoch 22, loss 1.066198706626892\n",
      "epoch 23, iter 0, loss 0.5493824481964111\n",
      "epoch 23, iter 1, loss 0.4623512625694275\n",
      "epoch 23, loss 1.0117337107658386\n",
      "epoch 24, iter 0, loss 0.47830143570899963\n",
      "epoch 24, iter 1, loss 0.496692955493927\n",
      "epoch 24, loss 0.9749943912029266\n",
      "epoch 25, iter 0, loss 0.49571120738983154\n",
      "epoch 25, iter 1, loss 0.41449928283691406\n",
      "epoch 25, loss 0.9102104902267456\n",
      "epoch 26, iter 0, loss 0.466739296913147\n",
      "epoch 26, iter 1, loss 0.3913150727748871\n",
      "epoch 26, loss 0.8580543696880341\n",
      "epoch 27, iter 0, loss 0.43734002113342285\n",
      "epoch 27, iter 1, loss 0.39605647325515747\n",
      "epoch 27, loss 0.8333964943885803\n",
      "epoch 28, iter 0, loss 0.4014066457748413\n",
      "epoch 28, iter 1, loss 0.37327104806900024\n",
      "epoch 28, loss 0.7746776938438416\n",
      "epoch 29, iter 0, loss 0.4114726185798645\n",
      "epoch 29, iter 1, loss 0.35231301188468933\n",
      "epoch 29, loss 0.7637856304645538\n",
      "epoch 30, iter 0, loss 0.4103073477745056\n",
      "epoch 30, iter 1, loss 0.32879552245140076\n",
      "epoch 30, loss 0.7391028702259064\n",
      "epoch 31, iter 0, loss 0.31784671545028687\n",
      "epoch 31, iter 1, loss 0.49746471643447876\n",
      "epoch 31, loss 0.8153114318847656\n",
      "epoch 32, iter 0, loss 0.36464062333106995\n",
      "epoch 32, iter 1, loss 0.40092748403549194\n",
      "epoch 32, loss 0.7655681073665619\n",
      "epoch 33, iter 0, loss 0.2534685730934143\n",
      "epoch 33, iter 1, loss 0.4220120310783386\n",
      "epoch 33, loss 0.6754806041717529\n",
      "epoch 34, iter 0, loss 0.25207316875457764\n",
      "epoch 34, iter 1, loss 0.3495492935180664\n",
      "epoch 34, loss 0.601622462272644\n",
      "epoch 35, iter 0, loss 0.31545621156692505\n",
      "epoch 35, iter 1, loss 0.17292280495166779\n",
      "epoch 35, loss 0.48837901651859283\n",
      "epoch 36, iter 0, loss 0.238628089427948\n",
      "epoch 36, iter 1, loss 0.22455672919750214\n",
      "epoch 36, loss 0.46318481862545013\n",
      "epoch 37, iter 0, loss 0.11199100315570831\n",
      "epoch 37, iter 1, loss 0.32980310916900635\n",
      "epoch 37, loss 0.44179411232471466\n",
      "epoch 38, iter 0, loss 0.22284957766532898\n",
      "epoch 38, iter 1, loss 0.200718954205513\n",
      "epoch 38, loss 0.423568531870842\n",
      "epoch 39, iter 0, loss 0.13770654797554016\n",
      "epoch 39, iter 1, loss 0.27919381856918335\n",
      "epoch 39, loss 0.4169003665447235\n",
      "epoch 40, iter 0, loss 0.1276358962059021\n",
      "epoch 40, iter 1, loss 0.23207227885723114\n",
      "epoch 40, loss 0.35970817506313324\n",
      "epoch 41, iter 0, loss 0.08885893225669861\n",
      "epoch 41, iter 1, loss 0.24799174070358276\n",
      "epoch 41, loss 0.33685067296028137\n",
      "epoch 42, iter 0, loss 0.16804321110248566\n",
      "epoch 42, iter 1, loss 0.0682794451713562\n",
      "epoch 42, loss 0.23632265627384186\n",
      "epoch 43, iter 0, loss 0.10790807008743286\n",
      "epoch 43, iter 1, loss 0.1031775176525116\n",
      "epoch 43, loss 0.21108558773994446\n",
      "epoch 44, iter 0, loss 0.07393492758274078\n",
      "epoch 44, iter 1, loss 0.10468795895576477\n",
      "epoch 44, loss 0.17862288653850555\n",
      "epoch 45, iter 0, loss 0.11881010234355927\n",
      "epoch 45, iter 1, loss 0.027896299958229065\n",
      "epoch 45, loss 0.14670640230178833\n",
      "epoch 46, iter 0, loss 0.12683972716331482\n",
      "epoch 46, iter 1, loss 0.03397458791732788\n",
      "epoch 46, loss 0.1608143150806427\n",
      "epoch 47, iter 0, loss 0.11866419017314911\n",
      "epoch 47, iter 1, loss 0.04789665341377258\n",
      "epoch 47, loss 0.1665608435869217\n",
      "epoch 48, iter 0, loss 0.07939636707305908\n",
      "epoch 48, iter 1, loss 0.05993889272212982\n",
      "epoch 48, loss 0.1393352597951889\n",
      "epoch 49, iter 0, loss 0.031744182109832764\n",
      "epoch 49, iter 1, loss 0.18377044796943665\n",
      "epoch 49, loss 0.2155146300792694\n",
      "epoch 50, iter 0, loss 0.025281012058258057\n",
      "epoch 50, iter 1, loss 0.19102376699447632\n",
      "epoch 50, loss 0.21630477905273438\n",
      "epoch 51, iter 0, loss 0.11513891816139221\n",
      "epoch 51, iter 1, loss 0.046711087226867676\n",
      "epoch 51, loss 0.1618500053882599\n",
      "epoch 52, iter 0, loss 0.1668875813484192\n",
      "epoch 52, iter 1, loss 0.00027871131896972656\n",
      "epoch 52, loss 0.16716629266738892\n",
      "epoch 53, iter 0, loss 0.15226949751377106\n",
      "epoch 53, iter 1, loss 0.008588790893554688\n",
      "epoch 53, loss 0.16085828840732574\n",
      "epoch 54, iter 0, loss 0.022077322006225586\n",
      "epoch 54, iter 1, loss 0.13040076196193695\n",
      "epoch 54, loss 0.15247808396816254\n",
      "epoch 55, iter 0, loss -0.010235339403152466\n",
      "epoch 55, iter 1, loss 0.19171231985092163\n",
      "epoch 55, loss 0.18147698044776917\n",
      "epoch 56, iter 0, loss 0.1107715368270874\n",
      "epoch 56, iter 1, loss -0.02856704592704773\n",
      "epoch 56, loss 0.08220449090003967\n",
      "epoch 57, iter 0, loss 0.08318081498146057\n",
      "epoch 57, iter 1, loss -0.0036630779504776\n",
      "epoch 57, loss 0.07951773703098297\n",
      "epoch 58, iter 0, loss 0.03710690140724182\n",
      "epoch 58, iter 1, loss 0.04115113615989685\n",
      "epoch 58, loss 0.07825803756713867\n",
      "epoch 59, iter 0, loss 0.06724810600280762\n",
      "epoch 59, iter 1, loss -0.04465954005718231\n",
      "epoch 59, loss 0.022588565945625305\n",
      "epoch 60, iter 0, loss 0.07874377071857452\n",
      "epoch 60, iter 1, loss 0.003697514533996582\n",
      "epoch 60, loss 0.0824412852525711\n",
      "epoch 61, iter 0, loss 0.029258668422698975\n",
      "epoch 61, iter 1, loss -0.020648077130317688\n",
      "epoch 61, loss 0.008610591292381287\n",
      "epoch 62, iter 0, loss 0.04265758395195007\n",
      "epoch 62, iter 1, loss 0.0028748810291290283\n",
      "epoch 62, loss 0.0455324649810791\n",
      "epoch 63, iter 0, loss -0.06391754746437073\n",
      "epoch 63, iter 1, loss 0.18688595294952393\n",
      "epoch 63, loss 0.1229684054851532\n",
      "epoch 64, iter 0, loss -0.04950055480003357\n",
      "epoch 64, iter 1, loss 0.1337174028158188\n",
      "epoch 64, loss 0.08421684801578522\n",
      "epoch 65, iter 0, loss 0.03874997794628143\n",
      "epoch 65, iter 1, loss -0.059049636125564575\n",
      "epoch 65, loss -0.020299658179283142\n",
      "epoch 66, iter 0, loss -0.028434544801712036\n",
      "epoch 66, iter 1, loss 0.04175931215286255\n",
      "epoch 66, loss 0.013324767351150513\n",
      "epoch 67, iter 0, loss 0.027910619974136353\n",
      "epoch 67, iter 1, loss -0.03299736976623535\n",
      "epoch 67, loss -0.005086749792098999\n",
      "epoch 68, iter 0, loss 0.041594743728637695\n",
      "epoch 68, iter 1, loss -0.06350928544998169\n",
      "epoch 68, loss -0.021914541721343994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69, iter 0, loss -0.053568035364151\n",
      "epoch 69, iter 1, loss 0.11251762509346008\n",
      "epoch 69, loss 0.05894958972930908\n",
      "epoch 70, iter 0, loss -0.10434049367904663\n",
      "epoch 70, iter 1, loss 0.1589774787425995\n",
      "epoch 70, loss 0.054636985063552856\n",
      "epoch 71, iter 0, loss -0.04331454634666443\n",
      "epoch 71, iter 1, loss 0.08693572878837585\n",
      "epoch 71, loss 0.043621182441711426\n",
      "epoch 72, iter 0, loss 0.04387129843235016\n",
      "epoch 72, iter 1, loss -0.08751058578491211\n",
      "epoch 72, loss -0.04363928735256195\n",
      "epoch 73, iter 0, loss 0.047128498554229736\n",
      "epoch 73, iter 1, loss -0.05993691086769104\n",
      "epoch 73, loss -0.012808412313461304\n",
      "epoch 74, iter 0, loss 0.03512918949127197\n",
      "epoch 74, iter 1, loss -0.0725255012512207\n",
      "epoch 74, loss -0.03739631175994873\n",
      "epoch 75, iter 0, loss -0.007801458239555359\n",
      "epoch 75, iter 1, loss 0.025013893842697144\n",
      "epoch 75, loss 0.017212435603141785\n",
      "epoch 76, iter 0, loss -0.0083647221326828\n",
      "epoch 76, iter 1, loss -0.014265298843383789\n",
      "epoch 76, loss -0.02263002097606659\n",
      "epoch 77, iter 0, loss -0.06464239954948425\n",
      "epoch 77, iter 1, loss 0.07192960381507874\n",
      "epoch 77, loss 0.007287204265594482\n",
      "epoch 78, iter 0, loss 0.0035077333450317383\n",
      "epoch 78, iter 1, loss -0.06733021140098572\n",
      "epoch 78, loss -0.06382247805595398\n",
      "epoch 79, iter 0, loss -0.011994332075119019\n",
      "epoch 79, iter 1, loss -0.03411814570426941\n",
      "epoch 79, loss -0.04611247777938843\n",
      "epoch 80, iter 0, loss -0.10916593670845032\n",
      "epoch 80, iter 1, loss 0.13092897832393646\n",
      "epoch 80, loss 0.021763041615486145\n",
      "epoch 81, iter 0, loss 0.0047997236251831055\n",
      "epoch 81, iter 1, loss -0.05961163341999054\n",
      "epoch 81, loss -0.054811909794807434\n",
      "epoch 82, iter 0, loss 0.022824451327323914\n",
      "epoch 82, iter 1, loss -0.0660521537065506\n",
      "epoch 82, loss -0.043227702379226685\n",
      "epoch 83, iter 0, loss 0.02052140235900879\n",
      "epoch 83, iter 1, loss -0.04049624502658844\n",
      "epoch 83, loss -0.01997484266757965\n",
      "epoch 84, iter 0, loss -0.0801943689584732\n",
      "epoch 84, iter 1, loss 0.08977389335632324\n",
      "epoch 84, loss 0.009579524397850037\n",
      "epoch 85, iter 0, loss -0.04073062539100647\n",
      "epoch 85, iter 1, loss 0.05131734907627106\n",
      "epoch 85, loss 0.010586723685264587\n",
      "epoch 86, iter 0, loss 0.03974445164203644\n",
      "epoch 86, iter 1, loss -0.10557898879051208\n",
      "epoch 86, loss -0.06583453714847565\n",
      "epoch 87, iter 0, loss 0.011634990572929382\n",
      "epoch 87, iter 1, loss -0.10118785500526428\n",
      "epoch 87, loss -0.0895528644323349\n",
      "epoch 88, iter 0, loss -0.009282365441322327\n",
      "epoch 88, iter 1, loss -0.024757519364356995\n",
      "epoch 88, loss -0.03403988480567932\n",
      "epoch 89, iter 0, loss 0.011278018355369568\n",
      "epoch 89, iter 1, loss -0.06201229989528656\n",
      "epoch 89, loss -0.05073428153991699\n",
      "epoch 90, iter 0, loss 0.0016159862279891968\n",
      "epoch 90, iter 1, loss -0.1011563092470169\n",
      "epoch 90, loss -0.09954032301902771\n",
      "epoch 91, iter 0, loss -0.12283787131309509\n",
      "epoch 91, iter 1, loss 0.10732288658618927\n",
      "epoch 91, loss -0.015514984726905823\n",
      "epoch 92, iter 0, loss -0.1350821554660797\n",
      "epoch 92, iter 1, loss 0.12712280452251434\n",
      "epoch 92, loss -0.007959350943565369\n",
      "epoch 93, iter 0, loss -0.114783376455307\n",
      "epoch 93, iter 1, loss 0.08476831018924713\n",
      "epoch 93, loss -0.030015066266059875\n",
      "epoch 94, iter 0, loss -0.11400669813156128\n",
      "epoch 94, iter 1, loss 0.07042534649372101\n",
      "epoch 94, loss -0.04358135163784027\n",
      "epoch 95, iter 0, loss 0.02870391309261322\n",
      "epoch 95, iter 1, loss -0.172007218003273\n",
      "epoch 95, loss -0.1433033049106598\n",
      "epoch 96, iter 0, loss 0.015516549348831177\n",
      "epoch 96, iter 1, loss -0.14922267198562622\n",
      "epoch 96, loss -0.13370612263679504\n",
      "epoch 97, iter 0, loss 0.030265286564826965\n",
      "epoch 97, iter 1, loss -0.17554326355457306\n",
      "epoch 97, loss -0.1452779769897461\n",
      "epoch 98, iter 0, loss -0.15389128029346466\n",
      "epoch 98, iter 1, loss 0.11144360154867172\n",
      "epoch 98, loss -0.04244767874479294\n",
      "epoch 99, iter 0, loss -0.00984443724155426\n",
      "epoch 99, iter 1, loss -0.13688187301158905\n",
      "epoch 99, loss -0.1467263102531433\n",
      "2019-03-30 22:50:16.442493, fold=0, rep=0, eta=0d 0h 30m 55s \n",
      "{'fold': 0, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.024074867367744446, 'train_time': 363.298329167068, 'trained_epochs': 100, 'prior_train_nmll': -0.05847178399562836, 'train_nll': -690.2867431640625, 'test_nll': -316.22406005859375, 'train_mse': 0.020735906437039375, 'state_dict_file': 'model_state_dict_-3264739281010625575.pkl'}\n",
      "epoch 0, iter 0, loss 2.0221099853515625\n",
      "epoch 0, iter 1, loss 2.3289403915405273\n",
      "epoch 0, loss 4.35105037689209\n",
      "epoch 1, iter 0, loss 2.3002610206604004\n",
      "epoch 1, iter 1, loss 2.056938648223877\n",
      "epoch 1, loss 4.357199668884277\n",
      "epoch 2, iter 0, loss 1.967965841293335\n",
      "epoch 2, iter 1, loss 1.9616458415985107\n",
      "epoch 2, loss 3.9296116828918457\n",
      "epoch 3, iter 0, loss 1.8240809440612793\n",
      "epoch 3, iter 1, loss 1.646131992340088\n",
      "epoch 3, loss 3.470212936401367\n",
      "epoch 4, iter 0, loss 1.6396960020065308\n",
      "epoch 4, iter 1, loss 1.5994651317596436\n",
      "epoch 4, loss 3.2391611337661743\n",
      "epoch 5, iter 0, loss 1.5479717254638672\n",
      "epoch 5, iter 1, loss 1.4718104600906372\n",
      "epoch 5, loss 3.0197821855545044\n",
      "epoch 6, iter 0, loss 1.4383116960525513\n",
      "epoch 6, iter 1, loss 1.4372798204421997\n",
      "epoch 6, loss 2.875591516494751\n",
      "epoch 7, iter 0, loss 1.422666311264038\n",
      "epoch 7, iter 1, loss 1.349287986755371\n",
      "epoch 7, loss 2.771954298019409\n",
      "epoch 8, iter 0, loss 1.328627109527588\n",
      "epoch 8, iter 1, loss 1.3185192346572876\n",
      "epoch 8, loss 2.6471463441848755\n",
      "epoch 9, iter 0, loss 1.2977057695388794\n",
      "epoch 9, iter 1, loss 1.2678967714309692\n",
      "epoch 9, loss 2.5656025409698486\n",
      "epoch 10, iter 0, loss 1.231308937072754\n",
      "epoch 10, iter 1, loss 1.219347357749939\n",
      "epoch 10, loss 2.450656294822693\n",
      "epoch 11, iter 0, loss 1.1847846508026123\n",
      "epoch 11, iter 1, loss 1.1620240211486816\n",
      "epoch 11, loss 2.346808671951294\n",
      "epoch 12, iter 0, loss 1.1216931343078613\n",
      "epoch 12, iter 1, loss 1.1250131130218506\n",
      "epoch 12, loss 2.246706247329712\n",
      "epoch 13, iter 0, loss 1.0781501531600952\n",
      "epoch 13, iter 1, loss 1.053051471710205\n",
      "epoch 13, loss 2.1312016248703003\n",
      "epoch 14, iter 0, loss 1.0217312574386597\n",
      "epoch 14, iter 1, loss 1.0043445825576782\n",
      "epoch 14, loss 2.026075839996338\n",
      "epoch 15, iter 0, loss 0.9696878790855408\n",
      "epoch 15, iter 1, loss 0.9356879591941833\n",
      "epoch 15, loss 1.9053758382797241\n",
      "epoch 16, iter 0, loss 0.908017098903656\n",
      "epoch 16, iter 1, loss 0.887082040309906\n",
      "epoch 16, loss 1.795099139213562\n",
      "epoch 17, iter 0, loss 0.8692948222160339\n",
      "epoch 17, iter 1, loss 0.7928479313850403\n",
      "epoch 17, loss 1.6621427536010742\n",
      "epoch 18, iter 0, loss 0.765288233757019\n",
      "epoch 18, iter 1, loss 0.8003199100494385\n",
      "epoch 18, loss 1.5656081438064575\n",
      "epoch 19, iter 0, loss 0.7232117056846619\n",
      "epoch 19, iter 1, loss 0.7041329145431519\n",
      "epoch 19, loss 1.4273446202278137\n",
      "epoch 20, iter 0, loss 0.6554439663887024\n",
      "epoch 20, iter 1, loss 0.6695033311843872\n",
      "epoch 20, loss 1.3249472975730896\n",
      "epoch 21, iter 0, loss 0.6071509122848511\n",
      "epoch 21, iter 1, loss 0.6015219688415527\n",
      "epoch 21, loss 1.2086728811264038\n",
      "epoch 22, iter 0, loss 0.6009162068367004\n",
      "epoch 22, iter 1, loss 0.4920312464237213\n",
      "epoch 22, loss 1.0929474532604218\n",
      "epoch 23, iter 0, loss 0.5123715996742249\n",
      "epoch 23, iter 1, loss 0.5405417680740356\n",
      "epoch 23, loss 1.0529133677482605\n",
      "epoch 24, iter 0, loss 0.4660283923149109\n",
      "epoch 24, iter 1, loss 0.5110678672790527\n",
      "epoch 24, loss 0.9770962595939636\n",
      "epoch 25, iter 0, loss 0.4879510700702667\n",
      "epoch 25, iter 1, loss 0.41354817152023315\n",
      "epoch 25, loss 0.9014992415904999\n",
      "epoch 26, iter 0, loss 0.48966681957244873\n",
      "epoch 26, iter 1, loss 0.3897714614868164\n",
      "epoch 26, loss 0.8794382810592651\n",
      "epoch 27, iter 0, loss 0.40056902170181274\n",
      "epoch 27, iter 1, loss 0.47678321599960327\n",
      "epoch 27, loss 0.877352237701416\n",
      "epoch 28, iter 0, loss 0.3965097665786743\n",
      "epoch 28, iter 1, loss 0.4883112609386444\n",
      "epoch 28, loss 0.8848210275173187\n",
      "epoch 29, iter 0, loss 0.41085055470466614\n",
      "epoch 29, iter 1, loss 0.47644543647766113\n",
      "epoch 29, loss 0.8872959911823273\n",
      "epoch 30, iter 0, loss 0.4544215202331543\n",
      "epoch 30, iter 1, loss 0.3235149383544922\n",
      "epoch 30, loss 0.7779364585876465\n",
      "epoch 31, iter 0, loss 0.3546736240386963\n",
      "epoch 31, iter 1, loss 0.3995476961135864\n",
      "epoch 31, loss 0.7542213201522827\n",
      "epoch 32, iter 0, loss 0.36409202218055725\n",
      "epoch 32, iter 1, loss 0.325967013835907\n",
      "epoch 32, loss 0.6900590360164642\n",
      "epoch 33, iter 0, loss 0.31876352429389954\n",
      "epoch 33, iter 1, loss 0.3460007607936859\n",
      "epoch 33, loss 0.6647642850875854\n",
      "epoch 34, iter 0, loss 0.28461119532585144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34, iter 1, loss 0.35267215967178345\n",
      "epoch 34, loss 0.6372833549976349\n",
      "epoch 35, iter 0, loss 0.34579551219940186\n",
      "epoch 35, iter 1, loss 0.19423922896385193\n",
      "epoch 35, loss 0.5400347411632538\n",
      "epoch 36, iter 0, loss 0.3094378411769867\n",
      "epoch 36, iter 1, loss 0.17783218622207642\n",
      "epoch 36, loss 0.4872700273990631\n",
      "epoch 37, iter 0, loss 0.24192894995212555\n",
      "epoch 37, iter 1, loss 0.19699513912200928\n",
      "epoch 37, loss 0.4389240890741348\n",
      "epoch 38, iter 0, loss 0.2650487422943115\n",
      "epoch 38, iter 1, loss 0.07401424646377563\n",
      "epoch 38, loss 0.33906298875808716\n",
      "epoch 39, iter 0, loss 0.21799927949905396\n",
      "epoch 39, iter 1, loss 0.05195137858390808\n",
      "epoch 39, loss 0.26995065808296204\n",
      "epoch 40, iter 0, loss 0.1500631421804428\n",
      "epoch 40, iter 1, loss 0.09598502516746521\n",
      "epoch 40, loss 0.24604816734790802\n",
      "epoch 41, iter 0, loss 0.07041224837303162\n",
      "epoch 41, iter 1, loss 0.15886563062667847\n",
      "epoch 41, loss 0.22927787899971008\n",
      "epoch 42, iter 0, loss 0.12104121595621109\n",
      "epoch 42, iter 1, loss 0.07281254231929779\n",
      "epoch 42, loss 0.19385375827550888\n",
      "epoch 43, iter 0, loss 0.15495681762695312\n",
      "epoch 43, iter 1, loss 0.047211527824401855\n",
      "epoch 43, loss 0.20216834545135498\n",
      "epoch 44, iter 0, loss 0.07223059237003326\n",
      "epoch 44, iter 1, loss 0.1791743040084839\n",
      "epoch 44, loss 0.25140489637851715\n",
      "epoch 45, iter 0, loss 0.09432095289230347\n",
      "epoch 45, iter 1, loss 0.12317435443401337\n",
      "epoch 45, loss 0.21749530732631683\n",
      "epoch 46, iter 0, loss 0.15795163810253143\n",
      "epoch 46, iter 1, loss 0.0868556797504425\n",
      "epoch 46, loss 0.24480731785297394\n",
      "epoch 47, iter 0, loss 0.05393500626087189\n",
      "epoch 47, iter 1, loss 0.2279926836490631\n",
      "epoch 47, loss 0.281927689909935\n",
      "epoch 48, iter 0, loss 0.16477030515670776\n",
      "epoch 48, iter 1, loss 0.012433826923370361\n",
      "epoch 48, loss 0.17720413208007812\n",
      "epoch 49, iter 0, loss 0.1706007719039917\n",
      "epoch 49, iter 1, loss 0.031502753496170044\n",
      "epoch 49, loss 0.20210352540016174\n",
      "epoch 50, iter 0, loss 0.04898054897785187\n",
      "epoch 50, iter 1, loss 0.19616183638572693\n",
      "epoch 50, loss 0.2451423853635788\n",
      "epoch 51, iter 0, loss 0.12013900279998779\n",
      "epoch 51, iter 1, loss 0.0679272711277008\n",
      "epoch 51, loss 0.1880662739276886\n",
      "epoch 52, iter 0, loss 0.02886126935482025\n",
      "epoch 52, iter 1, loss 0.21026071906089783\n",
      "epoch 52, loss 0.23912198841571808\n",
      "epoch 53, iter 0, loss 0.11708720028400421\n",
      "epoch 53, iter 1, loss 0.044122278690338135\n",
      "epoch 53, loss 0.16120947897434235\n",
      "epoch 54, iter 0, loss 0.03731192648410797\n",
      "epoch 54, iter 1, loss 0.17569714784622192\n",
      "epoch 54, loss 0.2130090743303299\n",
      "epoch 55, iter 0, loss 0.06940868496894836\n",
      "epoch 55, iter 1, loss 0.14618179202079773\n",
      "epoch 55, loss 0.2155904769897461\n",
      "epoch 56, iter 0, loss 0.05308222770690918\n",
      "epoch 56, iter 1, loss 0.12560300529003143\n",
      "epoch 56, loss 0.1786852329969406\n",
      "epoch 57, iter 0, loss -0.0009814202785491943\n",
      "epoch 57, iter 1, loss 0.16210170090198517\n",
      "epoch 57, loss 0.16112028062343597\n",
      "epoch 58, iter 0, loss -0.00859677791595459\n",
      "epoch 58, iter 1, loss 0.15997347235679626\n",
      "epoch 58, loss 0.15137669444084167\n",
      "epoch 59, iter 0, loss -0.030707508325576782\n",
      "epoch 59, iter 1, loss 0.13976582884788513\n",
      "epoch 59, loss 0.10905832052230835\n",
      "epoch 60, iter 0, loss 0.030088037252426147\n",
      "epoch 60, iter 1, loss -0.020517081022262573\n",
      "epoch 60, loss 0.009570956230163574\n",
      "epoch 61, iter 0, loss -0.029365554451942444\n",
      "epoch 61, iter 1, loss 0.05107395350933075\n",
      "epoch 61, loss 0.021708399057388306\n",
      "epoch 62, iter 0, loss 0.026275411248207092\n",
      "epoch 62, iter 1, loss -0.061064302921295166\n",
      "epoch 62, loss -0.034788891673088074\n",
      "epoch 63, iter 0, loss -0.06523525714874268\n",
      "epoch 63, iter 1, loss 0.06193932890892029\n",
      "epoch 63, loss -0.0032959282398223877\n",
      "epoch 64, iter 0, loss 0.014172330498695374\n",
      "epoch 64, iter 1, loss -0.05642583966255188\n",
      "epoch 64, loss -0.042253509163856506\n",
      "epoch 65, iter 0, loss 0.048459842801094055\n",
      "epoch 65, iter 1, loss -0.06846478581428528\n",
      "epoch 65, loss -0.020004943013191223\n",
      "epoch 66, iter 0, loss -0.12146031856536865\n",
      "epoch 66, iter 1, loss 0.21018649637699127\n",
      "epoch 66, loss 0.08872617781162262\n",
      "epoch 67, iter 0, loss 0.042238831520080566\n",
      "epoch 67, iter 1, loss -0.06548461318016052\n",
      "epoch 67, loss -0.023245781660079956\n",
      "epoch 68, iter 0, loss 0.033017173409461975\n",
      "epoch 68, iter 1, loss -0.07891091704368591\n",
      "epoch 68, loss -0.04589374363422394\n",
      "epoch 69, iter 0, loss 0.04485757648944855\n",
      "epoch 69, iter 1, loss -0.05438414216041565\n",
      "epoch 69, loss -0.009526565670967102\n",
      "epoch 70, iter 0, loss 0.05655071139335632\n",
      "epoch 70, iter 1, loss -0.10115140676498413\n",
      "epoch 70, loss -0.04460069537162781\n",
      "epoch 71, iter 0, loss -0.05995422601699829\n",
      "epoch 71, iter 1, loss 0.07121244072914124\n",
      "epoch 71, loss 0.011258214712142944\n",
      "epoch 72, iter 0, loss 0.02316756546497345\n",
      "epoch 72, iter 1, loss -0.028941363096237183\n",
      "epoch 72, loss -0.005773797631263733\n",
      "epoch 73, iter 0, loss 0.058928146958351135\n",
      "epoch 73, iter 1, loss -0.12635895609855652\n",
      "epoch 73, loss -0.06743080914020538\n",
      "epoch 74, iter 0, loss -0.12471351027488708\n",
      "epoch 74, iter 1, loss 0.1782432347536087\n",
      "epoch 74, loss 0.05352972447872162\n",
      "epoch 75, iter 0, loss 0.044594764709472656\n",
      "epoch 75, iter 1, loss -0.12983375787734985\n",
      "epoch 75, loss -0.0852389931678772\n",
      "epoch 76, iter 0, loss -0.07276168465614319\n",
      "epoch 76, iter 1, loss 0.11304347217082977\n",
      "epoch 76, loss 0.040281787514686584\n",
      "epoch 77, iter 0, loss 0.02185991406440735\n",
      "epoch 77, iter 1, loss -0.054753392934799194\n",
      "epoch 77, loss -0.032893478870391846\n",
      "epoch 78, iter 0, loss -0.004249781370162964\n",
      "epoch 78, iter 1, loss -0.01930658519268036\n",
      "epoch 78, loss -0.023556366562843323\n",
      "epoch 79, iter 0, loss -0.11910036206245422\n",
      "epoch 79, iter 1, loss 0.13612520694732666\n",
      "epoch 79, loss 0.017024844884872437\n",
      "epoch 80, iter 0, loss -0.07133689522743225\n",
      "epoch 80, iter 1, loss -0.008199214935302734\n",
      "epoch 80, loss -0.07953611016273499\n",
      "epoch 81, iter 0, loss -0.019644081592559814\n",
      "epoch 81, iter 1, loss -0.09034107625484467\n",
      "epoch 81, loss -0.10998515784740448\n",
      "epoch 82, iter 0, loss 0.015365630388259888\n",
      "epoch 82, iter 1, loss -0.13266976177692413\n",
      "epoch 82, loss -0.11730413138866425\n",
      "epoch 83, iter 0, loss -0.019718274474143982\n",
      "epoch 83, iter 1, loss -0.09869144856929779\n",
      "epoch 83, loss -0.11840972304344177\n",
      "epoch 84, iter 0, loss 0.01694221794605255\n",
      "epoch 84, iter 1, loss -0.14810124039649963\n",
      "epoch 84, loss -0.13115902245044708\n",
      "epoch 85, iter 0, loss 0.0036995112895965576\n",
      "epoch 85, iter 1, loss -0.11615955829620361\n",
      "epoch 85, loss -0.11246004700660706\n",
      "epoch 86, iter 0, loss -0.05000591278076172\n",
      "epoch 86, iter 1, loss 0.016586467623710632\n",
      "epoch 86, loss -0.033419445157051086\n",
      "epoch 87, iter 0, loss -0.03886842727661133\n",
      "epoch 87, iter 1, loss 0.06685842573642731\n",
      "epoch 87, loss 0.02798999845981598\n",
      "epoch 88, iter 0, loss 0.05349799990653992\n",
      "epoch 88, iter 1, loss -0.07049176096916199\n",
      "epoch 88, loss -0.01699376106262207\n",
      "epoch 89, iter 0, loss 0.0032905936241149902\n",
      "epoch 89, iter 1, loss -0.030770808458328247\n",
      "epoch 89, loss -0.027480214834213257\n",
      "epoch 90, iter 0, loss 0.04577198624610901\n",
      "epoch 90, iter 1, loss -0.06838825345039368\n",
      "epoch 90, loss -0.022616267204284668\n",
      "epoch 91, iter 0, loss -0.06942957639694214\n",
      "epoch 91, iter 1, loss 0.07634991407394409\n",
      "epoch 91, loss 0.006920337677001953\n",
      "epoch 92, iter 0, loss 0.02069765329360962\n",
      "epoch 92, iter 1, loss -0.05746006965637207\n",
      "epoch 92, loss -0.03676241636276245\n",
      "epoch 93, iter 0, loss -0.12122300267219543\n",
      "epoch 93, iter 1, loss 0.1279188096523285\n",
      "epoch 93, loss 0.006695806980133057\n",
      "epoch 94, iter 0, loss 0.03372354805469513\n",
      "epoch 94, iter 1, loss -0.0937851071357727\n",
      "epoch 94, loss -0.060061559081077576\n",
      "epoch 95, iter 0, loss -0.0006354302167892456\n",
      "epoch 95, iter 1, loss -0.10751736164093018\n",
      "epoch 95, loss -0.10815279185771942\n",
      "epoch 96, iter 0, loss -0.13985735177993774\n",
      "epoch 96, iter 1, loss 0.0669492781162262\n",
      "epoch 96, loss -0.07290807366371155\n",
      "epoch 97, iter 0, loss -0.005365997552871704\n",
      "epoch 97, iter 1, loss -0.15107758343219757\n",
      "epoch 97, loss -0.15644358098506927\n",
      "epoch 98, iter 0, loss -0.11801783740520477\n",
      "epoch 98, iter 1, loss 0.0026569664478302\n",
      "epoch 98, loss -0.11536087095737457\n",
      "epoch 99, iter 0, loss -0.12135618925094604\n",
      "epoch 99, iter 1, loss 0.012789368629455566\n",
      "epoch 99, loss -0.10856682062149048\n",
      "2019-03-30 22:56:26.845889, fold=0, rep=1, eta=0d 0h 24m 43s \n",
      "{'fold': 0, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.02849399484694004, 'train_time': 370.40299251605757, 'trained_epochs': 100, 'prior_train_nmll': -0.08060167729854584, 'train_nll': -705.1739501953125, 'test_nll': -290.73211669921875, 'train_mse': 0.02117278054356575, 'state_dict_file': 'model_state_dict_-5907330982047012106.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, iter 0, loss 1.9813473224639893\n",
      "epoch 0, iter 1, loss 2.3022189140319824\n",
      "epoch 0, loss 4.283566236495972\n",
      "epoch 1, iter 0, loss 2.1979660987854004\n",
      "epoch 1, iter 1, loss 2.0169754028320312\n",
      "epoch 1, loss 4.214941501617432\n",
      "epoch 2, iter 0, loss 1.9387973546981812\n",
      "epoch 2, iter 1, loss 1.9363386631011963\n",
      "epoch 2, loss 3.8751360177993774\n",
      "epoch 3, iter 0, loss 1.788520336151123\n",
      "epoch 3, iter 1, loss 1.67030668258667\n",
      "epoch 3, loss 3.458827018737793\n",
      "epoch 4, iter 0, loss 1.6358284950256348\n",
      "epoch 4, iter 1, loss 1.603543996810913\n",
      "epoch 4, loss 3.239372491836548\n",
      "epoch 5, iter 0, loss 1.5364105701446533\n",
      "epoch 5, iter 1, loss 1.4621129035949707\n",
      "epoch 5, loss 2.998523473739624\n",
      "epoch 6, iter 0, loss 1.449113368988037\n",
      "epoch 6, iter 1, loss 1.4367212057113647\n",
      "epoch 6, loss 2.885834574699402\n",
      "epoch 7, iter 0, loss 1.3908032178878784\n",
      "epoch 7, iter 1, loss 1.3641324043273926\n",
      "epoch 7, loss 2.754935622215271\n",
      "epoch 8, iter 0, loss 1.3264979124069214\n",
      "epoch 8, iter 1, loss 1.3210117816925049\n",
      "epoch 8, loss 2.6475096940994263\n",
      "epoch 9, iter 0, loss 1.2945419549942017\n",
      "epoch 9, iter 1, loss 1.2583675384521484\n",
      "epoch 9, loss 2.55290949344635\n",
      "epoch 10, iter 0, loss 1.2297861576080322\n",
      "epoch 10, iter 1, loss 1.2129149436950684\n",
      "epoch 10, loss 2.4427011013031006\n",
      "epoch 11, iter 0, loss 1.1851671934127808\n",
      "epoch 11, iter 1, loss 1.1687403917312622\n",
      "epoch 11, loss 2.353907585144043\n",
      "epoch 12, iter 0, loss 1.1266707181930542\n",
      "epoch 12, iter 1, loss 1.1096279621124268\n",
      "epoch 12, loss 2.236298680305481\n",
      "epoch 13, iter 0, loss 1.078629732131958\n",
      "epoch 13, iter 1, loss 1.0523910522460938\n",
      "epoch 13, loss 2.1310207843780518\n",
      "epoch 14, iter 0, loss 1.0212843418121338\n",
      "epoch 14, iter 1, loss 0.9935114979743958\n",
      "epoch 14, loss 2.0147958397865295\n",
      "epoch 15, iter 0, loss 0.966241717338562\n",
      "epoch 15, iter 1, loss 0.933875560760498\n",
      "epoch 15, loss 1.90011727809906\n",
      "epoch 16, iter 0, loss 0.9077901840209961\n",
      "epoch 16, iter 1, loss 0.8721120953559875\n",
      "epoch 16, loss 1.7799022793769836\n",
      "epoch 17, iter 0, loss 0.8349230289459229\n",
      "epoch 17, iter 1, loss 0.8149076700210571\n",
      "epoch 17, loss 1.64983069896698\n",
      "epoch 18, iter 0, loss 0.7719334363937378\n",
      "epoch 18, iter 1, loss 0.7434316277503967\n",
      "epoch 18, loss 1.5153650641441345\n",
      "epoch 19, iter 0, loss 0.7048783898353577\n",
      "epoch 19, iter 1, loss 0.6873953938484192\n",
      "epoch 19, loss 1.3922737836837769\n",
      "epoch 20, iter 0, loss 0.6478859186172485\n",
      "epoch 20, iter 1, loss 0.623805582523346\n",
      "epoch 20, loss 1.2716915011405945\n",
      "epoch 21, iter 0, loss 0.5958946943283081\n",
      "epoch 21, iter 1, loss 0.5553690791130066\n",
      "epoch 21, loss 1.1512637734413147\n",
      "epoch 22, iter 0, loss 0.5342999696731567\n",
      "epoch 22, iter 1, loss 0.5185391902923584\n",
      "epoch 22, loss 1.0528391599655151\n",
      "epoch 23, iter 0, loss 0.5087018013000488\n",
      "epoch 23, iter 1, loss 0.4465998411178589\n",
      "epoch 23, loss 0.9553016424179077\n",
      "epoch 24, iter 0, loss 0.4455363154411316\n",
      "epoch 24, iter 1, loss 0.45043182373046875\n",
      "epoch 24, loss 0.8959681391716003\n",
      "epoch 25, iter 0, loss 0.4237261414527893\n",
      "epoch 25, iter 1, loss 0.4023553729057312\n",
      "epoch 25, loss 0.8260815143585205\n",
      "epoch 26, iter 0, loss 0.41407686471939087\n",
      "epoch 26, iter 1, loss 0.3631117045879364\n",
      "epoch 26, loss 0.7771885693073273\n",
      "epoch 27, iter 0, loss 0.3848525881767273\n",
      "epoch 27, iter 1, loss 0.37748515605926514\n",
      "epoch 27, loss 0.7623377442359924\n",
      "epoch 28, iter 0, loss 0.35157719254493713\n",
      "epoch 28, iter 1, loss 0.40331098437309265\n",
      "epoch 28, loss 0.7548881769180298\n",
      "epoch 29, iter 0, loss 0.3802867829799652\n",
      "epoch 29, iter 1, loss 0.3778004050254822\n",
      "epoch 29, loss 0.7580871880054474\n",
      "epoch 30, iter 0, loss 0.3874506652355194\n",
      "epoch 30, iter 1, loss 0.3694015145301819\n",
      "epoch 30, loss 0.7568521797657013\n",
      "epoch 31, iter 0, loss 0.3607954680919647\n",
      "epoch 31, iter 1, loss 0.3809913694858551\n",
      "epoch 31, loss 0.7417868375778198\n",
      "epoch 32, iter 0, loss 0.315473735332489\n",
      "epoch 32, iter 1, loss 0.2990420460700989\n",
      "epoch 32, loss 0.6145157814025879\n",
      "epoch 33, iter 0, loss 0.29047149419784546\n",
      "epoch 33, iter 1, loss 0.3800452947616577\n",
      "epoch 33, loss 0.6705167889595032\n",
      "epoch 34, iter 0, loss 0.23525050282478333\n",
      "epoch 34, iter 1, loss 0.35399097204208374\n",
      "epoch 34, loss 0.5892414748668671\n",
      "epoch 35, iter 0, loss 0.21452590823173523\n",
      "epoch 35, iter 1, loss 0.2552546560764313\n",
      "epoch 35, loss 0.4697805643081665\n",
      "epoch 36, iter 0, loss 0.21569401025772095\n",
      "epoch 36, iter 1, loss 0.2386215329170227\n",
      "epoch 36, loss 0.45431554317474365\n",
      "epoch 37, iter 0, loss 0.23734813928604126\n",
      "epoch 37, iter 1, loss 0.14597541093826294\n",
      "epoch 37, loss 0.3833235502243042\n",
      "epoch 38, iter 0, loss 0.21345509588718414\n",
      "epoch 38, iter 1, loss 0.15396374464035034\n",
      "epoch 38, loss 0.3674188405275345\n",
      "epoch 39, iter 0, loss 0.13868719339370728\n",
      "epoch 39, iter 1, loss 0.17604875564575195\n",
      "epoch 39, loss 0.31473594903945923\n",
      "epoch 40, iter 0, loss 0.16097500920295715\n",
      "epoch 40, iter 1, loss 0.1385488212108612\n",
      "epoch 40, loss 0.29952383041381836\n",
      "epoch 41, iter 0, loss 0.1253204345703125\n",
      "epoch 41, iter 1, loss 0.12372808158397675\n",
      "epoch 41, loss 0.24904851615428925\n",
      "epoch 42, iter 0, loss 0.13933293521404266\n",
      "epoch 42, iter 1, loss 0.06806297600269318\n",
      "epoch 42, loss 0.20739591121673584\n",
      "epoch 43, iter 0, loss 0.12698090076446533\n",
      "epoch 43, iter 1, loss 0.03414091467857361\n",
      "epoch 43, loss 0.16112181544303894\n",
      "epoch 44, iter 0, loss 0.08455225825309753\n",
      "epoch 44, iter 1, loss 0.09205490350723267\n",
      "epoch 44, loss 0.1766071617603302\n",
      "epoch 45, iter 0, loss 0.059412360191345215\n",
      "epoch 45, iter 1, loss 0.1151445209980011\n",
      "epoch 45, loss 0.1745568811893463\n",
      "epoch 46, iter 0, loss 0.011715888977050781\n",
      "epoch 46, iter 1, loss 0.15045469999313354\n",
      "epoch 46, loss 0.16217058897018433\n",
      "epoch 47, iter 0, loss 0.0794350802898407\n",
      "epoch 47, iter 1, loss 0.01928877830505371\n",
      "epoch 47, loss 0.09872385859489441\n",
      "epoch 48, iter 0, loss -0.01542133092880249\n",
      "epoch 48, iter 1, loss 0.23291881382465363\n",
      "epoch 48, loss 0.21749748289585114\n",
      "epoch 49, iter 0, loss 0.024007797241210938\n",
      "epoch 49, iter 1, loss 0.17781177163124084\n",
      "epoch 49, loss 0.20181956887245178\n",
      "epoch 50, iter 0, loss 0.11422178149223328\n",
      "epoch 50, iter 1, loss 0.0597856342792511\n",
      "epoch 50, loss 0.17400741577148438\n",
      "epoch 51, iter 0, loss 0.052226632833480835\n",
      "epoch 51, iter 1, loss 0.13884346187114716\n",
      "epoch 51, loss 0.191070094704628\n",
      "epoch 52, iter 0, loss 0.14708983898162842\n",
      "epoch 52, iter 1, loss 0.008249938488006592\n",
      "epoch 52, loss 0.155339777469635\n",
      "epoch 53, iter 0, loss 0.08770880103111267\n",
      "epoch 53, iter 1, loss 0.07458341121673584\n",
      "epoch 53, loss 0.1622922122478485\n",
      "epoch 54, iter 0, loss 0.09663724899291992\n",
      "epoch 54, iter 1, loss 0.04602755606174469\n",
      "epoch 54, loss 0.1426648050546646\n",
      "epoch 55, iter 0, loss 0.04855777323246002\n",
      "epoch 55, iter 1, loss 0.10346941649913788\n",
      "epoch 55, loss 0.1520271897315979\n",
      "epoch 56, iter 0, loss 0.034761905670166016\n",
      "epoch 56, iter 1, loss 0.09422376751899719\n",
      "epoch 56, loss 0.1289856731891632\n",
      "epoch 57, iter 0, loss 0.06979608535766602\n",
      "epoch 57, iter 1, loss -0.021040499210357666\n",
      "epoch 57, loss 0.04875558614730835\n",
      "epoch 58, iter 0, loss 0.02886757254600525\n",
      "epoch 58, iter 1, loss 0.005263149738311768\n",
      "epoch 58, loss 0.03413072228431702\n",
      "epoch 59, iter 0, loss 0.030557721853256226\n",
      "epoch 59, iter 1, loss -0.056037113070487976\n",
      "epoch 59, loss -0.02547939121723175\n",
      "epoch 60, iter 0, loss 0.025447338819503784\n",
      "epoch 60, iter 1, loss -0.05586633086204529\n",
      "epoch 60, loss -0.030418992042541504\n",
      "epoch 61, iter 0, loss -0.039345115423202515\n",
      "epoch 61, iter 1, loss 0.03792227804660797\n",
      "epoch 61, loss -0.0014228373765945435\n",
      "epoch 62, iter 0, loss -0.0017271041870117188\n",
      "epoch 62, iter 1, loss -0.05171257257461548\n",
      "epoch 62, loss -0.0534396767616272\n",
      "epoch 63, iter 0, loss 0.008171230554580688\n",
      "epoch 63, iter 1, loss -0.040774405002593994\n",
      "epoch 63, loss -0.032603174448013306\n",
      "epoch 64, iter 0, loss -0.038641929626464844\n",
      "epoch 64, iter 1, loss 0.035300031304359436\n",
      "epoch 64, loss -0.0033418983221054077\n",
      "epoch 65, iter 0, loss 0.033334165811538696\n",
      "epoch 65, iter 1, loss -0.020812660455703735\n",
      "epoch 65, loss 0.012521505355834961\n",
      "epoch 66, iter 0, loss 0.059017568826675415\n",
      "epoch 66, iter 1, loss -0.03227683901786804\n",
      "epoch 66, loss 0.026740729808807373\n",
      "epoch 67, iter 0, loss 0.010992050170898438\n",
      "epoch 67, iter 1, loss -0.01572287082672119\n",
      "epoch 67, loss -0.004730820655822754\n",
      "epoch 68, iter 0, loss -0.02632308006286621\n",
      "epoch 68, iter 1, loss 0.05455058813095093\n",
      "epoch 68, loss 0.028227508068084717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69, iter 0, loss 0.03346869349479675\n",
      "epoch 69, iter 1, loss -0.026767432689666748\n",
      "epoch 69, loss 0.006701260805130005\n",
      "epoch 70, iter 0, loss -0.06579464673995972\n",
      "epoch 70, iter 1, loss 0.10358405113220215\n",
      "epoch 70, loss 0.03778940439224243\n",
      "epoch 71, iter 0, loss -0.05452603101730347\n",
      "epoch 71, iter 1, loss 0.10816752910614014\n",
      "epoch 71, loss 0.05364149808883667\n",
      "epoch 72, iter 0, loss 0.03811459243297577\n",
      "epoch 72, iter 1, loss -0.031345516443252563\n",
      "epoch 72, loss 0.006769075989723206\n",
      "epoch 73, iter 0, loss 0.05528721213340759\n",
      "epoch 73, iter 1, loss -0.09603545069694519\n",
      "epoch 73, loss -0.0407482385635376\n",
      "epoch 74, iter 0, loss -0.012719780206680298\n",
      "epoch 74, iter 1, loss 0.009543776512145996\n",
      "epoch 74, loss -0.0031760036945343018\n",
      "epoch 75, iter 0, loss -0.006491601467132568\n",
      "epoch 75, iter 1, loss -0.03285166621208191\n",
      "epoch 75, loss -0.03934326767921448\n",
      "epoch 76, iter 0, loss 0.03663046658039093\n",
      "epoch 76, iter 1, loss -0.11650896072387695\n",
      "epoch 76, loss -0.07987849414348602\n",
      "epoch 77, iter 0, loss -0.040511518716812134\n",
      "epoch 77, iter 1, loss -0.031675487756729126\n",
      "epoch 77, loss -0.07218700647354126\n",
      "epoch 78, iter 0, loss -0.03319978713989258\n",
      "epoch 78, iter 1, loss -0.05948951840400696\n",
      "epoch 78, loss -0.09268930554389954\n",
      "epoch 79, iter 0, loss -0.0024848878383636475\n",
      "epoch 79, iter 1, loss -0.11054649949073792\n",
      "epoch 79, loss -0.11303138732910156\n",
      "epoch 80, iter 0, loss -0.02380189299583435\n",
      "epoch 80, iter 1, loss -0.09527602791786194\n",
      "epoch 80, loss -0.11907792091369629\n",
      "epoch 81, iter 0, loss -0.021536171436309814\n",
      "epoch 81, iter 1, loss -0.06962767243385315\n",
      "epoch 81, loss -0.09116384387016296\n",
      "epoch 82, iter 0, loss -0.06215018033981323\n",
      "epoch 82, iter 1, loss -0.01575133204460144\n",
      "epoch 82, loss -0.07790151238441467\n",
      "epoch 83, iter 0, loss 0.012771278619766235\n",
      "epoch 83, iter 1, loss -0.13640332221984863\n",
      "epoch 83, loss -0.1236320436000824\n",
      "epoch 84, iter 0, loss -0.07610425353050232\n",
      "epoch 84, iter 1, loss 0.006164848804473877\n",
      "epoch 84, loss -0.06993940472602844\n",
      "epoch 85, iter 0, loss 0.028553172945976257\n",
      "epoch 85, iter 1, loss -0.1366494596004486\n",
      "epoch 85, loss -0.10809628665447235\n",
      "epoch 86, iter 0, loss -0.11559456586837769\n",
      "epoch 86, iter 1, loss 0.11526985466480255\n",
      "epoch 86, loss -0.0003247112035751343\n",
      "epoch 87, iter 0, loss -0.023643851280212402\n",
      "epoch 87, iter 1, loss 0.028206929564476013\n",
      "epoch 87, loss 0.004563078284263611\n",
      "epoch 88, iter 0, loss -0.0056269168853759766\n",
      "epoch 88, iter 1, loss -0.02947738766670227\n",
      "epoch 88, loss -0.03510430455207825\n",
      "epoch 89, iter 0, loss 0.017456352710723877\n",
      "epoch 89, iter 1, loss -0.04969629645347595\n",
      "epoch 89, loss -0.032239943742752075\n",
      "epoch 90, iter 0, loss -0.0018918216228485107\n",
      "epoch 90, iter 1, loss -0.04369544982910156\n",
      "epoch 90, loss -0.04558727145195007\n",
      "epoch 91, iter 0, loss -0.12841975688934326\n",
      "epoch 91, iter 1, loss 0.1479349434375763\n",
      "epoch 91, loss 0.019515186548233032\n",
      "epoch 92, iter 0, loss -0.025771185755729675\n",
      "epoch 92, iter 1, loss -0.03907361626625061\n",
      "epoch 92, loss -0.06484480202198029\n",
      "epoch 93, iter 0, loss -0.06534972786903381\n",
      "epoch 93, iter 1, loss -0.009157136082649231\n",
      "epoch 93, loss -0.07450686395168304\n",
      "epoch 94, iter 0, loss -0.07081842422485352\n",
      "epoch 94, iter 1, loss -0.007204145193099976\n",
      "epoch 94, loss -0.07802256941795349\n",
      "epoch 95, iter 0, loss -0.08323457837104797\n",
      "epoch 95, iter 1, loss -0.01549685001373291\n",
      "epoch 95, loss -0.09873142838478088\n",
      "epoch 96, iter 0, loss -0.07247915863990784\n",
      "epoch 96, iter 1, loss -0.06901547312736511\n",
      "epoch 96, loss -0.14149463176727295\n",
      "epoch 97, iter 0, loss -0.10387706756591797\n",
      "epoch 97, iter 1, loss -0.012970775365829468\n",
      "epoch 97, loss -0.11684784293174744\n",
      "epoch 98, iter 0, loss -0.06663881242275238\n",
      "epoch 98, iter 1, loss -0.10140900313854218\n",
      "epoch 98, loss -0.16804781556129456\n",
      "epoch 99, iter 0, loss -0.06552425026893616\n",
      "epoch 99, iter 1, loss -0.062369659543037415\n",
      "epoch 99, loss -0.12789390981197357\n",
      "2019-03-30 23:02:49.517573, fold=1, rep=0, eta=0d 0h 18m 44s \n",
      "{'fold': 1, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.029054909944534302, 'train_time': 374.838460664032, 'trained_epochs': 100, 'prior_train_nmll': -0.07315868139266968, 'train_nll': -778.5208740234375, 'test_nll': -305.47601318359375, 'train_mse': 0.017950959503650665, 'state_dict_file': 'model_state_dict_6922288829097631790.pkl'}\n",
      "epoch 0, iter 0, loss 1.9740039110183716\n",
      "epoch 0, iter 1, loss 2.2792415618896484\n",
      "epoch 0, loss 4.25324547290802\n",
      "epoch 1, iter 0, loss 2.1847567558288574\n",
      "epoch 1, iter 1, loss 1.9761344194412231\n",
      "epoch 1, loss 4.160891175270081\n",
      "epoch 2, iter 0, loss 1.9151866436004639\n",
      "epoch 2, iter 1, loss 1.9532527923583984\n",
      "epoch 2, loss 3.8684394359588623\n",
      "epoch 3, iter 0, loss 1.782500147819519\n",
      "epoch 3, iter 1, loss 1.6531792879104614\n",
      "epoch 3, loss 3.4356794357299805\n",
      "epoch 4, iter 0, loss 1.6312243938446045\n",
      "epoch 4, iter 1, loss 1.5945476293563843\n",
      "epoch 4, loss 3.2257720232009888\n",
      "epoch 5, iter 0, loss 1.5201077461242676\n",
      "epoch 5, iter 1, loss 1.4614650011062622\n",
      "epoch 5, loss 2.98157274723053\n",
      "epoch 6, iter 0, loss 1.4375674724578857\n",
      "epoch 6, iter 1, loss 1.4374998807907104\n",
      "epoch 6, loss 2.875067353248596\n",
      "epoch 7, iter 0, loss 1.3938708305358887\n",
      "epoch 7, iter 1, loss 1.3568029403686523\n",
      "epoch 7, loss 2.750673770904541\n",
      "epoch 8, iter 0, loss 1.327451229095459\n",
      "epoch 8, iter 1, loss 1.3143924474716187\n",
      "epoch 8, loss 2.6418436765670776\n",
      "epoch 9, iter 0, loss 1.2923346757888794\n",
      "epoch 9, iter 1, loss 1.262514591217041\n",
      "epoch 9, loss 2.5548492670059204\n",
      "epoch 10, iter 0, loss 1.229642391204834\n",
      "epoch 10, iter 1, loss 1.2121080160140991\n",
      "epoch 10, loss 2.441750407218933\n",
      "epoch 11, iter 0, loss 1.1796833276748657\n",
      "epoch 11, iter 1, loss 1.1678869724273682\n",
      "epoch 11, loss 2.347570300102234\n",
      "epoch 12, iter 0, loss 1.1242753267288208\n",
      "epoch 12, iter 1, loss 1.1045386791229248\n",
      "epoch 12, loss 2.2288140058517456\n",
      "epoch 13, iter 0, loss 1.0733344554901123\n",
      "epoch 13, iter 1, loss 1.0536370277404785\n",
      "epoch 13, loss 2.126971483230591\n",
      "epoch 14, iter 0, loss 1.020566701889038\n",
      "epoch 14, iter 1, loss 0.9821125268936157\n",
      "epoch 14, loss 2.002679228782654\n",
      "epoch 15, iter 0, loss 0.9595526456832886\n",
      "epoch 15, iter 1, loss 0.9242950081825256\n",
      "epoch 15, loss 1.8838476538658142\n",
      "epoch 16, iter 0, loss 0.8941487669944763\n",
      "epoch 16, iter 1, loss 0.8808194398880005\n",
      "epoch 16, loss 1.7749682068824768\n",
      "epoch 17, iter 0, loss 0.8345767259597778\n",
      "epoch 17, iter 1, loss 0.8074641823768616\n",
      "epoch 17, loss 1.6420409083366394\n",
      "epoch 18, iter 0, loss 0.7791199088096619\n",
      "epoch 18, iter 1, loss 0.7334357500076294\n",
      "epoch 18, loss 1.5125556588172913\n",
      "epoch 19, iter 0, loss 0.7143043279647827\n",
      "epoch 19, iter 1, loss 0.684108555316925\n",
      "epoch 19, loss 1.3984128832817078\n",
      "epoch 20, iter 0, loss 0.6519553065299988\n",
      "epoch 20, iter 1, loss 0.6154313683509827\n",
      "epoch 20, loss 1.2673866748809814\n",
      "epoch 21, iter 0, loss 0.6027695536613464\n",
      "epoch 21, iter 1, loss 0.5567566156387329\n",
      "epoch 21, loss 1.1595261693000793\n",
      "epoch 22, iter 0, loss 0.5708130598068237\n",
      "epoch 22, iter 1, loss 0.4989998936653137\n",
      "epoch 22, loss 1.0698129534721375\n",
      "epoch 23, iter 0, loss 0.5070958733558655\n",
      "epoch 23, iter 1, loss 0.4942152798175812\n",
      "epoch 23, loss 1.0013111531734467\n",
      "epoch 24, iter 0, loss 0.4866703748703003\n",
      "epoch 24, iter 1, loss 0.4371887445449829\n",
      "epoch 24, loss 0.9238591194152832\n",
      "epoch 25, iter 0, loss 0.4665073752403259\n",
      "epoch 25, iter 1, loss 0.44612830877304077\n",
      "epoch 25, loss 0.9126356840133667\n",
      "epoch 26, iter 0, loss 0.44232696294784546\n",
      "epoch 26, iter 1, loss 0.43902429938316345\n",
      "epoch 26, loss 0.8813512623310089\n",
      "epoch 27, iter 0, loss 0.4683920741081238\n",
      "epoch 27, iter 1, loss 0.43231773376464844\n",
      "epoch 27, loss 0.9007098078727722\n",
      "epoch 28, iter 0, loss 0.41920581459999084\n",
      "epoch 28, iter 1, loss 0.44846755266189575\n",
      "epoch 28, loss 0.8676733672618866\n",
      "epoch 29, iter 0, loss 0.4130832850933075\n",
      "epoch 29, iter 1, loss 0.4129256308078766\n",
      "epoch 29, loss 0.8260089159011841\n",
      "epoch 30, iter 0, loss 0.43628859519958496\n",
      "epoch 30, iter 1, loss 0.29468032717704773\n",
      "epoch 30, loss 0.7309689223766327\n",
      "epoch 31, iter 0, loss 0.39559584856033325\n",
      "epoch 31, iter 1, loss 0.3541434109210968\n",
      "epoch 31, loss 0.74973925948143\n",
      "epoch 32, iter 0, loss 0.3218434453010559\n",
      "epoch 32, iter 1, loss 0.33505335450172424\n",
      "epoch 32, loss 0.6568967998027802\n",
      "epoch 33, iter 0, loss 0.30247175693511963\n",
      "epoch 33, iter 1, loss 0.3490832448005676\n",
      "epoch 33, loss 0.6515550017356873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34, iter 0, loss 0.33544743061065674\n",
      "epoch 34, iter 1, loss 0.22021165490150452\n",
      "epoch 34, loss 0.5556590855121613\n",
      "epoch 35, iter 0, loss 0.30514371395111084\n",
      "epoch 35, iter 1, loss 0.20682594180107117\n",
      "epoch 35, loss 0.511969655752182\n",
      "epoch 36, iter 0, loss 0.2688256502151489\n",
      "epoch 36, iter 1, loss 0.20848029851913452\n",
      "epoch 36, loss 0.47730594873428345\n",
      "epoch 37, iter 0, loss 0.16645759344100952\n",
      "epoch 37, iter 1, loss 0.35324013233184814\n",
      "epoch 37, loss 0.5196977257728577\n",
      "epoch 38, iter 0, loss 0.17278632521629333\n",
      "epoch 38, iter 1, loss 0.27701589465141296\n",
      "epoch 38, loss 0.4498022198677063\n",
      "epoch 39, iter 0, loss 0.24077282845973969\n",
      "epoch 39, iter 1, loss 0.13671867549419403\n",
      "epoch 39, loss 0.3774915039539337\n",
      "epoch 40, iter 0, loss 0.21465729176998138\n",
      "epoch 40, iter 1, loss 0.08757881820201874\n",
      "epoch 40, loss 0.3022361099720001\n",
      "epoch 41, iter 0, loss 0.13073429465293884\n",
      "epoch 41, iter 1, loss 0.17514491081237793\n",
      "epoch 41, loss 0.3058792054653168\n",
      "epoch 42, iter 0, loss 0.11270047724246979\n",
      "epoch 42, iter 1, loss 0.1750788688659668\n",
      "epoch 42, loss 0.2877793461084366\n",
      "epoch 43, iter 0, loss 0.12963984906673431\n",
      "epoch 43, iter 1, loss 0.07544730603694916\n",
      "epoch 43, loss 0.20508715510368347\n",
      "epoch 44, iter 0, loss 0.10517878830432892\n",
      "epoch 44, iter 1, loss 0.09170740842819214\n",
      "epoch 44, loss 0.19688619673252106\n",
      "epoch 45, iter 0, loss 0.06592750549316406\n",
      "epoch 45, iter 1, loss 0.10980138182640076\n",
      "epoch 45, loss 0.17572888731956482\n",
      "epoch 46, iter 0, loss 0.09133768081665039\n",
      "epoch 46, iter 1, loss 0.10915680229663849\n",
      "epoch 46, loss 0.20049448311328888\n",
      "epoch 47, iter 0, loss 0.0771641880273819\n",
      "epoch 47, iter 1, loss 0.05445230007171631\n",
      "epoch 47, loss 0.1316164880990982\n",
      "epoch 48, iter 0, loss 0.05646952986717224\n",
      "epoch 48, iter 1, loss 0.1296752542257309\n",
      "epoch 48, loss 0.18614478409290314\n",
      "epoch 49, iter 0, loss 0.11967387795448303\n",
      "epoch 49, iter 1, loss 0.06956090033054352\n",
      "epoch 49, loss 0.18923477828502655\n",
      "epoch 50, iter 0, loss 0.06567026674747467\n",
      "epoch 50, iter 1, loss 0.1370517909526825\n",
      "epoch 50, loss 0.20272205770015717\n",
      "epoch 51, iter 0, loss 0.035506486892700195\n",
      "epoch 51, iter 1, loss 0.2022186517715454\n",
      "epoch 51, loss 0.2377251386642456\n",
      "epoch 52, iter 0, loss 0.0564858615398407\n",
      "epoch 52, iter 1, loss 0.14365139603614807\n",
      "epoch 52, loss 0.20013725757598877\n",
      "epoch 53, iter 0, loss 0.07385373115539551\n",
      "epoch 53, iter 1, loss 0.03495708107948303\n",
      "epoch 53, loss 0.10881081223487854\n",
      "epoch 54, iter 0, loss 0.03658938407897949\n",
      "epoch 54, iter 1, loss 0.11884701251983643\n",
      "epoch 54, loss 0.15543639659881592\n",
      "epoch 55, iter 0, loss 0.03590092062950134\n",
      "epoch 55, iter 1, loss 0.0889081358909607\n",
      "epoch 55, loss 0.12480905652046204\n",
      "epoch 56, iter 0, loss 0.06443481147289276\n",
      "epoch 56, iter 1, loss 0.03769506514072418\n",
      "epoch 56, loss 0.10212987661361694\n",
      "epoch 57, iter 0, loss 0.02670958638191223\n",
      "epoch 57, iter 1, loss 0.059870555996894836\n",
      "epoch 57, loss 0.08658014237880707\n",
      "epoch 58, iter 0, loss 0.054333046078681946\n",
      "epoch 58, iter 1, loss 0.024969860911369324\n",
      "epoch 58, loss 0.07930290699005127\n",
      "epoch 59, iter 0, loss 0.05669066309928894\n",
      "epoch 59, iter 1, loss 0.005545377731323242\n",
      "epoch 59, loss 0.06223604083061218\n",
      "epoch 60, iter 0, loss 0.0012486279010772705\n",
      "epoch 60, iter 1, loss 0.06766709685325623\n",
      "epoch 60, loss 0.0689157247543335\n",
      "epoch 61, iter 0, loss -0.06421145796775818\n",
      "epoch 61, iter 1, loss 0.14910338819026947\n",
      "epoch 61, loss 0.08489193022251129\n",
      "epoch 62, iter 0, loss 0.01626613736152649\n",
      "epoch 62, iter 1, loss -0.02262517809867859\n",
      "epoch 62, loss -0.0063590407371521\n",
      "epoch 63, iter 0, loss -0.0474277138710022\n",
      "epoch 63, iter 1, loss 0.12295745313167572\n",
      "epoch 63, loss 0.07552973926067352\n",
      "epoch 64, iter 0, loss 0.04961185157299042\n",
      "epoch 64, iter 1, loss -0.07619473338127136\n",
      "epoch 64, loss -0.026582881808280945\n",
      "epoch 65, iter 0, loss 0.010284751653671265\n",
      "epoch 65, iter 1, loss 0.005147725343704224\n",
      "epoch 65, loss 0.015432476997375488\n",
      "epoch 66, iter 0, loss 0.0168057382106781\n",
      "epoch 66, iter 1, loss -0.014146864414215088\n",
      "epoch 66, loss 0.0026588737964630127\n",
      "epoch 67, iter 0, loss -0.016302824020385742\n",
      "epoch 67, iter 1, loss 0.004744738340377808\n",
      "epoch 67, loss -0.011558085680007935\n",
      "epoch 68, iter 0, loss 0.0374734103679657\n",
      "epoch 68, iter 1, loss -0.08108717203140259\n",
      "epoch 68, loss -0.04361376166343689\n",
      "epoch 69, iter 0, loss -0.07093071937561035\n",
      "epoch 69, iter 1, loss 0.09452950954437256\n",
      "epoch 69, loss 0.023598790168762207\n",
      "epoch 70, iter 0, loss -0.04894113540649414\n",
      "epoch 70, iter 1, loss 0.10866715013980865\n",
      "epoch 70, loss 0.059726014733314514\n",
      "epoch 71, iter 0, loss 0.03986355662345886\n",
      "epoch 71, iter 1, loss -0.047119587659835815\n",
      "epoch 71, loss -0.007256031036376953\n",
      "epoch 72, iter 0, loss 0.07183346152305603\n",
      "epoch 72, iter 1, loss -0.09014460444450378\n",
      "epoch 72, loss -0.018311142921447754\n",
      "epoch 73, iter 0, loss 0.020688116550445557\n",
      "epoch 73, iter 1, loss -0.005599230527877808\n",
      "epoch 73, loss 0.015088886022567749\n",
      "epoch 74, iter 0, loss 0.06247773766517639\n",
      "epoch 74, iter 1, loss -0.08559775352478027\n",
      "epoch 74, loss -0.023120015859603882\n",
      "epoch 75, iter 0, loss 0.02241542935371399\n",
      "epoch 75, iter 1, loss -0.04632231593132019\n",
      "epoch 75, loss -0.0239068865776062\n",
      "epoch 76, iter 0, loss -0.016385585069656372\n",
      "epoch 76, iter 1, loss 0.006635069847106934\n",
      "epoch 76, loss -0.009750515222549438\n",
      "epoch 77, iter 0, loss -0.02328479290008545\n",
      "epoch 77, iter 1, loss 0.003964245319366455\n",
      "epoch 77, loss -0.019320547580718994\n",
      "epoch 78, iter 0, loss 0.034390851855278015\n",
      "epoch 78, iter 1, loss -0.10115131735801697\n",
      "epoch 78, loss -0.06676046550273895\n",
      "epoch 79, iter 0, loss -0.004058778285980225\n",
      "epoch 79, iter 1, loss -0.0013863146305084229\n",
      "epoch 79, loss -0.0054450929164886475\n",
      "epoch 80, iter 0, loss -0.005719482898712158\n",
      "epoch 80, iter 1, loss -0.042213380336761475\n",
      "epoch 80, loss -0.04793286323547363\n",
      "epoch 81, iter 0, loss -0.0519641637802124\n",
      "epoch 81, iter 1, loss 0.11027708649635315\n",
      "epoch 81, loss 0.05831292271614075\n",
      "epoch 82, iter 0, loss -0.07681095600128174\n",
      "epoch 82, iter 1, loss 0.13365043699741364\n",
      "epoch 82, loss 0.0568394809961319\n",
      "epoch 83, iter 0, loss -0.03105941414833069\n",
      "epoch 83, iter 1, loss 0.0833425521850586\n",
      "epoch 83, loss 0.052283138036727905\n",
      "epoch 84, iter 0, loss 0.01985672116279602\n",
      "epoch 84, iter 1, loss 0.049139514565467834\n",
      "epoch 84, loss 0.06899623572826385\n",
      "epoch 85, iter 0, loss -0.014131754636764526\n",
      "epoch 85, iter 1, loss 0.02440410852432251\n",
      "epoch 85, loss 0.010272353887557983\n",
      "epoch 86, iter 0, loss 0.004520028829574585\n",
      "epoch 86, iter 1, loss 0.004943966865539551\n",
      "epoch 86, loss 0.009463995695114136\n",
      "epoch 87, iter 0, loss -0.005492478609085083\n",
      "epoch 87, iter 1, loss -0.02246001362800598\n",
      "epoch 87, loss -0.027952492237091064\n",
      "epoch 88, iter 0, loss -0.04478636384010315\n",
      "epoch 88, iter 1, loss 0.024052292108535767\n",
      "epoch 88, loss -0.020734071731567383\n",
      "epoch 89, iter 0, loss -0.09791293740272522\n",
      "epoch 89, iter 1, loss 0.09833228588104248\n",
      "epoch 89, loss 0.00041934847831726074\n",
      "epoch 90, iter 0, loss -0.053030818700790405\n",
      "epoch 90, iter 1, loss 0.0013162046670913696\n",
      "epoch 90, loss -0.051714614033699036\n",
      "epoch 91, iter 0, loss -0.03135579824447632\n",
      "epoch 91, iter 1, loss -0.016361624002456665\n",
      "epoch 91, loss -0.04771742224693298\n",
      "epoch 92, iter 0, loss -0.0031534433364868164\n",
      "epoch 92, iter 1, loss -0.10603773593902588\n",
      "epoch 92, loss -0.1091911792755127\n",
      "epoch 93, iter 0, loss -0.02931419014930725\n",
      "epoch 93, iter 1, loss -0.05731934309005737\n",
      "epoch 93, loss -0.08663353323936462\n",
      "epoch 94, iter 0, loss -0.02069583535194397\n",
      "epoch 94, iter 1, loss -0.10210992395877838\n",
      "epoch 94, loss -0.12280575931072235\n",
      "epoch 95, iter 0, loss 0.01562705636024475\n",
      "epoch 95, iter 1, loss -0.14448294043540955\n",
      "epoch 95, loss -0.1288558840751648\n",
      "epoch 96, iter 0, loss -0.1122882068157196\n",
      "epoch 96, iter 1, loss 0.05587533116340637\n",
      "epoch 96, loss -0.05641287565231323\n",
      "epoch 97, iter 0, loss -0.09828466176986694\n",
      "epoch 97, iter 1, loss 0.030952930450439453\n",
      "epoch 97, loss -0.06733173131942749\n",
      "epoch 98, iter 0, loss -0.13033193349838257\n",
      "epoch 98, iter 1, loss 0.07930059731006622\n",
      "epoch 98, loss -0.051031336188316345\n",
      "epoch 99, iter 0, loss -0.00030094385147094727\n",
      "epoch 99, iter 1, loss -0.0784527063369751\n",
      "epoch 99, loss -0.07875365018844604\n",
      "2019-03-30 23:09:14.527010, fold=1, rep=1, eta=0d 0h 12m 34s \n",
      "{'fold': 1, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.034559719264507294, 'train_time': 385.00923267705366, 'trained_epochs': 100, 'prior_train_nmll': -0.008738934993743896, 'train_nll': -685.0338134765625, 'test_nll': -253.61993408203125, 'train_mse': 0.022537298500537872, 'state_dict_file': 'model_state_dict_-6744374239659966282.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, iter 0, loss 2.003568172454834\n",
      "epoch 0, iter 1, loss 2.4073474407196045\n",
      "epoch 0, loss 4.4109156131744385\n",
      "epoch 1, iter 0, loss 2.2530364990234375\n",
      "epoch 1, iter 1, loss 2.132333278656006\n",
      "epoch 1, loss 4.385369777679443\n",
      "epoch 2, iter 0, loss 1.9963572025299072\n",
      "epoch 2, iter 1, loss 1.9762517213821411\n",
      "epoch 2, loss 3.9726089239120483\n",
      "epoch 3, iter 0, loss 1.7956664562225342\n",
      "epoch 3, iter 1, loss 1.6403212547302246\n",
      "epoch 3, loss 3.435987710952759\n",
      "epoch 4, iter 0, loss 1.6208508014678955\n",
      "epoch 4, iter 1, loss 1.6116912364959717\n",
      "epoch 4, loss 3.232542037963867\n",
      "epoch 5, iter 0, loss 1.5421574115753174\n",
      "epoch 5, iter 1, loss 1.4548637866973877\n",
      "epoch 5, loss 2.997021198272705\n",
      "epoch 6, iter 0, loss 1.4374459981918335\n",
      "epoch 6, iter 1, loss 1.4405384063720703\n",
      "epoch 6, loss 2.877984404563904\n",
      "epoch 7, iter 0, loss 1.4021116495132446\n",
      "epoch 7, iter 1, loss 1.3665558099746704\n",
      "epoch 7, loss 2.768667459487915\n",
      "epoch 8, iter 0, loss 1.3250519037246704\n",
      "epoch 8, iter 1, loss 1.3128646612167358\n",
      "epoch 8, loss 2.6379165649414062\n",
      "epoch 9, iter 0, loss 1.2943012714385986\n",
      "epoch 9, iter 1, loss 1.269273042678833\n",
      "epoch 9, loss 2.5635743141174316\n",
      "epoch 10, iter 0, loss 1.2280625104904175\n",
      "epoch 10, iter 1, loss 1.2106884717941284\n",
      "epoch 10, loss 2.438750982284546\n",
      "epoch 11, iter 0, loss 1.1854478120803833\n",
      "epoch 11, iter 1, loss 1.1656547784805298\n",
      "epoch 11, loss 2.351102590560913\n",
      "epoch 12, iter 0, loss 1.1370148658752441\n",
      "epoch 12, iter 1, loss 1.0970083475112915\n",
      "epoch 12, loss 2.2340232133865356\n",
      "epoch 13, iter 0, loss 1.06771981716156\n",
      "epoch 13, iter 1, loss 1.0663509368896484\n",
      "epoch 13, loss 2.1340707540512085\n",
      "epoch 14, iter 0, loss 1.025012731552124\n",
      "epoch 14, iter 1, loss 0.9846041798591614\n",
      "epoch 14, loss 2.0096169114112854\n",
      "epoch 15, iter 0, loss 0.95302814245224\n",
      "epoch 15, iter 1, loss 0.9463268518447876\n",
      "epoch 15, loss 1.8993549942970276\n",
      "epoch 16, iter 0, loss 0.9040743112564087\n",
      "epoch 16, iter 1, loss 0.8663526773452759\n",
      "epoch 16, loss 1.7704269886016846\n",
      "epoch 17, iter 0, loss 0.8404036164283752\n",
      "epoch 17, iter 1, loss 0.8059977889060974\n",
      "epoch 17, loss 1.6464014053344727\n",
      "epoch 18, iter 0, loss 0.7804309725761414\n",
      "epoch 18, iter 1, loss 0.7416404485702515\n",
      "epoch 18, loss 1.5220714211463928\n",
      "epoch 19, iter 0, loss 0.707297682762146\n",
      "epoch 19, iter 1, loss 0.6937105655670166\n",
      "epoch 19, loss 1.4010082483291626\n",
      "epoch 20, iter 0, loss 0.6564084887504578\n",
      "epoch 20, iter 1, loss 0.6251431107521057\n",
      "epoch 20, loss 1.2815515995025635\n",
      "epoch 21, iter 0, loss 0.599330723285675\n",
      "epoch 21, iter 1, loss 0.5637703537940979\n",
      "epoch 21, loss 1.163101077079773\n",
      "epoch 22, iter 0, loss 0.535776674747467\n",
      "epoch 22, iter 1, loss 0.5579184293746948\n",
      "epoch 22, loss 1.0936951041221619\n",
      "epoch 23, iter 0, loss 0.5260821580886841\n",
      "epoch 23, iter 1, loss 0.47360602021217346\n",
      "epoch 23, loss 0.9996881783008575\n",
      "epoch 24, iter 0, loss 0.4527323246002197\n",
      "epoch 24, iter 1, loss 0.4479348361492157\n",
      "epoch 24, loss 0.9006671607494354\n",
      "epoch 25, iter 0, loss 0.4445522427558899\n",
      "epoch 25, iter 1, loss 0.3818625807762146\n",
      "epoch 25, loss 0.8264148235321045\n",
      "epoch 26, iter 0, loss 0.41669076681137085\n",
      "epoch 26, iter 1, loss 0.37331730127334595\n",
      "epoch 26, loss 0.7900080680847168\n",
      "epoch 27, iter 0, loss 0.41180354356765747\n",
      "epoch 27, iter 1, loss 0.3303526043891907\n",
      "epoch 27, loss 0.7421561479568481\n",
      "epoch 28, iter 0, loss 0.4197739362716675\n",
      "epoch 28, iter 1, loss 0.3454779386520386\n",
      "epoch 28, loss 0.765251874923706\n",
      "epoch 29, iter 0, loss 0.3770233690738678\n",
      "epoch 29, iter 1, loss 0.3948762118816376\n",
      "epoch 29, loss 0.7718995809555054\n",
      "epoch 30, iter 0, loss 0.381924033164978\n",
      "epoch 30, iter 1, loss 0.45517832040786743\n",
      "epoch 30, loss 0.8371023535728455\n",
      "epoch 31, iter 0, loss 0.3759496808052063\n",
      "epoch 31, iter 1, loss 0.40574395656585693\n",
      "epoch 31, loss 0.7816936373710632\n",
      "epoch 32, iter 0, loss 0.3695375919342041\n",
      "epoch 32, iter 1, loss 0.33991575241088867\n",
      "epoch 32, loss 0.7094533443450928\n",
      "epoch 33, iter 0, loss 0.27864888310432434\n",
      "epoch 33, iter 1, loss 0.48691239953041077\n",
      "epoch 33, loss 0.7655612826347351\n",
      "epoch 34, iter 0, loss 0.37726646661758423\n",
      "epoch 34, iter 1, loss 0.22647860646247864\n",
      "epoch 34, loss 0.6037450730800629\n",
      "epoch 35, iter 0, loss 0.2704179883003235\n",
      "epoch 35, iter 1, loss 0.30564844608306885\n",
      "epoch 35, loss 0.5760664343833923\n",
      "epoch 36, iter 0, loss 0.2826421558856964\n",
      "epoch 36, iter 1, loss 0.2347986400127411\n",
      "epoch 36, loss 0.5174407958984375\n",
      "epoch 37, iter 0, loss 0.2598957419395447\n",
      "epoch 37, iter 1, loss 0.17137236893177032\n",
      "epoch 37, loss 0.431268110871315\n",
      "epoch 38, iter 0, loss 0.15539860725402832\n",
      "epoch 38, iter 1, loss 0.295174241065979\n",
      "epoch 38, loss 0.4505728483200073\n",
      "epoch 39, iter 0, loss 0.23267236351966858\n",
      "epoch 39, iter 1, loss 0.14004507660865784\n",
      "epoch 39, loss 0.3727174401283264\n",
      "epoch 40, iter 0, loss 0.10252523422241211\n",
      "epoch 40, iter 1, loss 0.273128867149353\n",
      "epoch 40, loss 0.37565410137176514\n",
      "epoch 41, iter 0, loss 0.14816200733184814\n",
      "epoch 41, iter 1, loss 0.16978341341018677\n",
      "epoch 41, loss 0.3179454207420349\n",
      "epoch 42, iter 0, loss 0.09657832980155945\n",
      "epoch 42, iter 1, loss 0.2318471074104309\n",
      "epoch 42, loss 0.32842543721199036\n",
      "epoch 43, iter 0, loss 0.10851503908634186\n",
      "epoch 43, iter 1, loss 0.15797123312950134\n",
      "epoch 43, loss 0.2664862722158432\n",
      "epoch 44, iter 0, loss 0.12646305561065674\n",
      "epoch 44, iter 1, loss 0.11245662719011307\n",
      "epoch 44, loss 0.2389196828007698\n",
      "epoch 45, iter 0, loss 0.043551117181777954\n",
      "epoch 45, iter 1, loss 0.2178746908903122\n",
      "epoch 45, loss 0.26142580807209015\n",
      "epoch 46, iter 0, loss 0.15986737608909607\n",
      "epoch 46, iter 1, loss 0.02066856622695923\n",
      "epoch 46, loss 0.1805359423160553\n",
      "epoch 47, iter 0, loss 0.10079821944236755\n",
      "epoch 47, iter 1, loss 0.12860411405563354\n",
      "epoch 47, loss 0.2294023334980011\n",
      "epoch 48, iter 0, loss 0.11148455739021301\n",
      "epoch 48, iter 1, loss 0.10871607065200806\n",
      "epoch 48, loss 0.22020062804222107\n",
      "epoch 49, iter 0, loss 0.13377878069877625\n",
      "epoch 49, iter 1, loss 0.044934019446372986\n",
      "epoch 49, loss 0.17871280014514923\n",
      "epoch 50, iter 0, loss 0.08742892742156982\n",
      "epoch 50, iter 1, loss 0.13846006989479065\n",
      "epoch 50, loss 0.22588899731636047\n",
      "epoch 51, iter 0, loss 0.016576051712036133\n",
      "epoch 51, iter 1, loss 0.2702244222164154\n",
      "epoch 51, loss 0.28680047392845154\n",
      "epoch 52, iter 0, loss 0.16655325889587402\n",
      "epoch 52, iter 1, loss 0.05529336631298065\n",
      "epoch 52, loss 0.22184662520885468\n",
      "epoch 53, iter 0, loss 0.17376254498958588\n",
      "epoch 53, iter 1, loss 0.038170382380485535\n",
      "epoch 53, loss 0.2119329273700714\n",
      "epoch 54, iter 0, loss 0.11338797211647034\n",
      "epoch 54, iter 1, loss 0.11255809664726257\n",
      "epoch 54, loss 0.2259460687637329\n",
      "epoch 55, iter 0, loss 0.19189973175525665\n",
      "epoch 55, iter 1, loss 0.023446664214134216\n",
      "epoch 55, loss 0.21534639596939087\n",
      "epoch 56, iter 0, loss 0.17058065533638\n",
      "epoch 56, iter 1, loss 0.06522233784198761\n",
      "epoch 56, loss 0.23580299317836761\n",
      "epoch 57, iter 0, loss 0.07298645377159119\n",
      "epoch 57, iter 1, loss 0.16638731956481934\n",
      "epoch 57, loss 0.23937377333641052\n",
      "epoch 58, iter 0, loss 0.12250535190105438\n",
      "epoch 58, iter 1, loss 0.09103938937187195\n",
      "epoch 58, loss 0.21354474127292633\n",
      "epoch 59, iter 0, loss 0.01715072989463806\n",
      "epoch 59, iter 1, loss 0.20614179968833923\n",
      "epoch 59, loss 0.2232925295829773\n",
      "epoch 60, iter 0, loss 0.07834187150001526\n",
      "epoch 60, iter 1, loss 0.09447488188743591\n",
      "epoch 60, loss 0.17281675338745117\n",
      "epoch 61, iter 0, loss -0.015932828187942505\n",
      "epoch 61, iter 1, loss 0.24070008099079132\n",
      "epoch 61, loss 0.22476725280284882\n",
      "epoch 62, iter 0, loss 0.0573921799659729\n",
      "epoch 62, iter 1, loss 0.10337133705615997\n",
      "epoch 62, loss 0.16076351702213287\n",
      "epoch 63, iter 0, loss 0.08866624534130096\n",
      "epoch 63, iter 1, loss 0.07632116973400116\n",
      "epoch 63, loss 0.16498741507530212\n",
      "epoch 64, iter 0, loss 0.11486199498176575\n",
      "epoch 64, iter 1, loss 0.024015069007873535\n",
      "epoch 64, loss 0.13887706398963928\n",
      "epoch 65, iter 0, loss 0.035043865442276\n",
      "epoch 65, iter 1, loss 0.14679282903671265\n",
      "epoch 65, loss 0.18183669447898865\n",
      "epoch 66, iter 0, loss 0.03447440266609192\n",
      "epoch 66, iter 1, loss 0.12488095462322235\n",
      "epoch 66, loss 0.15935535728931427\n",
      "epoch 67, iter 0, loss 0.003509834408760071\n",
      "epoch 67, iter 1, loss 0.15306615829467773\n",
      "epoch 67, loss 0.1565759927034378\n",
      "epoch 68, iter 0, loss 0.08204162120819092\n",
      "epoch 68, iter 1, loss 0.04931493103504181\n",
      "epoch 68, loss 0.13135655224323273\n",
      "epoch 69, iter 0, loss 0.053213685750961304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69, iter 1, loss 0.04743358492851257\n",
      "epoch 69, loss 0.10064727067947388\n",
      "epoch 70, iter 0, loss 0.06873336434364319\n",
      "epoch 70, iter 1, loss 0.028287500143051147\n",
      "epoch 70, loss 0.09702086448669434\n",
      "epoch 71, iter 0, loss 0.06113545596599579\n",
      "epoch 71, iter 1, loss 0.03927619755268097\n",
      "epoch 71, loss 0.10041165351867676\n",
      "epoch 72, iter 0, loss 0.007640466094017029\n",
      "epoch 72, iter 1, loss 0.10595497488975525\n",
      "epoch 72, loss 0.11359544098377228\n",
      "epoch 73, iter 0, loss 0.09673136472702026\n",
      "epoch 73, iter 1, loss 0.0023931264877319336\n",
      "epoch 73, loss 0.0991244912147522\n",
      "epoch 74, iter 0, loss 0.043350234627723694\n",
      "epoch 74, iter 1, loss 0.061232104897499084\n",
      "epoch 74, loss 0.10458233952522278\n",
      "epoch 75, iter 0, loss 0.14773938059806824\n",
      "epoch 75, iter 1, loss -0.07154145836830139\n",
      "epoch 75, loss 0.07619792222976685\n",
      "epoch 76, iter 0, loss 0.012648403644561768\n",
      "epoch 76, iter 1, loss 0.10902281105518341\n",
      "epoch 76, loss 0.12167121469974518\n",
      "epoch 77, iter 0, loss -0.027447566390037537\n",
      "epoch 77, iter 1, loss 0.17201511561870575\n",
      "epoch 77, loss 0.1445675492286682\n",
      "epoch 78, iter 0, loss 0.06977605819702148\n",
      "epoch 78, iter 1, loss 0.04395362734794617\n",
      "epoch 78, loss 0.11372968554496765\n",
      "epoch 79, iter 0, loss -0.00471070408821106\n",
      "epoch 79, iter 1, loss 0.14946921169757843\n",
      "epoch 79, loss 0.14475850760936737\n",
      "epoch 80, iter 0, loss 0.06230290234088898\n",
      "epoch 80, iter 1, loss 0.05598330497741699\n",
      "epoch 80, loss 0.11828620731830597\n",
      "epoch 81, iter 0, loss 0.12346561253070831\n",
      "epoch 81, iter 1, loss -0.047471076250076294\n",
      "epoch 81, loss 0.07599453628063202\n",
      "epoch 82, iter 0, loss -0.011941403150558472\n",
      "epoch 82, iter 1, loss 0.1198643371462822\n",
      "epoch 82, loss 0.10792293399572372\n",
      "epoch 83, iter 0, loss -0.0263090580701828\n",
      "epoch 83, iter 1, loss 0.13474640250205994\n",
      "epoch 83, loss 0.10843734443187714\n",
      "epoch 84, iter 0, loss 0.035931527614593506\n",
      "epoch 84, iter 1, loss -0.00996437668800354\n",
      "epoch 84, loss 0.025967150926589966\n",
      "epoch 85, iter 0, loss 0.06251208484172821\n",
      "epoch 85, iter 1, loss -0.046753570437431335\n",
      "epoch 85, loss 0.015758514404296875\n",
      "epoch 86, iter 0, loss 0.060112565755844116\n",
      "epoch 86, iter 1, loss -0.046564847230911255\n",
      "epoch 86, loss 0.013547718524932861\n",
      "epoch 87, iter 0, loss 0.04883319139480591\n",
      "epoch 87, iter 1, loss -0.0647253543138504\n",
      "epoch 87, loss -0.015892162919044495\n",
      "epoch 88, iter 0, loss -0.09005467593669891\n",
      "epoch 88, iter 1, loss 0.16801361739635468\n",
      "epoch 88, loss 0.07795894145965576\n",
      "epoch 89, iter 0, loss 0.017467498779296875\n",
      "epoch 89, iter 1, loss -0.0015719234943389893\n",
      "epoch 89, loss 0.015895575284957886\n",
      "epoch 90, iter 0, loss -0.056499674916267395\n",
      "epoch 90, iter 1, loss 0.1354472041130066\n",
      "epoch 90, loss 0.0789475291967392\n",
      "epoch 91, iter 0, loss -0.039815306663513184\n",
      "epoch 91, iter 1, loss 0.10142539441585541\n",
      "epoch 91, loss 0.061610087752342224\n",
      "epoch 92, iter 0, loss 0.0021861791610717773\n",
      "epoch 92, iter 1, loss 0.05179643630981445\n",
      "epoch 92, loss 0.05398261547088623\n",
      "epoch 93, iter 0, loss -0.03243038058280945\n",
      "epoch 93, iter 1, loss 0.07979120314121246\n",
      "epoch 93, loss 0.047360822558403015\n",
      "epoch 94, iter 0, loss 0.00031560659408569336\n",
      "epoch 94, iter 1, loss 0.06516653299331665\n",
      "epoch 94, loss 0.06548213958740234\n",
      "epoch 95, iter 0, loss 0.017161652445793152\n",
      "epoch 95, iter 1, loss 0.03370560705661774\n",
      "epoch 95, loss 0.05086725950241089\n",
      "epoch 96, iter 0, loss 0.10387073457241058\n",
      "epoch 96, iter 1, loss -0.05361923575401306\n",
      "epoch 96, loss 0.05025149881839752\n",
      "epoch 97, iter 0, loss -0.05144768953323364\n",
      "epoch 97, iter 1, loss 0.2337053120136261\n",
      "epoch 97, loss 0.18225762248039246\n",
      "epoch 98, iter 0, loss 0.013520509004592896\n",
      "epoch 98, iter 1, loss 0.0722426176071167\n",
      "epoch 98, loss 0.0857631266117096\n",
      "epoch 99, iter 0, loss 0.06394588947296143\n",
      "epoch 99, iter 1, loss -0.02192579209804535\n",
      "epoch 99, loss 0.04202009737491608\n",
      "2019-03-30 23:15:11.549118, fold=2, rep=0, eta=0d 0h 6m 13s \n",
      "{'fold': 2, 'repeat': 0, 'n': 2565, 'd': 126, 'mse': 0.012882736511528492, 'train_time': 349.17221044306643, 'trained_epochs': 100, 'prior_train_nmll': 0.025783836841583252, 'train_nll': -559.9671630859375, 'test_nll': -355.59613037109375, 'train_mse': 0.023979468271136284, 'state_dict_file': 'model_state_dict_7016074241288158454.pkl'}\n",
      "epoch 0, iter 0, loss 1.993256688117981\n",
      "epoch 0, iter 1, loss 2.355884552001953\n",
      "epoch 0, loss 4.349141240119934\n",
      "epoch 1, iter 0, loss 2.2248921394348145\n",
      "epoch 1, iter 1, loss 2.073476791381836\n",
      "epoch 1, loss 4.29836893081665\n",
      "epoch 2, iter 0, loss 1.9708144664764404\n",
      "epoch 2, iter 1, loss 1.9459025859832764\n",
      "epoch 2, loss 3.916717052459717\n",
      "epoch 3, iter 0, loss 1.7992098331451416\n",
      "epoch 3, iter 1, loss 1.631584882736206\n",
      "epoch 3, loss 3.4307947158813477\n",
      "epoch 4, iter 0, loss 1.608702540397644\n",
      "epoch 4, iter 1, loss 1.6223971843719482\n",
      "epoch 4, loss 3.2310997247695923\n",
      "epoch 5, iter 0, loss 1.5348265171051025\n",
      "epoch 5, iter 1, loss 1.464780330657959\n",
      "epoch 5, loss 2.9996068477630615\n",
      "epoch 6, iter 0, loss 1.435623288154602\n",
      "epoch 6, iter 1, loss 1.4373376369476318\n",
      "epoch 6, loss 2.872960925102234\n",
      "epoch 7, iter 0, loss 1.3993852138519287\n",
      "epoch 7, iter 1, loss 1.3678913116455078\n",
      "epoch 7, loss 2.7672765254974365\n",
      "epoch 8, iter 0, loss 1.3185909986495972\n",
      "epoch 8, iter 1, loss 1.3241679668426514\n",
      "epoch 8, loss 2.6427589654922485\n",
      "epoch 9, iter 0, loss 1.2935010194778442\n",
      "epoch 9, iter 1, loss 1.2604732513427734\n",
      "epoch 9, loss 2.5539742708206177\n",
      "epoch 10, iter 0, loss 1.2235331535339355\n",
      "epoch 10, iter 1, loss 1.2072248458862305\n",
      "epoch 10, loss 2.430757999420166\n",
      "epoch 11, iter 0, loss 1.1790647506713867\n",
      "epoch 11, iter 1, loss 1.1577223539352417\n",
      "epoch 11, loss 2.3367871046066284\n",
      "epoch 12, iter 0, loss 1.1262180805206299\n",
      "epoch 12, iter 1, loss 1.094740390777588\n",
      "epoch 12, loss 2.2209584712982178\n",
      "epoch 13, iter 0, loss 1.0656846761703491\n",
      "epoch 13, iter 1, loss 1.0397437810897827\n",
      "epoch 13, loss 2.105428457260132\n",
      "epoch 14, iter 0, loss 1.0141674280166626\n",
      "epoch 14, iter 1, loss 0.9670422077178955\n",
      "epoch 14, loss 1.981209635734558\n",
      "epoch 15, iter 0, loss 0.9470725655555725\n",
      "epoch 15, iter 1, loss 0.9190694689750671\n",
      "epoch 15, loss 1.8661420345306396\n",
      "epoch 16, iter 0, loss 0.8831366300582886\n",
      "epoch 16, iter 1, loss 0.8607895374298096\n",
      "epoch 16, loss 1.7439261674880981\n",
      "epoch 17, iter 0, loss 0.8240646719932556\n",
      "epoch 17, iter 1, loss 0.7941343188285828\n",
      "epoch 17, loss 1.6181989908218384\n",
      "epoch 18, iter 0, loss 0.762114405632019\n",
      "epoch 18, iter 1, loss 0.7237563133239746\n",
      "epoch 18, loss 1.4858707189559937\n",
      "epoch 19, iter 0, loss 0.6925057172775269\n",
      "epoch 19, iter 1, loss 0.6662914752960205\n",
      "epoch 19, loss 1.3587971925735474\n",
      "epoch 20, iter 0, loss 0.6283186674118042\n",
      "epoch 20, iter 1, loss 0.6181075572967529\n",
      "epoch 20, loss 1.2464262247085571\n",
      "epoch 21, iter 0, loss 0.5800154805183411\n",
      "epoch 21, iter 1, loss 0.553530216217041\n",
      "epoch 21, loss 1.133545696735382\n",
      "epoch 22, iter 0, loss 0.5271033644676208\n",
      "epoch 22, iter 1, loss 0.5138688683509827\n",
      "epoch 22, loss 1.0409722328186035\n",
      "epoch 23, iter 0, loss 0.49796417355537415\n",
      "epoch 23, iter 1, loss 0.4761374592781067\n",
      "epoch 23, loss 0.9741016328334808\n",
      "epoch 24, iter 0, loss 0.4989645779132843\n",
      "epoch 24, iter 1, loss 0.4028146266937256\n",
      "epoch 24, loss 0.9017792046070099\n",
      "epoch 25, iter 0, loss 0.4417862892150879\n",
      "epoch 25, iter 1, loss 0.435991495847702\n",
      "epoch 25, loss 0.8777777850627899\n",
      "epoch 26, iter 0, loss 0.3876495361328125\n",
      "epoch 26, iter 1, loss 0.47079139947891235\n",
      "epoch 26, loss 0.8584409356117249\n",
      "epoch 27, iter 0, loss 0.4360518455505371\n",
      "epoch 27, iter 1, loss 0.372965544462204\n",
      "epoch 27, loss 0.8090173900127411\n",
      "epoch 28, iter 0, loss 0.40436458587646484\n",
      "epoch 28, iter 1, loss 0.4006977081298828\n",
      "epoch 28, loss 0.8050622940063477\n",
      "epoch 29, iter 0, loss 0.4102717339992523\n",
      "epoch 29, iter 1, loss 0.36074915528297424\n",
      "epoch 29, loss 0.7710208892822266\n",
      "epoch 30, iter 0, loss 0.44494131207466125\n",
      "epoch 30, iter 1, loss 0.2860707640647888\n",
      "epoch 30, loss 0.7310120761394501\n",
      "epoch 31, iter 0, loss 0.4123151898384094\n",
      "epoch 31, iter 1, loss 0.33561795949935913\n",
      "epoch 31, loss 0.7479331493377686\n",
      "epoch 32, iter 0, loss 0.38423335552215576\n",
      "epoch 32, iter 1, loss 0.3286410868167877\n",
      "epoch 32, loss 0.7128744423389435\n",
      "epoch 33, iter 0, loss 0.3059505224227905\n",
      "epoch 33, iter 1, loss 0.39690572023391724\n",
      "epoch 33, loss 0.7028562426567078\n",
      "epoch 34, iter 0, loss 0.2174568772315979\n",
      "epoch 34, iter 1, loss 0.43516919016838074\n",
      "epoch 34, loss 0.6526260673999786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35, iter 0, loss 0.31180375814437866\n",
      "epoch 35, iter 1, loss 0.21965479850769043\n",
      "epoch 35, loss 0.5314585566520691\n",
      "epoch 36, iter 0, loss 0.23579925298690796\n",
      "epoch 36, iter 1, loss 0.2654776871204376\n",
      "epoch 36, loss 0.5012769401073456\n",
      "epoch 37, iter 0, loss 0.268110454082489\n",
      "epoch 37, iter 1, loss 0.1927366852760315\n",
      "epoch 37, loss 0.4608471393585205\n",
      "epoch 38, iter 0, loss 0.24607016146183014\n",
      "epoch 38, iter 1, loss 0.1297098994255066\n",
      "epoch 38, loss 0.37578006088733673\n",
      "epoch 39, iter 0, loss 0.19605597853660583\n",
      "epoch 39, iter 1, loss 0.19127286970615387\n",
      "epoch 39, loss 0.3873288482427597\n",
      "epoch 40, iter 0, loss 0.2053648829460144\n",
      "epoch 40, iter 1, loss 0.11590412259101868\n",
      "epoch 40, loss 0.3212690055370331\n",
      "epoch 41, iter 0, loss 0.18610240519046783\n",
      "epoch 41, iter 1, loss 0.10351242125034332\n",
      "epoch 41, loss 0.28961482644081116\n",
      "epoch 42, iter 0, loss 0.09805314242839813\n",
      "epoch 42, iter 1, loss 0.20103731751441956\n",
      "epoch 42, loss 0.2990904599428177\n",
      "epoch 43, iter 0, loss 0.05564339458942413\n",
      "epoch 43, iter 1, loss 0.22616855800151825\n",
      "epoch 43, loss 0.2818119525909424\n",
      "epoch 44, iter 0, loss 0.15396742522716522\n",
      "epoch 44, iter 1, loss 0.058539018034935\n",
      "epoch 44, loss 0.21250644326210022\n",
      "epoch 45, iter 0, loss 0.1593332290649414\n",
      "epoch 45, iter 1, loss 0.04754050076007843\n",
      "epoch 45, loss 0.20687372982501984\n",
      "epoch 46, iter 0, loss 0.07561075687408447\n",
      "epoch 46, iter 1, loss 0.18892987072467804\n",
      "epoch 46, loss 0.2645406275987625\n",
      "epoch 47, iter 0, loss 0.1372724324464798\n",
      "epoch 47, iter 1, loss 0.06823804974555969\n",
      "epoch 47, loss 0.2055104821920395\n",
      "epoch 48, iter 0, loss 0.09405311942100525\n",
      "epoch 48, iter 1, loss 0.2008524388074875\n",
      "epoch 48, loss 0.29490555822849274\n",
      "epoch 49, iter 0, loss 0.17089003324508667\n",
      "epoch 49, iter 1, loss 0.06076064705848694\n",
      "epoch 49, loss 0.2316506803035736\n",
      "epoch 50, iter 0, loss 0.03040221333503723\n",
      "epoch 50, iter 1, loss 0.32130712270736694\n",
      "epoch 50, loss 0.3517093360424042\n",
      "epoch 51, iter 0, loss 0.08282878994941711\n",
      "epoch 51, iter 1, loss 0.1952456384897232\n",
      "epoch 51, loss 0.2780744284391403\n",
      "epoch 52, iter 0, loss 0.1285453587770462\n",
      "epoch 52, iter 1, loss 0.11102545261383057\n",
      "epoch 52, loss 0.23957081139087677\n",
      "epoch 53, iter 0, loss 0.03935118019580841\n",
      "epoch 53, iter 1, loss 0.2423204928636551\n",
      "epoch 53, loss 0.2816716730594635\n",
      "epoch 54, iter 0, loss 0.13081127405166626\n",
      "epoch 54, iter 1, loss 0.053028225898742676\n",
      "epoch 54, loss 0.18383949995040894\n",
      "epoch 55, iter 0, loss 0.11436694860458374\n",
      "epoch 55, iter 1, loss 0.03166036307811737\n",
      "epoch 55, loss 0.1460273116827011\n",
      "epoch 56, iter 0, loss 0.11697984486818314\n",
      "epoch 56, iter 1, loss 0.03697782754898071\n",
      "epoch 56, loss 0.15395767241716385\n",
      "epoch 57, iter 0, loss 0.16886673867702484\n",
      "epoch 57, iter 1, loss -0.08998382091522217\n",
      "epoch 57, loss 0.07888291776180267\n",
      "epoch 58, iter 0, loss -0.006676822900772095\n",
      "epoch 58, iter 1, loss 0.20251934230327606\n",
      "epoch 58, loss 0.19584251940250397\n",
      "epoch 59, iter 0, loss 0.04920284450054169\n",
      "epoch 59, iter 1, loss 0.07816441357135773\n",
      "epoch 59, loss 0.12736725807189941\n",
      "epoch 60, iter 0, loss -0.03130874037742615\n",
      "epoch 60, iter 1, loss 0.22081051766872406\n",
      "epoch 60, loss 0.1895017772912979\n",
      "epoch 61, iter 0, loss 0.08059114217758179\n",
      "epoch 61, iter 1, loss 0.03734244406223297\n",
      "epoch 61, loss 0.11793358623981476\n",
      "epoch 62, iter 0, loss 0.08580656349658966\n",
      "epoch 62, iter 1, loss 0.03537467122077942\n",
      "epoch 62, loss 0.12118123471736908\n",
      "epoch 63, iter 0, loss 0.07417649030685425\n",
      "epoch 63, iter 1, loss 0.08006827533245087\n",
      "epoch 63, loss 0.15424476563930511\n",
      "epoch 64, iter 0, loss 0.1096343845129013\n",
      "epoch 64, iter 1, loss 0.01834246516227722\n",
      "epoch 64, loss 0.12797684967517853\n",
      "epoch 65, iter 0, loss 0.10187174379825592\n",
      "epoch 65, iter 1, loss 0.05389644205570221\n",
      "epoch 65, loss 0.15576818585395813\n",
      "epoch 66, iter 0, loss 0.14975102245807648\n",
      "epoch 66, iter 1, loss -0.01970323920249939\n",
      "epoch 66, loss 0.1300477832555771\n",
      "epoch 67, iter 0, loss 0.14259979128837585\n",
      "epoch 67, iter 1, loss 0.015695303678512573\n",
      "epoch 67, loss 0.15829509496688843\n",
      "epoch 68, iter 0, loss 0.1170520931482315\n",
      "epoch 68, iter 1, loss 0.0010387301445007324\n",
      "epoch 68, loss 0.11809082329273224\n",
      "epoch 69, iter 0, loss 0.135747030377388\n",
      "epoch 69, iter 1, loss -0.00894811749458313\n",
      "epoch 69, loss 0.12679891288280487\n",
      "epoch 70, iter 0, loss 0.09599260985851288\n",
      "epoch 70, iter 1, loss 0.02079354226589203\n",
      "epoch 70, loss 0.11678615212440491\n",
      "epoch 71, iter 0, loss 0.12007570266723633\n",
      "epoch 71, iter 1, loss -0.030980601906776428\n",
      "epoch 71, loss 0.0890951007604599\n",
      "epoch 72, iter 0, loss 0.00010541081428527832\n",
      "epoch 72, iter 1, loss 0.16308197379112244\n",
      "epoch 72, loss 0.16318738460540771\n",
      "epoch 73, iter 0, loss 0.0983676016330719\n",
      "epoch 73, iter 1, loss -0.009554430842399597\n",
      "epoch 73, loss 0.0888131707906723\n",
      "epoch 74, iter 0, loss 0.1100711077451706\n",
      "epoch 74, iter 1, loss -0.03641107678413391\n",
      "epoch 74, loss 0.07366003096103668\n",
      "epoch 75, iter 0, loss 0.13135239481925964\n",
      "epoch 75, iter 1, loss -0.06761100888252258\n",
      "epoch 75, loss 0.06374138593673706\n",
      "epoch 76, iter 0, loss 0.012519985437393188\n",
      "epoch 76, iter 1, loss 0.11311443150043488\n",
      "epoch 76, loss 0.12563441693782806\n",
      "epoch 77, iter 0, loss 0.1267944872379303\n",
      "epoch 77, iter 1, loss -0.039805322885513306\n",
      "epoch 77, loss 0.08698916435241699\n",
      "epoch 78, iter 0, loss 0.005284547805786133\n",
      "epoch 78, iter 1, loss 0.1433400809764862\n",
      "epoch 78, loss 0.14862462878227234\n",
      "epoch 79, iter 0, loss -0.028124824166297913\n",
      "epoch 79, iter 1, loss 0.1539791226387024\n",
      "epoch 79, loss 0.12585429847240448\n",
      "epoch 80, iter 0, loss -0.07485982775688171\n",
      "epoch 80, iter 1, loss 0.2609308063983917\n",
      "epoch 80, loss 0.18607097864151\n",
      "epoch 81, iter 0, loss 0.08101494610309601\n",
      "epoch 81, iter 1, loss 0.006513833999633789\n",
      "epoch 81, loss 0.0875287801027298\n",
      "epoch 82, iter 0, loss 0.07153269648551941\n",
      "epoch 82, iter 1, loss 0.062233105301856995\n",
      "epoch 82, loss 0.1337658017873764\n",
      "epoch 83, iter 0, loss 0.0443776398897171\n",
      "epoch 83, iter 1, loss 0.08721476793289185\n",
      "epoch 83, loss 0.13159240782260895\n",
      "epoch 84, iter 0, loss 0.08026985824108124\n",
      "epoch 84, iter 1, loss 0.04106388986110687\n",
      "epoch 84, loss 0.12133374810218811\n",
      "epoch 85, iter 0, loss -0.0056369006633758545\n",
      "epoch 85, iter 1, loss 0.1634143590927124\n",
      "epoch 85, loss 0.15777745842933655\n",
      "epoch 86, iter 0, loss 0.010337993502616882\n",
      "epoch 86, iter 1, loss 0.15587541460990906\n",
      "epoch 86, loss 0.16621340811252594\n",
      "epoch 87, iter 0, loss 0.05019427835941315\n",
      "epoch 87, iter 1, loss 0.07004258036613464\n",
      "epoch 87, loss 0.12023685872554779\n",
      "epoch 88, iter 0, loss 0.08236154913902283\n",
      "epoch 88, iter 1, loss 0.03316129744052887\n",
      "epoch 88, loss 0.1155228465795517\n",
      "epoch 89, iter 0, loss 0.04059047996997833\n",
      "epoch 89, iter 1, loss 0.062317460775375366\n",
      "epoch 89, loss 0.1029079407453537\n",
      "epoch 90, iter 0, loss 0.025807127356529236\n",
      "epoch 90, iter 1, loss 0.10873690247535706\n",
      "epoch 90, loss 0.1345440298318863\n",
      "epoch 91, iter 0, loss 0.08828407526016235\n",
      "epoch 91, iter 1, loss -0.025026574730873108\n",
      "epoch 91, loss 0.06325750052928925\n",
      "epoch 92, iter 0, loss -0.033528491854667664\n",
      "epoch 92, iter 1, loss 0.15550237894058228\n",
      "epoch 92, loss 0.12197388708591461\n",
      "epoch 93, iter 0, loss 0.09662216901779175\n",
      "epoch 93, iter 1, loss -0.09020985662937164\n",
      "epoch 93, loss 0.006412312388420105\n",
      "epoch 94, iter 0, loss 0.09877108037471771\n",
      "epoch 94, iter 1, loss -0.07525797188282013\n",
      "epoch 94, loss 0.023513108491897583\n",
      "epoch 95, iter 0, loss -0.0812978446483612\n",
      "epoch 95, iter 1, loss 0.1682911217212677\n",
      "epoch 95, loss 0.0869932770729065\n",
      "epoch 96, iter 0, loss -0.08745415508747101\n",
      "epoch 96, iter 1, loss 0.14779365062713623\n",
      "epoch 96, loss 0.06033949553966522\n",
      "epoch 97, iter 0, loss -0.10454049706459045\n",
      "epoch 97, iter 1, loss 0.1970793604850769\n",
      "epoch 97, loss 0.09253886342048645\n",
      "epoch 98, iter 0, loss -0.09095612168312073\n",
      "epoch 98, iter 1, loss 0.20467373728752136\n",
      "epoch 98, loss 0.11371761560440063\n",
      "epoch 99, iter 0, loss 0.047503143548965454\n",
      "epoch 99, iter 1, loss -0.027601957321166992\n",
      "epoch 99, loss 0.019901186227798462\n",
      "2019-03-30 23:21:13.467187, fold=2, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 2, 'repeat': 1, 'n': 2565, 'd': 126, 'mse': 0.011547986418008804, 'train_time': 361.91783946892247, 'trained_epochs': 100, 'prior_train_nmll': 0.03196151554584503, 'train_nll': -602.8045654296875, 'test_nll': -397.05438232421875, 'train_mse': 0.02216755971312523, 'state_dict_file': 'model_state_dict_-3429308961514514007.pkl'}\n",
      "Begin dataset  skillcraft\n",
      "Begin with SVI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, iter 0, loss 1.9991238117218018\n",
      "epoch 0, iter 1, loss 2.19984769821167\n",
      "epoch 0, iter 2, loss 2.3414413928985596\n",
      "epoch 0, loss 6.540412902832031\n",
      "epoch 1, iter 0, loss 2.135169267654419\n",
      "epoch 1, iter 1, loss 2.0810604095458984\n",
      "epoch 1, iter 2, loss 1.9389878511428833\n",
      "epoch 1, loss 6.155217528343201\n",
      "epoch 2, iter 0, loss 1.9199280738830566\n",
      "epoch 2, iter 1, loss 1.8237265348434448\n",
      "epoch 2, iter 2, loss 1.8290338516235352\n",
      "epoch 2, loss 5.572688460350037\n",
      "epoch 3, iter 0, loss 1.7458943128585815\n",
      "epoch 3, iter 1, loss 1.686457872390747\n",
      "epoch 3, iter 2, loss 1.6164333820343018\n",
      "epoch 3, loss 5.04878556728363\n",
      "epoch 4, iter 0, loss 1.6709017753601074\n",
      "epoch 4, iter 1, loss 1.6263644695281982\n",
      "epoch 4, iter 2, loss 1.630469560623169\n",
      "epoch 4, loss 4.927735805511475\n",
      "epoch 5, iter 0, loss 1.5871659517288208\n",
      "epoch 5, iter 1, loss 1.5859638452529907\n",
      "epoch 5, iter 2, loss 1.600774884223938\n",
      "epoch 5, loss 4.7739046812057495\n",
      "epoch 6, iter 0, loss 1.5450571775436401\n",
      "epoch 6, iter 1, loss 1.5414525270462036\n",
      "epoch 6, iter 2, loss 1.4897629022598267\n",
      "epoch 6, loss 4.57627260684967\n",
      "epoch 7, iter 0, loss 1.5140082836151123\n",
      "epoch 7, iter 1, loss 1.4936431646347046\n",
      "epoch 7, iter 2, loss 1.4629087448120117\n",
      "epoch 7, loss 4.470560193061829\n",
      "epoch 8, iter 0, loss 1.4733211994171143\n",
      "epoch 8, iter 1, loss 1.457137107849121\n",
      "epoch 8, iter 2, loss 1.428533673286438\n",
      "epoch 8, loss 4.358991980552673\n",
      "epoch 9, iter 0, loss 1.442800760269165\n",
      "epoch 9, iter 1, loss 1.4093183279037476\n",
      "epoch 9, iter 2, loss 1.4021881818771362\n",
      "epoch 9, loss 4.254307270050049\n",
      "epoch 10, iter 0, loss 1.3870590925216675\n",
      "epoch 10, iter 1, loss 1.3851803541183472\n",
      "epoch 10, iter 2, loss 1.366722583770752\n",
      "epoch 10, loss 4.138962030410767\n",
      "epoch 11, iter 0, loss 1.3545976877212524\n",
      "epoch 11, iter 1, loss 1.3374766111373901\n",
      "epoch 11, iter 2, loss 1.3180413246154785\n",
      "epoch 11, loss 4.010115623474121\n",
      "epoch 12, iter 0, loss 1.3087836503982544\n",
      "epoch 12, iter 1, loss 1.3085037469863892\n",
      "epoch 12, iter 2, loss 1.2570242881774902\n",
      "epoch 12, loss 3.874311685562134\n",
      "epoch 13, iter 0, loss 1.283284068107605\n",
      "epoch 13, iter 1, loss 1.2652952671051025\n",
      "epoch 13, iter 2, loss 1.2474932670593262\n",
      "epoch 13, loss 3.7960726022720337\n",
      "epoch 14, iter 0, loss 1.240463376045227\n",
      "epoch 14, iter 1, loss 1.232740879058838\n",
      "epoch 14, iter 2, loss 1.220241904258728\n",
      "epoch 14, loss 3.693446159362793\n",
      "epoch 15, iter 0, loss 1.212101936340332\n",
      "epoch 15, iter 1, loss 1.2180876731872559\n",
      "epoch 15, iter 2, loss 1.2452447414398193\n",
      "epoch 15, loss 3.6754343509674072\n",
      "epoch 16, iter 0, loss 1.2111103534698486\n",
      "epoch 16, iter 1, loss 1.1951653957366943\n",
      "epoch 16, iter 2, loss 1.1866122484207153\n",
      "epoch 16, loss 3.5928879976272583\n",
      "epoch 17, iter 0, loss 1.1821492910385132\n",
      "epoch 17, iter 1, loss 1.2221825122833252\n",
      "epoch 17, iter 2, loss 1.1196023225784302\n",
      "epoch 17, loss 3.5239341259002686\n",
      "epoch 18, iter 0, loss 1.2011791467666626\n",
      "epoch 18, iter 1, loss 1.183142066001892\n",
      "epoch 18, iter 2, loss 1.2171324491500854\n",
      "epoch 18, loss 3.60145366191864\n",
      "epoch 19, iter 0, loss 1.1989872455596924\n",
      "epoch 19, iter 1, loss 1.1987038850784302\n",
      "epoch 19, iter 2, loss 1.117141604423523\n",
      "epoch 19, loss 3.5148327350616455\n",
      "epoch 20, iter 0, loss 1.2138426303863525\n",
      "epoch 20, iter 1, loss 1.1658493280410767\n",
      "epoch 20, iter 2, loss 1.2894935607910156\n",
      "epoch 20, loss 3.669185519218445\n",
      "epoch 21, iter 0, loss 1.1859679222106934\n",
      "epoch 21, iter 1, loss 1.1808698177337646\n",
      "epoch 21, iter 2, loss 1.2835361957550049\n",
      "epoch 21, loss 3.650373935699463\n",
      "epoch 22, iter 0, loss 1.2034850120544434\n",
      "epoch 22, iter 1, loss 1.1941556930541992\n",
      "epoch 22, iter 2, loss 1.0785760879516602\n",
      "epoch 22, loss 3.4762167930603027\n",
      "epoch 23, iter 0, loss 1.17625892162323\n",
      "epoch 23, iter 1, loss 1.1973081827163696\n",
      "epoch 23, iter 2, loss 1.1287846565246582\n",
      "epoch 23, loss 3.502351760864258\n",
      "epoch 24, iter 0, loss 1.1847093105316162\n",
      "epoch 24, iter 1, loss 1.1560052633285522\n",
      "epoch 24, iter 2, loss 1.1265534162521362\n",
      "epoch 24, loss 3.4672679901123047\n",
      "epoch 25, iter 0, loss 1.1599005460739136\n",
      "epoch 25, iter 1, loss 1.1560200452804565\n",
      "epoch 25, iter 2, loss 1.2375092506408691\n",
      "epoch 25, loss 3.5534298419952393\n",
      "epoch 26, iter 0, loss 1.1463818550109863\n",
      "epoch 26, iter 1, loss 1.1593925952911377\n",
      "epoch 26, iter 2, loss 1.2006862163543701\n",
      "epoch 26, loss 3.506460666656494\n",
      "epoch 27, iter 0, loss 1.1247432231903076\n",
      "epoch 27, iter 1, loss 1.1838459968566895\n",
      "epoch 27, iter 2, loss 1.16159987449646\n",
      "epoch 27, loss 3.470189094543457\n",
      "epoch 28, iter 0, loss 1.1717355251312256\n",
      "epoch 28, iter 1, loss 1.1370103359222412\n",
      "epoch 28, iter 2, loss 1.127742886543274\n",
      "epoch 28, loss 3.4364887475967407\n",
      "epoch 29, iter 0, loss 1.1376057863235474\n",
      "epoch 29, iter 1, loss 1.1474418640136719\n",
      "epoch 29, iter 2, loss 1.1334620714187622\n",
      "epoch 29, loss 3.4185097217559814\n",
      "epoch 30, iter 0, loss 1.1211923360824585\n",
      "epoch 30, iter 1, loss 1.1544076204299927\n",
      "epoch 30, iter 2, loss 1.1523841619491577\n",
      "epoch 30, loss 3.427984118461609\n",
      "epoch 31, iter 0, loss 1.1495234966278076\n",
      "epoch 31, iter 1, loss 1.1304783821105957\n",
      "epoch 31, iter 2, loss 1.0948152542114258\n",
      "epoch 31, loss 3.374817132949829\n",
      "epoch 32, iter 0, loss 1.1328281164169312\n",
      "epoch 32, iter 1, loss 1.134606957435608\n",
      "epoch 32, iter 2, loss 1.153632402420044\n",
      "epoch 32, loss 3.421067476272583\n",
      "epoch 33, iter 0, loss 1.1575281620025635\n",
      "epoch 33, iter 1, loss 1.1154754161834717\n",
      "epoch 33, iter 2, loss 1.1232101917266846\n",
      "epoch 33, loss 3.3962137699127197\n",
      "epoch 34, iter 0, loss 1.1462386846542358\n",
      "epoch 34, iter 1, loss 1.1534481048583984\n",
      "epoch 34, iter 2, loss 1.0722001791000366\n",
      "epoch 34, loss 3.371886968612671\n",
      "epoch 35, iter 0, loss 1.1312819719314575\n",
      "epoch 35, iter 1, loss 1.14055597782135\n",
      "epoch 35, iter 2, loss 1.1761142015457153\n",
      "epoch 35, loss 3.447952151298523\n",
      "epoch 36, iter 0, loss 1.1774102449417114\n",
      "epoch 36, iter 1, loss 1.0967209339141846\n",
      "epoch 36, iter 2, loss 1.126343846321106\n",
      "epoch 36, loss 3.400475025177002\n",
      "epoch 37, iter 0, loss 1.1336818933486938\n",
      "epoch 37, iter 1, loss 1.1345710754394531\n",
      "epoch 37, iter 2, loss 1.2167588472366333\n",
      "epoch 37, loss 3.4850118160247803\n",
      "epoch 38, iter 0, loss 1.1294149160385132\n",
      "epoch 38, iter 1, loss 1.1431028842926025\n",
      "epoch 38, iter 2, loss 1.1819878816604614\n",
      "epoch 38, loss 3.454505681991577\n",
      "epoch 39, iter 0, loss 1.134811520576477\n",
      "epoch 39, iter 1, loss 1.1334296464920044\n",
      "epoch 39, iter 2, loss 1.1647769212722778\n",
      "epoch 39, loss 3.4330180883407593\n",
      "epoch 40, iter 0, loss 1.1351580619812012\n",
      "epoch 40, iter 1, loss 1.138216257095337\n",
      "epoch 40, iter 2, loss 1.1425751447677612\n",
      "epoch 40, loss 3.4159494638442993\n",
      "epoch 41, iter 0, loss 1.1079754829406738\n",
      "epoch 41, iter 1, loss 1.1528712511062622\n",
      "epoch 41, iter 2, loss 1.1116074323654175\n",
      "epoch 41, loss 3.3724541664123535\n",
      "epoch 42, iter 0, loss 1.1355254650115967\n",
      "epoch 42, iter 1, loss 1.1304620504379272\n",
      "epoch 42, iter 2, loss 1.089927077293396\n",
      "epoch 42, loss 3.35591459274292\n",
      "epoch 43, iter 0, loss 1.1223760843276978\n",
      "epoch 43, iter 1, loss 1.1209101676940918\n",
      "epoch 43, iter 2, loss 1.1097146272659302\n",
      "epoch 43, loss 3.3530008792877197\n",
      "epoch 44, iter 0, loss 1.1101034879684448\n",
      "epoch 44, iter 1, loss 1.117201328277588\n",
      "epoch 44, iter 2, loss 1.1065154075622559\n",
      "epoch 44, loss 3.3338202238082886\n",
      "epoch 45, iter 0, loss 1.130286693572998\n",
      "epoch 45, iter 1, loss 1.130083441734314\n",
      "epoch 45, iter 2, loss 1.0303128957748413\n",
      "epoch 45, loss 3.2906830310821533\n",
      "epoch 46, iter 0, loss 1.1233655214309692\n",
      "epoch 46, iter 1, loss 1.115548014640808\n",
      "epoch 46, iter 2, loss 1.103342890739441\n",
      "epoch 46, loss 3.3422564268112183\n",
      "epoch 47, iter 0, loss 1.1164497137069702\n",
      "epoch 47, iter 1, loss 1.1174808740615845\n",
      "epoch 47, iter 2, loss 1.0856441259384155\n",
      "epoch 47, loss 3.31957471370697\n",
      "epoch 48, iter 0, loss 1.1020402908325195\n",
      "epoch 48, iter 1, loss 1.1332937479019165\n",
      "epoch 48, iter 2, loss 1.1204791069030762\n",
      "epoch 48, loss 3.355813145637512\n",
      "epoch 49, iter 0, loss 1.1307417154312134\n",
      "epoch 49, iter 1, loss 1.0978457927703857\n",
      "epoch 49, iter 2, loss 1.1078805923461914\n",
      "epoch 49, loss 3.3364681005477905\n",
      "epoch 50, iter 0, loss 1.1343830823898315\n",
      "epoch 50, iter 1, loss 1.0872818231582642\n",
      "epoch 50, iter 2, loss 1.1435693502426147\n",
      "epoch 50, loss 3.3652342557907104\n",
      "epoch 51, iter 0, loss 1.098481297492981\n",
      "epoch 51, iter 1, loss 1.123723030090332\n",
      "epoch 51, iter 2, loss 1.0360522270202637\n",
      "epoch 51, loss 3.2582565546035767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52, iter 0, loss 1.1159864664077759\n",
      "epoch 52, iter 1, loss 1.1159629821777344\n",
      "epoch 52, iter 2, loss 1.042317509651184\n",
      "epoch 52, loss 3.2742669582366943\n",
      "epoch 53, iter 0, loss 1.1343969106674194\n",
      "epoch 53, iter 1, loss 1.0872055292129517\n",
      "epoch 53, iter 2, loss 1.0006600618362427\n",
      "epoch 53, loss 3.2222625017166138\n",
      "epoch 54, iter 0, loss 1.095660924911499\n",
      "epoch 54, iter 1, loss 1.0974035263061523\n",
      "epoch 54, iter 2, loss 1.2063632011413574\n",
      "epoch 54, loss 3.399427652359009\n",
      "epoch 55, iter 0, loss 1.1051989793777466\n",
      "epoch 55, iter 1, loss 1.1142257452011108\n",
      "epoch 55, iter 2, loss 1.0451868772506714\n",
      "epoch 55, loss 3.264611601829529\n",
      "epoch 56, iter 0, loss 1.0920556783676147\n",
      "epoch 56, iter 1, loss 1.1198749542236328\n",
      "epoch 56, iter 2, loss 1.0731667280197144\n",
      "epoch 56, loss 3.285097360610962\n",
      "epoch 57, iter 0, loss 1.1126796007156372\n",
      "epoch 57, iter 1, loss 1.1019117832183838\n",
      "epoch 57, iter 2, loss 1.057226538658142\n",
      "epoch 57, loss 3.271817922592163\n",
      "epoch 58, iter 0, loss 1.0994821786880493\n",
      "epoch 58, iter 1, loss 1.0962467193603516\n",
      "epoch 58, iter 2, loss 1.1354833841323853\n",
      "epoch 58, loss 3.331212282180786\n",
      "epoch 59, iter 0, loss 1.118472933769226\n",
      "epoch 59, iter 1, loss 1.0753121376037598\n",
      "epoch 59, iter 2, loss 1.125939965248108\n",
      "epoch 59, loss 3.3197250366210938\n",
      "epoch 60, iter 0, loss 1.0913671255111694\n",
      "epoch 60, iter 1, loss 1.108933687210083\n",
      "epoch 60, iter 2, loss 1.185522198677063\n",
      "epoch 60, loss 3.3858230113983154\n",
      "epoch 61, iter 0, loss 1.1220463514328003\n",
      "epoch 61, iter 1, loss 1.1003895998001099\n",
      "epoch 61, iter 2, loss 1.0614519119262695\n",
      "epoch 61, loss 3.2838878631591797\n",
      "epoch 62, iter 0, loss 1.0998390913009644\n",
      "epoch 62, iter 1, loss 1.0903096199035645\n",
      "epoch 62, iter 2, loss 1.123853325843811\n",
      "epoch 62, loss 3.31400203704834\n",
      "epoch 63, iter 0, loss 1.0973399877548218\n",
      "epoch 63, iter 1, loss 1.098163366317749\n",
      "epoch 63, iter 2, loss 1.0656805038452148\n",
      "epoch 63, loss 3.2611838579177856\n",
      "epoch 64, iter 0, loss 1.1205350160598755\n",
      "epoch 64, iter 1, loss 1.0763235092163086\n",
      "epoch 64, iter 2, loss 1.0749170780181885\n",
      "epoch 64, loss 3.2717756032943726\n",
      "epoch 65, iter 0, loss 1.13297438621521\n",
      "epoch 65, iter 1, loss 1.0677595138549805\n",
      "epoch 65, iter 2, loss 1.0064102411270142\n",
      "epoch 65, loss 3.2071441411972046\n",
      "epoch 66, iter 0, loss 1.129363775253296\n",
      "epoch 66, iter 1, loss 1.0783978700637817\n",
      "epoch 66, iter 2, loss 0.9798527359962463\n",
      "epoch 66, loss 3.187614381313324\n",
      "epoch 67, iter 0, loss 1.097444772720337\n",
      "epoch 67, iter 1, loss 1.0919736623764038\n",
      "epoch 67, iter 2, loss 1.0510274171829224\n",
      "epoch 67, loss 3.240445852279663\n",
      "epoch 68, iter 0, loss 1.0924952030181885\n",
      "epoch 68, iter 1, loss 1.107507348060608\n",
      "epoch 68, iter 2, loss 1.1311293840408325\n",
      "epoch 68, loss 3.331131935119629\n",
      "epoch 69, iter 0, loss 1.1117420196533203\n",
      "epoch 69, iter 1, loss 1.0829812288284302\n",
      "epoch 69, iter 2, loss 1.1187983751296997\n",
      "epoch 69, loss 3.31352162361145\n",
      "epoch 70, iter 0, loss 1.078963279724121\n",
      "epoch 70, iter 1, loss 1.1101547479629517\n",
      "epoch 70, iter 2, loss 1.1600161790847778\n",
      "epoch 70, loss 3.3491342067718506\n",
      "epoch 71, iter 0, loss 1.0884464979171753\n",
      "epoch 71, iter 1, loss 1.0986275672912598\n",
      "epoch 71, iter 2, loss 1.2458425760269165\n",
      "epoch 71, loss 3.4329166412353516\n",
      "epoch 72, iter 0, loss 1.091353178024292\n",
      "epoch 72, iter 1, loss 1.1025240421295166\n",
      "epoch 72, iter 2, loss 1.0825941562652588\n",
      "epoch 72, loss 3.2764713764190674\n",
      "epoch 73, iter 0, loss 1.0818119049072266\n",
      "epoch 73, iter 1, loss 1.1069904565811157\n",
      "epoch 73, iter 2, loss 1.0913262367248535\n",
      "epoch 73, loss 3.280128598213196\n",
      "epoch 74, iter 0, loss 1.088747262954712\n",
      "epoch 74, iter 1, loss 1.0912370681762695\n",
      "epoch 74, iter 2, loss 1.1024060249328613\n",
      "epoch 74, loss 3.2823903560638428\n",
      "epoch 75, iter 0, loss 1.1021277904510498\n",
      "epoch 75, iter 1, loss 1.0815168619155884\n",
      "epoch 75, iter 2, loss 1.1128982305526733\n",
      "epoch 75, loss 3.2965428829193115\n",
      "epoch 76, iter 0, loss 1.0975315570831299\n",
      "epoch 76, iter 1, loss 1.0977239608764648\n",
      "epoch 76, iter 2, loss 1.0495609045028687\n",
      "epoch 76, loss 3.2448164224624634\n",
      "epoch 77, iter 0, loss 1.082409381866455\n",
      "epoch 77, iter 1, loss 1.0873862504959106\n",
      "epoch 77, iter 2, loss 1.1648977994918823\n",
      "epoch 77, loss 3.334693431854248\n",
      "epoch 78, iter 0, loss 1.087959885597229\n",
      "epoch 78, iter 1, loss 1.0893312692642212\n",
      "epoch 78, iter 2, loss 1.0880889892578125\n",
      "epoch 78, loss 3.2653801441192627\n",
      "epoch 79, iter 0, loss 1.0957471132278442\n",
      "epoch 79, iter 1, loss 1.0997443199157715\n",
      "epoch 79, iter 2, loss 1.049758791923523\n",
      "epoch 79, loss 3.2452502250671387\n",
      "epoch 80, iter 0, loss 1.1202915906906128\n",
      "epoch 80, iter 1, loss 1.0586684942245483\n",
      "epoch 80, iter 2, loss 1.0642982721328735\n",
      "epoch 80, loss 3.2432583570480347\n",
      "epoch 81, iter 0, loss 1.058045506477356\n",
      "epoch 81, iter 1, loss 1.1113592386245728\n",
      "epoch 81, iter 2, loss 1.0780102014541626\n",
      "epoch 81, loss 3.2474149465560913\n",
      "epoch 82, iter 0, loss 1.0833547115325928\n",
      "epoch 82, iter 1, loss 1.08454430103302\n",
      "epoch 82, iter 2, loss 1.1432106494903564\n",
      "epoch 82, loss 3.3111096620559692\n",
      "epoch 83, iter 0, loss 1.1124882698059082\n",
      "epoch 83, iter 1, loss 1.0639934539794922\n",
      "epoch 83, iter 2, loss 1.0728899240493774\n",
      "epoch 83, loss 3.249371647834778\n",
      "epoch 84, iter 0, loss 1.0804270505905151\n",
      "epoch 84, iter 1, loss 1.1047674417495728\n",
      "epoch 84, iter 2, loss 1.065507173538208\n",
      "epoch 84, loss 3.250701665878296\n",
      "epoch 85, iter 0, loss 1.0848346948623657\n",
      "epoch 85, iter 1, loss 1.0815322399139404\n",
      "epoch 85, iter 2, loss 1.102526068687439\n",
      "epoch 85, loss 3.268893003463745\n",
      "epoch 86, iter 0, loss 1.0979317426681519\n",
      "epoch 86, iter 1, loss 1.081602931022644\n",
      "epoch 86, iter 2, loss 1.0590214729309082\n",
      "epoch 86, loss 3.238556146621704\n",
      "epoch 87, iter 0, loss 1.0739322900772095\n",
      "epoch 87, iter 1, loss 1.1001640558242798\n",
      "epoch 87, iter 2, loss 0.9955909848213196\n",
      "epoch 87, loss 3.169687330722809\n",
      "epoch 88, iter 0, loss 1.0954288244247437\n",
      "epoch 88, iter 1, loss 1.0874367952346802\n",
      "epoch 88, iter 2, loss 1.0441827774047852\n",
      "epoch 88, loss 3.227048397064209\n",
      "epoch 89, iter 0, loss 1.1036380529403687\n",
      "epoch 89, iter 1, loss 1.0753870010375977\n",
      "epoch 89, iter 2, loss 1.0844087600708008\n",
      "epoch 89, loss 3.263433814048767\n",
      "epoch 90, iter 0, loss 1.069056749343872\n",
      "epoch 90, iter 1, loss 1.09248948097229\n",
      "epoch 90, iter 2, loss 1.0801295042037964\n",
      "epoch 90, loss 3.2416757345199585\n",
      "epoch 91, iter 0, loss 1.0676323175430298\n",
      "epoch 91, iter 1, loss 1.0827189683914185\n",
      "epoch 91, iter 2, loss 1.1682953834533691\n",
      "epoch 91, loss 3.3186466693878174\n",
      "epoch 92, iter 0, loss 1.0674693584442139\n",
      "epoch 92, iter 1, loss 1.100426197052002\n",
      "epoch 92, iter 2, loss 1.0400768518447876\n",
      "epoch 92, loss 3.2079724073410034\n",
      "epoch 93, iter 0, loss 1.0893677473068237\n",
      "epoch 93, iter 1, loss 1.087881088256836\n",
      "epoch 93, iter 2, loss 1.0539970397949219\n",
      "epoch 93, loss 3.2312458753585815\n",
      "epoch 94, iter 0, loss 1.077291488647461\n",
      "epoch 94, iter 1, loss 1.093454122543335\n",
      "epoch 94, iter 2, loss 1.0725845098495483\n",
      "epoch 94, loss 3.2433301210403442\n",
      "epoch 95, iter 0, loss 1.0669269561767578\n",
      "epoch 95, iter 1, loss 1.08882474899292\n",
      "epoch 95, iter 2, loss 1.098846435546875\n",
      "epoch 95, loss 3.2545981407165527\n",
      "epoch 96, iter 0, loss 1.0660055875778198\n",
      "epoch 96, iter 1, loss 1.0849257707595825\n",
      "epoch 96, iter 2, loss 1.145741581916809\n",
      "epoch 96, loss 3.2966729402542114\n",
      "epoch 97, iter 0, loss 1.044597864151001\n",
      "epoch 97, iter 1, loss 1.109920620918274\n",
      "epoch 97, iter 2, loss 1.0684071779251099\n",
      "epoch 97, loss 3.2229256629943848\n",
      "epoch 98, iter 0, loss 1.0635218620300293\n",
      "epoch 98, iter 1, loss 1.0873241424560547\n",
      "epoch 98, iter 2, loss 1.1407363414764404\n",
      "epoch 98, loss 3.2915823459625244\n",
      "epoch 99, iter 0, loss 1.078955054283142\n",
      "epoch 99, iter 1, loss 1.0775896310806274\n",
      "epoch 99, iter 2, loss 1.0391889810562134\n",
      "epoch 99, loss 3.195733666419983\n",
      "2019-03-30 23:29:55.816451, fold=0, rep=0, eta=0d 0h 43m 31s \n",
      "{'fold': 0, 'repeat': 0, 'n': 3338, 'd': 17, 'mse': 0.4421810209751129, 'train_time': 521.106818415923, 'trained_epochs': 100, 'prior_train_nmll': 1.0691723823547363, 'train_nll': 2121.90576171875, 'test_nll': 1119.6474609375, 'train_mse': 0.3801526427268982, 'state_dict_file': 'model_state_dict_-1758446708212598960.pkl'}\n",
      "epoch 0, iter 0, loss 1.9179184436798096\n",
      "epoch 0, iter 1, loss 2.2889771461486816\n",
      "epoch 0, iter 2, loss 2.2146623134613037\n",
      "epoch 0, loss 6.421557903289795\n",
      "epoch 1, iter 0, loss 2.1066782474517822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, iter 1, loss 2.087813377380371\n",
      "epoch 1, iter 2, loss 2.005000352859497\n",
      "epoch 1, loss 6.19949197769165\n",
      "epoch 2, iter 0, loss 1.890445590019226\n",
      "epoch 2, iter 1, loss 1.8104219436645508\n",
      "epoch 2, iter 2, loss 1.805678367614746\n",
      "epoch 2, loss 5.506545901298523\n",
      "epoch 3, iter 0, loss 1.7661099433898926\n",
      "epoch 3, iter 1, loss 1.6544294357299805\n",
      "epoch 3, iter 2, loss 1.661909580230713\n",
      "epoch 3, loss 5.082448959350586\n",
      "epoch 4, iter 0, loss 1.6680407524108887\n",
      "epoch 4, iter 1, loss 1.6225013732910156\n",
      "epoch 4, iter 2, loss 1.6372885704040527\n",
      "epoch 4, loss 4.927830696105957\n",
      "epoch 5, iter 0, loss 1.5850907564163208\n",
      "epoch 5, iter 1, loss 1.5791735649108887\n",
      "epoch 5, iter 2, loss 1.5957032442092896\n",
      "epoch 5, loss 4.759967565536499\n",
      "epoch 6, iter 0, loss 1.543051838874817\n",
      "epoch 6, iter 1, loss 1.5339338779449463\n",
      "epoch 6, iter 2, loss 1.5054417848587036\n",
      "epoch 6, loss 4.582427501678467\n",
      "epoch 7, iter 0, loss 1.498521089553833\n",
      "epoch 7, iter 1, loss 1.5009422302246094\n",
      "epoch 7, iter 2, loss 1.4917919635772705\n",
      "epoch 7, loss 4.491255283355713\n",
      "epoch 8, iter 0, loss 1.4568440914154053\n",
      "epoch 8, iter 1, loss 1.4631755352020264\n",
      "epoch 8, iter 2, loss 1.4378477334976196\n",
      "epoch 8, loss 4.357867360115051\n",
      "epoch 9, iter 0, loss 1.420461654663086\n",
      "epoch 9, iter 1, loss 1.422137975692749\n",
      "epoch 9, iter 2, loss 1.3958957195281982\n",
      "epoch 9, loss 4.238495349884033\n",
      "epoch 10, iter 0, loss 1.3790701627731323\n",
      "epoch 10, iter 1, loss 1.3897511959075928\n",
      "epoch 10, iter 2, loss 1.3413447141647339\n",
      "epoch 10, loss 4.110166072845459\n",
      "epoch 11, iter 0, loss 1.3550465106964111\n",
      "epoch 11, iter 1, loss 1.3296531438827515\n",
      "epoch 11, iter 2, loss 1.3064746856689453\n",
      "epoch 11, loss 3.991174340248108\n",
      "epoch 12, iter 0, loss 1.2934895753860474\n",
      "epoch 12, iter 1, loss 1.2980717420578003\n",
      "epoch 12, iter 2, loss 1.3049204349517822\n",
      "epoch 12, loss 3.89648175239563\n",
      "epoch 13, iter 0, loss 1.2756531238555908\n",
      "epoch 13, iter 1, loss 1.2587660551071167\n",
      "epoch 13, iter 2, loss 1.2196848392486572\n",
      "epoch 13, loss 3.7541040182113647\n",
      "epoch 14, iter 0, loss 1.2294695377349854\n",
      "epoch 14, iter 1, loss 1.2271308898925781\n",
      "epoch 14, iter 2, loss 1.215684175491333\n",
      "epoch 14, loss 3.6722846031188965\n",
      "epoch 15, iter 0, loss 1.203971028327942\n",
      "epoch 15, iter 1, loss 1.222175121307373\n",
      "epoch 15, iter 2, loss 1.2404289245605469\n",
      "epoch 15, loss 3.666575074195862\n",
      "epoch 16, iter 0, loss 1.221649169921875\n",
      "epoch 16, iter 1, loss 1.196761131286621\n",
      "epoch 16, iter 2, loss 1.1576192378997803\n",
      "epoch 16, loss 3.5760295391082764\n",
      "epoch 17, iter 0, loss 1.2142093181610107\n",
      "epoch 17, iter 1, loss 1.1850643157958984\n",
      "epoch 17, iter 2, loss 1.2439608573913574\n",
      "epoch 17, loss 3.6432344913482666\n",
      "epoch 18, iter 0, loss 1.194470763206482\n",
      "epoch 18, iter 1, loss 1.2137298583984375\n",
      "epoch 18, iter 2, loss 1.1155701875686646\n",
      "epoch 18, loss 3.523770809173584\n",
      "epoch 19, iter 0, loss 1.2189841270446777\n",
      "epoch 19, iter 1, loss 1.187187910079956\n",
      "epoch 19, iter 2, loss 1.1243877410888672\n",
      "epoch 19, loss 3.530559778213501\n",
      "epoch 20, iter 0, loss 1.217990517616272\n",
      "epoch 20, iter 1, loss 1.1798076629638672\n",
      "epoch 20, iter 2, loss 1.2172788381576538\n",
      "epoch 20, loss 3.615077018737793\n",
      "epoch 21, iter 0, loss 1.1929911375045776\n",
      "epoch 21, iter 1, loss 1.1983779668807983\n",
      "epoch 21, iter 2, loss 1.162239670753479\n",
      "epoch 21, loss 3.553608775138855\n",
      "epoch 22, iter 0, loss 1.1903712749481201\n",
      "epoch 22, iter 1, loss 1.179537057876587\n",
      "epoch 22, iter 2, loss 1.1934833526611328\n",
      "epoch 22, loss 3.56339168548584\n",
      "epoch 23, iter 0, loss 1.17113196849823\n",
      "epoch 23, iter 1, loss 1.181744933128357\n",
      "epoch 23, iter 2, loss 1.1844382286071777\n",
      "epoch 23, loss 3.5373151302337646\n",
      "epoch 24, iter 0, loss 1.1641504764556885\n",
      "epoch 24, iter 1, loss 1.1789003610610962\n",
      "epoch 24, iter 2, loss 1.2033474445343018\n",
      "epoch 24, loss 3.5463982820510864\n",
      "epoch 25, iter 0, loss 1.1628432273864746\n",
      "epoch 25, iter 1, loss 1.1724029779434204\n",
      "epoch 25, iter 2, loss 1.1162102222442627\n",
      "epoch 25, loss 3.4514564275741577\n",
      "epoch 26, iter 0, loss 1.1749244928359985\n",
      "epoch 26, iter 1, loss 1.1429980993270874\n",
      "epoch 26, iter 2, loss 1.1162809133529663\n",
      "epoch 26, loss 3.4342035055160522\n",
      "epoch 27, iter 0, loss 1.1614863872528076\n",
      "epoch 27, iter 1, loss 1.146655797958374\n",
      "epoch 27, iter 2, loss 1.1015396118164062\n",
      "epoch 27, loss 3.409681797027588\n",
      "epoch 28, iter 0, loss 1.1639716625213623\n",
      "epoch 28, iter 1, loss 1.1131930351257324\n",
      "epoch 28, iter 2, loss 1.1694293022155762\n",
      "epoch 28, loss 3.446593999862671\n",
      "epoch 29, iter 0, loss 1.1288918256759644\n",
      "epoch 29, iter 1, loss 1.1373602151870728\n",
      "epoch 29, iter 2, loss 1.1785526275634766\n",
      "epoch 29, loss 3.4448046684265137\n",
      "epoch 30, iter 0, loss 1.1551357507705688\n",
      "epoch 30, iter 1, loss 1.11001718044281\n",
      "epoch 30, iter 2, loss 1.1656429767608643\n",
      "epoch 30, loss 3.430795907974243\n",
      "epoch 31, iter 0, loss 1.1467390060424805\n",
      "epoch 31, iter 1, loss 1.1349563598632812\n",
      "epoch 31, iter 2, loss 1.111067771911621\n",
      "epoch 31, loss 3.392763137817383\n",
      "epoch 32, iter 0, loss 1.1168776750564575\n",
      "epoch 32, iter 1, loss 1.1493232250213623\n",
      "epoch 32, iter 2, loss 1.1557512283325195\n",
      "epoch 32, loss 3.4219521284103394\n",
      "epoch 33, iter 0, loss 1.118133306503296\n",
      "epoch 33, iter 1, loss 1.1413427591323853\n",
      "epoch 33, iter 2, loss 1.154505968093872\n",
      "epoch 33, loss 3.4139820337295532\n",
      "epoch 34, iter 0, loss 1.1461248397827148\n",
      "epoch 34, iter 1, loss 1.1247750520706177\n",
      "epoch 34, iter 2, loss 1.0881620645523071\n",
      "epoch 34, loss 3.3590619564056396\n",
      "epoch 35, iter 0, loss 1.141976237297058\n",
      "epoch 35, iter 1, loss 1.1276758909225464\n",
      "epoch 35, iter 2, loss 1.1075732707977295\n",
      "epoch 35, loss 3.377225399017334\n",
      "epoch 36, iter 0, loss 1.1334178447723389\n",
      "epoch 36, iter 1, loss 1.1178499460220337\n",
      "epoch 36, iter 2, loss 1.1142542362213135\n",
      "epoch 36, loss 3.365522027015686\n",
      "epoch 37, iter 0, loss 1.0908974409103394\n",
      "epoch 37, iter 1, loss 1.1551544666290283\n",
      "epoch 37, iter 2, loss 1.1503156423568726\n",
      "epoch 37, loss 3.3963675498962402\n",
      "epoch 38, iter 0, loss 1.1530084609985352\n",
      "epoch 38, iter 1, loss 1.0997278690338135\n",
      "epoch 38, iter 2, loss 1.126400351524353\n",
      "epoch 38, loss 3.3791366815567017\n",
      "epoch 39, iter 0, loss 1.1274689435958862\n",
      "epoch 39, iter 1, loss 1.117203950881958\n",
      "epoch 39, iter 2, loss 1.1862268447875977\n",
      "epoch 39, loss 3.430899739265442\n",
      "epoch 40, iter 0, loss 1.130460500717163\n",
      "epoch 40, iter 1, loss 1.1256661415100098\n",
      "epoch 40, iter 2, loss 1.1103854179382324\n",
      "epoch 40, loss 3.3665120601654053\n",
      "epoch 41, iter 0, loss 1.1250109672546387\n",
      "epoch 41, iter 1, loss 1.131118655204773\n",
      "epoch 41, iter 2, loss 1.088748574256897\n",
      "epoch 41, loss 3.3448781967163086\n",
      "epoch 42, iter 0, loss 1.1207213401794434\n",
      "epoch 42, iter 1, loss 1.121726393699646\n",
      "epoch 42, iter 2, loss 1.1116658449172974\n",
      "epoch 42, loss 3.3541135787963867\n",
      "epoch 43, iter 0, loss 1.1114115715026855\n",
      "epoch 43, iter 1, loss 1.1281532049179077\n",
      "epoch 43, iter 2, loss 1.129173755645752\n",
      "epoch 43, loss 3.368738532066345\n",
      "epoch 44, iter 0, loss 1.1046026945114136\n",
      "epoch 44, iter 1, loss 1.1280032396316528\n",
      "epoch 44, iter 2, loss 1.180845022201538\n",
      "epoch 44, loss 3.4134509563446045\n",
      "epoch 45, iter 0, loss 1.099775791168213\n",
      "epoch 45, iter 1, loss 1.1449605226516724\n",
      "epoch 45, iter 2, loss 1.1298458576202393\n",
      "epoch 45, loss 3.3745821714401245\n",
      "epoch 46, iter 0, loss 1.1415786743164062\n",
      "epoch 46, iter 1, loss 1.108165979385376\n",
      "epoch 46, iter 2, loss 1.060589075088501\n",
      "epoch 46, loss 3.310333728790283\n",
      "epoch 47, iter 0, loss 1.1125946044921875\n",
      "epoch 47, iter 1, loss 1.1292705535888672\n",
      "epoch 47, iter 2, loss 1.0237092971801758\n",
      "epoch 47, loss 3.2655744552612305\n",
      "epoch 48, iter 0, loss 1.1190767288208008\n",
      "epoch 48, iter 1, loss 1.0867729187011719\n",
      "epoch 48, iter 2, loss 1.2127935886383057\n",
      "epoch 48, loss 3.4186432361602783\n",
      "epoch 49, iter 0, loss 1.0971838235855103\n",
      "epoch 49, iter 1, loss 1.0934489965438843\n",
      "epoch 49, iter 2, loss 1.2461774349212646\n",
      "epoch 49, loss 3.436810255050659\n",
      "epoch 50, iter 0, loss 1.0939478874206543\n",
      "epoch 50, iter 1, loss 1.1150888204574585\n",
      "epoch 50, iter 2, loss 1.113100528717041\n",
      "epoch 50, loss 3.322137236595154\n",
      "epoch 51, iter 0, loss 1.1121842861175537\n",
      "epoch 51, iter 1, loss 1.1020317077636719\n",
      "epoch 51, iter 2, loss 1.0988078117370605\n",
      "epoch 51, loss 3.313023805618286\n",
      "epoch 52, iter 0, loss 1.091308355331421\n",
      "epoch 52, iter 1, loss 1.127511739730835\n",
      "epoch 52, iter 2, loss 1.0402883291244507\n",
      "epoch 52, loss 3.2591084241867065\n",
      "epoch 53, iter 0, loss 1.0739450454711914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53, iter 1, loss 1.121263027191162\n",
      "epoch 53, iter 2, loss 1.1665329933166504\n",
      "epoch 53, loss 3.361741065979004\n",
      "epoch 54, iter 0, loss 1.0721718072891235\n",
      "epoch 54, iter 1, loss 1.1275542974472046\n",
      "epoch 54, iter 2, loss 1.1415139436721802\n",
      "epoch 54, loss 3.3412400484085083\n",
      "epoch 55, iter 0, loss 1.1060330867767334\n",
      "epoch 55, iter 1, loss 1.118139624595642\n",
      "epoch 55, iter 2, loss 1.0216069221496582\n",
      "epoch 55, loss 3.2457796335220337\n",
      "epoch 56, iter 0, loss 1.1057676076889038\n",
      "epoch 56, iter 1, loss 1.094556212425232\n",
      "epoch 56, iter 2, loss 1.0989916324615479\n",
      "epoch 56, loss 3.2993154525756836\n",
      "epoch 57, iter 0, loss 1.0888909101486206\n",
      "epoch 57, iter 1, loss 1.1191085577011108\n",
      "epoch 57, iter 2, loss 1.1126669645309448\n",
      "epoch 57, loss 3.3206664323806763\n",
      "epoch 58, iter 0, loss 1.116782307624817\n",
      "epoch 58, iter 1, loss 1.0809653997421265\n",
      "epoch 58, iter 2, loss 1.1134055852890015\n",
      "epoch 58, loss 3.311153292655945\n",
      "epoch 59, iter 0, loss 1.091439962387085\n",
      "epoch 59, iter 1, loss 1.1124638319015503\n",
      "epoch 59, iter 2, loss 1.1425347328186035\n",
      "epoch 59, loss 3.3464385271072388\n",
      "epoch 60, iter 0, loss 1.0666807889938354\n",
      "epoch 60, iter 1, loss 1.1270402669906616\n",
      "epoch 60, iter 2, loss 1.1387903690338135\n",
      "epoch 60, loss 3.3325114250183105\n",
      "epoch 61, iter 0, loss 1.101824164390564\n",
      "epoch 61, iter 1, loss 1.0861551761627197\n",
      "epoch 61, iter 2, loss 1.155367136001587\n",
      "epoch 61, loss 3.3433464765548706\n",
      "epoch 62, iter 0, loss 1.1019713878631592\n",
      "epoch 62, iter 1, loss 1.0941274166107178\n",
      "epoch 62, iter 2, loss 1.0896259546279907\n",
      "epoch 62, loss 3.2857247591018677\n",
      "epoch 63, iter 0, loss 1.1159754991531372\n",
      "epoch 63, iter 1, loss 1.0875163078308105\n",
      "epoch 63, iter 2, loss 1.118294596672058\n",
      "epoch 63, loss 3.321786403656006\n",
      "epoch 64, iter 0, loss 1.083828091621399\n",
      "epoch 64, iter 1, loss 1.0975673198699951\n",
      "epoch 64, iter 2, loss 1.1691803932189941\n",
      "epoch 64, loss 3.350575804710388\n",
      "epoch 65, iter 0, loss 1.0608640909194946\n",
      "epoch 65, iter 1, loss 1.138632893562317\n",
      "epoch 65, iter 2, loss 1.0450962781906128\n",
      "epoch 65, loss 3.2445932626724243\n",
      "epoch 66, iter 0, loss 1.0979217290878296\n",
      "epoch 66, iter 1, loss 1.0990842580795288\n",
      "epoch 66, iter 2, loss 1.086925745010376\n",
      "epoch 66, loss 3.2839317321777344\n",
      "epoch 67, iter 0, loss 1.1256163120269775\n",
      "epoch 67, iter 1, loss 1.087689995765686\n",
      "epoch 67, iter 2, loss 1.0817584991455078\n",
      "epoch 67, loss 3.2950648069381714\n",
      "epoch 68, iter 0, loss 1.1328353881835938\n",
      "epoch 68, iter 1, loss 1.0836800336837769\n",
      "epoch 68, iter 2, loss 1.1254864931106567\n",
      "epoch 68, loss 3.3420019149780273\n",
      "epoch 69, iter 0, loss 1.1403940916061401\n",
      "epoch 69, iter 1, loss 1.0754852294921875\n",
      "epoch 69, iter 2, loss 1.096740484237671\n",
      "epoch 69, loss 3.3126198053359985\n",
      "epoch 70, iter 0, loss 1.0823155641555786\n",
      "epoch 70, iter 1, loss 1.1270424127578735\n",
      "epoch 70, iter 2, loss 1.1460362672805786\n",
      "epoch 70, loss 3.3553942441940308\n",
      "epoch 71, iter 0, loss 1.0957951545715332\n",
      "epoch 71, iter 1, loss 1.1165223121643066\n",
      "epoch 71, iter 2, loss 1.0928610563278198\n",
      "epoch 71, loss 3.3051785230636597\n",
      "epoch 72, iter 0, loss 1.0894997119903564\n",
      "epoch 72, iter 1, loss 1.0934408903121948\n",
      "epoch 72, iter 2, loss 1.124206304550171\n",
      "epoch 72, loss 3.307146906852722\n",
      "epoch 73, iter 0, loss 1.0879735946655273\n",
      "epoch 73, iter 1, loss 1.1081029176712036\n",
      "epoch 73, iter 2, loss 1.103211760520935\n",
      "epoch 73, loss 3.299288272857666\n",
      "epoch 74, iter 0, loss 1.1242191791534424\n",
      "epoch 74, iter 1, loss 1.0869417190551758\n",
      "epoch 74, iter 2, loss 1.0497560501098633\n",
      "epoch 74, loss 3.2609169483184814\n",
      "epoch 75, iter 0, loss 1.0824260711669922\n",
      "epoch 75, iter 1, loss 1.0891402959823608\n",
      "epoch 75, iter 2, loss 1.1532859802246094\n",
      "epoch 75, loss 3.3248523473739624\n",
      "epoch 76, iter 0, loss 1.0744447708129883\n",
      "epoch 76, iter 1, loss 1.1144472360610962\n",
      "epoch 76, iter 2, loss 1.0358307361602783\n",
      "epoch 76, loss 3.224722743034363\n",
      "epoch 77, iter 0, loss 1.1223946809768677\n",
      "epoch 77, iter 1, loss 1.0721153020858765\n",
      "epoch 77, iter 2, loss 1.050685167312622\n",
      "epoch 77, loss 3.245195150375366\n",
      "epoch 78, iter 0, loss 1.0965627431869507\n",
      "epoch 78, iter 1, loss 1.0695559978485107\n",
      "epoch 78, iter 2, loss 1.1143590211868286\n",
      "epoch 78, loss 3.28047776222229\n",
      "epoch 79, iter 0, loss 1.068176031112671\n",
      "epoch 79, iter 1, loss 1.0998753309249878\n",
      "epoch 79, iter 2, loss 1.1357839107513428\n",
      "epoch 79, loss 3.3038352727890015\n",
      "epoch 80, iter 0, loss 1.0976340770721436\n",
      "epoch 80, iter 1, loss 1.0969277620315552\n",
      "epoch 80, iter 2, loss 1.0533840656280518\n",
      "epoch 80, loss 3.2479459047317505\n",
      "epoch 81, iter 0, loss 1.114966869354248\n",
      "epoch 81, iter 1, loss 1.0862150192260742\n",
      "epoch 81, iter 2, loss 1.099509835243225\n",
      "epoch 81, loss 3.3006917238235474\n",
      "epoch 82, iter 0, loss 1.098246693611145\n",
      "epoch 82, iter 1, loss 1.0692100524902344\n",
      "epoch 82, iter 2, loss 1.0833687782287598\n",
      "epoch 82, loss 3.250825524330139\n",
      "epoch 83, iter 0, loss 1.1053814888000488\n",
      "epoch 83, iter 1, loss 1.055471658706665\n",
      "epoch 83, iter 2, loss 1.102487325668335\n",
      "epoch 83, loss 3.263340473175049\n",
      "epoch 84, iter 0, loss 1.0630345344543457\n",
      "epoch 84, iter 1, loss 1.0871844291687012\n",
      "epoch 84, iter 2, loss 1.1097996234893799\n",
      "epoch 84, loss 3.2600185871124268\n",
      "epoch 85, iter 0, loss 1.0705227851867676\n",
      "epoch 85, iter 1, loss 1.095411777496338\n",
      "epoch 85, iter 2, loss 1.1185474395751953\n",
      "epoch 85, loss 3.284482002258301\n",
      "epoch 86, iter 0, loss 1.0645067691802979\n",
      "epoch 86, iter 1, loss 1.0922999382019043\n",
      "epoch 86, iter 2, loss 1.1167789697647095\n",
      "epoch 86, loss 3.2735856771469116\n",
      "epoch 87, iter 0, loss 1.0875035524368286\n",
      "epoch 87, iter 1, loss 1.0817081928253174\n",
      "epoch 87, iter 2, loss 1.0740022659301758\n",
      "epoch 87, loss 3.2432140111923218\n",
      "epoch 88, iter 0, loss 1.0965298414230347\n",
      "epoch 88, iter 1, loss 1.0742216110229492\n",
      "epoch 88, iter 2, loss 1.0449086427688599\n",
      "epoch 88, loss 3.2156600952148438\n",
      "epoch 89, iter 0, loss 1.0782639980316162\n",
      "epoch 89, iter 1, loss 1.0991712808609009\n",
      "epoch 89, iter 2, loss 1.008812427520752\n",
      "epoch 89, loss 3.186247706413269\n",
      "epoch 90, iter 0, loss 1.0827311277389526\n",
      "epoch 90, iter 1, loss 1.0904979705810547\n",
      "epoch 90, iter 2, loss 1.043984055519104\n",
      "epoch 90, loss 3.2172131538391113\n",
      "epoch 91, iter 0, loss 1.093191146850586\n",
      "epoch 91, iter 1, loss 1.0661941766738892\n",
      "epoch 91, iter 2, loss 1.0750577449798584\n",
      "epoch 91, loss 3.2344430685043335\n",
      "epoch 92, iter 0, loss 1.0740820169448853\n",
      "epoch 92, iter 1, loss 1.1096363067626953\n",
      "epoch 92, iter 2, loss 1.0817084312438965\n",
      "epoch 92, loss 3.265426754951477\n",
      "epoch 93, iter 0, loss 1.0806779861450195\n",
      "epoch 93, iter 1, loss 1.0655747652053833\n",
      "epoch 93, iter 2, loss 1.1216542720794678\n",
      "epoch 93, loss 3.2679070234298706\n",
      "epoch 94, iter 0, loss 1.0677473545074463\n",
      "epoch 94, iter 1, loss 1.1053918600082397\n",
      "epoch 94, iter 2, loss 1.0914669036865234\n",
      "epoch 94, loss 3.2646061182022095\n",
      "epoch 95, iter 0, loss 1.1313095092773438\n",
      "epoch 95, iter 1, loss 1.037196159362793\n",
      "epoch 95, iter 2, loss 1.106156826019287\n",
      "epoch 95, loss 3.274662494659424\n",
      "epoch 96, iter 0, loss 1.0686030387878418\n",
      "epoch 96, iter 1, loss 1.0837900638580322\n",
      "epoch 96, iter 2, loss 1.1205440759658813\n",
      "epoch 96, loss 3.2729371786117554\n",
      "epoch 97, iter 0, loss 1.0848987102508545\n",
      "epoch 97, iter 1, loss 1.0787734985351562\n",
      "epoch 97, iter 2, loss 1.0955007076263428\n",
      "epoch 97, loss 3.2591729164123535\n",
      "epoch 98, iter 0, loss 1.0774322748184204\n",
      "epoch 98, iter 1, loss 1.099941372871399\n",
      "epoch 98, iter 2, loss 1.0450010299682617\n",
      "epoch 98, loss 3.222374677658081\n",
      "epoch 99, iter 0, loss 1.0727298259735107\n",
      "epoch 99, iter 1, loss 1.0940117835998535\n",
      "epoch 99, iter 2, loss 1.0583354234695435\n",
      "epoch 99, loss 3.2250770330429077\n",
      "2019-03-30 23:38:54.774703, fold=0, rep=1, eta=0d 0h 35m 22s \n",
      "{'fold': 0, 'repeat': 1, 'n': 3338, 'd': 17, 'mse': 0.43783533573150635, 'train_time': 538.9580134509597, 'trained_epochs': 100, 'prior_train_nmll': 1.0838981866836548, 'train_nll': 2120.01904296875, 'test_nll': 1115.369384765625, 'train_mse': 0.3834737241268158, 'state_dict_file': 'model_state_dict_1126841604739188780.pkl'}\n",
      "epoch 0, iter 0, loss 1.9638947248458862\n",
      "epoch 0, iter 1, loss 2.2097809314727783\n",
      "epoch 0, iter 2, loss 2.4177169799804688\n",
      "epoch 0, loss 6.591392636299133\n",
      "epoch 1, iter 0, loss 2.1318538188934326\n",
      "epoch 1, iter 1, loss 2.0403506755828857\n",
      "epoch 1, iter 2, loss 2.091582775115967\n",
      "epoch 1, loss 6.263787269592285\n",
      "epoch 2, iter 0, loss 1.920226812362671\n",
      "epoch 2, iter 1, loss 1.835921287536621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, iter 2, loss 1.807695746421814\n",
      "epoch 2, loss 5.563843846321106\n",
      "epoch 3, iter 0, loss 1.7530041933059692\n",
      "epoch 3, iter 1, loss 1.674086570739746\n",
      "epoch 3, iter 2, loss 1.6062260866165161\n",
      "epoch 3, loss 5.0333168506622314\n",
      "epoch 4, iter 0, loss 1.6489827632904053\n",
      "epoch 4, iter 1, loss 1.6505489349365234\n",
      "epoch 4, iter 2, loss 1.6249513626098633\n",
      "epoch 4, loss 4.924483060836792\n",
      "epoch 5, iter 0, loss 1.6035692691802979\n",
      "epoch 5, iter 1, loss 1.583113431930542\n",
      "epoch 5, iter 2, loss 1.5666435956954956\n",
      "epoch 5, loss 4.7533262968063354\n",
      "epoch 6, iter 0, loss 1.5642677545547485\n",
      "epoch 6, iter 1, loss 1.5287866592407227\n",
      "epoch 6, iter 2, loss 1.4929789304733276\n",
      "epoch 6, loss 4.586033344268799\n",
      "epoch 7, iter 0, loss 1.5136171579360962\n",
      "epoch 7, iter 1, loss 1.491824746131897\n",
      "epoch 7, iter 2, loss 1.5249682664871216\n",
      "epoch 7, loss 4.530410170555115\n",
      "epoch 8, iter 0, loss 1.4703835248947144\n",
      "epoch 8, iter 1, loss 1.4539295434951782\n",
      "epoch 8, iter 2, loss 1.4663418531417847\n",
      "epoch 8, loss 4.390654921531677\n",
      "epoch 9, iter 0, loss 1.4231700897216797\n",
      "epoch 9, iter 1, loss 1.4342705011367798\n",
      "epoch 9, iter 2, loss 1.4250110387802124\n",
      "epoch 9, loss 4.282451629638672\n",
      "epoch 10, iter 0, loss 1.4010339975357056\n",
      "epoch 10, iter 1, loss 1.3819853067398071\n",
      "epoch 10, iter 2, loss 1.3838635683059692\n",
      "epoch 10, loss 4.166882872581482\n",
      "epoch 11, iter 0, loss 1.3458445072174072\n",
      "epoch 11, iter 1, loss 1.3714337348937988\n",
      "epoch 11, iter 2, loss 1.3063645362854004\n",
      "epoch 11, loss 4.0236427783966064\n",
      "epoch 12, iter 0, loss 1.3236974477767944\n",
      "epoch 12, iter 1, loss 1.313817024230957\n",
      "epoch 12, iter 2, loss 1.3105381727218628\n",
      "epoch 12, loss 3.9480526447296143\n",
      "epoch 13, iter 0, loss 1.3148103952407837\n",
      "epoch 13, iter 1, loss 1.2556852102279663\n",
      "epoch 13, iter 2, loss 1.255632996559143\n",
      "epoch 13, loss 3.826128602027893\n",
      "epoch 14, iter 0, loss 1.260886788368225\n",
      "epoch 14, iter 1, loss 1.240564227104187\n",
      "epoch 14, iter 2, loss 1.2925533056259155\n",
      "epoch 14, loss 3.7940043210983276\n",
      "epoch 15, iter 0, loss 1.2451754808425903\n",
      "epoch 15, iter 1, loss 1.2240318059921265\n",
      "epoch 15, iter 2, loss 1.2535439729690552\n",
      "epoch 15, loss 3.722751259803772\n",
      "epoch 16, iter 0, loss 1.225117802619934\n",
      "epoch 16, iter 1, loss 1.2253354787826538\n",
      "epoch 16, iter 2, loss 1.2206008434295654\n",
      "epoch 16, loss 3.6710541248321533\n",
      "epoch 17, iter 0, loss 1.2070696353912354\n",
      "epoch 17, iter 1, loss 1.2249172925949097\n",
      "epoch 17, iter 2, loss 1.2805211544036865\n",
      "epoch 17, loss 3.7125080823898315\n",
      "epoch 18, iter 0, loss 1.1913526058197021\n",
      "epoch 18, iter 1, loss 1.2547568082809448\n",
      "epoch 18, iter 2, loss 1.1330169439315796\n",
      "epoch 18, loss 3.5791263580322266\n",
      "epoch 19, iter 0, loss 1.2175891399383545\n",
      "epoch 19, iter 1, loss 1.2103588581085205\n",
      "epoch 19, iter 2, loss 1.2689509391784668\n",
      "epoch 19, loss 3.696898937225342\n",
      "epoch 20, iter 0, loss 1.2045649290084839\n",
      "epoch 20, iter 1, loss 1.2206207513809204\n",
      "epoch 20, iter 2, loss 1.1981838941574097\n",
      "epoch 20, loss 3.623369574546814\n",
      "epoch 21, iter 0, loss 1.2228672504425049\n",
      "epoch 21, iter 1, loss 1.2073131799697876\n",
      "epoch 21, iter 2, loss 1.1343060731887817\n",
      "epoch 21, loss 3.564486503601074\n",
      "epoch 22, iter 0, loss 1.1844823360443115\n",
      "epoch 22, iter 1, loss 1.1942754983901978\n",
      "epoch 22, iter 2, loss 1.370821475982666\n",
      "epoch 22, loss 3.7495793104171753\n",
      "epoch 23, iter 0, loss 1.211620569229126\n",
      "epoch 23, iter 1, loss 1.1951228380203247\n",
      "epoch 23, iter 2, loss 1.180861473083496\n",
      "epoch 23, loss 3.5876048803329468\n",
      "epoch 24, iter 0, loss 1.2067450284957886\n",
      "epoch 24, iter 1, loss 1.179734706878662\n",
      "epoch 24, iter 2, loss 1.2426156997680664\n",
      "epoch 24, loss 3.629095435142517\n",
      "epoch 25, iter 0, loss 1.201529622077942\n",
      "epoch 25, iter 1, loss 1.175888180732727\n",
      "epoch 25, iter 2, loss 1.1909043788909912\n",
      "epoch 25, loss 3.56832218170166\n",
      "epoch 26, iter 0, loss 1.167153239250183\n",
      "epoch 26, iter 1, loss 1.1951245069503784\n",
      "epoch 26, iter 2, loss 1.2634456157684326\n",
      "epoch 26, loss 3.625723361968994\n",
      "epoch 27, iter 0, loss 1.1904867887496948\n",
      "epoch 27, iter 1, loss 1.1550662517547607\n",
      "epoch 27, iter 2, loss 1.238635778427124\n",
      "epoch 27, loss 3.5841888189315796\n",
      "epoch 28, iter 0, loss 1.1344845294952393\n",
      "epoch 28, iter 1, loss 1.2035741806030273\n",
      "epoch 28, iter 2, loss 1.2311023473739624\n",
      "epoch 28, loss 3.569161057472229\n",
      "epoch 29, iter 0, loss 1.1715587377548218\n",
      "epoch 29, iter 1, loss 1.170455813407898\n",
      "epoch 29, iter 2, loss 1.1543636322021484\n",
      "epoch 29, loss 3.496378183364868\n",
      "epoch 30, iter 0, loss 1.1957405805587769\n",
      "epoch 30, iter 1, loss 1.1446776390075684\n",
      "epoch 30, iter 2, loss 1.118821144104004\n",
      "epoch 30, loss 3.459239363670349\n",
      "epoch 31, iter 0, loss 1.193405270576477\n",
      "epoch 31, iter 1, loss 1.1332873106002808\n",
      "epoch 31, iter 2, loss 1.1585593223571777\n",
      "epoch 31, loss 3.4852519035339355\n",
      "epoch 32, iter 0, loss 1.136738896369934\n",
      "epoch 32, iter 1, loss 1.1655184030532837\n",
      "epoch 32, iter 2, loss 1.2481606006622314\n",
      "epoch 32, loss 3.550417900085449\n",
      "epoch 33, iter 0, loss 1.15720534324646\n",
      "epoch 33, iter 1, loss 1.1606495380401611\n",
      "epoch 33, iter 2, loss 1.1934404373168945\n",
      "epoch 33, loss 3.5112953186035156\n",
      "epoch 34, iter 0, loss 1.1449005603790283\n",
      "epoch 34, iter 1, loss 1.194507360458374\n",
      "epoch 34, iter 2, loss 1.119357943534851\n",
      "epoch 34, loss 3.4587658643722534\n",
      "epoch 35, iter 0, loss 1.171308994293213\n",
      "epoch 35, iter 1, loss 1.1467691659927368\n",
      "epoch 35, iter 2, loss 1.216802716255188\n",
      "epoch 35, loss 3.5348808765411377\n",
      "epoch 36, iter 0, loss 1.1763240098953247\n",
      "epoch 36, iter 1, loss 1.1525224447250366\n",
      "epoch 36, iter 2, loss 1.152459979057312\n",
      "epoch 36, loss 3.4813064336776733\n",
      "epoch 37, iter 0, loss 1.160354733467102\n",
      "epoch 37, iter 1, loss 1.1743991374969482\n",
      "epoch 37, iter 2, loss 1.0873676538467407\n",
      "epoch 37, loss 3.422121524810791\n",
      "epoch 38, iter 0, loss 1.1405845880508423\n",
      "epoch 38, iter 1, loss 1.1636226177215576\n",
      "epoch 38, iter 2, loss 1.2236618995666504\n",
      "epoch 38, loss 3.5278691053390503\n",
      "epoch 39, iter 0, loss 1.1371402740478516\n",
      "epoch 39, iter 1, loss 1.1773402690887451\n",
      "epoch 39, iter 2, loss 1.1637418270111084\n",
      "epoch 39, loss 3.478222370147705\n",
      "epoch 40, iter 0, loss 1.1796241998672485\n",
      "epoch 40, iter 1, loss 1.153854489326477\n",
      "epoch 40, iter 2, loss 1.0968679189682007\n",
      "epoch 40, loss 3.4303466081619263\n",
      "epoch 41, iter 0, loss 1.1676064729690552\n",
      "epoch 41, iter 1, loss 1.1645978689193726\n",
      "epoch 41, iter 2, loss 1.1005226373672485\n",
      "epoch 41, loss 3.4327269792556763\n",
      "epoch 42, iter 0, loss 1.1668745279312134\n",
      "epoch 42, iter 1, loss 1.154175043106079\n",
      "epoch 42, iter 2, loss 1.063420057296753\n",
      "epoch 42, loss 3.3844696283340454\n",
      "epoch 43, iter 0, loss 1.1379311084747314\n",
      "epoch 43, iter 1, loss 1.1790821552276611\n",
      "epoch 43, iter 2, loss 1.0587230920791626\n",
      "epoch 43, loss 3.375736355781555\n",
      "epoch 44, iter 0, loss 1.1921205520629883\n",
      "epoch 44, iter 1, loss 1.1244202852249146\n",
      "epoch 44, iter 2, loss 1.0783326625823975\n",
      "epoch 44, loss 3.3948734998703003\n",
      "epoch 45, iter 0, loss 1.1507208347320557\n",
      "epoch 45, iter 1, loss 1.166939377784729\n",
      "epoch 45, iter 2, loss 1.0720258951187134\n",
      "epoch 45, loss 3.389686107635498\n",
      "epoch 46, iter 0, loss 1.15878427028656\n",
      "epoch 46, iter 1, loss 1.1469459533691406\n",
      "epoch 46, iter 2, loss 1.1321066617965698\n",
      "epoch 46, loss 3.4378368854522705\n",
      "epoch 47, iter 0, loss 1.1027992963790894\n",
      "epoch 47, iter 1, loss 1.1661899089813232\n",
      "epoch 47, iter 2, loss 1.2217931747436523\n",
      "epoch 47, loss 3.490782380104065\n",
      "epoch 48, iter 0, loss 1.121883511543274\n",
      "epoch 48, iter 1, loss 1.1641358137130737\n",
      "epoch 48, iter 2, loss 1.1674995422363281\n",
      "epoch 48, loss 3.453518867492676\n",
      "epoch 49, iter 0, loss 1.1452789306640625\n",
      "epoch 49, iter 1, loss 1.124442219734192\n",
      "epoch 49, iter 2, loss 1.2502058744430542\n",
      "epoch 49, loss 3.5199270248413086\n",
      "epoch 50, iter 0, loss 1.1346356868743896\n",
      "epoch 50, iter 1, loss 1.1567028760910034\n",
      "epoch 50, iter 2, loss 1.1197271347045898\n",
      "epoch 50, loss 3.411065697669983\n",
      "epoch 51, iter 0, loss 1.1487618684768677\n",
      "epoch 51, iter 1, loss 1.1182174682617188\n",
      "epoch 51, iter 2, loss 1.2067316770553589\n",
      "epoch 51, loss 3.4737110137939453\n",
      "epoch 52, iter 0, loss 1.1477546691894531\n",
      "epoch 52, iter 1, loss 1.134656310081482\n",
      "epoch 52, iter 2, loss 1.131356954574585\n",
      "epoch 52, loss 3.41376793384552\n",
      "epoch 53, iter 0, loss 1.1371541023254395\n",
      "epoch 53, iter 1, loss 1.155080795288086\n",
      "epoch 53, iter 2, loss 1.0807818174362183\n",
      "epoch 53, loss 3.3730167150497437\n",
      "epoch 54, iter 0, loss 1.1084150075912476\n",
      "epoch 54, iter 1, loss 1.1698790788650513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54, iter 2, loss 1.0914654731750488\n",
      "epoch 54, loss 3.3697595596313477\n",
      "epoch 55, iter 0, loss 1.1197096109390259\n",
      "epoch 55, iter 1, loss 1.1470656394958496\n",
      "epoch 55, iter 2, loss 1.1798624992370605\n",
      "epoch 55, loss 3.446637749671936\n",
      "epoch 56, iter 0, loss 1.1249948740005493\n",
      "epoch 56, iter 1, loss 1.1321909427642822\n",
      "epoch 56, iter 2, loss 1.2543354034423828\n",
      "epoch 56, loss 3.5115212202072144\n",
      "epoch 57, iter 0, loss 1.1479891538619995\n",
      "epoch 57, iter 1, loss 1.1500028371810913\n",
      "epoch 57, iter 2, loss 1.10267174243927\n",
      "epoch 57, loss 3.400663733482361\n",
      "epoch 58, iter 0, loss 1.1095354557037354\n",
      "epoch 58, iter 1, loss 1.1681522130966187\n",
      "epoch 58, iter 2, loss 1.1846543550491333\n",
      "epoch 58, loss 3.4623420238494873\n",
      "epoch 59, iter 0, loss 1.1540861129760742\n",
      "epoch 59, iter 1, loss 1.146674394607544\n",
      "epoch 59, iter 2, loss 1.0906665325164795\n",
      "epoch 59, loss 3.3914270401000977\n",
      "epoch 60, iter 0, loss 1.1292283535003662\n",
      "epoch 60, iter 1, loss 1.149462103843689\n",
      "epoch 60, iter 2, loss 1.1898152828216553\n",
      "epoch 60, loss 3.4685057401657104\n",
      "epoch 61, iter 0, loss 1.1112403869628906\n",
      "epoch 61, iter 1, loss 1.1708121299743652\n",
      "epoch 61, iter 2, loss 1.022399663925171\n",
      "epoch 61, loss 3.3044521808624268\n",
      "epoch 62, iter 0, loss 1.1464471817016602\n",
      "epoch 62, iter 1, loss 1.1306912899017334\n",
      "epoch 62, iter 2, loss 1.0867334604263306\n",
      "epoch 62, loss 3.363871932029724\n",
      "epoch 63, iter 0, loss 1.113591194152832\n",
      "epoch 63, iter 1, loss 1.135739803314209\n",
      "epoch 63, iter 2, loss 1.1667708158493042\n",
      "epoch 63, loss 3.416101813316345\n",
      "epoch 64, iter 0, loss 1.1203243732452393\n",
      "epoch 64, iter 1, loss 1.155314564704895\n",
      "epoch 64, iter 2, loss 1.102585792541504\n",
      "epoch 64, loss 3.378224730491638\n",
      "epoch 65, iter 0, loss 1.112616777420044\n",
      "epoch 65, iter 1, loss 1.1454917192459106\n",
      "epoch 65, iter 2, loss 1.1171599626541138\n",
      "epoch 65, loss 3.3752684593200684\n",
      "epoch 66, iter 0, loss 1.1440441608428955\n",
      "epoch 66, iter 1, loss 1.1194628477096558\n",
      "epoch 66, iter 2, loss 1.1956822872161865\n",
      "epoch 66, loss 3.459189295768738\n",
      "epoch 67, iter 0, loss 1.1616817712783813\n",
      "epoch 67, iter 1, loss 1.1235889196395874\n",
      "epoch 67, iter 2, loss 1.1067408323287964\n",
      "epoch 67, loss 3.392011523246765\n",
      "epoch 68, iter 0, loss 1.100123643875122\n",
      "epoch 68, iter 1, loss 1.154563546180725\n",
      "epoch 68, iter 2, loss 1.1556708812713623\n",
      "epoch 68, loss 3.4103580713272095\n",
      "epoch 69, iter 0, loss 1.1506456136703491\n",
      "epoch 69, iter 1, loss 1.1197919845581055\n",
      "epoch 69, iter 2, loss 1.0940910577774048\n",
      "epoch 69, loss 3.3645286560058594\n",
      "epoch 70, iter 0, loss 1.1311509609222412\n",
      "epoch 70, iter 1, loss 1.1318118572235107\n",
      "epoch 70, iter 2, loss 1.1862764358520508\n",
      "epoch 70, loss 3.4492392539978027\n",
      "epoch 71, iter 0, loss 1.1283762454986572\n",
      "epoch 71, iter 1, loss 1.1283483505249023\n",
      "epoch 71, iter 2, loss 1.1564552783966064\n",
      "epoch 71, loss 3.413179874420166\n",
      "epoch 72, iter 0, loss 1.1408960819244385\n",
      "epoch 72, iter 1, loss 1.109169602394104\n",
      "epoch 72, iter 2, loss 1.119861364364624\n",
      "epoch 72, loss 3.3699270486831665\n",
      "epoch 73, iter 0, loss 1.109843134880066\n",
      "epoch 73, iter 1, loss 1.1244362592697144\n",
      "epoch 73, iter 2, loss 1.1763103008270264\n",
      "epoch 73, loss 3.4105896949768066\n",
      "epoch 74, iter 0, loss 1.1283366680145264\n",
      "epoch 74, iter 1, loss 1.1197516918182373\n",
      "epoch 74, iter 2, loss 1.1581889390945435\n",
      "epoch 74, loss 3.406277298927307\n",
      "epoch 75, iter 0, loss 1.1022230386734009\n",
      "epoch 75, iter 1, loss 1.1399836540222168\n",
      "epoch 75, iter 2, loss 1.1254878044128418\n",
      "epoch 75, loss 3.3676944971084595\n",
      "epoch 76, iter 0, loss 1.1458311080932617\n",
      "epoch 76, iter 1, loss 1.1100696325302124\n",
      "epoch 76, iter 2, loss 1.0147331953048706\n",
      "epoch 76, loss 3.2706339359283447\n",
      "epoch 77, iter 0, loss 1.0975937843322754\n",
      "epoch 77, iter 1, loss 1.1026878356933594\n",
      "epoch 77, iter 2, loss 1.3454054594039917\n",
      "epoch 77, loss 3.5456870794296265\n",
      "epoch 78, iter 0, loss 1.095613718032837\n",
      "epoch 78, iter 1, loss 1.1838310956954956\n",
      "epoch 78, iter 2, loss 1.0923795700073242\n",
      "epoch 78, loss 3.3718243837356567\n",
      "epoch 79, iter 0, loss 1.1349270343780518\n",
      "epoch 79, iter 1, loss 1.1252095699310303\n",
      "epoch 79, iter 2, loss 1.1115124225616455\n",
      "epoch 79, loss 3.3716490268707275\n",
      "epoch 80, iter 0, loss 1.1183995008468628\n",
      "epoch 80, iter 1, loss 1.144158124923706\n",
      "epoch 80, iter 2, loss 1.1194826364517212\n",
      "epoch 80, loss 3.38204026222229\n",
      "epoch 81, iter 0, loss 1.1247212886810303\n",
      "epoch 81, iter 1, loss 1.1166927814483643\n",
      "epoch 81, iter 2, loss 1.1052601337432861\n",
      "epoch 81, loss 3.3466742038726807\n",
      "epoch 82, iter 0, loss 1.1213703155517578\n",
      "epoch 82, iter 1, loss 1.1213172674179077\n",
      "epoch 82, iter 2, loss 1.1253025531768799\n",
      "epoch 82, loss 3.3679901361465454\n",
      "epoch 83, iter 0, loss 1.100130319595337\n",
      "epoch 83, iter 1, loss 1.1506829261779785\n",
      "epoch 83, iter 2, loss 1.0745209455490112\n",
      "epoch 83, loss 3.3253341913223267\n",
      "epoch 84, iter 0, loss 1.1306251287460327\n",
      "epoch 84, iter 1, loss 1.1056244373321533\n",
      "epoch 84, iter 2, loss 1.1111607551574707\n",
      "epoch 84, loss 3.3474103212356567\n",
      "epoch 85, iter 0, loss 1.1067955493927002\n",
      "epoch 85, iter 1, loss 1.1338874101638794\n",
      "epoch 85, iter 2, loss 1.1313002109527588\n",
      "epoch 85, loss 3.3719831705093384\n",
      "epoch 86, iter 0, loss 1.1177427768707275\n",
      "epoch 86, iter 1, loss 1.1313084363937378\n",
      "epoch 86, iter 2, loss 1.146204948425293\n",
      "epoch 86, loss 3.3952561616897583\n",
      "epoch 87, iter 0, loss 1.1287719011306763\n",
      "epoch 87, iter 1, loss 1.1239986419677734\n",
      "epoch 87, iter 2, loss 1.0788066387176514\n",
      "epoch 87, loss 3.331577181816101\n",
      "epoch 88, iter 0, loss 1.1574146747589111\n",
      "epoch 88, iter 1, loss 1.094111442565918\n",
      "epoch 88, iter 2, loss 1.0492510795593262\n",
      "epoch 88, loss 3.3007771968841553\n",
      "epoch 89, iter 0, loss 1.1262933015823364\n",
      "epoch 89, iter 1, loss 1.0933148860931396\n",
      "epoch 89, iter 2, loss 1.2196813821792603\n",
      "epoch 89, loss 3.4392895698547363\n",
      "epoch 90, iter 0, loss 1.0864207744598389\n",
      "epoch 90, iter 1, loss 1.1385258436203003\n",
      "epoch 90, iter 2, loss 1.1163599491119385\n",
      "epoch 90, loss 3.3413065671920776\n",
      "epoch 91, iter 0, loss 1.1400336027145386\n",
      "epoch 91, iter 1, loss 1.1065763235092163\n",
      "epoch 91, iter 2, loss 1.044762372970581\n",
      "epoch 91, loss 3.291372299194336\n",
      "epoch 92, iter 0, loss 1.1304621696472168\n",
      "epoch 92, iter 1, loss 1.1095726490020752\n",
      "epoch 92, iter 2, loss 1.0586661100387573\n",
      "epoch 92, loss 3.2987009286880493\n",
      "epoch 93, iter 0, loss 1.096273422241211\n",
      "epoch 93, iter 1, loss 1.1255465745925903\n",
      "epoch 93, iter 2, loss 1.2418081760406494\n",
      "epoch 93, loss 3.4636281728744507\n",
      "epoch 94, iter 0, loss 1.1187241077423096\n",
      "epoch 94, iter 1, loss 1.1048343181610107\n",
      "epoch 94, iter 2, loss 1.1537619829177856\n",
      "epoch 94, loss 3.377320408821106\n",
      "epoch 95, iter 0, loss 1.113126277923584\n",
      "epoch 95, iter 1, loss 1.1097420454025269\n",
      "epoch 95, iter 2, loss 1.1494839191436768\n",
      "epoch 95, loss 3.3723522424697876\n",
      "epoch 96, iter 0, loss 1.1449705362319946\n",
      "epoch 96, iter 1, loss 1.080155611038208\n",
      "epoch 96, iter 2, loss 1.113418698310852\n",
      "epoch 96, loss 3.3385448455810547\n",
      "epoch 97, iter 0, loss 1.1380277872085571\n",
      "epoch 97, iter 1, loss 1.0716617107391357\n",
      "epoch 97, iter 2, loss 1.2114629745483398\n",
      "epoch 97, loss 3.4211524724960327\n",
      "epoch 98, iter 0, loss 1.0911935567855835\n",
      "epoch 98, iter 1, loss 1.1304991245269775\n",
      "epoch 98, iter 2, loss 1.1254944801330566\n",
      "epoch 98, loss 3.3471871614456177\n",
      "epoch 99, iter 0, loss 1.0858935117721558\n",
      "epoch 99, iter 1, loss 1.1347979307174683\n",
      "epoch 99, iter 2, loss 1.084686517715454\n",
      "epoch 99, loss 3.305377960205078\n",
      "2019-03-30 23:47:25.586115, fold=1, rep=0, eta=0d 0h 26m 12s \n",
      "{'fold': 1, 'repeat': 0, 'n': 3338, 'd': 17, 'mse': 0.4333813190460205, 'train_time': 509.5806405451149, 'trained_epochs': 100, 'prior_train_nmll': 1.1056710481643677, 'train_nll': 2194.91259765625, 'test_nll': 1121.2872314453125, 'train_mse': 0.4036383330821991, 'state_dict_file': 'model_state_dict_-7163484801637433496.pkl'}\n",
      "epoch 0, iter 0, loss 2.0045957565307617\n",
      "epoch 0, iter 1, loss 2.210338830947876\n",
      "epoch 0, iter 2, loss 2.343435049057007\n",
      "epoch 0, loss 6.5583696365356445\n",
      "epoch 1, iter 0, loss 2.130052089691162\n",
      "epoch 1, iter 1, loss 2.080474615097046\n",
      "epoch 1, iter 2, loss 2.089477300643921\n",
      "epoch 1, loss 6.300004005432129\n",
      "epoch 2, iter 0, loss 1.9608638286590576\n",
      "epoch 2, iter 1, loss 1.8300050497055054\n",
      "epoch 2, iter 2, loss 1.8382872343063354\n",
      "epoch 2, loss 5.629156112670898\n",
      "epoch 3, iter 0, loss 1.770338773727417\n",
      "epoch 3, iter 1, loss 1.6989997625350952\n",
      "epoch 3, iter 2, loss 1.64349365234375\n",
      "epoch 3, loss 5.112832188606262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, iter 0, loss 1.6680289506912231\n",
      "epoch 4, iter 1, loss 1.667689323425293\n",
      "epoch 4, iter 2, loss 1.644209623336792\n",
      "epoch 4, loss 4.979927897453308\n",
      "epoch 5, iter 0, loss 1.638145089149475\n",
      "epoch 5, iter 1, loss 1.5869132280349731\n",
      "epoch 5, iter 2, loss 1.5686659812927246\n",
      "epoch 5, loss 4.793724298477173\n",
      "epoch 6, iter 0, loss 1.5591410398483276\n",
      "epoch 6, iter 1, loss 1.5590234994888306\n",
      "epoch 6, iter 2, loss 1.5549588203430176\n",
      "epoch 6, loss 4.673123359680176\n",
      "epoch 7, iter 0, loss 1.5363229513168335\n",
      "epoch 7, iter 1, loss 1.515955924987793\n",
      "epoch 7, iter 2, loss 1.5048316717147827\n",
      "epoch 7, loss 4.557110548019409\n",
      "epoch 8, iter 0, loss 1.4988545179367065\n",
      "epoch 8, iter 1, loss 1.4674367904663086\n",
      "epoch 8, iter 2, loss 1.5080233812332153\n",
      "epoch 8, loss 4.4743146896362305\n",
      "epoch 9, iter 0, loss 1.4565470218658447\n",
      "epoch 9, iter 1, loss 1.4455721378326416\n",
      "epoch 9, iter 2, loss 1.4019050598144531\n",
      "epoch 9, loss 4.3040242195129395\n",
      "epoch 10, iter 0, loss 1.422382116317749\n",
      "epoch 10, iter 1, loss 1.4046605825424194\n",
      "epoch 10, iter 2, loss 1.391154170036316\n",
      "epoch 10, loss 4.218196868896484\n",
      "epoch 11, iter 0, loss 1.3904306888580322\n",
      "epoch 11, iter 1, loss 1.3618369102478027\n",
      "epoch 11, iter 2, loss 1.3297003507614136\n",
      "epoch 11, loss 4.0819679498672485\n",
      "epoch 12, iter 0, loss 1.3622057437896729\n",
      "epoch 12, iter 1, loss 1.3148914575576782\n",
      "epoch 12, iter 2, loss 1.2728521823883057\n",
      "epoch 12, loss 3.9499493837356567\n",
      "epoch 13, iter 0, loss 1.310841679573059\n",
      "epoch 13, iter 1, loss 1.3054519891738892\n",
      "epoch 13, iter 2, loss 1.2238476276397705\n",
      "epoch 13, loss 3.8401412963867188\n",
      "epoch 14, iter 0, loss 1.2749602794647217\n",
      "epoch 14, iter 1, loss 1.2678900957107544\n",
      "epoch 14, iter 2, loss 1.2183374166488647\n",
      "epoch 14, loss 3.761187791824341\n",
      "epoch 15, iter 0, loss 1.2470169067382812\n",
      "epoch 15, iter 1, loss 1.2334259748458862\n",
      "epoch 15, iter 2, loss 1.2909163236618042\n",
      "epoch 15, loss 3.7713592052459717\n",
      "epoch 16, iter 0, loss 1.2353798151016235\n",
      "epoch 16, iter 1, loss 1.2285053730010986\n",
      "epoch 16, iter 2, loss 1.2220722436904907\n",
      "epoch 16, loss 3.685957431793213\n",
      "epoch 17, iter 0, loss 1.2093682289123535\n",
      "epoch 17, iter 1, loss 1.2525148391723633\n",
      "epoch 17, iter 2, loss 1.2024649381637573\n",
      "epoch 17, loss 3.664348006248474\n",
      "epoch 18, iter 0, loss 1.2068252563476562\n",
      "epoch 18, iter 1, loss 1.241774082183838\n",
      "epoch 18, iter 2, loss 1.1664416790008545\n",
      "epoch 18, loss 3.6150410175323486\n",
      "epoch 19, iter 0, loss 1.2074158191680908\n",
      "epoch 19, iter 1, loss 1.2425371408462524\n",
      "epoch 19, iter 2, loss 1.2046135663986206\n",
      "epoch 19, loss 3.654566526412964\n",
      "epoch 20, iter 0, loss 1.229469895362854\n",
      "epoch 20, iter 1, loss 1.220746636390686\n",
      "epoch 20, iter 2, loss 1.189324975013733\n",
      "epoch 20, loss 3.639541506767273\n",
      "epoch 21, iter 0, loss 1.2179049253463745\n",
      "epoch 21, iter 1, loss 1.2174315452575684\n",
      "epoch 21, iter 2, loss 1.1655471324920654\n",
      "epoch 21, loss 3.6008836030960083\n",
      "epoch 22, iter 0, loss 1.183210849761963\n",
      "epoch 22, iter 1, loss 1.2453052997589111\n",
      "epoch 22, iter 2, loss 1.2309448719024658\n",
      "epoch 22, loss 3.65946102142334\n",
      "epoch 23, iter 0, loss 1.2260496616363525\n",
      "epoch 23, iter 1, loss 1.1904007196426392\n",
      "epoch 23, iter 2, loss 1.170362949371338\n",
      "epoch 23, loss 3.5868133306503296\n",
      "epoch 24, iter 0, loss 1.2056845426559448\n",
      "epoch 24, iter 1, loss 1.2109838724136353\n",
      "epoch 24, iter 2, loss 1.111696720123291\n",
      "epoch 24, loss 3.528365135192871\n",
      "epoch 25, iter 0, loss 1.1777610778808594\n",
      "epoch 25, iter 1, loss 1.1919037103652954\n",
      "epoch 25, iter 2, loss 1.2296627759933472\n",
      "epoch 25, loss 3.599327564239502\n",
      "epoch 26, iter 0, loss 1.2154655456542969\n",
      "epoch 26, iter 1, loss 1.145161747932434\n",
      "epoch 26, iter 2, loss 1.2312095165252686\n",
      "epoch 26, loss 3.5918368101119995\n",
      "epoch 27, iter 0, loss 1.1630158424377441\n",
      "epoch 27, iter 1, loss 1.1888333559036255\n",
      "epoch 27, iter 2, loss 1.215499758720398\n",
      "epoch 27, loss 3.5673489570617676\n",
      "epoch 28, iter 0, loss 1.1710810661315918\n",
      "epoch 28, iter 1, loss 1.1790058612823486\n",
      "epoch 28, iter 2, loss 1.1638970375061035\n",
      "epoch 28, loss 3.513983964920044\n",
      "epoch 29, iter 0, loss 1.1690568923950195\n",
      "epoch 29, iter 1, loss 1.1690585613250732\n",
      "epoch 29, iter 2, loss 1.193814992904663\n",
      "epoch 29, loss 3.531930446624756\n",
      "epoch 30, iter 0, loss 1.1459368467330933\n",
      "epoch 30, iter 1, loss 1.1713920831680298\n",
      "epoch 30, iter 2, loss 1.2000269889831543\n",
      "epoch 30, loss 3.5173559188842773\n",
      "epoch 31, iter 0, loss 1.173591136932373\n",
      "epoch 31, iter 1, loss 1.1472396850585938\n",
      "epoch 31, iter 2, loss 1.162330985069275\n",
      "epoch 31, loss 3.4831618070602417\n",
      "epoch 32, iter 0, loss 1.1391509771347046\n",
      "epoch 32, iter 1, loss 1.1603139638900757\n",
      "epoch 32, iter 2, loss 1.2686996459960938\n",
      "epoch 32, loss 3.568164587020874\n",
      "epoch 33, iter 0, loss 1.2131233215332031\n",
      "epoch 33, iter 1, loss 1.1051971912384033\n",
      "epoch 33, iter 2, loss 1.172743558883667\n",
      "epoch 33, loss 3.4910640716552734\n",
      "epoch 34, iter 0, loss 1.1321778297424316\n",
      "epoch 34, iter 1, loss 1.1853814125061035\n",
      "epoch 34, iter 2, loss 1.1939735412597656\n",
      "epoch 34, loss 3.511532783508301\n",
      "epoch 35, iter 0, loss 1.1875642538070679\n",
      "epoch 35, iter 1, loss 1.1578187942504883\n",
      "epoch 35, iter 2, loss 1.115051031112671\n",
      "epoch 35, loss 3.460434079170227\n",
      "epoch 36, iter 0, loss 1.1729758977890015\n",
      "epoch 36, iter 1, loss 1.1482359170913696\n",
      "epoch 36, iter 2, loss 1.170723557472229\n",
      "epoch 36, loss 3.4919353723526\n",
      "epoch 37, iter 0, loss 1.1421051025390625\n",
      "epoch 37, iter 1, loss 1.1849291324615479\n",
      "epoch 37, iter 2, loss 1.1321372985839844\n",
      "epoch 37, loss 3.4591715335845947\n",
      "epoch 38, iter 0, loss 1.140873670578003\n",
      "epoch 38, iter 1, loss 1.1943058967590332\n",
      "epoch 38, iter 2, loss 1.1282613277435303\n",
      "epoch 38, loss 3.4634408950805664\n",
      "epoch 39, iter 0, loss 1.1626907587051392\n",
      "epoch 39, iter 1, loss 1.1679534912109375\n",
      "epoch 39, iter 2, loss 1.093503713607788\n",
      "epoch 39, loss 3.4241479635238647\n",
      "epoch 40, iter 0, loss 1.1586315631866455\n",
      "epoch 40, iter 1, loss 1.1524513959884644\n",
      "epoch 40, iter 2, loss 1.2603528499603271\n",
      "epoch 40, loss 3.571435809135437\n",
      "epoch 41, iter 0, loss 1.1371434926986694\n",
      "epoch 41, iter 1, loss 1.182940125465393\n",
      "epoch 41, iter 2, loss 1.145215630531311\n",
      "epoch 41, loss 3.4652992486953735\n",
      "epoch 42, iter 0, loss 1.1494946479797363\n",
      "epoch 42, iter 1, loss 1.1683082580566406\n",
      "epoch 42, iter 2, loss 1.0781069993972778\n",
      "epoch 42, loss 3.395909905433655\n",
      "epoch 43, iter 0, loss 1.1741989850997925\n",
      "epoch 43, iter 1, loss 1.1420539617538452\n",
      "epoch 43, iter 2, loss 1.113103985786438\n",
      "epoch 43, loss 3.4293569326400757\n",
      "epoch 44, iter 0, loss 1.159563422203064\n",
      "epoch 44, iter 1, loss 1.1461727619171143\n",
      "epoch 44, iter 2, loss 1.1408576965332031\n",
      "epoch 44, loss 3.4465938806533813\n",
      "epoch 45, iter 0, loss 1.1930643320083618\n",
      "epoch 45, iter 1, loss 1.127090334892273\n",
      "epoch 45, iter 2, loss 1.080228328704834\n",
      "epoch 45, loss 3.4003829956054688\n",
      "epoch 46, iter 0, loss 1.1612610816955566\n",
      "epoch 46, iter 1, loss 1.128462314605713\n",
      "epoch 46, iter 2, loss 1.228450894355774\n",
      "epoch 46, loss 3.5181742906570435\n",
      "epoch 47, iter 0, loss 1.1782712936401367\n",
      "epoch 47, iter 1, loss 1.138314127922058\n",
      "epoch 47, iter 2, loss 1.1142927408218384\n",
      "epoch 47, loss 3.430878162384033\n",
      "epoch 48, iter 0, loss 1.1501195430755615\n",
      "epoch 48, iter 1, loss 1.153458833694458\n",
      "epoch 48, iter 2, loss 1.1733882427215576\n",
      "epoch 48, loss 3.476966619491577\n",
      "epoch 49, iter 0, loss 1.1653615236282349\n",
      "epoch 49, iter 1, loss 1.1172404289245605\n",
      "epoch 49, iter 2, loss 1.2095472812652588\n",
      "epoch 49, loss 3.492149233818054\n",
      "epoch 50, iter 0, loss 1.1593093872070312\n",
      "epoch 50, iter 1, loss 1.1299339532852173\n",
      "epoch 50, iter 2, loss 1.1633899211883545\n",
      "epoch 50, loss 3.452633261680603\n",
      "epoch 51, iter 0, loss 1.14215087890625\n",
      "epoch 51, iter 1, loss 1.1075372695922852\n",
      "epoch 51, iter 2, loss 1.336499571800232\n",
      "epoch 51, loss 3.586187720298767\n",
      "epoch 52, iter 0, loss 1.1535195112228394\n",
      "epoch 52, iter 1, loss 1.1182236671447754\n",
      "epoch 52, iter 2, loss 1.21086585521698\n",
      "epoch 52, loss 3.4826090335845947\n",
      "epoch 53, iter 0, loss 1.1660908460617065\n",
      "epoch 53, iter 1, loss 1.1276872158050537\n",
      "epoch 53, iter 2, loss 1.1153870820999146\n",
      "epoch 53, loss 3.409165143966675\n",
      "epoch 54, iter 0, loss 1.1348096132278442\n",
      "epoch 54, iter 1, loss 1.1472922563552856\n",
      "epoch 54, iter 2, loss 1.195403814315796\n",
      "epoch 54, loss 3.477505683898926\n",
      "epoch 55, iter 0, loss 1.137400507926941\n",
      "epoch 55, iter 1, loss 1.1459362506866455\n",
      "epoch 55, iter 2, loss 1.134774088859558\n",
      "epoch 55, loss 3.4181108474731445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56, iter 0, loss 1.1519259214401245\n",
      "epoch 56, iter 1, loss 1.1050997972488403\n",
      "epoch 56, iter 2, loss 1.2211580276489258\n",
      "epoch 56, loss 3.4781837463378906\n",
      "epoch 57, iter 0, loss 1.149872064590454\n",
      "epoch 57, iter 1, loss 1.125244140625\n",
      "epoch 57, iter 2, loss 1.133630633354187\n",
      "epoch 57, loss 3.408746838569641\n",
      "epoch 58, iter 0, loss 1.1462242603302002\n",
      "epoch 58, iter 1, loss 1.1273587942123413\n",
      "epoch 58, iter 2, loss 1.1605629920959473\n",
      "epoch 58, loss 3.4341460466384888\n",
      "epoch 59, iter 0, loss 1.1597626209259033\n",
      "epoch 59, iter 1, loss 1.1196882724761963\n",
      "epoch 59, iter 2, loss 1.1070787906646729\n",
      "epoch 59, loss 3.3865296840667725\n",
      "epoch 60, iter 0, loss 1.1511619091033936\n",
      "epoch 60, iter 1, loss 1.1262279748916626\n",
      "epoch 60, iter 2, loss 1.1302533149719238\n",
      "epoch 60, loss 3.40764319896698\n",
      "epoch 61, iter 0, loss 1.1336848735809326\n",
      "epoch 61, iter 1, loss 1.1497302055358887\n",
      "epoch 61, iter 2, loss 1.1670024394989014\n",
      "epoch 61, loss 3.4504175186157227\n",
      "epoch 62, iter 0, loss 1.1231108903884888\n",
      "epoch 62, iter 1, loss 1.1848632097244263\n",
      "epoch 62, iter 2, loss 1.035408616065979\n",
      "epoch 62, loss 3.343382716178894\n",
      "epoch 63, iter 0, loss 1.1173558235168457\n",
      "epoch 63, iter 1, loss 1.1731383800506592\n",
      "epoch 63, iter 2, loss 1.0901755094528198\n",
      "epoch 63, loss 3.3806697130203247\n",
      "epoch 64, iter 0, loss 1.1336050033569336\n",
      "epoch 64, iter 1, loss 1.139596939086914\n",
      "epoch 64, iter 2, loss 1.181948184967041\n",
      "epoch 64, loss 3.4551501274108887\n",
      "epoch 65, iter 0, loss 1.1717164516448975\n",
      "epoch 65, iter 1, loss 1.127681016921997\n",
      "epoch 65, iter 2, loss 1.025921106338501\n",
      "epoch 65, loss 3.3253185749053955\n",
      "epoch 66, iter 0, loss 1.133815884590149\n",
      "epoch 66, iter 1, loss 1.1356794834136963\n",
      "epoch 66, iter 2, loss 1.0614591836929321\n",
      "epoch 66, loss 3.3309545516967773\n",
      "epoch 67, iter 0, loss 1.1425049304962158\n",
      "epoch 67, iter 1, loss 1.128605842590332\n",
      "epoch 67, iter 2, loss 1.0836448669433594\n",
      "epoch 67, loss 3.3547556400299072\n",
      "epoch 68, iter 0, loss 1.1435980796813965\n",
      "epoch 68, iter 1, loss 1.1196469068527222\n",
      "epoch 68, iter 2, loss 1.0511655807495117\n",
      "epoch 68, loss 3.3144105672836304\n",
      "epoch 69, iter 0, loss 1.154809832572937\n",
      "epoch 69, iter 1, loss 1.1084517240524292\n",
      "epoch 69, iter 2, loss 1.1269402503967285\n",
      "epoch 69, loss 3.3902018070220947\n",
      "epoch 70, iter 0, loss 1.105471134185791\n",
      "epoch 70, iter 1, loss 1.1390435695648193\n",
      "epoch 70, iter 2, loss 1.1864957809448242\n",
      "epoch 70, loss 3.4310104846954346\n",
      "epoch 71, iter 0, loss 1.147497534751892\n",
      "epoch 71, iter 1, loss 1.1157984733581543\n",
      "epoch 71, iter 2, loss 1.0914347171783447\n",
      "epoch 71, loss 3.354730725288391\n",
      "epoch 72, iter 0, loss 1.1001718044281006\n",
      "epoch 72, iter 1, loss 1.1420971155166626\n",
      "epoch 72, iter 2, loss 1.1655477285385132\n",
      "epoch 72, loss 3.4078166484832764\n",
      "epoch 73, iter 0, loss 1.1427571773529053\n",
      "epoch 73, iter 1, loss 1.094785213470459\n",
      "epoch 73, iter 2, loss 1.1022772789001465\n",
      "epoch 73, loss 3.3398196697235107\n",
      "epoch 74, iter 0, loss 1.1150593757629395\n",
      "epoch 74, iter 1, loss 1.1194349527359009\n",
      "epoch 74, iter 2, loss 1.114202618598938\n",
      "epoch 74, loss 3.3486969470977783\n",
      "epoch 75, iter 0, loss 1.071800708770752\n",
      "epoch 75, iter 1, loss 1.1593666076660156\n",
      "epoch 75, iter 2, loss 1.153165578842163\n",
      "epoch 75, loss 3.3843328952789307\n",
      "epoch 76, iter 0, loss 1.1032590866088867\n",
      "epoch 76, iter 1, loss 1.1386734247207642\n",
      "epoch 76, iter 2, loss 1.0506994724273682\n",
      "epoch 76, loss 3.292631983757019\n",
      "epoch 77, iter 0, loss 1.1391009092330933\n",
      "epoch 77, iter 1, loss 1.093562126159668\n",
      "epoch 77, iter 2, loss 1.1105865240097046\n",
      "epoch 77, loss 3.343249559402466\n",
      "epoch 78, iter 0, loss 1.1087876558303833\n",
      "epoch 78, iter 1, loss 1.1114497184753418\n",
      "epoch 78, iter 2, loss 1.2236801385879517\n",
      "epoch 78, loss 3.4439175128936768\n",
      "epoch 79, iter 0, loss 1.1251895427703857\n",
      "epoch 79, iter 1, loss 1.1200706958770752\n",
      "epoch 79, iter 2, loss 1.1655097007751465\n",
      "epoch 79, loss 3.4107699394226074\n",
      "epoch 80, iter 0, loss 1.1174145936965942\n",
      "epoch 80, iter 1, loss 1.1327247619628906\n",
      "epoch 80, iter 2, loss 1.1444445848464966\n",
      "epoch 80, loss 3.3945839405059814\n",
      "epoch 81, iter 0, loss 1.1362438201904297\n",
      "epoch 81, iter 1, loss 1.1256136894226074\n",
      "epoch 81, iter 2, loss 1.1963717937469482\n",
      "epoch 81, loss 3.4582293033599854\n",
      "epoch 82, iter 0, loss 1.144763708114624\n",
      "epoch 82, iter 1, loss 1.1315592527389526\n",
      "epoch 82, iter 2, loss 1.0051944255828857\n",
      "epoch 82, loss 3.2815173864364624\n",
      "epoch 83, iter 0, loss 1.129887580871582\n",
      "epoch 83, iter 1, loss 1.1185269355773926\n",
      "epoch 83, iter 2, loss 1.1256258487701416\n",
      "epoch 83, loss 3.374040365219116\n",
      "epoch 84, iter 0, loss 1.1190282106399536\n",
      "epoch 84, iter 1, loss 1.134857416152954\n",
      "epoch 84, iter 2, loss 1.0388630628585815\n",
      "epoch 84, loss 3.2927486896514893\n",
      "epoch 85, iter 0, loss 1.1534662246704102\n",
      "epoch 85, iter 1, loss 1.072020411491394\n",
      "epoch 85, iter 2, loss 1.141772985458374\n",
      "epoch 85, loss 3.3672596216201782\n",
      "epoch 86, iter 0, loss 1.095552921295166\n",
      "epoch 86, iter 1, loss 1.156930923461914\n",
      "epoch 86, iter 2, loss 1.0795272588729858\n",
      "epoch 86, loss 3.332011103630066\n",
      "epoch 87, iter 0, loss 1.1090264320373535\n",
      "epoch 87, iter 1, loss 1.1290526390075684\n",
      "epoch 87, iter 2, loss 1.1595206260681152\n",
      "epoch 87, loss 3.397599697113037\n",
      "epoch 88, iter 0, loss 1.1710926294326782\n",
      "epoch 88, iter 1, loss 1.0751869678497314\n",
      "epoch 88, iter 2, loss 1.0965255498886108\n",
      "epoch 88, loss 3.3428051471710205\n",
      "epoch 89, iter 0, loss 1.1382321119308472\n",
      "epoch 89, iter 1, loss 1.1054177284240723\n",
      "epoch 89, iter 2, loss 1.0473262071609497\n",
      "epoch 89, loss 3.290976047515869\n",
      "epoch 90, iter 0, loss 1.1266717910766602\n",
      "epoch 90, iter 1, loss 1.093083381652832\n",
      "epoch 90, iter 2, loss 1.121246576309204\n",
      "epoch 90, loss 3.3410017490386963\n",
      "epoch 91, iter 0, loss 1.0921900272369385\n",
      "epoch 91, iter 1, loss 1.1323702335357666\n",
      "epoch 91, iter 2, loss 1.1173551082611084\n",
      "epoch 91, loss 3.3419153690338135\n",
      "epoch 92, iter 0, loss 1.1213793754577637\n",
      "epoch 92, iter 1, loss 1.1015398502349854\n",
      "epoch 92, iter 2, loss 1.11170494556427\n",
      "epoch 92, loss 3.334624171257019\n",
      "epoch 93, iter 0, loss 1.0500211715698242\n",
      "epoch 93, iter 1, loss 1.1564687490463257\n",
      "epoch 93, iter 2, loss 1.1524288654327393\n",
      "epoch 93, loss 3.358918786048889\n",
      "epoch 94, iter 0, loss 1.0853163003921509\n",
      "epoch 94, iter 1, loss 1.158010721206665\n",
      "epoch 94, iter 2, loss 1.0464497804641724\n",
      "epoch 94, loss 3.2897768020629883\n",
      "epoch 95, iter 0, loss 1.0937659740447998\n",
      "epoch 95, iter 1, loss 1.1346485614776611\n",
      "epoch 95, iter 2, loss 1.0908368825912476\n",
      "epoch 95, loss 3.3192514181137085\n",
      "epoch 96, iter 0, loss 1.1105877161026\n",
      "epoch 96, iter 1, loss 1.1243500709533691\n",
      "epoch 96, iter 2, loss 1.1046720743179321\n",
      "epoch 96, loss 3.3396098613739014\n",
      "epoch 97, iter 0, loss 1.1248936653137207\n",
      "epoch 97, iter 1, loss 1.1090223789215088\n",
      "epoch 97, iter 2, loss 1.0532855987548828\n",
      "epoch 97, loss 3.2872016429901123\n",
      "epoch 98, iter 0, loss 1.1167216300964355\n",
      "epoch 98, iter 1, loss 1.1031497716903687\n",
      "epoch 98, iter 2, loss 1.1639397144317627\n",
      "epoch 98, loss 3.383811116218567\n",
      "epoch 99, iter 0, loss 1.089974284172058\n",
      "epoch 99, iter 1, loss 1.1281858682632446\n",
      "epoch 99, iter 2, loss 1.1615421772003174\n",
      "epoch 99, loss 3.37970232963562\n",
      "2019-03-30 23:55:47.326519, fold=1, rep=1, eta=0d 0h 17m 16s \n",
      "{'fold': 1, 'repeat': 1, 'n': 3338, 'd': 17, 'mse': 0.43177589774131775, 'train_time': 501.7399397029076, 'trained_epochs': 100, 'prior_train_nmll': 1.105574369430542, 'train_nll': 2193.661376953125, 'test_nll': 1117.6441650390625, 'train_mse': 0.4064301550388336, 'state_dict_file': 'model_state_dict_1744517072698489241.pkl'}\n",
      "epoch 0, iter 0, loss 1.9709030389785767\n",
      "epoch 0, iter 1, loss 2.1872200965881348\n",
      "epoch 0, iter 2, loss 2.3902485370635986\n",
      "epoch 0, loss 6.54837167263031\n",
      "epoch 1, iter 0, loss 2.146911144256592\n",
      "epoch 1, iter 1, loss 2.0635664463043213\n",
      "epoch 1, iter 2, loss 2.0205979347229004\n",
      "epoch 1, loss 6.2310755252838135\n",
      "epoch 2, iter 0, loss 1.9102668762207031\n",
      "epoch 2, iter 1, loss 1.8484450578689575\n",
      "epoch 2, iter 2, loss 1.8679301738739014\n",
      "epoch 2, loss 5.626642107963562\n",
      "epoch 3, iter 0, loss 1.766814112663269\n",
      "epoch 3, iter 1, loss 1.6906983852386475\n",
      "epoch 3, iter 2, loss 1.5939768552780151\n",
      "epoch 3, loss 5.051489353179932\n",
      "epoch 4, iter 0, loss 1.650750756263733\n",
      "epoch 4, iter 1, loss 1.663170337677002\n",
      "epoch 4, iter 2, loss 1.6101669073104858\n",
      "epoch 4, loss 4.924088001251221\n",
      "epoch 5, iter 0, loss 1.59880793094635\n",
      "epoch 5, iter 1, loss 1.5979636907577515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, iter 2, loss 1.5554578304290771\n",
      "epoch 5, loss 4.752229452133179\n",
      "epoch 6, iter 0, loss 1.550818681716919\n",
      "epoch 6, iter 1, loss 1.5357047319412231\n",
      "epoch 6, iter 2, loss 1.5967305898666382\n",
      "epoch 6, loss 4.68325400352478\n",
      "epoch 7, iter 0, loss 1.5158904790878296\n",
      "epoch 7, iter 1, loss 1.5101253986358643\n",
      "epoch 7, iter 2, loss 1.487189531326294\n",
      "epoch 7, loss 4.513205409049988\n",
      "epoch 8, iter 0, loss 1.474258303642273\n",
      "epoch 8, iter 1, loss 1.4742487668991089\n",
      "epoch 8, iter 2, loss 1.4292268753051758\n",
      "epoch 8, loss 4.377733945846558\n",
      "epoch 9, iter 0, loss 1.4458749294281006\n",
      "epoch 9, iter 1, loss 1.434617042541504\n",
      "epoch 9, iter 2, loss 1.4138550758361816\n",
      "epoch 9, loss 4.294347047805786\n",
      "epoch 10, iter 0, loss 1.4147101640701294\n",
      "epoch 10, iter 1, loss 1.396045207977295\n",
      "epoch 10, iter 2, loss 1.3517032861709595\n",
      "epoch 10, loss 4.162458658218384\n",
      "epoch 11, iter 0, loss 1.3795297145843506\n",
      "epoch 11, iter 1, loss 1.3520197868347168\n",
      "epoch 11, iter 2, loss 1.3249489068984985\n",
      "epoch 11, loss 4.056498408317566\n",
      "epoch 12, iter 0, loss 1.3234670162200928\n",
      "epoch 12, iter 1, loss 1.3286864757537842\n",
      "epoch 12, iter 2, loss 1.3004416227340698\n",
      "epoch 12, loss 3.9525951147079468\n",
      "epoch 13, iter 0, loss 1.2911014556884766\n",
      "epoch 13, iter 1, loss 1.284099817276001\n",
      "epoch 13, iter 2, loss 1.2841579914093018\n",
      "epoch 13, loss 3.8593592643737793\n",
      "epoch 14, iter 0, loss 1.2785946130752563\n",
      "epoch 14, iter 1, loss 1.2449883222579956\n",
      "epoch 14, iter 2, loss 1.192670464515686\n",
      "epoch 14, loss 3.716253399848938\n",
      "epoch 15, iter 0, loss 1.243955135345459\n",
      "epoch 15, iter 1, loss 1.2354419231414795\n",
      "epoch 15, iter 2, loss 1.1997088193893433\n",
      "epoch 15, loss 3.6791058778762817\n",
      "epoch 16, iter 0, loss 1.2311623096466064\n",
      "epoch 16, iter 1, loss 1.193612813949585\n",
      "epoch 16, iter 2, loss 1.356511116027832\n",
      "epoch 16, loss 3.7812862396240234\n",
      "epoch 17, iter 0, loss 1.2127506732940674\n",
      "epoch 17, iter 1, loss 1.2348065376281738\n",
      "epoch 17, iter 2, loss 1.2215533256530762\n",
      "epoch 17, loss 3.6691105365753174\n",
      "epoch 18, iter 0, loss 1.2153830528259277\n",
      "epoch 18, iter 1, loss 1.2307175397872925\n",
      "epoch 18, iter 2, loss 1.2148816585540771\n",
      "epoch 18, loss 3.6609822511672974\n",
      "epoch 19, iter 0, loss 1.2091046571731567\n",
      "epoch 19, iter 1, loss 1.2335386276245117\n",
      "epoch 19, iter 2, loss 1.211617350578308\n",
      "epoch 19, loss 3.6542606353759766\n",
      "epoch 20, iter 0, loss 1.1998071670532227\n",
      "epoch 20, iter 1, loss 1.2367935180664062\n",
      "epoch 20, iter 2, loss 1.187828779220581\n",
      "epoch 20, loss 3.62442946434021\n",
      "epoch 21, iter 0, loss 1.2272284030914307\n",
      "epoch 21, iter 1, loss 1.1980834007263184\n",
      "epoch 21, iter 2, loss 1.1676886081695557\n",
      "epoch 21, loss 3.5930004119873047\n",
      "epoch 22, iter 0, loss 1.2124090194702148\n",
      "epoch 22, iter 1, loss 1.2120591402053833\n",
      "epoch 22, iter 2, loss 1.105531096458435\n",
      "epoch 22, loss 3.529999256134033\n",
      "epoch 23, iter 0, loss 1.2284889221191406\n",
      "epoch 23, iter 1, loss 1.164817452430725\n",
      "epoch 23, iter 2, loss 1.1986768245697021\n",
      "epoch 23, loss 3.591983199119568\n",
      "epoch 24, iter 0, loss 1.2170411348342896\n",
      "epoch 24, iter 1, loss 1.1629031896591187\n",
      "epoch 24, iter 2, loss 1.2372890710830688\n",
      "epoch 24, loss 3.617233395576477\n",
      "epoch 25, iter 0, loss 1.180643916130066\n",
      "epoch 25, iter 1, loss 1.1741139888763428\n",
      "epoch 25, iter 2, loss 1.3101005554199219\n",
      "epoch 25, loss 3.6648584604263306\n",
      "epoch 26, iter 0, loss 1.1855896711349487\n",
      "epoch 26, iter 1, loss 1.1784205436706543\n",
      "epoch 26, iter 2, loss 1.1452943086624146\n",
      "epoch 26, loss 3.5093045234680176\n",
      "epoch 27, iter 0, loss 1.159250259399414\n",
      "epoch 27, iter 1, loss 1.1987440586090088\n",
      "epoch 27, iter 2, loss 1.1350603103637695\n",
      "epoch 27, loss 3.4930546283721924\n",
      "epoch 28, iter 0, loss 1.1480077505111694\n",
      "epoch 28, iter 1, loss 1.1862751245498657\n",
      "epoch 28, iter 2, loss 1.1617028713226318\n",
      "epoch 28, loss 3.495985746383667\n",
      "epoch 29, iter 0, loss 1.1413419246673584\n",
      "epoch 29, iter 1, loss 1.1717679500579834\n",
      "epoch 29, iter 2, loss 1.1744260787963867\n",
      "epoch 29, loss 3.4875359535217285\n",
      "epoch 30, iter 0, loss 1.1738743782043457\n",
      "epoch 30, iter 1, loss 1.154389500617981\n",
      "epoch 30, iter 2, loss 1.1597747802734375\n",
      "epoch 30, loss 3.488038659095764\n",
      "epoch 31, iter 0, loss 1.1616593599319458\n",
      "epoch 31, iter 1, loss 1.1517181396484375\n",
      "epoch 31, iter 2, loss 1.1880269050598145\n",
      "epoch 31, loss 3.5014044046401978\n",
      "epoch 32, iter 0, loss 1.1521942615509033\n",
      "epoch 32, iter 1, loss 1.1505064964294434\n",
      "epoch 32, iter 2, loss 1.1755523681640625\n",
      "epoch 32, loss 3.478253126144409\n",
      "epoch 33, iter 0, loss 1.1315630674362183\n",
      "epoch 33, iter 1, loss 1.1725281476974487\n",
      "epoch 33, iter 2, loss 1.1989153623580933\n",
      "epoch 33, loss 3.5030065774917603\n",
      "epoch 34, iter 0, loss 1.1284316778182983\n",
      "epoch 34, iter 1, loss 1.1854429244995117\n",
      "epoch 34, iter 2, loss 1.0905749797821045\n",
      "epoch 34, loss 3.4044495820999146\n",
      "epoch 35, iter 0, loss 1.1433205604553223\n",
      "epoch 35, iter 1, loss 1.1766504049301147\n",
      "epoch 35, iter 2, loss 1.1214097738265991\n",
      "epoch 35, loss 3.441380739212036\n",
      "epoch 36, iter 0, loss 1.161695122718811\n",
      "epoch 36, iter 1, loss 1.1599839925765991\n",
      "epoch 36, iter 2, loss 1.1323316097259521\n",
      "epoch 36, loss 3.4540107250213623\n",
      "epoch 37, iter 0, loss 1.151453971862793\n",
      "epoch 37, iter 1, loss 1.1504648923873901\n",
      "epoch 37, iter 2, loss 1.221540927886963\n",
      "epoch 37, loss 3.523459792137146\n",
      "epoch 38, iter 0, loss 1.1670714616775513\n",
      "epoch 38, iter 1, loss 1.1508890390396118\n",
      "epoch 38, iter 2, loss 1.1479605436325073\n",
      "epoch 38, loss 3.4659210443496704\n",
      "epoch 39, iter 0, loss 1.169450044631958\n",
      "epoch 39, iter 1, loss 1.1421610116958618\n",
      "epoch 39, iter 2, loss 1.173780083656311\n",
      "epoch 39, loss 3.485391139984131\n",
      "epoch 40, iter 0, loss 1.1227922439575195\n",
      "epoch 40, iter 1, loss 1.1488381624221802\n",
      "epoch 40, iter 2, loss 1.288125991821289\n",
      "epoch 40, loss 3.5597563982009888\n",
      "epoch 41, iter 0, loss 1.1735700368881226\n",
      "epoch 41, iter 1, loss 1.124751091003418\n",
      "epoch 41, iter 2, loss 1.1497559547424316\n",
      "epoch 41, loss 3.448077082633972\n",
      "epoch 42, iter 0, loss 1.1308236122131348\n",
      "epoch 42, iter 1, loss 1.178073763847351\n",
      "epoch 42, iter 2, loss 1.1122324466705322\n",
      "epoch 42, loss 3.421129822731018\n",
      "epoch 43, iter 0, loss 1.1702295541763306\n",
      "epoch 43, iter 1, loss 1.1387690305709839\n",
      "epoch 43, iter 2, loss 1.1091251373291016\n",
      "epoch 43, loss 3.418123722076416\n",
      "epoch 44, iter 0, loss 1.1301132440567017\n",
      "epoch 44, iter 1, loss 1.139121651649475\n",
      "epoch 44, iter 2, loss 1.2734324932098389\n",
      "epoch 44, loss 3.5426673889160156\n",
      "epoch 45, iter 0, loss 1.1296820640563965\n",
      "epoch 45, iter 1, loss 1.1452947854995728\n",
      "epoch 45, iter 2, loss 1.1892595291137695\n",
      "epoch 45, loss 3.4642363786697388\n",
      "epoch 46, iter 0, loss 1.1842622756958008\n",
      "epoch 46, iter 1, loss 1.1078550815582275\n",
      "epoch 46, iter 2, loss 1.1330921649932861\n",
      "epoch 46, loss 3.4252095222473145\n",
      "epoch 47, iter 0, loss 1.1499278545379639\n",
      "epoch 47, iter 1, loss 1.1315076351165771\n",
      "epoch 47, iter 2, loss 1.1815524101257324\n",
      "epoch 47, loss 3.4629878997802734\n",
      "epoch 48, iter 0, loss 1.1305317878723145\n",
      "epoch 48, iter 1, loss 1.1569470167160034\n",
      "epoch 48, iter 2, loss 1.0760407447814941\n",
      "epoch 48, loss 3.363519549369812\n",
      "epoch 49, iter 0, loss 1.1589548587799072\n",
      "epoch 49, iter 1, loss 1.1305209398269653\n",
      "epoch 49, iter 2, loss 1.1054670810699463\n",
      "epoch 49, loss 3.394942879676819\n",
      "epoch 50, iter 0, loss 1.1195006370544434\n",
      "epoch 50, iter 1, loss 1.1510754823684692\n",
      "epoch 50, iter 2, loss 1.1800158023834229\n",
      "epoch 50, loss 3.4505919218063354\n",
      "epoch 51, iter 0, loss 1.1516753435134888\n",
      "epoch 51, iter 1, loss 1.1272822618484497\n",
      "epoch 51, iter 2, loss 1.1258862018585205\n",
      "epoch 51, loss 3.404843807220459\n",
      "epoch 52, iter 0, loss 1.1547966003417969\n",
      "epoch 52, iter 1, loss 1.1257174015045166\n",
      "epoch 52, iter 2, loss 1.1138249635696411\n",
      "epoch 52, loss 3.3943389654159546\n",
      "epoch 53, iter 0, loss 1.1143778562545776\n",
      "epoch 53, iter 1, loss 1.162319302558899\n",
      "epoch 53, iter 2, loss 1.1097867488861084\n",
      "epoch 53, loss 3.386483907699585\n",
      "epoch 54, iter 0, loss 1.1102083921432495\n",
      "epoch 54, iter 1, loss 1.1616042852401733\n",
      "epoch 54, iter 2, loss 1.151494026184082\n",
      "epoch 54, loss 3.423306703567505\n",
      "epoch 55, iter 0, loss 1.116862177848816\n",
      "epoch 55, iter 1, loss 1.1520683765411377\n",
      "epoch 55, iter 2, loss 1.1753575801849365\n",
      "epoch 55, loss 3.44428813457489\n",
      "epoch 56, iter 0, loss 1.137204885482788\n",
      "epoch 56, iter 1, loss 1.1195366382598877\n",
      "epoch 56, iter 2, loss 1.1501827239990234\n",
      "epoch 56, loss 3.406924247741699\n",
      "epoch 57, iter 0, loss 1.1613823175430298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57, iter 1, loss 1.1175446510314941\n",
      "epoch 57, iter 2, loss 0.9995443224906921\n",
      "epoch 57, loss 3.278471291065216\n",
      "epoch 58, iter 0, loss 1.1523966789245605\n",
      "epoch 58, iter 1, loss 1.1121708154678345\n",
      "epoch 58, iter 2, loss 1.07130765914917\n",
      "epoch 58, loss 3.335875153541565\n",
      "epoch 59, iter 0, loss 1.0969783067703247\n",
      "epoch 59, iter 1, loss 1.1612868309020996\n",
      "epoch 59, iter 2, loss 1.0780576467514038\n",
      "epoch 59, loss 3.336322784423828\n",
      "epoch 60, iter 0, loss 1.1385586261749268\n",
      "epoch 60, iter 1, loss 1.1193171739578247\n",
      "epoch 60, iter 2, loss 1.0995930433273315\n",
      "epoch 60, loss 3.357468843460083\n",
      "epoch 61, iter 0, loss 1.1164312362670898\n",
      "epoch 61, iter 1, loss 1.1355034112930298\n",
      "epoch 61, iter 2, loss 1.1211798191070557\n",
      "epoch 61, loss 3.3731144666671753\n",
      "epoch 62, iter 0, loss 1.1577081680297852\n",
      "epoch 62, iter 1, loss 1.1216539144515991\n",
      "epoch 62, iter 2, loss 1.0441023111343384\n",
      "epoch 62, loss 3.3234643936157227\n",
      "epoch 63, iter 0, loss 1.1305654048919678\n",
      "epoch 63, iter 1, loss 1.127502679824829\n",
      "epoch 63, iter 2, loss 1.116262435913086\n",
      "epoch 63, loss 3.374330520629883\n",
      "epoch 64, iter 0, loss 1.1748970746994019\n",
      "epoch 64, iter 1, loss 1.101325273513794\n",
      "epoch 64, iter 2, loss 1.0390230417251587\n",
      "epoch 64, loss 3.3152453899383545\n",
      "epoch 65, iter 0, loss 1.1128431558609009\n",
      "epoch 65, iter 1, loss 1.1454259157180786\n",
      "epoch 65, iter 2, loss 1.080483078956604\n",
      "epoch 65, loss 3.3387521505355835\n",
      "epoch 66, iter 0, loss 1.093190312385559\n",
      "epoch 66, iter 1, loss 1.1344566345214844\n",
      "epoch 66, iter 2, loss 1.2112410068511963\n",
      "epoch 66, loss 3.4388879537582397\n",
      "epoch 67, iter 0, loss 1.1320642232894897\n",
      "epoch 67, iter 1, loss 1.1059224605560303\n",
      "epoch 67, iter 2, loss 1.1547373533248901\n",
      "epoch 67, loss 3.39272403717041\n",
      "epoch 68, iter 0, loss 1.1459602117538452\n",
      "epoch 68, iter 1, loss 1.1138851642608643\n",
      "epoch 68, iter 2, loss 1.0582504272460938\n",
      "epoch 68, loss 3.3180958032608032\n",
      "epoch 69, iter 0, loss 1.1372668743133545\n",
      "epoch 69, iter 1, loss 1.0949101448059082\n",
      "epoch 69, iter 2, loss 1.1250629425048828\n",
      "epoch 69, loss 3.3572399616241455\n",
      "epoch 70, iter 0, loss 1.1254441738128662\n",
      "epoch 70, iter 1, loss 1.127814769744873\n",
      "epoch 70, iter 2, loss 1.0917171239852905\n",
      "epoch 70, loss 3.34497606754303\n",
      "epoch 71, iter 0, loss 1.1199740171432495\n",
      "epoch 71, iter 1, loss 1.0958800315856934\n",
      "epoch 71, iter 2, loss 1.2871426343917847\n",
      "epoch 71, loss 3.5029966831207275\n",
      "epoch 72, iter 0, loss 1.126168966293335\n",
      "epoch 72, iter 1, loss 1.1104393005371094\n",
      "epoch 72, iter 2, loss 1.0738637447357178\n",
      "epoch 72, loss 3.310472011566162\n",
      "epoch 73, iter 0, loss 1.0978949069976807\n",
      "epoch 73, iter 1, loss 1.1178364753723145\n",
      "epoch 73, iter 2, loss 1.2801522016525269\n",
      "epoch 73, loss 3.495883584022522\n",
      "epoch 74, iter 0, loss 1.1441500186920166\n",
      "epoch 74, iter 1, loss 1.1046029329299927\n",
      "epoch 74, iter 2, loss 1.1414252519607544\n",
      "epoch 74, loss 3.3901782035827637\n",
      "epoch 75, iter 0, loss 1.1174592971801758\n",
      "epoch 75, iter 1, loss 1.1316434144973755\n",
      "epoch 75, iter 2, loss 1.100341558456421\n",
      "epoch 75, loss 3.349444270133972\n",
      "epoch 76, iter 0, loss 1.1425774097442627\n",
      "epoch 76, iter 1, loss 1.1133512258529663\n",
      "epoch 76, iter 2, loss 1.0859417915344238\n",
      "epoch 76, loss 3.341870427131653\n",
      "epoch 77, iter 0, loss 1.118362307548523\n",
      "epoch 77, iter 1, loss 1.1320403814315796\n",
      "epoch 77, iter 2, loss 1.0983628034591675\n",
      "epoch 77, loss 3.34876549243927\n",
      "epoch 78, iter 0, loss 1.1100605726242065\n",
      "epoch 78, iter 1, loss 1.1223738193511963\n",
      "epoch 78, iter 2, loss 1.1125717163085938\n",
      "epoch 78, loss 3.3450061082839966\n",
      "epoch 79, iter 0, loss 1.1436775922775269\n",
      "epoch 79, iter 1, loss 1.0933520793914795\n",
      "epoch 79, iter 2, loss 1.1056079864501953\n",
      "epoch 79, loss 3.3426376581192017\n",
      "epoch 80, iter 0, loss 1.1228022575378418\n",
      "epoch 80, iter 1, loss 1.1157715320587158\n",
      "epoch 80, iter 2, loss 1.060508131980896\n",
      "epoch 80, loss 3.2990819215774536\n",
      "epoch 81, iter 0, loss 1.104446530342102\n",
      "epoch 81, iter 1, loss 1.1284449100494385\n",
      "epoch 81, iter 2, loss 1.138380527496338\n",
      "epoch 81, loss 3.3712719678878784\n",
      "epoch 82, iter 0, loss 1.1176460981369019\n",
      "epoch 82, iter 1, loss 1.1304242610931396\n",
      "epoch 82, iter 2, loss 1.0639983415603638\n",
      "epoch 82, loss 3.3120687007904053\n",
      "epoch 83, iter 0, loss 1.1093274354934692\n",
      "epoch 83, iter 1, loss 1.1159522533416748\n",
      "epoch 83, iter 2, loss 1.1825876235961914\n",
      "epoch 83, loss 3.4078673124313354\n",
      "epoch 84, iter 0, loss 1.146724820137024\n",
      "epoch 84, iter 1, loss 1.0984188318252563\n",
      "epoch 84, iter 2, loss 1.063847303390503\n",
      "epoch 84, loss 3.308990955352783\n",
      "epoch 85, iter 0, loss 1.1254860162734985\n",
      "epoch 85, iter 1, loss 1.115557312965393\n",
      "epoch 85, iter 2, loss 1.0747625827789307\n",
      "epoch 85, loss 3.3158059120178223\n",
      "epoch 86, iter 0, loss 1.146146535873413\n",
      "epoch 86, iter 1, loss 1.0931875705718994\n",
      "epoch 86, iter 2, loss 1.108187198638916\n",
      "epoch 86, loss 3.3475213050842285\n",
      "epoch 87, iter 0, loss 1.1251193284988403\n",
      "epoch 87, iter 1, loss 1.1103266477584839\n",
      "epoch 87, iter 2, loss 1.0992225408554077\n",
      "epoch 87, loss 3.334668517112732\n",
      "epoch 88, iter 0, loss 1.1032956838607788\n",
      "epoch 88, iter 1, loss 1.13754403591156\n",
      "epoch 88, iter 2, loss 1.0440394878387451\n",
      "epoch 88, loss 3.284879207611084\n",
      "epoch 89, iter 0, loss 1.113712191581726\n",
      "epoch 89, iter 1, loss 1.1184158325195312\n",
      "epoch 89, iter 2, loss 1.1284520626068115\n",
      "epoch 89, loss 3.360580086708069\n",
      "epoch 90, iter 0, loss 1.11831796169281\n",
      "epoch 90, iter 1, loss 1.1031057834625244\n",
      "epoch 90, iter 2, loss 1.079392910003662\n",
      "epoch 90, loss 3.3008166551589966\n",
      "epoch 91, iter 0, loss 1.097558617591858\n",
      "epoch 91, iter 1, loss 1.1277838945388794\n",
      "epoch 91, iter 2, loss 1.1293432712554932\n",
      "epoch 91, loss 3.3546857833862305\n",
      "epoch 92, iter 0, loss 1.1370548009872437\n",
      "epoch 92, iter 1, loss 1.0781872272491455\n",
      "epoch 92, iter 2, loss 1.091099500656128\n",
      "epoch 92, loss 3.306341528892517\n",
      "epoch 93, iter 0, loss 1.1035830974578857\n",
      "epoch 93, iter 1, loss 1.1035033464431763\n",
      "epoch 93, iter 2, loss 1.1484508514404297\n",
      "epoch 93, loss 3.3555372953414917\n",
      "epoch 94, iter 0, loss 1.1247152090072632\n",
      "epoch 94, iter 1, loss 1.1134672164916992\n",
      "epoch 94, iter 2, loss 1.020149827003479\n",
      "epoch 94, loss 3.2583322525024414\n",
      "epoch 95, iter 0, loss 1.1109117269515991\n",
      "epoch 95, iter 1, loss 1.0972702503204346\n",
      "epoch 95, iter 2, loss 1.181612491607666\n",
      "epoch 95, loss 3.3897944688796997\n",
      "epoch 96, iter 0, loss 1.124456524848938\n",
      "epoch 96, iter 1, loss 1.1086103916168213\n",
      "epoch 96, iter 2, loss 1.0718196630477905\n",
      "epoch 96, loss 3.30488657951355\n",
      "epoch 97, iter 0, loss 1.128943920135498\n",
      "epoch 97, iter 1, loss 1.0945885181427002\n",
      "epoch 97, iter 2, loss 1.1382206678390503\n",
      "epoch 97, loss 3.3617531061172485\n",
      "epoch 98, iter 0, loss 1.0978741645812988\n",
      "epoch 98, iter 1, loss 1.140918254852295\n",
      "epoch 98, iter 2, loss 1.1499030590057373\n",
      "epoch 98, loss 3.388695478439331\n",
      "epoch 99, iter 0, loss 1.135995864868164\n",
      "epoch 99, iter 1, loss 1.093867540359497\n",
      "epoch 99, iter 2, loss 1.1002569198608398\n",
      "epoch 99, loss 3.330120325088501\n",
      "2019-03-31 00:04:07.711932, fold=2, rep=0, eta=0d 0h 8m 34s \n",
      "{'fold': 2, 'repeat': 0, 'n': 3338, 'd': 17, 'mse': 0.42822009325027466, 'train_time': 499.1450141130481, 'trained_epochs': 100, 'prior_train_nmll': 1.1138765811920166, 'train_nll': 2190.985107421875, 'test_nll': 1114.490478515625, 'train_mse': 0.4063141644001007, 'state_dict_file': 'model_state_dict_8166032515142180698.pkl'}\n",
      "epoch 0, iter 0, loss 1.9394961595535278\n",
      "epoch 0, iter 1, loss 2.240225315093994\n",
      "epoch 0, iter 2, loss 2.2791457176208496\n",
      "epoch 0, loss 6.458867192268372\n",
      "epoch 1, iter 0, loss 2.136955499649048\n",
      "epoch 1, iter 1, loss 2.072038412094116\n",
      "epoch 1, iter 2, loss 2.0169010162353516\n",
      "epoch 1, loss 6.225894927978516\n",
      "epoch 2, iter 0, loss 1.9142202138900757\n",
      "epoch 2, iter 1, loss 1.833113431930542\n",
      "epoch 2, iter 2, loss 1.7671446800231934\n",
      "epoch 2, loss 5.514478325843811\n",
      "epoch 3, iter 0, loss 1.7495170831680298\n",
      "epoch 3, iter 1, loss 1.6901803016662598\n",
      "epoch 3, iter 2, loss 1.5747655630111694\n",
      "epoch 3, loss 5.014462947845459\n",
      "epoch 4, iter 0, loss 1.6520358324050903\n",
      "epoch 4, iter 1, loss 1.6522071361541748\n",
      "epoch 4, iter 2, loss 1.6079121828079224\n",
      "epoch 4, loss 4.9121551513671875\n",
      "epoch 5, iter 0, loss 1.5751155614852905\n",
      "epoch 5, iter 1, loss 1.5987937450408936\n",
      "epoch 5, iter 2, loss 1.5950608253479004\n",
      "epoch 5, loss 4.7689701318740845\n",
      "epoch 6, iter 0, loss 1.5348939895629883\n",
      "epoch 6, iter 1, loss 1.5512443780899048\n",
      "epoch 6, iter 2, loss 1.5379626750946045\n",
      "epoch 6, loss 4.624101042747498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, iter 0, loss 1.5288509130477905\n",
      "epoch 7, iter 1, loss 1.4910427331924438\n",
      "epoch 7, iter 2, loss 1.4581584930419922\n",
      "epoch 7, loss 4.478052139282227\n",
      "epoch 8, iter 0, loss 1.4611963033676147\n",
      "epoch 8, iter 1, loss 1.472192645072937\n",
      "epoch 8, iter 2, loss 1.456992506980896\n",
      "epoch 8, loss 4.390381455421448\n",
      "epoch 9, iter 0, loss 1.4132119417190552\n",
      "epoch 9, iter 1, loss 1.4494962692260742\n",
      "epoch 9, iter 2, loss 1.437567949295044\n",
      "epoch 9, loss 4.300276160240173\n",
      "epoch 10, iter 0, loss 1.3951061964035034\n",
      "epoch 10, iter 1, loss 1.3942530155181885\n",
      "epoch 10, iter 2, loss 1.3591358661651611\n",
      "epoch 10, loss 4.148495078086853\n",
      "epoch 11, iter 0, loss 1.365604043006897\n",
      "epoch 11, iter 1, loss 1.3476840257644653\n",
      "epoch 11, iter 2, loss 1.347918152809143\n",
      "epoch 11, loss 4.061206221580505\n",
      "epoch 12, iter 0, loss 1.342388391494751\n",
      "epoch 12, iter 1, loss 1.2978054285049438\n",
      "epoch 12, iter 2, loss 1.272964596748352\n",
      "epoch 12, loss 3.913158416748047\n",
      "epoch 13, iter 0, loss 1.2808531522750854\n",
      "epoch 13, iter 1, loss 1.280959963798523\n",
      "epoch 13, iter 2, loss 1.2738096714019775\n",
      "epoch 13, loss 3.835622787475586\n",
      "epoch 14, iter 0, loss 1.2770366668701172\n",
      "epoch 14, iter 1, loss 1.221301555633545\n",
      "epoch 14, iter 2, loss 1.2691199779510498\n",
      "epoch 14, loss 3.767458200454712\n",
      "epoch 15, iter 0, loss 1.2181806564331055\n",
      "epoch 15, iter 1, loss 1.2616947889328003\n",
      "epoch 15, iter 2, loss 1.2122571468353271\n",
      "epoch 15, loss 3.692132592201233\n",
      "epoch 16, iter 0, loss 1.231610894203186\n",
      "epoch 16, iter 1, loss 1.2168171405792236\n",
      "epoch 16, iter 2, loss 1.2065448760986328\n",
      "epoch 16, loss 3.6549729108810425\n",
      "epoch 17, iter 0, loss 1.236034631729126\n",
      "epoch 17, iter 1, loss 1.1852402687072754\n",
      "epoch 17, iter 2, loss 1.2613377571105957\n",
      "epoch 17, loss 3.682612657546997\n",
      "epoch 18, iter 0, loss 1.1971397399902344\n",
      "epoch 18, iter 1, loss 1.2104815244674683\n",
      "epoch 18, iter 2, loss 1.3604700565338135\n",
      "epoch 18, loss 3.768091320991516\n",
      "epoch 19, iter 0, loss 1.213890790939331\n",
      "epoch 19, iter 1, loss 1.2290934324264526\n",
      "epoch 19, iter 2, loss 1.1325042247772217\n",
      "epoch 19, loss 3.5754884481430054\n",
      "epoch 20, iter 0, loss 1.2413276433944702\n",
      "epoch 20, iter 1, loss 1.1773536205291748\n",
      "epoch 20, iter 2, loss 1.2337676286697388\n",
      "epoch 20, loss 3.652448892593384\n",
      "epoch 21, iter 0, loss 1.2119483947753906\n",
      "epoch 21, iter 1, loss 1.2056431770324707\n",
      "epoch 21, iter 2, loss 1.1795119047164917\n",
      "epoch 21, loss 3.597103476524353\n",
      "epoch 22, iter 0, loss 1.1946040391921997\n",
      "epoch 22, iter 1, loss 1.2046189308166504\n",
      "epoch 22, iter 2, loss 1.2303838729858398\n",
      "epoch 22, loss 3.62960684299469\n",
      "epoch 23, iter 0, loss 1.1902813911437988\n",
      "epoch 23, iter 1, loss 1.2088310718536377\n",
      "epoch 23, iter 2, loss 1.1361340284347534\n",
      "epoch 23, loss 3.53524649143219\n",
      "epoch 24, iter 0, loss 1.193772315979004\n",
      "epoch 24, iter 1, loss 1.1713942289352417\n",
      "epoch 24, iter 2, loss 1.2378064393997192\n",
      "epoch 24, loss 3.602972984313965\n",
      "epoch 25, iter 0, loss 1.1761916875839233\n",
      "epoch 25, iter 1, loss 1.1770069599151611\n",
      "epoch 25, iter 2, loss 1.1901366710662842\n",
      "epoch 25, loss 3.5433353185653687\n",
      "epoch 26, iter 0, loss 1.2063730955123901\n",
      "epoch 26, iter 1, loss 1.150206208229065\n",
      "epoch 26, iter 2, loss 1.159303903579712\n",
      "epoch 26, loss 3.515883207321167\n",
      "epoch 27, iter 0, loss 1.1921367645263672\n",
      "epoch 27, iter 1, loss 1.1634095907211304\n",
      "epoch 27, iter 2, loss 1.1818467378616333\n",
      "epoch 27, loss 3.537393093109131\n",
      "epoch 28, iter 0, loss 1.1718717813491821\n",
      "epoch 28, iter 1, loss 1.1681935787200928\n",
      "epoch 28, iter 2, loss 1.1579670906066895\n",
      "epoch 28, loss 3.4980324506759644\n",
      "epoch 29, iter 0, loss 1.1625961065292358\n",
      "epoch 29, iter 1, loss 1.1604756116867065\n",
      "epoch 29, iter 2, loss 1.227174162864685\n",
      "epoch 29, loss 3.5502458810806274\n",
      "epoch 30, iter 0, loss 1.1875920295715332\n",
      "epoch 30, iter 1, loss 1.1620522737503052\n",
      "epoch 30, iter 2, loss 1.0131137371063232\n",
      "epoch 30, loss 3.3627580404281616\n",
      "epoch 31, iter 0, loss 1.144335389137268\n",
      "epoch 31, iter 1, loss 1.1799800395965576\n",
      "epoch 31, iter 2, loss 1.175977110862732\n",
      "epoch 31, loss 3.5002925395965576\n",
      "epoch 32, iter 0, loss 1.1623672246932983\n",
      "epoch 32, iter 1, loss 1.1384756565093994\n",
      "epoch 32, iter 2, loss 1.2104034423828125\n",
      "epoch 32, loss 3.5112463235855103\n",
      "epoch 33, iter 0, loss 1.1733529567718506\n",
      "epoch 33, iter 1, loss 1.1382718086242676\n",
      "epoch 33, iter 2, loss 1.1855567693710327\n",
      "epoch 33, loss 3.497181534767151\n",
      "epoch 34, iter 0, loss 1.1386414766311646\n",
      "epoch 34, iter 1, loss 1.1530930995941162\n",
      "epoch 34, iter 2, loss 1.2374569177627563\n",
      "epoch 34, loss 3.529191493988037\n",
      "epoch 35, iter 0, loss 1.1818008422851562\n",
      "epoch 35, iter 1, loss 1.1285130977630615\n",
      "epoch 35, iter 2, loss 1.1445317268371582\n",
      "epoch 35, loss 3.454845666885376\n",
      "epoch 36, iter 0, loss 1.1725209951400757\n",
      "epoch 36, iter 1, loss 1.1285380125045776\n",
      "epoch 36, iter 2, loss 1.1451458930969238\n",
      "epoch 36, loss 3.446204900741577\n",
      "epoch 37, iter 0, loss 1.1708171367645264\n",
      "epoch 37, iter 1, loss 1.1329337358474731\n",
      "epoch 37, iter 2, loss 1.1224541664123535\n",
      "epoch 37, loss 3.426205039024353\n",
      "epoch 38, iter 0, loss 1.1393033266067505\n",
      "epoch 38, iter 1, loss 1.1564918756484985\n",
      "epoch 38, iter 2, loss 1.1463313102722168\n",
      "epoch 38, loss 3.442126512527466\n",
      "epoch 39, iter 0, loss 1.1288245916366577\n",
      "epoch 39, iter 1, loss 1.163527011871338\n",
      "epoch 39, iter 2, loss 1.165820598602295\n",
      "epoch 39, loss 3.4581722021102905\n",
      "epoch 40, iter 0, loss 1.1954338550567627\n",
      "epoch 40, iter 1, loss 1.100449562072754\n",
      "epoch 40, iter 2, loss 1.086197853088379\n",
      "epoch 40, loss 3.3820812702178955\n",
      "epoch 41, iter 0, loss 1.1301207542419434\n",
      "epoch 41, iter 1, loss 1.1422019004821777\n",
      "epoch 41, iter 2, loss 1.2576606273651123\n",
      "epoch 41, loss 3.5299832820892334\n",
      "epoch 42, iter 0, loss 1.1249403953552246\n",
      "epoch 42, iter 1, loss 1.1426037549972534\n",
      "epoch 42, iter 2, loss 1.2371771335601807\n",
      "epoch 42, loss 3.5047212839126587\n",
      "epoch 43, iter 0, loss 1.1581001281738281\n",
      "epoch 43, iter 1, loss 1.1153215169906616\n",
      "epoch 43, iter 2, loss 1.3176565170288086\n",
      "epoch 43, loss 3.5910781621932983\n",
      "epoch 44, iter 0, loss 1.1717357635498047\n",
      "epoch 44, iter 1, loss 1.1195576190948486\n",
      "epoch 44, iter 2, loss 1.165366291999817\n",
      "epoch 44, loss 3.45665967464447\n",
      "epoch 45, iter 0, loss 1.1592909097671509\n",
      "epoch 45, iter 1, loss 1.1229451894760132\n",
      "epoch 45, iter 2, loss 1.1711846590042114\n",
      "epoch 45, loss 3.4534207582473755\n",
      "epoch 46, iter 0, loss 1.111176609992981\n",
      "epoch 46, iter 1, loss 1.1739310026168823\n",
      "epoch 46, iter 2, loss 1.1427536010742188\n",
      "epoch 46, loss 3.427861213684082\n",
      "epoch 47, iter 0, loss 1.137994408607483\n",
      "epoch 47, iter 1, loss 1.151039719581604\n",
      "epoch 47, iter 2, loss 1.1117759943008423\n",
      "epoch 47, loss 3.400810122489929\n",
      "epoch 48, iter 0, loss 1.101427435874939\n",
      "epoch 48, iter 1, loss 1.1569339036941528\n",
      "epoch 48, iter 2, loss 1.2083252668380737\n",
      "epoch 48, loss 3.4666866064071655\n",
      "epoch 49, iter 0, loss 1.1579961776733398\n",
      "epoch 49, iter 1, loss 1.1136157512664795\n",
      "epoch 49, iter 2, loss 1.1426608562469482\n",
      "epoch 49, loss 3.4142727851867676\n",
      "epoch 50, iter 0, loss 1.15398108959198\n",
      "epoch 50, iter 1, loss 1.1132451295852661\n",
      "epoch 50, iter 2, loss 1.1110159158706665\n",
      "epoch 50, loss 3.3782421350479126\n",
      "epoch 51, iter 0, loss 1.1593635082244873\n",
      "epoch 51, iter 1, loss 1.1226106882095337\n",
      "epoch 51, iter 2, loss 1.097482442855835\n",
      "epoch 51, loss 3.379456639289856\n",
      "epoch 52, iter 0, loss 1.1447335481643677\n",
      "epoch 52, iter 1, loss 1.1127979755401611\n",
      "epoch 52, iter 2, loss 1.250230073928833\n",
      "epoch 52, loss 3.507761597633362\n",
      "epoch 53, iter 0, loss 1.141844630241394\n",
      "epoch 53, iter 1, loss 1.1477712392807007\n",
      "epoch 53, iter 2, loss 1.1189733743667603\n",
      "epoch 53, loss 3.408589243888855\n",
      "epoch 54, iter 0, loss 1.161091923713684\n",
      "epoch 54, iter 1, loss 1.113369107246399\n",
      "epoch 54, iter 2, loss 1.1445105075836182\n",
      "epoch 54, loss 3.418971538543701\n",
      "epoch 55, iter 0, loss 1.164666771888733\n",
      "epoch 55, iter 1, loss 1.1106008291244507\n",
      "epoch 55, iter 2, loss 1.1581350564956665\n",
      "epoch 55, loss 3.43340265750885\n",
      "epoch 56, iter 0, loss 1.1182087659835815\n",
      "epoch 56, iter 1, loss 1.1548500061035156\n",
      "epoch 56, iter 2, loss 1.1415205001831055\n",
      "epoch 56, loss 3.4145792722702026\n",
      "epoch 57, iter 0, loss 1.0957273244857788\n",
      "epoch 57, iter 1, loss 1.171877145767212\n",
      "epoch 57, iter 2, loss 1.1850626468658447\n",
      "epoch 57, loss 3.4526671171188354\n",
      "epoch 58, iter 0, loss 1.1455496549606323\n",
      "epoch 58, iter 1, loss 1.1383174657821655\n",
      "epoch 58, iter 2, loss 1.0684630870819092\n",
      "epoch 58, loss 3.352330207824707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59, iter 0, loss 1.1498439311981201\n",
      "epoch 59, iter 1, loss 1.1223284006118774\n",
      "epoch 59, iter 2, loss 1.0468419790267944\n",
      "epoch 59, loss 3.319014310836792\n",
      "epoch 60, iter 0, loss 1.104854702949524\n",
      "epoch 60, iter 1, loss 1.163885474205017\n",
      "epoch 60, iter 2, loss 1.1301484107971191\n",
      "epoch 60, loss 3.39888858795166\n",
      "epoch 61, iter 0, loss 1.1303431987762451\n",
      "epoch 61, iter 1, loss 1.1297472715377808\n",
      "epoch 61, iter 2, loss 1.1143125295639038\n",
      "epoch 61, loss 3.3744029998779297\n",
      "epoch 62, iter 0, loss 1.1483285427093506\n",
      "epoch 62, iter 1, loss 1.1339964866638184\n",
      "epoch 62, iter 2, loss 0.9851846694946289\n",
      "epoch 62, loss 3.267509698867798\n",
      "epoch 63, iter 0, loss 1.1152558326721191\n",
      "epoch 63, iter 1, loss 1.1566269397735596\n",
      "epoch 63, iter 2, loss 1.053936243057251\n",
      "epoch 63, loss 3.3258190155029297\n",
      "epoch 64, iter 0, loss 1.145226001739502\n",
      "epoch 64, iter 1, loss 1.1252046823501587\n",
      "epoch 64, iter 2, loss 1.1560224294662476\n",
      "epoch 64, loss 3.426453113555908\n",
      "epoch 65, iter 0, loss 1.1427806615829468\n",
      "epoch 65, iter 1, loss 1.126238465309143\n",
      "epoch 65, iter 2, loss 1.1765828132629395\n",
      "epoch 65, loss 3.4456019401550293\n",
      "epoch 66, iter 0, loss 1.1043578386306763\n",
      "epoch 66, iter 1, loss 1.1482089757919312\n",
      "epoch 66, iter 2, loss 1.1304399967193604\n",
      "epoch 66, loss 3.3830068111419678\n",
      "epoch 67, iter 0, loss 1.1490916013717651\n",
      "epoch 67, iter 1, loss 1.0945427417755127\n",
      "epoch 67, iter 2, loss 1.1975739002227783\n",
      "epoch 67, loss 3.441208243370056\n",
      "epoch 68, iter 0, loss 1.1566749811172485\n",
      "epoch 68, iter 1, loss 1.101024866104126\n",
      "epoch 68, iter 2, loss 1.0788254737854004\n",
      "epoch 68, loss 3.336525321006775\n",
      "epoch 69, iter 0, loss 1.1307594776153564\n",
      "epoch 69, iter 1, loss 1.1056547164916992\n",
      "epoch 69, iter 2, loss 1.1687729358673096\n",
      "epoch 69, loss 3.4051871299743652\n",
      "epoch 70, iter 0, loss 1.1359543800354004\n",
      "epoch 70, iter 1, loss 1.0985735654830933\n",
      "epoch 70, iter 2, loss 1.1260817050933838\n",
      "epoch 70, loss 3.3606096506118774\n",
      "epoch 71, iter 0, loss 1.1170450448989868\n",
      "epoch 71, iter 1, loss 1.1252408027648926\n",
      "epoch 71, iter 2, loss 1.1519865989685059\n",
      "epoch 71, loss 3.3942724466323853\n",
      "epoch 72, iter 0, loss 1.135501503944397\n",
      "epoch 72, iter 1, loss 1.1061512231826782\n",
      "epoch 72, iter 2, loss 1.1243987083435059\n",
      "epoch 72, loss 3.366051435470581\n",
      "epoch 73, iter 0, loss 1.0980099439620972\n",
      "epoch 73, iter 1, loss 1.1448050737380981\n",
      "epoch 73, iter 2, loss 1.1269527673721313\n",
      "epoch 73, loss 3.3697677850723267\n",
      "epoch 74, iter 0, loss 1.108683705329895\n",
      "epoch 74, iter 1, loss 1.128465175628662\n",
      "epoch 74, iter 2, loss 1.1213507652282715\n",
      "epoch 74, loss 3.3584996461868286\n",
      "epoch 75, iter 0, loss 1.0959619283676147\n",
      "epoch 75, iter 1, loss 1.132420539855957\n",
      "epoch 75, iter 2, loss 1.1445095539093018\n",
      "epoch 75, loss 3.3728920221328735\n",
      "epoch 76, iter 0, loss 1.1245261430740356\n",
      "epoch 76, iter 1, loss 1.1276689767837524\n",
      "epoch 76, iter 2, loss 1.092956781387329\n",
      "epoch 76, loss 3.345151901245117\n",
      "epoch 77, iter 0, loss 1.1116108894348145\n",
      "epoch 77, iter 1, loss 1.137638807296753\n",
      "epoch 77, iter 2, loss 1.0603313446044922\n",
      "epoch 77, loss 3.3095810413360596\n",
      "epoch 78, iter 0, loss 1.1045671701431274\n",
      "epoch 78, iter 1, loss 1.1355088949203491\n",
      "epoch 78, iter 2, loss 1.1344424486160278\n",
      "epoch 78, loss 3.3745185136795044\n",
      "epoch 79, iter 0, loss 1.0951980352401733\n",
      "epoch 79, iter 1, loss 1.1326344013214111\n",
      "epoch 79, iter 2, loss 1.1317743062973022\n",
      "epoch 79, loss 3.3596067428588867\n",
      "epoch 80, iter 0, loss 1.0983431339263916\n",
      "epoch 80, iter 1, loss 1.1365227699279785\n",
      "epoch 80, iter 2, loss 1.1568821668624878\n",
      "epoch 80, loss 3.391748070716858\n",
      "epoch 81, iter 0, loss 1.1332073211669922\n",
      "epoch 81, iter 1, loss 1.084335207939148\n",
      "epoch 81, iter 2, loss 1.1734849214553833\n",
      "epoch 81, loss 3.3910274505615234\n",
      "epoch 82, iter 0, loss 1.104547381401062\n",
      "epoch 82, iter 1, loss 1.124719500541687\n",
      "epoch 82, iter 2, loss 1.143782377243042\n",
      "epoch 82, loss 3.373049259185791\n",
      "epoch 83, iter 0, loss 1.1063742637634277\n",
      "epoch 83, iter 1, loss 1.1167759895324707\n",
      "epoch 83, iter 2, loss 1.0794368982315063\n",
      "epoch 83, loss 3.302587151527405\n",
      "epoch 84, iter 0, loss 1.1257734298706055\n",
      "epoch 84, iter 1, loss 1.0822627544403076\n",
      "epoch 84, iter 2, loss 1.2752739191055298\n",
      "epoch 84, loss 3.483310103416443\n",
      "epoch 85, iter 0, loss 1.0818588733673096\n",
      "epoch 85, iter 1, loss 1.1138060092926025\n",
      "epoch 85, iter 2, loss 1.256722092628479\n",
      "epoch 85, loss 3.452386975288391\n",
      "epoch 86, iter 0, loss 1.0907715559005737\n",
      "epoch 86, iter 1, loss 1.1295428276062012\n",
      "epoch 86, iter 2, loss 1.162664771080017\n",
      "epoch 86, loss 3.382979154586792\n",
      "epoch 87, iter 0, loss 1.130972146987915\n",
      "epoch 87, iter 1, loss 1.0860881805419922\n",
      "epoch 87, iter 2, loss 1.135141372680664\n",
      "epoch 87, loss 3.3522017002105713\n",
      "epoch 88, iter 0, loss 1.1070280075073242\n",
      "epoch 88, iter 1, loss 1.1126896142959595\n",
      "epoch 88, iter 2, loss 1.1433402299880981\n",
      "epoch 88, loss 3.363057851791382\n",
      "epoch 89, iter 0, loss 1.0707714557647705\n",
      "epoch 89, iter 1, loss 1.165172815322876\n",
      "epoch 89, iter 2, loss 1.0287939310073853\n",
      "epoch 89, loss 3.2647382020950317\n",
      "epoch 90, iter 0, loss 1.1045852899551392\n",
      "epoch 90, iter 1, loss 1.1284457445144653\n",
      "epoch 90, iter 2, loss 1.064103126525879\n",
      "epoch 90, loss 3.2971341609954834\n",
      "epoch 91, iter 0, loss 1.1411771774291992\n",
      "epoch 91, iter 1, loss 1.1044464111328125\n",
      "epoch 91, iter 2, loss 1.03425931930542\n",
      "epoch 91, loss 3.2798829078674316\n",
      "epoch 92, iter 0, loss 1.1225495338439941\n",
      "epoch 92, iter 1, loss 1.1161521673202515\n",
      "epoch 92, iter 2, loss 1.1110668182373047\n",
      "epoch 92, loss 3.3497685194015503\n",
      "epoch 93, iter 0, loss 1.138105034828186\n",
      "epoch 93, iter 1, loss 1.096514344215393\n",
      "epoch 93, iter 2, loss 1.1553572416305542\n",
      "epoch 93, loss 3.3899766206741333\n",
      "epoch 94, iter 0, loss 1.1299902200698853\n",
      "epoch 94, iter 1, loss 1.0933458805084229\n",
      "epoch 94, iter 2, loss 1.1522117853164673\n",
      "epoch 94, loss 3.3755478858947754\n",
      "epoch 95, iter 0, loss 1.1320366859436035\n",
      "epoch 95, iter 1, loss 1.089002013206482\n",
      "epoch 95, iter 2, loss 1.20108962059021\n",
      "epoch 95, loss 3.4221283197402954\n",
      "epoch 96, iter 0, loss 1.110740303993225\n",
      "epoch 96, iter 1, loss 1.1205285787582397\n",
      "epoch 96, iter 2, loss 1.1170192956924438\n",
      "epoch 96, loss 3.3482881784439087\n",
      "epoch 97, iter 0, loss 1.129773736000061\n",
      "epoch 97, iter 1, loss 1.1068718433380127\n",
      "epoch 97, iter 2, loss 1.060415267944336\n",
      "epoch 97, loss 3.2970608472824097\n",
      "epoch 98, iter 0, loss 1.084774374961853\n",
      "epoch 98, iter 1, loss 1.1241399049758911\n",
      "epoch 98, iter 2, loss 1.170854926109314\n",
      "epoch 98, loss 3.379769206047058\n",
      "epoch 99, iter 0, loss 1.084298849105835\n",
      "epoch 99, iter 1, loss 1.123357892036438\n",
      "epoch 99, iter 2, loss 1.1853053569793701\n",
      "epoch 99, loss 3.392962098121643\n",
      "2019-03-31 00:12:34.317677, fold=2, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 2, 'repeat': 1, 'n': 3338, 'd': 17, 'mse': 0.43406808376312256, 'train_time': 506.6055287960917, 'trained_epochs': 100, 'prior_train_nmll': 1.1050785779953003, 'train_nll': 2194.01123046875, 'test_nll': 1115.2347412109375, 'train_mse': 0.40816619992256165, 'state_dict_file': 'model_state_dict_-7417402834003472939.pkl'}\n",
      "Begin dataset  sml\n",
      "Begin with SVI\n",
      "epoch 0, iter 0, loss 1.9481818675994873\n",
      "epoch 0, iter 1, loss 2.0882654190063477\n",
      "epoch 0, iter 2, loss 2.137683868408203\n",
      "epoch 0, loss 6.174131155014038\n",
      "epoch 1, iter 0, loss 1.9584777355194092\n",
      "epoch 1, iter 1, loss 1.8853952884674072\n",
      "epoch 1, iter 2, loss 1.8964308500289917\n",
      "epoch 1, loss 5.740303874015808\n",
      "epoch 2, iter 0, loss 1.7441471815109253\n",
      "epoch 2, iter 1, loss 1.556398868560791\n",
      "epoch 2, iter 2, loss 1.5401456356048584\n",
      "epoch 2, loss 4.840691685676575\n",
      "epoch 3, iter 0, loss 1.5540153980255127\n",
      "epoch 3, iter 1, loss 1.4929605722427368\n",
      "epoch 3, iter 2, loss 1.431138038635254\n",
      "epoch 3, loss 4.478114008903503\n",
      "epoch 4, iter 0, loss 1.4043209552764893\n",
      "epoch 4, iter 1, loss 1.388683557510376\n",
      "epoch 4, iter 2, loss 1.3574777841567993\n",
      "epoch 4, loss 4.1504822969436646\n",
      "epoch 5, iter 0, loss 1.332519769668579\n",
      "epoch 5, iter 1, loss 1.3060829639434814\n",
      "epoch 5, iter 2, loss 1.2940231561660767\n",
      "epoch 5, loss 3.932625889778137\n",
      "epoch 6, iter 0, loss 1.2625411748886108\n",
      "epoch 6, iter 1, loss 1.230623483657837\n",
      "epoch 6, iter 2, loss 1.2130911350250244\n",
      "epoch 6, loss 3.706255793571472\n",
      "epoch 7, iter 0, loss 1.1876710653305054\n",
      "epoch 7, iter 1, loss 1.1622389554977417\n",
      "epoch 7, iter 2, loss 1.1313635110855103\n",
      "epoch 7, loss 3.4812735319137573\n",
      "epoch 8, iter 0, loss 1.1058319807052612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, iter 1, loss 1.0789190530776978\n",
      "epoch 8, iter 2, loss 1.0551338195800781\n",
      "epoch 8, loss 3.239884853363037\n",
      "epoch 9, iter 0, loss 1.0276068449020386\n",
      "epoch 9, iter 1, loss 0.9891640543937683\n",
      "epoch 9, iter 2, loss 0.9609690308570862\n",
      "epoch 9, loss 2.977739930152893\n",
      "epoch 10, iter 0, loss 0.9293436408042908\n",
      "epoch 10, iter 1, loss 0.8964516520500183\n",
      "epoch 10, iter 2, loss 0.862398087978363\n",
      "epoch 10, loss 2.688193380832672\n",
      "epoch 11, iter 0, loss 0.8244092464447021\n",
      "epoch 11, iter 1, loss 0.7995121479034424\n",
      "epoch 11, iter 2, loss 0.7615549564361572\n",
      "epoch 11, loss 2.3854763507843018\n",
      "epoch 12, iter 0, loss 0.720584511756897\n",
      "epoch 12, iter 1, loss 0.6852157115936279\n",
      "epoch 12, iter 2, loss 0.6594017148017883\n",
      "epoch 12, loss 2.0652019381523132\n",
      "epoch 13, iter 0, loss 0.6151906251907349\n",
      "epoch 13, iter 1, loss 0.5817123055458069\n",
      "epoch 13, iter 2, loss 0.5639065504074097\n",
      "epoch 13, loss 1.7608094811439514\n",
      "epoch 14, iter 0, loss 0.5292098522186279\n",
      "epoch 14, iter 1, loss 0.4858599305152893\n",
      "epoch 14, iter 2, loss 0.4763827919960022\n",
      "epoch 14, loss 1.4914525747299194\n",
      "epoch 15, iter 0, loss 0.4522596597671509\n",
      "epoch 15, iter 1, loss 0.43952512741088867\n",
      "epoch 15, iter 2, loss 0.42398446798324585\n",
      "epoch 15, loss 1.3157692551612854\n",
      "epoch 16, iter 0, loss 0.43290311098098755\n",
      "epoch 16, iter 1, loss 0.3913589119911194\n",
      "epoch 16, iter 2, loss 0.40696391463279724\n",
      "epoch 16, loss 1.2312259376049042\n",
      "epoch 17, iter 0, loss 0.4068015217781067\n",
      "epoch 17, iter 1, loss 0.3806180953979492\n",
      "epoch 17, iter 2, loss 0.3735466003417969\n",
      "epoch 17, loss 1.1609662175178528\n",
      "epoch 18, iter 0, loss 0.3729378283023834\n",
      "epoch 18, iter 1, loss 0.36059996485710144\n",
      "epoch 18, iter 2, loss 0.4180046319961548\n",
      "epoch 18, loss 1.1515424251556396\n",
      "epoch 19, iter 0, loss 0.3893180191516876\n",
      "epoch 19, iter 1, loss 0.3835483193397522\n",
      "epoch 19, iter 2, loss 0.3901252746582031\n",
      "epoch 19, loss 1.162991613149643\n",
      "epoch 20, iter 0, loss 0.392007976770401\n",
      "epoch 20, iter 1, loss 0.3632197976112366\n",
      "epoch 20, iter 2, loss 0.37632039189338684\n",
      "epoch 20, loss 1.1315481662750244\n",
      "epoch 21, iter 0, loss 0.3909969925880432\n",
      "epoch 21, iter 1, loss 0.34077492356300354\n",
      "epoch 21, iter 2, loss 0.34736332297325134\n",
      "epoch 21, loss 1.079135239124298\n",
      "epoch 22, iter 0, loss 0.33002549409866333\n",
      "epoch 22, iter 1, loss 0.3015543222427368\n",
      "epoch 22, iter 2, loss 0.2638837397098541\n",
      "epoch 22, loss 0.8954635560512543\n",
      "epoch 23, iter 0, loss 0.2906721830368042\n",
      "epoch 23, iter 1, loss 0.2505553364753723\n",
      "epoch 23, iter 2, loss 0.21201324462890625\n",
      "epoch 23, loss 0.7532407641410828\n",
      "epoch 24, iter 0, loss 0.23394790291786194\n",
      "epoch 24, iter 1, loss 0.207098126411438\n",
      "epoch 24, iter 2, loss 0.16738483309745789\n",
      "epoch 24, loss 0.6084308624267578\n",
      "epoch 25, iter 0, loss 0.17363937199115753\n",
      "epoch 25, iter 1, loss 0.1668534278869629\n",
      "epoch 25, iter 2, loss 0.1557726413011551\n",
      "epoch 25, loss 0.4962654411792755\n",
      "epoch 26, iter 0, loss 0.12661977112293243\n",
      "epoch 26, iter 1, loss 0.10600201785564423\n",
      "epoch 26, iter 2, loss 0.106292724609375\n",
      "epoch 26, loss 0.33891451358795166\n",
      "epoch 27, iter 0, loss 0.09333893656730652\n",
      "epoch 27, iter 1, loss 0.05615244805812836\n",
      "epoch 27, iter 2, loss 0.06994730234146118\n",
      "epoch 27, loss 0.21943868696689606\n",
      "epoch 28, iter 0, loss 0.029606893658638\n",
      "epoch 28, iter 1, loss 0.04272928833961487\n",
      "epoch 28, iter 2, loss 0.006503596901893616\n",
      "epoch 28, loss 0.07883977890014648\n",
      "epoch 29, iter 0, loss 0.01482301950454712\n",
      "epoch 29, iter 1, loss -0.017320767045021057\n",
      "epoch 29, iter 2, loss -0.01752418279647827\n",
      "epoch 29, loss -0.02002193033695221\n",
      "epoch 30, iter 0, loss -0.029974907636642456\n",
      "epoch 30, iter 1, loss -0.03906607627868652\n",
      "epoch 30, iter 2, loss -0.030008330941200256\n",
      "epoch 30, loss -0.09904931485652924\n",
      "epoch 31, iter 0, loss -0.05195620656013489\n",
      "epoch 31, iter 1, loss -0.0637584775686264\n",
      "epoch 31, iter 2, loss -0.03620615601539612\n",
      "epoch 31, loss -0.1519208401441574\n",
      "epoch 32, iter 0, loss -0.048783138394355774\n",
      "epoch 32, iter 1, loss -0.05916094779968262\n",
      "epoch 32, iter 2, loss -0.05579093098640442\n",
      "epoch 32, loss -0.1637350171804428\n",
      "epoch 33, iter 0, loss -0.037085920572280884\n",
      "epoch 33, iter 1, loss -0.048970699310302734\n",
      "epoch 33, iter 2, loss -0.027028977870941162\n",
      "epoch 33, loss -0.11308559775352478\n",
      "epoch 34, iter 0, loss -0.047250211238861084\n",
      "epoch 34, iter 1, loss -0.025400102138519287\n",
      "epoch 34, iter 2, loss -0.019168496131896973\n",
      "epoch 34, loss -0.09181880950927734\n",
      "epoch 35, iter 0, loss -0.013986438512802124\n",
      "epoch 35, iter 1, loss -0.01417812705039978\n",
      "epoch 35, iter 2, loss 0.027538985013961792\n",
      "epoch 35, loss -0.0006255805492401123\n",
      "epoch 36, iter 0, loss -0.025035500526428223\n",
      "epoch 36, iter 1, loss -0.015831172466278076\n",
      "epoch 36, iter 2, loss -0.0031509101390838623\n",
      "epoch 36, loss -0.04401758313179016\n",
      "epoch 37, iter 0, loss -0.008875101804733276\n",
      "epoch 37, iter 1, loss -0.03831532597541809\n",
      "epoch 37, iter 2, loss -0.0212877094745636\n",
      "epoch 37, loss -0.06847813725471497\n",
      "epoch 38, iter 0, loss -0.03581947088241577\n",
      "epoch 38, iter 1, loss -0.03228646516799927\n",
      "epoch 38, iter 2, loss -0.0689222663640976\n",
      "epoch 38, loss -0.13702820241451263\n",
      "epoch 39, iter 0, loss -0.037025272846221924\n",
      "epoch 39, iter 1, loss -0.06650093197822571\n",
      "epoch 39, iter 2, loss -0.0670790821313858\n",
      "epoch 39, loss -0.17060528695583344\n",
      "epoch 40, iter 0, loss -0.09469231963157654\n",
      "epoch 40, iter 1, loss -0.07910662889480591\n",
      "epoch 40, iter 2, loss -0.056270018219947815\n",
      "epoch 40, loss -0.23006896674633026\n",
      "epoch 41, iter 0, loss -0.0866207480430603\n",
      "epoch 41, iter 1, loss -0.10015220940113068\n",
      "epoch 41, iter 2, loss -0.09234195947647095\n",
      "epoch 41, loss -0.2791149169206619\n",
      "epoch 42, iter 0, loss -0.09409353137016296\n",
      "epoch 42, iter 1, loss -0.1262543499469757\n",
      "epoch 42, iter 2, loss -0.11942894756793976\n",
      "epoch 42, loss -0.33977682888507843\n",
      "epoch 43, iter 0, loss -0.13032761216163635\n",
      "epoch 43, iter 1, loss -0.10653583705425262\n",
      "epoch 43, iter 2, loss -0.14005190134048462\n",
      "epoch 43, loss -0.3769153505563736\n",
      "epoch 44, iter 0, loss -0.1386515200138092\n",
      "epoch 44, iter 1, loss -0.147250235080719\n",
      "epoch 44, iter 2, loss -0.14821697771549225\n",
      "epoch 44, loss -0.43411873281002045\n",
      "epoch 45, iter 0, loss -0.15006358921527863\n",
      "epoch 45, iter 1, loss -0.15370990335941315\n",
      "epoch 45, iter 2, loss -0.13080759346485138\n",
      "epoch 45, loss -0.43458108603954315\n",
      "epoch 46, iter 0, loss -0.16366340219974518\n",
      "epoch 46, iter 1, loss -0.1232534795999527\n",
      "epoch 46, iter 2, loss -0.1506575644016266\n",
      "epoch 46, loss -0.43757444620132446\n",
      "epoch 47, iter 0, loss -0.12101607024669647\n",
      "epoch 47, iter 1, loss -0.11861076951026917\n",
      "epoch 47, iter 2, loss -0.0964154303073883\n",
      "epoch 47, loss -0.33604227006435394\n",
      "epoch 48, iter 0, loss -0.09634998440742493\n",
      "epoch 48, iter 1, loss -0.09503093361854553\n",
      "epoch 48, iter 2, loss -0.1507161557674408\n",
      "epoch 48, loss -0.34209707379341125\n",
      "epoch 49, iter 0, loss -0.11965745687484741\n",
      "epoch 49, iter 1, loss -0.1392422765493393\n",
      "epoch 49, iter 2, loss -0.05326361954212189\n",
      "epoch 49, loss -0.3121633529663086\n",
      "epoch 50, iter 0, loss -0.11476652324199677\n",
      "epoch 50, iter 1, loss -0.10067029297351837\n",
      "epoch 50, iter 2, loss -0.11289151012897491\n",
      "epoch 50, loss -0.32832832634449005\n",
      "epoch 51, iter 0, loss -0.08898255228996277\n",
      "epoch 51, iter 1, loss -0.13849683105945587\n",
      "epoch 51, iter 2, loss -0.10027250647544861\n",
      "epoch 51, loss -0.32775188982486725\n",
      "epoch 52, iter 0, loss -0.12379637360572815\n",
      "epoch 52, iter 1, loss -0.12632499635219574\n",
      "epoch 52, iter 2, loss -0.13689574599266052\n",
      "epoch 52, loss -0.3870171159505844\n",
      "epoch 53, iter 0, loss -0.14011169970035553\n",
      "epoch 53, iter 1, loss -0.14074715971946716\n",
      "epoch 53, iter 2, loss -0.14602740108966827\n",
      "epoch 53, loss -0.42688626050949097\n",
      "epoch 54, iter 0, loss -0.14100931584835052\n",
      "epoch 54, iter 1, loss -0.15634973347187042\n",
      "epoch 54, iter 2, loss -0.15492182970046997\n",
      "epoch 54, loss -0.4522808790206909\n",
      "epoch 55, iter 0, loss -0.14979812502861023\n",
      "epoch 55, iter 1, loss -0.17094019055366516\n",
      "epoch 55, iter 2, loss -0.15875357389450073\n",
      "epoch 55, loss -0.4794918894767761\n",
      "epoch 56, iter 0, loss -0.19020596146583557\n",
      "epoch 56, iter 1, loss -0.17060135304927826\n",
      "epoch 56, iter 2, loss -0.13324497640132904\n",
      "epoch 56, loss -0.49405229091644287\n",
      "epoch 57, iter 0, loss -0.1674620509147644\n",
      "epoch 57, iter 1, loss -0.1780642420053482\n",
      "epoch 57, iter 2, loss -0.1500854790210724\n",
      "epoch 57, loss -0.495611771941185\n",
      "epoch 58, iter 0, loss -0.18972790241241455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58, iter 1, loss -0.1798497438430786\n",
      "epoch 58, iter 2, loss -0.14988884329795837\n",
      "epoch 58, loss -0.5194664895534515\n",
      "epoch 59, iter 0, loss -0.18843917548656464\n",
      "epoch 59, iter 1, loss -0.16485881805419922\n",
      "epoch 59, iter 2, loss -0.17552243173122406\n",
      "epoch 59, loss -0.5288204252719879\n",
      "epoch 60, iter 0, loss -0.17339922487735748\n",
      "epoch 60, iter 1, loss -0.1765691339969635\n",
      "epoch 60, iter 2, loss -0.17108403146266937\n",
      "epoch 60, loss -0.5210523903369904\n",
      "epoch 61, iter 0, loss -0.2038203626871109\n",
      "epoch 61, iter 1, loss -0.1597377061843872\n",
      "epoch 61, iter 2, loss -0.20335981249809265\n",
      "epoch 61, loss -0.5669178813695908\n",
      "epoch 62, iter 0, loss -0.19753187894821167\n",
      "epoch 62, iter 1, loss -0.1858549863100052\n",
      "epoch 62, iter 2, loss -0.19151093065738678\n",
      "epoch 62, loss -0.5748977959156036\n",
      "epoch 63, iter 0, loss -0.1650426834821701\n",
      "epoch 63, iter 1, loss -0.20167505741119385\n",
      "epoch 63, iter 2, loss -0.2148016393184662\n",
      "epoch 63, loss -0.5815193802118301\n",
      "epoch 64, iter 0, loss -0.18056470155715942\n",
      "epoch 64, iter 1, loss -0.19591931998729706\n",
      "epoch 64, iter 2, loss -0.2031164914369583\n",
      "epoch 64, loss -0.5796005129814148\n",
      "epoch 65, iter 0, loss -0.19764402508735657\n",
      "epoch 65, iter 1, loss -0.19842085242271423\n",
      "epoch 65, iter 2, loss -0.188092902302742\n",
      "epoch 65, loss -0.5841577798128128\n",
      "epoch 66, iter 0, loss -0.19640611112117767\n",
      "epoch 66, iter 1, loss -0.22568847239017487\n",
      "epoch 66, iter 2, loss -0.1547446995973587\n",
      "epoch 66, loss -0.5768392831087112\n",
      "epoch 67, iter 0, loss -0.20848850905895233\n",
      "epoch 67, iter 1, loss -0.1893310248851776\n",
      "epoch 67, iter 2, loss -0.17900408804416656\n",
      "epoch 67, loss -0.5768236219882965\n",
      "epoch 68, iter 0, loss -0.20731769502162933\n",
      "epoch 68, iter 1, loss -0.1944684237241745\n",
      "epoch 68, iter 2, loss -0.1999097615480423\n",
      "epoch 68, loss -0.6016958802938461\n",
      "epoch 69, iter 0, loss -0.19066227972507477\n",
      "epoch 69, iter 1, loss -0.21991181373596191\n",
      "epoch 69, iter 2, loss -0.2008899599313736\n",
      "epoch 69, loss -0.6114640533924103\n",
      "epoch 70, iter 0, loss -0.21156863868236542\n",
      "epoch 70, iter 1, loss -0.20043139159679413\n",
      "epoch 70, iter 2, loss -0.2320069819688797\n",
      "epoch 70, loss -0.6440070122480392\n",
      "epoch 71, iter 0, loss -0.18119335174560547\n",
      "epoch 71, iter 1, loss -0.22257432341575623\n",
      "epoch 71, iter 2, loss -0.20850896835327148\n",
      "epoch 71, loss -0.6122766435146332\n",
      "epoch 72, iter 0, loss -0.20394405722618103\n",
      "epoch 72, iter 1, loss -0.18507952988147736\n",
      "epoch 72, iter 2, loss -0.21117083728313446\n",
      "epoch 72, loss -0.6001944243907928\n",
      "epoch 73, iter 0, loss -0.20266570150852203\n",
      "epoch 73, iter 1, loss -0.18946819007396698\n",
      "epoch 73, iter 2, loss -0.19220608472824097\n",
      "epoch 73, loss -0.58433997631073\n",
      "epoch 74, iter 0, loss -0.2153094857931137\n",
      "epoch 74, iter 1, loss -0.17303518950939178\n",
      "epoch 74, iter 2, loss -0.21264420449733734\n",
      "epoch 74, loss -0.6009888797998428\n",
      "epoch 75, iter 0, loss -0.2050332874059677\n",
      "epoch 75, iter 1, loss -0.1888563483953476\n",
      "epoch 75, iter 2, loss -0.18682712316513062\n",
      "epoch 75, loss -0.5807167589664459\n",
      "epoch 76, iter 0, loss -0.225828617811203\n",
      "epoch 76, iter 1, loss -0.1747887283563614\n",
      "epoch 76, iter 2, loss -0.19857458770275116\n",
      "epoch 76, loss -0.5991919338703156\n",
      "epoch 77, iter 0, loss -0.20463471114635468\n",
      "epoch 77, iter 1, loss -0.20263446867465973\n",
      "epoch 77, iter 2, loss -0.20647284388542175\n",
      "epoch 77, loss -0.6137420237064362\n",
      "epoch 78, iter 0, loss -0.2089582234621048\n",
      "epoch 78, iter 1, loss -0.22798317670822144\n",
      "epoch 78, iter 2, loss -0.19298453629016876\n",
      "epoch 78, loss -0.629925936460495\n",
      "epoch 79, iter 0, loss -0.23537670075893402\n",
      "epoch 79, iter 1, loss -0.20705412328243256\n",
      "epoch 79, iter 2, loss -0.22102415561676025\n",
      "epoch 79, loss -0.6634549796581268\n",
      "epoch 80, iter 0, loss -0.23398983478546143\n",
      "epoch 80, iter 1, loss -0.22472935914993286\n",
      "epoch 80, iter 2, loss -0.23024989664554596\n",
      "epoch 80, loss -0.6889690905809402\n",
      "epoch 81, iter 0, loss -0.20737923681735992\n",
      "epoch 81, iter 1, loss -0.20402801036834717\n",
      "epoch 81, iter 2, loss -0.26343658566474915\n",
      "epoch 81, loss -0.6748438328504562\n",
      "epoch 82, iter 0, loss -0.21581432223320007\n",
      "epoch 82, iter 1, loss -0.2525520920753479\n",
      "epoch 82, iter 2, loss -0.23050712049007416\n",
      "epoch 82, loss -0.6988735347986221\n",
      "epoch 83, iter 0, loss -0.24088206887245178\n",
      "epoch 83, iter 1, loss -0.23486436903476715\n",
      "epoch 83, iter 2, loss -0.24715809524059296\n",
      "epoch 83, loss -0.7229045331478119\n",
      "epoch 84, iter 0, loss -0.24339614808559418\n",
      "epoch 84, iter 1, loss -0.23232276737689972\n",
      "epoch 84, iter 2, loss -0.21386116743087769\n",
      "epoch 84, loss -0.6895800828933716\n",
      "epoch 85, iter 0, loss -0.21940840780735016\n",
      "epoch 85, iter 1, loss -0.2240460366010666\n",
      "epoch 85, iter 2, loss -0.22478605806827545\n",
      "epoch 85, loss -0.6682405024766922\n",
      "epoch 86, iter 0, loss -0.2264452874660492\n",
      "epoch 86, iter 1, loss -0.2151760458946228\n",
      "epoch 86, iter 2, loss -0.20710881054401398\n",
      "epoch 86, loss -0.648730143904686\n",
      "epoch 87, iter 0, loss -0.23728686571121216\n",
      "epoch 87, iter 1, loss -0.1975545883178711\n",
      "epoch 87, iter 2, loss -0.2137528955936432\n",
      "epoch 87, loss -0.6485943496227264\n",
      "epoch 88, iter 0, loss -0.22456201910972595\n",
      "epoch 88, iter 1, loss -0.20993579924106598\n",
      "epoch 88, iter 2, loss -0.21144403517246246\n",
      "epoch 88, loss -0.6459418535232544\n",
      "epoch 89, iter 0, loss -0.19296759366989136\n",
      "epoch 89, iter 1, loss -0.21751666069030762\n",
      "epoch 89, iter 2, loss -0.2127225399017334\n",
      "epoch 89, loss -0.6232067942619324\n",
      "epoch 90, iter 0, loss -0.21131685376167297\n",
      "epoch 90, iter 1, loss -0.22707103192806244\n",
      "epoch 90, iter 2, loss -0.20559072494506836\n",
      "epoch 90, loss -0.6439786106348038\n",
      "epoch 91, iter 0, loss -0.23545046150684357\n",
      "epoch 91, iter 1, loss -0.21658417582511902\n",
      "epoch 91, iter 2, loss -0.21075206995010376\n",
      "epoch 91, loss -0.6627867072820663\n",
      "epoch 92, iter 0, loss -0.21547459065914154\n",
      "epoch 92, iter 1, loss -0.2337779402732849\n",
      "epoch 92, iter 2, loss -0.23667846620082855\n",
      "epoch 92, loss -0.685930997133255\n",
      "epoch 93, iter 0, loss -0.221896693110466\n",
      "epoch 93, iter 1, loss -0.2230173647403717\n",
      "epoch 93, iter 2, loss -0.23433539271354675\n",
      "epoch 93, loss -0.6792494505643845\n",
      "epoch 94, iter 0, loss -0.22927628457546234\n",
      "epoch 94, iter 1, loss -0.22930605709552765\n",
      "epoch 94, iter 2, loss -0.22058440744876862\n",
      "epoch 94, loss -0.6791667491197586\n",
      "epoch 95, iter 0, loss -0.2560052275657654\n",
      "epoch 95, iter 1, loss -0.2037585824728012\n",
      "epoch 95, iter 2, loss -0.2400720864534378\n",
      "epoch 95, loss -0.6998358964920044\n",
      "epoch 96, iter 0, loss -0.23664343357086182\n",
      "epoch 96, iter 1, loss -0.2178451418876648\n",
      "epoch 96, iter 2, loss -0.24732688069343567\n",
      "epoch 96, loss -0.7018154561519623\n",
      "epoch 97, iter 0, loss -0.24801234900951385\n",
      "epoch 97, iter 1, loss -0.23002660274505615\n",
      "epoch 97, iter 2, loss -0.2660679519176483\n",
      "epoch 97, loss -0.7441069036722183\n",
      "epoch 98, iter 0, loss -0.2522590160369873\n",
      "epoch 98, iter 1, loss -0.2412305772304535\n",
      "epoch 98, iter 2, loss -0.22049526870250702\n",
      "epoch 98, loss -0.7139848619699478\n",
      "epoch 99, iter 0, loss -0.2573603391647339\n",
      "epoch 99, iter 1, loss -0.21723946928977966\n",
      "epoch 99, iter 2, loss -0.2547716498374939\n",
      "epoch 99, loss -0.7293714582920074\n",
      "2019-03-31 00:22:57.535073, fold=0, rep=0, eta=0d 0h 51m 55s \n",
      "{'fold': 0, 'repeat': 0, 'n': 4137, 'd': 20, 'mse': 0.013949411921203136, 'train_time': 621.8103003019933, 'trained_epochs': 100, 'prior_train_nmll': -0.17360885441303253, 'train_nll': -1653.863037109375, 'test_nll': -836.7156982421875, 'train_mse': 0.014608682133257389, 'state_dict_file': 'model_state_dict_4728967980167442671.pkl'}\n",
      "epoch 0, iter 0, loss 1.9602234363555908\n",
      "epoch 0, iter 1, loss 2.0751843452453613\n",
      "epoch 0, iter 2, loss 2.106863260269165\n",
      "epoch 0, loss 6.142271041870117\n",
      "epoch 1, iter 0, loss 1.9521441459655762\n",
      "epoch 1, iter 1, loss 1.8886475563049316\n",
      "epoch 1, iter 2, loss 1.9051597118377686\n",
      "epoch 1, loss 5.745951414108276\n",
      "epoch 2, iter 0, loss 1.7454094886779785\n",
      "epoch 2, iter 1, loss 1.5907225608825684\n",
      "epoch 2, iter 2, loss 1.5419766902923584\n",
      "epoch 2, loss 4.878108739852905\n",
      "epoch 3, iter 0, loss 1.5519918203353882\n",
      "epoch 3, iter 1, loss 1.5144665241241455\n",
      "epoch 3, iter 2, loss 1.4357988834381104\n",
      "epoch 3, loss 4.502257227897644\n",
      "epoch 4, iter 0, loss 1.416628122329712\n",
      "epoch 4, iter 1, loss 1.3990960121154785\n",
      "epoch 4, iter 2, loss 1.3640599250793457\n",
      "epoch 4, loss 4.179784059524536\n",
      "epoch 5, iter 0, loss 1.34063720703125\n",
      "epoch 5, iter 1, loss 1.3236769437789917\n",
      "epoch 5, iter 2, loss 1.2939622402191162\n",
      "epoch 5, loss 3.958276391029358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, iter 0, loss 1.2675909996032715\n",
      "epoch 6, iter 1, loss 1.2448164224624634\n",
      "epoch 6, iter 2, loss 1.2247008085250854\n",
      "epoch 6, loss 3.7371082305908203\n",
      "epoch 7, iter 0, loss 1.192747712135315\n",
      "epoch 7, iter 1, loss 1.1707847118377686\n",
      "epoch 7, iter 2, loss 1.1477890014648438\n",
      "epoch 7, loss 3.5113214254379272\n",
      "epoch 8, iter 0, loss 1.1176503896713257\n",
      "epoch 8, iter 1, loss 1.0921109914779663\n",
      "epoch 8, iter 2, loss 1.0629853010177612\n",
      "epoch 8, loss 3.2727466821670532\n",
      "epoch 9, iter 0, loss 1.035597324371338\n",
      "epoch 9, iter 1, loss 1.002626657485962\n",
      "epoch 9, iter 2, loss 0.9679922461509705\n",
      "epoch 9, loss 3.0062162280082703\n",
      "epoch 10, iter 0, loss 0.9426077604293823\n",
      "epoch 10, iter 1, loss 0.9089648723602295\n",
      "epoch 10, iter 2, loss 0.872872531414032\n",
      "epoch 10, loss 2.724445164203644\n",
      "epoch 11, iter 0, loss 0.8393537402153015\n",
      "epoch 11, iter 1, loss 0.8044841289520264\n",
      "epoch 11, iter 2, loss 0.7699630260467529\n",
      "epoch 11, loss 2.413800895214081\n",
      "epoch 12, iter 0, loss 0.734709620475769\n",
      "epoch 12, iter 1, loss 0.6988076567649841\n",
      "epoch 12, iter 2, loss 0.6720713376998901\n",
      "epoch 12, loss 2.1055886149406433\n",
      "epoch 13, iter 0, loss 0.6315799355506897\n",
      "epoch 13, iter 1, loss 0.5967859625816345\n",
      "epoch 13, iter 2, loss 0.5714971423149109\n",
      "epoch 13, loss 1.799863040447235\n",
      "epoch 14, iter 0, loss 0.538475751876831\n",
      "epoch 14, iter 1, loss 0.5135817527770996\n",
      "epoch 14, iter 2, loss 0.491759717464447\n",
      "epoch 14, loss 1.5438172221183777\n",
      "epoch 15, iter 0, loss 0.472190260887146\n",
      "epoch 15, iter 1, loss 0.44041645526885986\n",
      "epoch 15, iter 2, loss 0.4338456690311432\n",
      "epoch 15, loss 1.346452385187149\n",
      "epoch 16, iter 0, loss 0.4211568832397461\n",
      "epoch 16, iter 1, loss 0.41759514808654785\n",
      "epoch 16, iter 2, loss 0.3930476903915405\n",
      "epoch 16, loss 1.2317997217178345\n",
      "epoch 17, iter 0, loss 0.4073629677295685\n",
      "epoch 17, iter 1, loss 0.38921722769737244\n",
      "epoch 17, iter 2, loss 0.39933109283447266\n",
      "epoch 17, loss 1.1959112882614136\n",
      "epoch 18, iter 0, loss 0.4127715826034546\n",
      "epoch 18, iter 1, loss 0.38363510370254517\n",
      "epoch 18, iter 2, loss 0.37574905157089233\n",
      "epoch 18, loss 1.172155737876892\n",
      "epoch 19, iter 0, loss 0.3850196301937103\n",
      "epoch 19, iter 1, loss 0.38758423924446106\n",
      "epoch 19, iter 2, loss 0.39334437251091003\n",
      "epoch 19, loss 1.1659482419490814\n",
      "epoch 20, iter 0, loss 0.3682597577571869\n",
      "epoch 20, iter 1, loss 0.40185850858688354\n",
      "epoch 20, iter 2, loss 0.3780090808868408\n",
      "epoch 20, loss 1.1481273472309113\n",
      "epoch 21, iter 0, loss 0.38177889585494995\n",
      "epoch 21, iter 1, loss 0.372164785861969\n",
      "epoch 21, iter 2, loss 0.3383120596408844\n",
      "epoch 21, loss 1.0922557413578033\n",
      "epoch 22, iter 0, loss 0.3346688151359558\n",
      "epoch 22, iter 1, loss 0.292539119720459\n",
      "epoch 22, iter 2, loss 0.32261210680007935\n",
      "epoch 22, loss 0.9498200416564941\n",
      "epoch 23, iter 0, loss 0.25040027499198914\n",
      "epoch 23, iter 1, loss 0.2747790217399597\n",
      "epoch 23, iter 2, loss 0.24466167390346527\n",
      "epoch 23, loss 0.7698409706354141\n",
      "epoch 24, iter 0, loss 0.2013949453830719\n",
      "epoch 24, iter 1, loss 0.20878978073596954\n",
      "epoch 24, iter 2, loss 0.19277307391166687\n",
      "epoch 24, loss 0.6029578000307083\n",
      "epoch 25, iter 0, loss 0.1673799753189087\n",
      "epoch 25, iter 1, loss 0.1454232931137085\n",
      "epoch 25, iter 2, loss 0.1463061273097992\n",
      "epoch 25, loss 0.4591093957424164\n",
      "epoch 26, iter 0, loss 0.12060455977916718\n",
      "epoch 26, iter 1, loss 0.11264564096927643\n",
      "epoch 26, iter 2, loss 0.08855722844600677\n",
      "epoch 26, loss 0.3218074291944504\n",
      "epoch 27, iter 0, loss 0.07800547778606415\n",
      "epoch 27, iter 1, loss 0.060654088854789734\n",
      "epoch 27, iter 2, loss 0.05642491579055786\n",
      "epoch 27, loss 0.19508448243141174\n",
      "epoch 28, iter 0, loss 0.034410566091537476\n",
      "epoch 28, iter 1, loss 0.01096402108669281\n",
      "epoch 28, iter 2, loss 0.017522484064102173\n",
      "epoch 28, loss 0.06289707124233246\n",
      "epoch 29, iter 0, loss -0.012322023510932922\n",
      "epoch 29, iter 1, loss 0.002655237913131714\n",
      "epoch 29, iter 2, loss -0.030297890305519104\n",
      "epoch 29, loss -0.03996467590332031\n",
      "epoch 30, iter 0, loss -0.02160710096359253\n",
      "epoch 30, iter 1, loss -0.031682223081588745\n",
      "epoch 30, iter 2, loss -0.010662674903869629\n",
      "epoch 30, loss -0.0639519989490509\n",
      "epoch 31, iter 0, loss -0.013396918773651123\n",
      "epoch 31, iter 1, loss -0.05556163191795349\n",
      "epoch 31, iter 2, loss -0.03726738691329956\n",
      "epoch 31, loss -0.10622593760490417\n",
      "epoch 32, iter 0, loss -0.022185444831848145\n",
      "epoch 32, iter 1, loss -0.01689508557319641\n",
      "epoch 32, iter 2, loss -0.019967347383499146\n",
      "epoch 32, loss -0.0590478777885437\n",
      "epoch 33, iter 0, loss -0.023748546838760376\n",
      "epoch 33, iter 1, loss -0.0017113089561462402\n",
      "epoch 33, iter 2, loss -0.023108601570129395\n",
      "epoch 33, loss -0.04856845736503601\n",
      "epoch 34, iter 0, loss -0.010164499282836914\n",
      "epoch 34, iter 1, loss 0.015246212482452393\n",
      "epoch 34, iter 2, loss -0.04401540756225586\n",
      "epoch 34, loss -0.03893369436264038\n",
      "epoch 35, iter 0, loss 0.0055439770221710205\n",
      "epoch 35, iter 1, loss -0.026266485452651978\n",
      "epoch 35, iter 2, loss -0.006506621837615967\n",
      "epoch 35, loss -0.027229130268096924\n",
      "epoch 36, iter 0, loss -0.02170655131340027\n",
      "epoch 36, iter 1, loss -0.0046949684619903564\n",
      "epoch 36, iter 2, loss 0.028602823615074158\n",
      "epoch 36, loss 0.0022013038396835327\n",
      "epoch 37, iter 0, loss -0.011046499013900757\n",
      "epoch 37, iter 1, loss -0.031927645206451416\n",
      "epoch 37, iter 2, loss -0.0024042725563049316\n",
      "epoch 37, loss -0.045378416776657104\n",
      "epoch 38, iter 0, loss -0.045765653252601624\n",
      "epoch 38, iter 1, loss -0.03530369699001312\n",
      "epoch 38, iter 2, loss -0.029758423566818237\n",
      "epoch 38, loss -0.11082777380943298\n",
      "epoch 39, iter 0, loss -0.056457728147506714\n",
      "epoch 39, iter 1, loss -0.06021273136138916\n",
      "epoch 39, iter 2, loss -0.03166128695011139\n",
      "epoch 39, loss -0.14833174645900726\n",
      "epoch 40, iter 0, loss -0.06925149261951447\n",
      "epoch 40, iter 1, loss -0.08212299644947052\n",
      "epoch 40, iter 2, loss -0.0909724086523056\n",
      "epoch 40, loss -0.2423468977212906\n",
      "epoch 41, iter 0, loss -0.11144377291202545\n",
      "epoch 41, iter 1, loss -0.08167079091072083\n",
      "epoch 41, iter 2, loss -0.09706616401672363\n",
      "epoch 41, loss -0.2901807278394699\n",
      "epoch 42, iter 0, loss -0.09758614003658295\n",
      "epoch 42, iter 1, loss -0.11879943311214447\n",
      "epoch 42, iter 2, loss -0.12328888475894928\n",
      "epoch 42, loss -0.3396744579076767\n",
      "epoch 43, iter 0, loss -0.10047797858715057\n",
      "epoch 43, iter 1, loss -0.14385798573493958\n",
      "epoch 43, iter 2, loss -0.15563395619392395\n",
      "epoch 43, loss -0.3999699205160141\n",
      "epoch 44, iter 0, loss -0.13239030539989471\n",
      "epoch 44, iter 1, loss -0.12076553702354431\n",
      "epoch 44, iter 2, loss -0.1472465842962265\n",
      "epoch 44, loss -0.4004024267196655\n",
      "epoch 45, iter 0, loss -0.12611857056617737\n",
      "epoch 45, iter 1, loss -0.15295249223709106\n",
      "epoch 45, iter 2, loss -0.10649611055850983\n",
      "epoch 45, loss -0.38556717336177826\n",
      "epoch 46, iter 0, loss -0.12353834509849548\n",
      "epoch 46, iter 1, loss -0.15341441333293915\n",
      "epoch 46, iter 2, loss -0.14973975718021393\n",
      "epoch 46, loss -0.42669251561164856\n",
      "epoch 47, iter 0, loss -0.11134026944637299\n",
      "epoch 47, iter 1, loss -0.16463033854961395\n",
      "epoch 47, iter 2, loss -0.11337320506572723\n",
      "epoch 47, loss -0.3893438130617142\n",
      "epoch 48, iter 0, loss -0.1453370749950409\n",
      "epoch 48, iter 1, loss -0.095099076628685\n",
      "epoch 48, iter 2, loss -0.11947797238826752\n",
      "epoch 48, loss -0.3599141240119934\n",
      "epoch 49, iter 0, loss -0.13765759766101837\n",
      "epoch 49, iter 1, loss -0.11520561575889587\n",
      "epoch 49, iter 2, loss -0.12076304852962494\n",
      "epoch 49, loss -0.3736262619495392\n",
      "epoch 50, iter 0, loss -0.14705529808998108\n",
      "epoch 50, iter 1, loss -0.10210783779621124\n",
      "epoch 50, iter 2, loss -0.1365913450717926\n",
      "epoch 50, loss -0.3857544809579849\n",
      "epoch 51, iter 0, loss -0.1338014304637909\n",
      "epoch 51, iter 1, loss -0.11978474259376526\n",
      "epoch 51, iter 2, loss -0.14733651280403137\n",
      "epoch 51, loss -0.4009226858615875\n",
      "epoch 52, iter 0, loss -0.1317826509475708\n",
      "epoch 52, iter 1, loss -0.16597609221935272\n",
      "epoch 52, iter 2, loss -0.1455177366733551\n",
      "epoch 52, loss -0.4432764798402786\n",
      "epoch 53, iter 0, loss -0.1678754836320877\n",
      "epoch 53, iter 1, loss -0.1354336440563202\n",
      "epoch 53, iter 2, loss -0.175557941198349\n",
      "epoch 53, loss -0.4788670688867569\n",
      "epoch 54, iter 0, loss -0.16690391302108765\n",
      "epoch 54, iter 1, loss -0.1657560020685196\n",
      "epoch 54, iter 2, loss -0.17028002440929413\n",
      "epoch 54, loss -0.5029399394989014\n",
      "epoch 55, iter 0, loss -0.1815488338470459\n",
      "epoch 55, iter 1, loss -0.13593505322933197\n",
      "epoch 55, iter 2, loss -0.1766039878129959\n",
      "epoch 55, loss -0.4940878748893738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56, iter 0, loss -0.14733286201953888\n",
      "epoch 56, iter 1, loss -0.1869824230670929\n",
      "epoch 56, iter 2, loss -0.1786334365606308\n",
      "epoch 56, loss -0.5129487216472626\n",
      "epoch 57, iter 0, loss -0.2015174925327301\n",
      "epoch 57, iter 1, loss -0.16509078443050385\n",
      "epoch 57, iter 2, loss -0.17988379299640656\n",
      "epoch 57, loss -0.5464920699596405\n",
      "epoch 58, iter 0, loss -0.17401562631130219\n",
      "epoch 58, iter 1, loss -0.18563273549079895\n",
      "epoch 58, iter 2, loss -0.19111087918281555\n",
      "epoch 58, loss -0.5507592409849167\n",
      "epoch 59, iter 0, loss -0.19152206182479858\n",
      "epoch 59, iter 1, loss -0.17473024129867554\n",
      "epoch 59, iter 2, loss -0.1583530455827713\n",
      "epoch 59, loss -0.5246053487062454\n",
      "epoch 60, iter 0, loss -0.16331236064434052\n",
      "epoch 60, iter 1, loss -0.1731075942516327\n",
      "epoch 60, iter 2, loss -0.16199259459972382\n",
      "epoch 60, loss -0.498412549495697\n",
      "epoch 61, iter 0, loss -0.18482406437397003\n",
      "epoch 61, iter 1, loss -0.1457127332687378\n",
      "epoch 61, iter 2, loss -0.19536083936691284\n",
      "epoch 61, loss -0.5258976370096207\n",
      "epoch 62, iter 0, loss -0.17359000444412231\n",
      "epoch 62, iter 1, loss -0.1673256754875183\n",
      "epoch 62, iter 2, loss -0.16448131203651428\n",
      "epoch 62, loss -0.5053969919681549\n",
      "epoch 63, iter 0, loss -0.17392610013484955\n",
      "epoch 63, iter 1, loss -0.17011260986328125\n",
      "epoch 63, iter 2, loss -0.18651524186134338\n",
      "epoch 63, loss -0.5305539518594742\n",
      "epoch 64, iter 0, loss -0.19603081047534943\n",
      "epoch 64, iter 1, loss -0.1707887500524521\n",
      "epoch 64, iter 2, loss -0.15681298077106476\n",
      "epoch 64, loss -0.5236325412988663\n",
      "epoch 65, iter 0, loss -0.19570131599903107\n",
      "epoch 65, iter 1, loss -0.18624360859394073\n",
      "epoch 65, iter 2, loss -0.1740277260541916\n",
      "epoch 65, loss -0.5559726506471634\n",
      "epoch 66, iter 0, loss -0.19483807682991028\n",
      "epoch 66, iter 1, loss -0.16547444462776184\n",
      "epoch 66, iter 2, loss -0.17562422156333923\n",
      "epoch 66, loss -0.5359367430210114\n",
      "epoch 67, iter 0, loss -0.18101496994495392\n",
      "epoch 67, iter 1, loss -0.18680328130722046\n",
      "epoch 67, iter 2, loss -0.19328226149082184\n",
      "epoch 67, loss -0.5611005127429962\n",
      "epoch 68, iter 0, loss -0.15903063118457794\n",
      "epoch 68, iter 1, loss -0.1941973865032196\n",
      "epoch 68, iter 2, loss -0.14691513776779175\n",
      "epoch 68, loss -0.5001431554555893\n",
      "epoch 69, iter 0, loss -0.17511442303657532\n",
      "epoch 69, iter 1, loss -0.1970333456993103\n",
      "epoch 69, iter 2, loss -0.12166391313076019\n",
      "epoch 69, loss -0.4938116818666458\n",
      "epoch 70, iter 0, loss -0.17549674212932587\n",
      "epoch 70, iter 1, loss -0.17391155660152435\n",
      "epoch 70, iter 2, loss -0.16229481995105743\n",
      "epoch 70, loss -0.5117031186819077\n",
      "epoch 71, iter 0, loss -0.19481395184993744\n",
      "epoch 71, iter 1, loss -0.17260372638702393\n",
      "epoch 71, iter 2, loss -0.1718364655971527\n",
      "epoch 71, loss -0.5392541438341141\n",
      "epoch 72, iter 0, loss -0.2035796046257019\n",
      "epoch 72, iter 1, loss -0.16817134618759155\n",
      "epoch 72, iter 2, loss -0.18672724068164825\n",
      "epoch 72, loss -0.5584781914949417\n",
      "epoch 73, iter 0, loss -0.2255818098783493\n",
      "epoch 73, iter 1, loss -0.16754749417304993\n",
      "epoch 73, iter 2, loss -0.1906772404909134\n",
      "epoch 73, loss -0.5838065445423126\n",
      "epoch 74, iter 0, loss -0.21001291275024414\n",
      "epoch 74, iter 1, loss -0.18445880711078644\n",
      "epoch 74, iter 2, loss -0.2050025314092636\n",
      "epoch 74, loss -0.5994742512702942\n",
      "epoch 75, iter 0, loss -0.2140374481678009\n",
      "epoch 75, iter 1, loss -0.2012825608253479\n",
      "epoch 75, iter 2, loss -0.23262493312358856\n",
      "epoch 75, loss -0.6479449421167374\n",
      "epoch 76, iter 0, loss -0.20920458436012268\n",
      "epoch 76, iter 1, loss -0.2375836819410324\n",
      "epoch 76, iter 2, loss -0.1818656325340271\n",
      "epoch 76, loss -0.6286538988351822\n",
      "epoch 77, iter 0, loss -0.20400899648666382\n",
      "epoch 77, iter 1, loss -0.23372836410999298\n",
      "epoch 77, iter 2, loss -0.1664411723613739\n",
      "epoch 77, loss -0.6041785329580307\n",
      "epoch 78, iter 0, loss -0.20762620866298676\n",
      "epoch 78, iter 1, loss -0.23023462295532227\n",
      "epoch 78, iter 2, loss -0.18517351150512695\n",
      "epoch 78, loss -0.623034343123436\n",
      "epoch 79, iter 0, loss -0.23061603307724\n",
      "epoch 79, iter 1, loss -0.21367022395133972\n",
      "epoch 79, iter 2, loss -0.1797482818365097\n",
      "epoch 79, loss -0.6240345388650894\n",
      "epoch 80, iter 0, loss -0.230522021651268\n",
      "epoch 80, iter 1, loss -0.18148422241210938\n",
      "epoch 80, iter 2, loss -0.19724009931087494\n",
      "epoch 80, loss -0.6092463433742523\n",
      "epoch 81, iter 0, loss -0.19554957747459412\n",
      "epoch 81, iter 1, loss -0.17835727334022522\n",
      "epoch 81, iter 2, loss -0.19239933788776398\n",
      "epoch 81, loss -0.5663061887025833\n",
      "epoch 82, iter 0, loss -0.20378077030181885\n",
      "epoch 82, iter 1, loss -0.20222510397434235\n",
      "epoch 82, iter 2, loss -0.19749848544597626\n",
      "epoch 82, loss -0.6035043597221375\n",
      "epoch 83, iter 0, loss -0.20113617181777954\n",
      "epoch 83, iter 1, loss -0.19846703112125397\n",
      "epoch 83, iter 2, loss -0.2405216544866562\n",
      "epoch 83, loss -0.6401248574256897\n",
      "epoch 84, iter 0, loss -0.21504949033260345\n",
      "epoch 84, iter 1, loss -0.2041953206062317\n",
      "epoch 84, iter 2, loss -0.2394959032535553\n",
      "epoch 84, loss -0.6587407141923904\n",
      "epoch 85, iter 0, loss -0.21586760878562927\n",
      "epoch 85, iter 1, loss -0.23503784835338593\n",
      "epoch 85, iter 2, loss -0.2198449969291687\n",
      "epoch 85, loss -0.6707504540681839\n",
      "epoch 86, iter 0, loss -0.23207394778728485\n",
      "epoch 86, iter 1, loss -0.23582014441490173\n",
      "epoch 86, iter 2, loss -0.23378632962703705\n",
      "epoch 86, loss -0.7016804218292236\n",
      "epoch 87, iter 0, loss -0.23991210758686066\n",
      "epoch 87, iter 1, loss -0.24635016918182373\n",
      "epoch 87, iter 2, loss -0.250130832195282\n",
      "epoch 87, loss -0.7363931089639664\n",
      "epoch 88, iter 0, loss -0.2538739740848541\n",
      "epoch 88, iter 1, loss -0.2587190568447113\n",
      "epoch 88, iter 2, loss -0.22778809070587158\n",
      "epoch 88, loss -0.740381121635437\n",
      "epoch 89, iter 0, loss -0.2265864461660385\n",
      "epoch 89, iter 1, loss -0.24827900528907776\n",
      "epoch 89, iter 2, loss -0.25465771555900574\n",
      "epoch 89, loss -0.729523167014122\n",
      "epoch 90, iter 0, loss -0.26275551319122314\n",
      "epoch 90, iter 1, loss -0.22654584050178528\n",
      "epoch 90, iter 2, loss -0.227430522441864\n",
      "epoch 90, loss -0.7167318761348724\n",
      "epoch 91, iter 0, loss -0.26815474033355713\n",
      "epoch 91, iter 1, loss -0.22268863022327423\n",
      "epoch 91, iter 2, loss -0.23966951668262482\n",
      "epoch 91, loss -0.7305128872394562\n",
      "epoch 92, iter 0, loss -0.2569853365421295\n",
      "epoch 92, iter 1, loss -0.24836643040180206\n",
      "epoch 92, iter 2, loss -0.20019611716270447\n",
      "epoch 92, loss -0.705547884106636\n",
      "epoch 93, iter 0, loss -0.2633247971534729\n",
      "epoch 93, iter 1, loss -0.19709335267543793\n",
      "epoch 93, iter 2, loss -0.23266716301441193\n",
      "epoch 93, loss -0.6930853128433228\n",
      "epoch 94, iter 0, loss -0.2425403743982315\n",
      "epoch 94, iter 1, loss -0.23067490756511688\n",
      "epoch 94, iter 2, loss -0.22124171257019043\n",
      "epoch 94, loss -0.6944569945335388\n",
      "epoch 95, iter 0, loss -0.24665020406246185\n",
      "epoch 95, iter 1, loss -0.22900238633155823\n",
      "epoch 95, iter 2, loss -0.18601350486278534\n",
      "epoch 95, loss -0.6616660952568054\n",
      "epoch 96, iter 0, loss -0.23468689620494843\n",
      "epoch 96, iter 1, loss -0.2162615805864334\n",
      "epoch 96, iter 2, loss -0.22303684055805206\n",
      "epoch 96, loss -0.6739853173494339\n",
      "epoch 97, iter 0, loss -0.2429581731557846\n",
      "epoch 97, iter 1, loss -0.22558431327342987\n",
      "epoch 97, iter 2, loss -0.2041510045528412\n",
      "epoch 97, loss -0.6726934909820557\n",
      "epoch 98, iter 0, loss -0.2238507717847824\n",
      "epoch 98, iter 1, loss -0.22899901866912842\n",
      "epoch 98, iter 2, loss -0.2447865903377533\n",
      "epoch 98, loss -0.6976363807916641\n",
      "epoch 99, iter 0, loss -0.22053606808185577\n",
      "epoch 99, iter 1, loss -0.2469337284564972\n",
      "epoch 99, iter 2, loss -0.2224353700876236\n",
      "epoch 99, loss -0.6899051666259766\n",
      "2019-03-31 00:33:01.991943, fold=0, rep=1, eta=0d 0h 40m 55s \n",
      "{'fold': 0, 'repeat': 1, 'n': 4137, 'd': 20, 'mse': 0.013747315853834152, 'train_time': 604.45659733098, 'trained_epochs': 100, 'prior_train_nmll': -0.18142959475517273, 'train_nll': -1611.841064453125, 'test_nll': -808.5728759765625, 'train_mse': 0.01482431497424841, 'state_dict_file': 'model_state_dict_5114432513571493183.pkl'}\n",
      "epoch 0, iter 0, loss 1.9644763469696045\n",
      "epoch 0, iter 1, loss 2.076463222503662\n",
      "epoch 0, iter 2, loss 2.1413004398345947\n",
      "epoch 0, loss 6.182240009307861\n",
      "epoch 1, iter 0, loss 1.980244755744934\n",
      "epoch 1, iter 1, loss 1.9026540517807007\n",
      "epoch 1, iter 2, loss 1.8589560985565186\n",
      "epoch 1, loss 5.741854906082153\n",
      "epoch 2, iter 0, loss 1.7360132932662964\n",
      "epoch 2, iter 1, loss 1.579017162322998\n",
      "epoch 2, iter 2, loss 1.5373852252960205\n",
      "epoch 2, loss 4.852415680885315\n",
      "epoch 3, iter 0, loss 1.548632264137268\n",
      "epoch 3, iter 1, loss 1.5046848058700562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, iter 2, loss 1.4336072206497192\n",
      "epoch 3, loss 4.4869242906570435\n",
      "epoch 4, iter 0, loss 1.3992135524749756\n",
      "epoch 4, iter 1, loss 1.394059181213379\n",
      "epoch 4, iter 2, loss 1.362878680229187\n",
      "epoch 4, loss 4.1561514139175415\n",
      "epoch 5, iter 0, loss 1.3297865390777588\n",
      "epoch 5, iter 1, loss 1.312618613243103\n",
      "epoch 5, iter 2, loss 1.2913451194763184\n",
      "epoch 5, loss 3.93375027179718\n",
      "epoch 6, iter 0, loss 1.2595621347427368\n",
      "epoch 6, iter 1, loss 1.2368699312210083\n",
      "epoch 6, iter 2, loss 1.2134613990783691\n",
      "epoch 6, loss 3.7098934650421143\n",
      "epoch 7, iter 0, loss 1.1877251863479614\n",
      "epoch 7, iter 1, loss 1.159009575843811\n",
      "epoch 7, iter 2, loss 1.1361982822418213\n",
      "epoch 7, loss 3.4829330444335938\n",
      "epoch 8, iter 0, loss 1.103775978088379\n",
      "epoch 8, iter 1, loss 1.0798747539520264\n",
      "epoch 8, iter 2, loss 1.0495402812957764\n",
      "epoch 8, loss 3.2331910133361816\n",
      "epoch 9, iter 0, loss 1.0196022987365723\n",
      "epoch 9, iter 1, loss 0.9962444305419922\n",
      "epoch 9, iter 2, loss 0.9570370316505432\n",
      "epoch 9, loss 2.9728837609291077\n",
      "epoch 10, iter 0, loss 0.9238151907920837\n",
      "epoch 10, iter 1, loss 0.8968049883842468\n",
      "epoch 10, iter 2, loss 0.8684908747673035\n",
      "epoch 10, loss 2.689111053943634\n",
      "epoch 11, iter 0, loss 0.8276352882385254\n",
      "epoch 11, iter 1, loss 0.79606032371521\n",
      "epoch 11, iter 2, loss 0.7625709176063538\n",
      "epoch 11, loss 2.386266529560089\n",
      "epoch 12, iter 0, loss 0.722550094127655\n",
      "epoch 12, iter 1, loss 0.6892261505126953\n",
      "epoch 12, iter 2, loss 0.6502470970153809\n",
      "epoch 12, loss 2.062023341655731\n",
      "epoch 13, iter 0, loss 0.6153324246406555\n",
      "epoch 13, iter 1, loss 0.5859964489936829\n",
      "epoch 13, iter 2, loss 0.5536938905715942\n",
      "epoch 13, loss 1.7550227642059326\n",
      "epoch 14, iter 0, loss 0.5286862254142761\n",
      "epoch 14, iter 1, loss 0.4901580512523651\n",
      "epoch 14, iter 2, loss 0.48100799322128296\n",
      "epoch 14, loss 1.4998522698879242\n",
      "epoch 15, iter 0, loss 0.4546566605567932\n",
      "epoch 15, iter 1, loss 0.4295542240142822\n",
      "epoch 15, iter 2, loss 0.42628175020217896\n",
      "epoch 15, loss 1.3104926347732544\n",
      "epoch 16, iter 0, loss 0.40093544125556946\n",
      "epoch 16, iter 1, loss 0.3882882595062256\n",
      "epoch 16, iter 2, loss 0.3931657671928406\n",
      "epoch 16, loss 1.1823894679546356\n",
      "epoch 17, iter 0, loss 0.38854068517684937\n",
      "epoch 17, iter 1, loss 0.3684845566749573\n",
      "epoch 17, iter 2, loss 0.3748147189617157\n",
      "epoch 17, loss 1.1318399608135223\n",
      "epoch 18, iter 0, loss 0.3775976598262787\n",
      "epoch 18, iter 1, loss 0.36025312542915344\n",
      "epoch 18, iter 2, loss 0.38952961564064026\n",
      "epoch 18, loss 1.1273804008960724\n",
      "epoch 19, iter 0, loss 0.37089186906814575\n",
      "epoch 19, iter 1, loss 0.3768309950828552\n",
      "epoch 19, iter 2, loss 0.38698503375053406\n",
      "epoch 19, loss 1.134707897901535\n",
      "epoch 20, iter 0, loss 0.393507182598114\n",
      "epoch 20, iter 1, loss 0.3598867952823639\n",
      "epoch 20, iter 2, loss 0.3724640905857086\n",
      "epoch 20, loss 1.1258580684661865\n",
      "epoch 21, iter 0, loss 0.3678150177001953\n",
      "epoch 21, iter 1, loss 0.3276675045490265\n",
      "epoch 21, iter 2, loss 0.35230696201324463\n",
      "epoch 21, loss 1.0477894842624664\n",
      "epoch 22, iter 0, loss 0.3181554973125458\n",
      "epoch 22, iter 1, loss 0.3000519275665283\n",
      "epoch 22, iter 2, loss 0.2615450322628021\n",
      "epoch 22, loss 0.8797524571418762\n",
      "epoch 23, iter 0, loss 0.24531209468841553\n",
      "epoch 23, iter 1, loss 0.2438143789768219\n",
      "epoch 23, iter 2, loss 0.21269062161445618\n",
      "epoch 23, loss 0.7018170952796936\n",
      "epoch 24, iter 0, loss 0.1804758906364441\n",
      "epoch 24, iter 1, loss 0.1806127429008484\n",
      "epoch 24, iter 2, loss 0.15543243288993835\n",
      "epoch 24, loss 0.5165210664272308\n",
      "epoch 25, iter 0, loss 0.14619098603725433\n",
      "epoch 25, iter 1, loss 0.1113748699426651\n",
      "epoch 25, iter 2, loss 0.10397474467754364\n",
      "epoch 25, loss 0.3615406006574631\n",
      "epoch 26, iter 0, loss 0.09836079180240631\n",
      "epoch 26, iter 1, loss 0.0693017989397049\n",
      "epoch 26, iter 2, loss 0.07683233916759491\n",
      "epoch 26, loss 0.24449492990970612\n",
      "epoch 27, iter 0, loss 0.06289470195770264\n",
      "epoch 27, iter 1, loss 0.03078034520149231\n",
      "epoch 27, iter 2, loss 0.0216892808675766\n",
      "epoch 27, loss 0.11536432802677155\n",
      "epoch 28, iter 0, loss -0.0027009397745132446\n",
      "epoch 28, iter 1, loss 0.02411399781703949\n",
      "epoch 28, iter 2, loss -0.004448533058166504\n",
      "epoch 28, loss 0.01696452498435974\n",
      "epoch 29, iter 0, loss -0.015622228384017944\n",
      "epoch 29, iter 1, loss -0.012300118803977966\n",
      "epoch 29, iter 2, loss -0.050711750984191895\n",
      "epoch 29, loss -0.0786340981721878\n",
      "epoch 30, iter 0, loss -0.040158942341804504\n",
      "epoch 30, iter 1, loss -0.039868757128715515\n",
      "epoch 30, iter 2, loss -0.0332467257976532\n",
      "epoch 30, loss -0.11327442526817322\n",
      "epoch 31, iter 0, loss -0.06147906184196472\n",
      "epoch 31, iter 1, loss -0.04085454344749451\n",
      "epoch 31, iter 2, loss -0.03282627463340759\n",
      "epoch 31, loss -0.13515987992286682\n",
      "epoch 32, iter 0, loss -0.045486241579055786\n",
      "epoch 32, iter 1, loss -0.0326727032661438\n",
      "epoch 32, iter 2, loss -0.021411269903182983\n",
      "epoch 32, loss -0.09957021474838257\n",
      "epoch 33, iter 0, loss -0.04433315992355347\n",
      "epoch 33, iter 1, loss -0.028997302055358887\n",
      "epoch 33, iter 2, loss -0.03012537956237793\n",
      "epoch 33, loss -0.10345584154129028\n",
      "epoch 34, iter 0, loss -0.0443289577960968\n",
      "epoch 34, iter 1, loss -0.01261061429977417\n",
      "epoch 34, iter 2, loss -0.04837989807128906\n",
      "epoch 34, loss -0.10531947016716003\n",
      "epoch 35, iter 0, loss -0.028363287448883057\n",
      "epoch 35, iter 1, loss -0.05085718631744385\n",
      "epoch 35, iter 2, loss -0.040788233280181885\n",
      "epoch 35, loss -0.12000870704650879\n",
      "epoch 36, iter 0, loss -0.045913636684417725\n",
      "epoch 36, iter 1, loss -0.03768312931060791\n",
      "epoch 36, iter 2, loss -0.041284769773483276\n",
      "epoch 36, loss -0.12488153576850891\n",
      "epoch 37, iter 0, loss -0.04821497201919556\n",
      "epoch 37, iter 1, loss -0.04679626226425171\n",
      "epoch 37, iter 2, loss -0.06270366907119751\n",
      "epoch 37, loss -0.15771490335464478\n",
      "epoch 38, iter 0, loss -0.07881920039653778\n",
      "epoch 38, iter 1, loss -0.050320178270339966\n",
      "epoch 38, iter 2, loss -0.0828353762626648\n",
      "epoch 38, loss -0.21197475492954254\n",
      "epoch 39, iter 0, loss -0.08288532495498657\n",
      "epoch 39, iter 1, loss -0.10259135067462921\n",
      "epoch 39, iter 2, loss -0.08574365079402924\n",
      "epoch 39, loss -0.271220326423645\n",
      "epoch 40, iter 0, loss -0.08573177456855774\n",
      "epoch 40, iter 1, loss -0.11086006462574005\n",
      "epoch 40, iter 2, loss -0.10051211714744568\n",
      "epoch 40, loss -0.29710395634174347\n",
      "epoch 41, iter 0, loss -0.11524318158626556\n",
      "epoch 41, iter 1, loss -0.13651151955127716\n",
      "epoch 41, iter 2, loss -0.10652178525924683\n",
      "epoch 41, loss -0.35827648639678955\n",
      "epoch 42, iter 0, loss -0.12547606229782104\n",
      "epoch 42, iter 1, loss -0.13186471164226532\n",
      "epoch 42, iter 2, loss -0.11296346783638\n",
      "epoch 42, loss -0.37030424177646637\n",
      "epoch 43, iter 0, loss -0.14650550484657288\n",
      "epoch 43, iter 1, loss -0.12780122458934784\n",
      "epoch 43, iter 2, loss -0.141188383102417\n",
      "epoch 43, loss -0.4154951125383377\n",
      "epoch 44, iter 0, loss -0.1479724943637848\n",
      "epoch 44, iter 1, loss -0.1314985305070877\n",
      "epoch 44, iter 2, loss -0.13521608710289001\n",
      "epoch 44, loss -0.4146871119737625\n",
      "epoch 45, iter 0, loss -0.12870745360851288\n",
      "epoch 45, iter 1, loss -0.16237258911132812\n",
      "epoch 45, iter 2, loss -0.127521812915802\n",
      "epoch 45, loss -0.418601855635643\n",
      "epoch 46, iter 0, loss -0.1571841686964035\n",
      "epoch 46, iter 1, loss -0.15812166035175323\n",
      "epoch 46, iter 2, loss -0.11813187599182129\n",
      "epoch 46, loss -0.433437705039978\n",
      "epoch 47, iter 0, loss -0.18005284667015076\n",
      "epoch 47, iter 1, loss -0.1536247432231903\n",
      "epoch 47, iter 2, loss -0.16436679661273956\n",
      "epoch 47, loss -0.4980443865060806\n",
      "epoch 48, iter 0, loss -0.16242137551307678\n",
      "epoch 48, iter 1, loss -0.17110779881477356\n",
      "epoch 48, iter 2, loss -0.15572026371955872\n",
      "epoch 48, loss -0.48924943804740906\n",
      "epoch 49, iter 0, loss -0.16962692141532898\n",
      "epoch 49, iter 1, loss -0.13303442299365997\n",
      "epoch 49, iter 2, loss -0.14848123490810394\n",
      "epoch 49, loss -0.4511425793170929\n",
      "epoch 50, iter 0, loss -0.15054666996002197\n",
      "epoch 50, iter 1, loss -0.16310842335224152\n",
      "epoch 50, iter 2, loss -0.14776310324668884\n",
      "epoch 50, loss -0.46141819655895233\n",
      "epoch 51, iter 0, loss -0.14743345975875854\n",
      "epoch 51, iter 1, loss -0.14145272970199585\n",
      "epoch 51, iter 2, loss -0.12495297193527222\n",
      "epoch 51, loss -0.4138391613960266\n",
      "epoch 52, iter 0, loss -0.1752404421567917\n",
      "epoch 52, iter 1, loss -0.13085494935512543\n",
      "epoch 52, iter 2, loss -0.12075857818126678\n",
      "epoch 52, loss -0.4268539696931839\n",
      "epoch 53, iter 0, loss -0.12884420156478882\n",
      "epoch 53, iter 1, loss -0.14445431530475616\n",
      "epoch 53, iter 2, loss -0.1605449914932251\n",
      "epoch 53, loss -0.4338435083627701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54, iter 0, loss -0.16507922112941742\n",
      "epoch 54, iter 1, loss -0.17878909409046173\n",
      "epoch 54, iter 2, loss -0.14645682275295258\n",
      "epoch 54, loss -0.4903251379728317\n",
      "epoch 55, iter 0, loss -0.17399537563323975\n",
      "epoch 55, iter 1, loss -0.17388799786567688\n",
      "epoch 55, iter 2, loss -0.19148896634578705\n",
      "epoch 55, loss -0.5393723398447037\n",
      "epoch 56, iter 0, loss -0.1976570188999176\n",
      "epoch 56, iter 1, loss -0.2132372409105301\n",
      "epoch 56, iter 2, loss -0.1748008131980896\n",
      "epoch 56, loss -0.5856950730085373\n",
      "epoch 57, iter 0, loss -0.20374955236911774\n",
      "epoch 57, iter 1, loss -0.1928609013557434\n",
      "epoch 57, iter 2, loss -0.21909348666667938\n",
      "epoch 57, loss -0.6157039403915405\n",
      "epoch 58, iter 0, loss -0.21080072224140167\n",
      "epoch 58, iter 1, loss -0.22705000638961792\n",
      "epoch 58, iter 2, loss -0.2078537791967392\n",
      "epoch 58, loss -0.6457045078277588\n",
      "epoch 59, iter 0, loss -0.21716946363449097\n",
      "epoch 59, iter 1, loss -0.21763037145137787\n",
      "epoch 59, iter 2, loss -0.18862062692642212\n",
      "epoch 59, loss -0.623420462012291\n",
      "epoch 60, iter 0, loss -0.23068946599960327\n",
      "epoch 60, iter 1, loss -0.18018898367881775\n",
      "epoch 60, iter 2, loss -0.18005524575710297\n",
      "epoch 60, loss -0.590933695435524\n",
      "epoch 61, iter 0, loss -0.2211514413356781\n",
      "epoch 61, iter 1, loss -0.1813644915819168\n",
      "epoch 61, iter 2, loss -0.2102375328540802\n",
      "epoch 61, loss -0.6127534657716751\n",
      "epoch 62, iter 0, loss -0.18610166013240814\n",
      "epoch 62, iter 1, loss -0.2021886557340622\n",
      "epoch 62, iter 2, loss -0.22360819578170776\n",
      "epoch 62, loss -0.6118985116481781\n",
      "epoch 63, iter 0, loss -0.1894015669822693\n",
      "epoch 63, iter 1, loss -0.19755490124225616\n",
      "epoch 63, iter 2, loss -0.16050797700881958\n",
      "epoch 63, loss -0.547464445233345\n",
      "epoch 64, iter 0, loss -0.1972622126340866\n",
      "epoch 64, iter 1, loss -0.1761879026889801\n",
      "epoch 64, iter 2, loss -0.1937769055366516\n",
      "epoch 64, loss -0.5672270208597183\n",
      "epoch 65, iter 0, loss -0.1881169229745865\n",
      "epoch 65, iter 1, loss -0.20808915793895721\n",
      "epoch 65, iter 2, loss -0.19692009687423706\n",
      "epoch 65, loss -0.5931261777877808\n",
      "epoch 66, iter 0, loss -0.1997203826904297\n",
      "epoch 66, iter 1, loss -0.20997992157936096\n",
      "epoch 66, iter 2, loss -0.1851681023836136\n",
      "epoch 66, loss -0.5948684066534042\n",
      "epoch 67, iter 0, loss -0.22786295413970947\n",
      "epoch 67, iter 1, loss -0.20234650373458862\n",
      "epoch 67, iter 2, loss -0.21282988786697388\n",
      "epoch 67, loss -0.643039345741272\n",
      "epoch 68, iter 0, loss -0.2215968668460846\n",
      "epoch 68, iter 1, loss -0.238189697265625\n",
      "epoch 68, iter 2, loss -0.21851564943790436\n",
      "epoch 68, loss -0.678302213549614\n",
      "epoch 69, iter 0, loss -0.24289749562740326\n",
      "epoch 69, iter 1, loss -0.24372941255569458\n",
      "epoch 69, iter 2, loss -0.22982805967330933\n",
      "epoch 69, loss -0.7164549678564072\n",
      "epoch 70, iter 0, loss -0.23426564037799835\n",
      "epoch 70, iter 1, loss -0.2596505284309387\n",
      "epoch 70, iter 2, loss -0.2536529302597046\n",
      "epoch 70, loss -0.7475690990686417\n",
      "epoch 71, iter 0, loss -0.24774734675884247\n",
      "epoch 71, iter 1, loss -0.2532651424407959\n",
      "epoch 71, iter 2, loss -0.26523441076278687\n",
      "epoch 71, loss -0.7662468999624252\n",
      "epoch 72, iter 0, loss -0.241162970662117\n",
      "epoch 72, iter 1, loss -0.2779484689235687\n",
      "epoch 72, iter 2, loss -0.23863722383975983\n",
      "epoch 72, loss -0.7577486634254456\n",
      "epoch 73, iter 0, loss -0.26570039987564087\n",
      "epoch 73, iter 1, loss -0.23388932645320892\n",
      "epoch 73, iter 2, loss -0.23582099378108978\n",
      "epoch 73, loss -0.7354107201099396\n",
      "epoch 74, iter 0, loss -0.2391170710325241\n",
      "epoch 74, iter 1, loss -0.2401515692472458\n",
      "epoch 74, iter 2, loss -0.2440335899591446\n",
      "epoch 74, loss -0.7233022302389145\n",
      "epoch 75, iter 0, loss -0.23590779304504395\n",
      "epoch 75, iter 1, loss -0.20054450631141663\n",
      "epoch 75, iter 2, loss -0.23633381724357605\n",
      "epoch 75, loss -0.6727861166000366\n",
      "epoch 76, iter 0, loss -0.23184257745742798\n",
      "epoch 76, iter 1, loss -0.2145380675792694\n",
      "epoch 76, iter 2, loss -0.21811163425445557\n",
      "epoch 76, loss -0.664492279291153\n",
      "epoch 77, iter 0, loss -0.21333612501621246\n",
      "epoch 77, iter 1, loss -0.20061632990837097\n",
      "epoch 77, iter 2, loss -0.23117145895957947\n",
      "epoch 77, loss -0.6451239138841629\n",
      "epoch 78, iter 0, loss -0.25818413496017456\n",
      "epoch 78, iter 1, loss -0.21394406259059906\n",
      "epoch 78, iter 2, loss -0.21272753179073334\n",
      "epoch 78, loss -0.684855729341507\n",
      "epoch 79, iter 0, loss -0.2172771692276001\n",
      "epoch 79, iter 1, loss -0.26108020544052124\n",
      "epoch 79, iter 2, loss -0.19830600917339325\n",
      "epoch 79, loss -0.6766633838415146\n",
      "epoch 80, iter 0, loss -0.2510175108909607\n",
      "epoch 80, iter 1, loss -0.21697117388248444\n",
      "epoch 80, iter 2, loss -0.2238452285528183\n",
      "epoch 80, loss -0.6918339133262634\n",
      "epoch 81, iter 0, loss -0.24708330631256104\n",
      "epoch 81, iter 1, loss -0.24754448235034943\n",
      "epoch 81, iter 2, loss -0.24728924036026\n",
      "epoch 81, loss -0.7419170290231705\n",
      "epoch 82, iter 0, loss -0.2849159836769104\n",
      "epoch 82, iter 1, loss -0.2419462949037552\n",
      "epoch 82, iter 2, loss -0.22195731103420258\n",
      "epoch 82, loss -0.7488195896148682\n",
      "epoch 83, iter 0, loss -0.2687203884124756\n",
      "epoch 83, iter 1, loss -0.2211589813232422\n",
      "epoch 83, iter 2, loss -0.2718062400817871\n",
      "epoch 83, loss -0.7616856098175049\n",
      "epoch 84, iter 0, loss -0.2643795609474182\n",
      "epoch 84, iter 1, loss -0.24270521104335785\n",
      "epoch 84, iter 2, loss -0.23954792320728302\n",
      "epoch 84, loss -0.7466326951980591\n",
      "epoch 85, iter 0, loss -0.2537631094455719\n",
      "epoch 85, iter 1, loss -0.22674429416656494\n",
      "epoch 85, iter 2, loss -0.26885363459587097\n",
      "epoch 85, loss -0.7493610382080078\n",
      "epoch 86, iter 0, loss -0.24141666293144226\n",
      "epoch 86, iter 1, loss -0.26246896386146545\n",
      "epoch 86, iter 2, loss -0.241776704788208\n",
      "epoch 86, loss -0.7456623315811157\n",
      "epoch 87, iter 0, loss -0.2574772238731384\n",
      "epoch 87, iter 1, loss -0.2501727342605591\n",
      "epoch 87, iter 2, loss -0.2467513233423233\n",
      "epoch 87, loss -0.7544012814760208\n",
      "epoch 88, iter 0, loss -0.25102391839027405\n",
      "epoch 88, iter 1, loss -0.22320669889450073\n",
      "epoch 88, iter 2, loss -0.27170324325561523\n",
      "epoch 88, loss -0.74593386054039\n",
      "epoch 89, iter 0, loss -0.2639298141002655\n",
      "epoch 89, iter 1, loss -0.23532313108444214\n",
      "epoch 89, iter 2, loss -0.2731993794441223\n",
      "epoch 89, loss -0.77245232462883\n",
      "epoch 90, iter 0, loss -0.26559877395629883\n",
      "epoch 90, iter 1, loss -0.2543049156665802\n",
      "epoch 90, iter 2, loss -0.26901569962501526\n",
      "epoch 90, loss -0.7889193892478943\n",
      "epoch 91, iter 0, loss -0.24191732704639435\n",
      "epoch 91, iter 1, loss -0.28005126118659973\n",
      "epoch 91, iter 2, loss -0.25860777497291565\n",
      "epoch 91, loss -0.7805763632059097\n",
      "epoch 92, iter 0, loss -0.2713537812232971\n",
      "epoch 92, iter 1, loss -0.27424079179763794\n",
      "epoch 92, iter 2, loss -0.24197475612163544\n",
      "epoch 92, loss -0.7875693291425705\n",
      "epoch 93, iter 0, loss -0.2739613950252533\n",
      "epoch 93, iter 1, loss -0.24092356860637665\n",
      "epoch 93, iter 2, loss -0.2759472131729126\n",
      "epoch 93, loss -0.7908321768045425\n",
      "epoch 94, iter 0, loss -0.26870131492614746\n",
      "epoch 94, iter 1, loss -0.2700275182723999\n",
      "epoch 94, iter 2, loss -0.24951858818531036\n",
      "epoch 94, loss -0.7882474213838577\n",
      "epoch 95, iter 0, loss -0.25992515683174133\n",
      "epoch 95, iter 1, loss -0.27253180742263794\n",
      "epoch 95, iter 2, loss -0.2411862164735794\n",
      "epoch 95, loss -0.7736431807279587\n",
      "epoch 96, iter 0, loss -0.2760086953639984\n",
      "epoch 96, iter 1, loss -0.2426651269197464\n",
      "epoch 96, iter 2, loss -0.26664257049560547\n",
      "epoch 96, loss -0.7853163927793503\n",
      "epoch 97, iter 0, loss -0.2703193426132202\n",
      "epoch 97, iter 1, loss -0.2643263339996338\n",
      "epoch 97, iter 2, loss -0.23669689893722534\n",
      "epoch 97, loss -0.7713425755500793\n",
      "epoch 98, iter 0, loss -0.2703476846218109\n",
      "epoch 98, iter 1, loss -0.22947826981544495\n",
      "epoch 98, iter 2, loss -0.24999070167541504\n",
      "epoch 98, loss -0.7498166561126709\n",
      "epoch 99, iter 0, loss -0.29811185598373413\n",
      "epoch 99, iter 1, loss -0.24690423905849457\n",
      "epoch 99, iter 2, loss -0.2278936356306076\n",
      "epoch 99, loss -0.7729097306728363\n",
      "2019-03-31 00:43:18.066755, fold=1, rep=0, eta=0d 0h 30m 43s \n",
      "{'fold': 1, 'repeat': 0, 'n': 4137, 'd': 20, 'mse': 0.013735827058553696, 'train_time': 614.6785335328896, 'trained_epochs': 100, 'prior_train_nmll': -0.19279512763023376, 'train_nll': -1693.223388671875, 'test_nll': -840.3167724609375, 'train_mse': 0.014447392895817757, 'state_dict_file': 'model_state_dict_6274405278018297689.pkl'}\n",
      "epoch 0, iter 0, loss 1.9549949169158936\n",
      "epoch 0, iter 1, loss 2.093639612197876\n",
      "epoch 0, iter 2, loss 2.133610725402832\n",
      "epoch 0, loss 6.182245254516602\n",
      "epoch 1, iter 0, loss 1.9728500843048096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, iter 1, loss 1.8723797798156738\n",
      "epoch 1, iter 2, loss 1.8942657709121704\n",
      "epoch 1, loss 5.739495635032654\n",
      "epoch 2, iter 0, loss 1.7206618785858154\n",
      "epoch 2, iter 1, loss 1.5736771821975708\n",
      "epoch 2, iter 2, loss 1.5392179489135742\n",
      "epoch 2, loss 4.8335570096969604\n",
      "epoch 3, iter 0, loss 1.5482406616210938\n",
      "epoch 3, iter 1, loss 1.4989392757415771\n",
      "epoch 3, iter 2, loss 1.4246591329574585\n",
      "epoch 3, loss 4.471839070320129\n",
      "epoch 4, iter 0, loss 1.4029713869094849\n",
      "epoch 4, iter 1, loss 1.392583966255188\n",
      "epoch 4, iter 2, loss 1.3610725402832031\n",
      "epoch 4, loss 4.156627893447876\n",
      "epoch 5, iter 0, loss 1.3302028179168701\n",
      "epoch 5, iter 1, loss 1.30936861038208\n",
      "epoch 5, iter 2, loss 1.2931466102600098\n",
      "epoch 5, loss 3.93271803855896\n",
      "epoch 6, iter 0, loss 1.2592434883117676\n",
      "epoch 6, iter 1, loss 1.2363603115081787\n",
      "epoch 6, iter 2, loss 1.2103519439697266\n",
      "epoch 6, loss 3.705955743789673\n",
      "epoch 7, iter 0, loss 1.1847079992294312\n",
      "epoch 7, iter 1, loss 1.1550171375274658\n",
      "epoch 7, iter 2, loss 1.1352094411849976\n",
      "epoch 7, loss 3.4749345779418945\n",
      "epoch 8, iter 0, loss 1.102676272392273\n",
      "epoch 8, iter 1, loss 1.0801539421081543\n",
      "epoch 8, iter 2, loss 1.051177740097046\n",
      "epoch 8, loss 3.234007954597473\n",
      "epoch 9, iter 0, loss 1.0189461708068848\n",
      "epoch 9, iter 1, loss 0.9871359467506409\n",
      "epoch 9, iter 2, loss 0.9603900909423828\n",
      "epoch 9, loss 2.9664722084999084\n",
      "epoch 10, iter 0, loss 0.9265227317810059\n",
      "epoch 10, iter 1, loss 0.8953419327735901\n",
      "epoch 10, iter 2, loss 0.8612486124038696\n",
      "epoch 10, loss 2.6831132769584656\n",
      "epoch 11, iter 0, loss 0.8227108120918274\n",
      "epoch 11, iter 1, loss 0.7929658889770508\n",
      "epoch 11, iter 2, loss 0.7690802812576294\n",
      "epoch 11, loss 2.3847569823265076\n",
      "epoch 12, iter 0, loss 0.72344571352005\n",
      "epoch 12, iter 1, loss 0.6875756978988647\n",
      "epoch 12, iter 2, loss 0.6530779004096985\n",
      "epoch 12, loss 2.0640993118286133\n",
      "epoch 13, iter 0, loss 0.6167876720428467\n",
      "epoch 13, iter 1, loss 0.5861085057258606\n",
      "epoch 13, iter 2, loss 0.5621483325958252\n",
      "epoch 13, loss 1.7650445103645325\n",
      "epoch 14, iter 0, loss 0.5191105604171753\n",
      "epoch 14, iter 1, loss 0.5002738833427429\n",
      "epoch 14, iter 2, loss 0.48246654868125916\n",
      "epoch 14, loss 1.5018509924411774\n",
      "epoch 15, iter 0, loss 0.4482789635658264\n",
      "epoch 15, iter 1, loss 0.43023163080215454\n",
      "epoch 15, iter 2, loss 0.41193169355392456\n",
      "epoch 15, loss 1.2904422879219055\n",
      "epoch 16, iter 0, loss 0.3982784152030945\n",
      "epoch 16, iter 1, loss 0.3753436803817749\n",
      "epoch 16, iter 2, loss 0.38600409030914307\n",
      "epoch 16, loss 1.1596261858940125\n",
      "epoch 17, iter 0, loss 0.380099356174469\n",
      "epoch 17, iter 1, loss 0.36193665862083435\n",
      "epoch 17, iter 2, loss 0.37675756216049194\n",
      "epoch 17, loss 1.1187935769557953\n",
      "epoch 18, iter 0, loss 0.39092394709587097\n",
      "epoch 18, iter 1, loss 0.3636285960674286\n",
      "epoch 18, iter 2, loss 0.372856467962265\n",
      "epoch 18, loss 1.1274090111255646\n",
      "epoch 19, iter 0, loss 0.4178354740142822\n",
      "epoch 19, iter 1, loss 0.39109766483306885\n",
      "epoch 19, iter 2, loss 0.36508285999298096\n",
      "epoch 19, loss 1.174015998840332\n",
      "epoch 20, iter 0, loss 0.44069892168045044\n",
      "epoch 20, iter 1, loss 0.38423603773117065\n",
      "epoch 20, iter 2, loss 0.4003198444843292\n",
      "epoch 20, loss 1.2252548038959503\n",
      "epoch 21, iter 0, loss 0.3920935094356537\n",
      "epoch 21, iter 1, loss 0.3651506304740906\n",
      "epoch 21, iter 2, loss 0.352179616689682\n",
      "epoch 21, loss 1.1094237565994263\n",
      "epoch 22, iter 0, loss 0.32762351632118225\n",
      "epoch 22, iter 1, loss 0.2833641767501831\n",
      "epoch 22, iter 2, loss 0.3243428170681\n",
      "epoch 22, loss 0.9353305101394653\n",
      "epoch 23, iter 0, loss 0.2716296911239624\n",
      "epoch 23, iter 1, loss 0.23178064823150635\n",
      "epoch 23, iter 2, loss 0.23382672667503357\n",
      "epoch 23, loss 0.7372370660305023\n",
      "epoch 24, iter 0, loss 0.20865821838378906\n",
      "epoch 24, iter 1, loss 0.19244137406349182\n",
      "epoch 24, iter 2, loss 0.1799124777317047\n",
      "epoch 24, loss 0.5810120701789856\n",
      "epoch 25, iter 0, loss 0.16073188185691833\n",
      "epoch 25, iter 1, loss 0.13527069985866547\n",
      "epoch 25, iter 2, loss 0.14242812991142273\n",
      "epoch 25, loss 0.43843071162700653\n",
      "epoch 26, iter 0, loss 0.11745767295360565\n",
      "epoch 26, iter 1, loss 0.09162421524524689\n",
      "epoch 26, iter 2, loss 0.07309868931770325\n",
      "epoch 26, loss 0.2821805775165558\n",
      "epoch 27, iter 0, loss 0.05960516631603241\n",
      "epoch 27, iter 1, loss 0.05332382023334503\n",
      "epoch 27, iter 2, loss 0.030151918530464172\n",
      "epoch 27, loss 0.1430809050798416\n",
      "epoch 28, iter 0, loss 0.009639352560043335\n",
      "epoch 28, iter 1, loss -0.009780213236808777\n",
      "epoch 28, iter 2, loss 0.014227122068405151\n",
      "epoch 28, loss 0.01408626139163971\n",
      "epoch 29, iter 0, loss -0.03216041624546051\n",
      "epoch 29, iter 1, loss -0.011656910181045532\n",
      "epoch 29, iter 2, loss -0.03286498785018921\n",
      "epoch 29, loss -0.07668231427669525\n",
      "epoch 30, iter 0, loss -0.04191307723522186\n",
      "epoch 30, iter 1, loss -0.039487287402153015\n",
      "epoch 30, iter 2, loss -0.04316934943199158\n",
      "epoch 30, loss -0.12456971406936646\n",
      "epoch 31, iter 0, loss -0.055611252784729004\n",
      "epoch 31, iter 1, loss -0.04881960153579712\n",
      "epoch 31, iter 2, loss -0.023811519145965576\n",
      "epoch 31, loss -0.1282423734664917\n",
      "epoch 32, iter 0, loss -0.023247092962265015\n",
      "epoch 32, iter 1, loss -0.04511567950248718\n",
      "epoch 32, iter 2, loss -0.01629960536956787\n",
      "epoch 32, loss -0.08466237783432007\n",
      "epoch 33, iter 0, loss -0.022306829690933228\n",
      "epoch 33, iter 1, loss -0.03614190220832825\n",
      "epoch 33, iter 2, loss -0.02365139126777649\n",
      "epoch 33, loss -0.08210012316703796\n",
      "epoch 34, iter 0, loss -0.002343416213989258\n",
      "epoch 34, iter 1, loss -0.05822920799255371\n",
      "epoch 34, iter 2, loss 0.005654633045196533\n",
      "epoch 34, loss -0.054917991161346436\n",
      "epoch 35, iter 0, loss -0.02216663956642151\n",
      "epoch 35, iter 1, loss -0.00874263048171997\n",
      "epoch 35, iter 2, loss -0.03783822059631348\n",
      "epoch 35, loss -0.06874749064445496\n",
      "epoch 36, iter 0, loss 0.007129877805709839\n",
      "epoch 36, iter 1, loss -0.02866309881210327\n",
      "epoch 36, iter 2, loss -0.021683812141418457\n",
      "epoch 36, loss -0.04321703314781189\n",
      "epoch 37, iter 0, loss -0.021868765354156494\n",
      "epoch 37, iter 1, loss -0.03569278120994568\n",
      "epoch 37, iter 2, loss -0.0457020103931427\n",
      "epoch 37, loss -0.10326355695724487\n",
      "epoch 38, iter 0, loss -0.04644574224948883\n",
      "epoch 38, iter 1, loss -0.03767700493335724\n",
      "epoch 38, iter 2, loss -0.04881121218204498\n",
      "epoch 38, loss -0.13293395936489105\n",
      "epoch 39, iter 0, loss -0.0842769593000412\n",
      "epoch 39, iter 1, loss -0.053570523858070374\n",
      "epoch 39, iter 2, loss -0.08373533189296722\n",
      "epoch 39, loss -0.2215828150510788\n",
      "epoch 40, iter 0, loss -0.09943321347236633\n",
      "epoch 40, iter 1, loss -0.07507550716400146\n",
      "epoch 40, iter 2, loss -0.10340507328510284\n",
      "epoch 40, loss -0.27791379392147064\n",
      "epoch 41, iter 0, loss -0.10933995246887207\n",
      "epoch 41, iter 1, loss -0.12056955695152283\n",
      "epoch 41, iter 2, loss -0.1094532310962677\n",
      "epoch 41, loss -0.3393627405166626\n",
      "epoch 42, iter 0, loss -0.14739933609962463\n",
      "epoch 42, iter 1, loss -0.12414422631263733\n",
      "epoch 42, iter 2, loss -0.09305782616138458\n",
      "epoch 42, loss -0.36460138857364655\n",
      "epoch 43, iter 0, loss -0.14044950902462006\n",
      "epoch 43, iter 1, loss -0.12058235704898834\n",
      "epoch 43, iter 2, loss -0.14313232898712158\n",
      "epoch 43, loss -0.40416419506073\n",
      "epoch 44, iter 0, loss -0.15866945683956146\n",
      "epoch 44, iter 1, loss -0.14860689640045166\n",
      "epoch 44, iter 2, loss -0.13606397807598114\n",
      "epoch 44, loss -0.44334033131599426\n",
      "epoch 45, iter 0, loss -0.1549604833126068\n",
      "epoch 45, iter 1, loss -0.1890825629234314\n",
      "epoch 45, iter 2, loss -0.0856475830078125\n",
      "epoch 45, loss -0.4296906292438507\n",
      "epoch 46, iter 0, loss -0.15826722979545593\n",
      "epoch 46, iter 1, loss -0.12800997495651245\n",
      "epoch 46, iter 2, loss -0.14730912446975708\n",
      "epoch 46, loss -0.43358632922172546\n",
      "epoch 47, iter 0, loss -0.1229495108127594\n",
      "epoch 47, iter 1, loss -0.11616116762161255\n",
      "epoch 47, iter 2, loss -0.15334239602088928\n",
      "epoch 47, loss -0.39245307445526123\n",
      "epoch 48, iter 0, loss -0.0777423083782196\n",
      "epoch 48, iter 1, loss -0.1199910044670105\n",
      "epoch 48, iter 2, loss -0.10417324304580688\n",
      "epoch 48, loss -0.301906555891037\n",
      "epoch 49, iter 0, loss -0.0949639081954956\n",
      "epoch 49, iter 1, loss -0.0617336630821228\n",
      "epoch 49, iter 2, loss -0.0931524932384491\n",
      "epoch 49, loss -0.2498500645160675\n",
      "epoch 50, iter 0, loss -0.06327220797538757\n",
      "epoch 50, iter 1, loss -0.1103968471288681\n",
      "epoch 50, iter 2, loss -0.08773760497570038\n",
      "epoch 50, loss -0.26140666007995605\n",
      "epoch 51, iter 0, loss -0.09229414165019989\n",
      "epoch 51, iter 1, loss -0.10392782092094421\n",
      "epoch 51, iter 2, loss -0.1339854747056961\n",
      "epoch 51, loss -0.3302074372768402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52, iter 0, loss -0.10673297941684723\n",
      "epoch 52, iter 1, loss -0.0920967310667038\n",
      "epoch 52, iter 2, loss -0.13499729335308075\n",
      "epoch 52, loss -0.3338270038366318\n",
      "epoch 53, iter 0, loss -0.11698874831199646\n",
      "epoch 53, iter 1, loss -0.11484041810035706\n",
      "epoch 53, iter 2, loss -0.13120554387569427\n",
      "epoch 53, loss -0.3630347102880478\n",
      "epoch 54, iter 0, loss -0.15387502312660217\n",
      "epoch 54, iter 1, loss -0.12956659495830536\n",
      "epoch 54, iter 2, loss -0.16268444061279297\n",
      "epoch 54, loss -0.4461260586977005\n",
      "epoch 55, iter 0, loss -0.15518392622470856\n",
      "epoch 55, iter 1, loss -0.15678122639656067\n",
      "epoch 55, iter 2, loss -0.1891935020685196\n",
      "epoch 55, loss -0.5011586546897888\n",
      "epoch 56, iter 0, loss -0.14671629667282104\n",
      "epoch 56, iter 1, loss -0.20827963948249817\n",
      "epoch 56, iter 2, loss -0.19425123929977417\n",
      "epoch 56, loss -0.5492471754550934\n",
      "epoch 57, iter 0, loss -0.21273837983608246\n",
      "epoch 57, iter 1, loss -0.1969010829925537\n",
      "epoch 57, iter 2, loss -0.23814243078231812\n",
      "epoch 57, loss -0.6477818936109543\n",
      "epoch 58, iter 0, loss -0.2194167971611023\n",
      "epoch 58, iter 1, loss -0.21781113743782043\n",
      "epoch 58, iter 2, loss -0.2164965569972992\n",
      "epoch 58, loss -0.6537244915962219\n",
      "epoch 59, iter 0, loss -0.22429120540618896\n",
      "epoch 59, iter 1, loss -0.21802257001399994\n",
      "epoch 59, iter 2, loss -0.2032810002565384\n",
      "epoch 59, loss -0.6455947756767273\n",
      "epoch 60, iter 0, loss -0.2296859323978424\n",
      "epoch 60, iter 1, loss -0.20206405222415924\n",
      "epoch 60, iter 2, loss -0.1766500174999237\n",
      "epoch 60, loss -0.6084000021219254\n",
      "epoch 61, iter 0, loss -0.2126484513282776\n",
      "epoch 61, iter 1, loss -0.19172245264053345\n",
      "epoch 61, iter 2, loss -0.17226266860961914\n",
      "epoch 61, loss -0.5766335725784302\n",
      "epoch 62, iter 0, loss -0.19975748658180237\n",
      "epoch 62, iter 1, loss -0.17259091138839722\n",
      "epoch 62, iter 2, loss -0.1843186765909195\n",
      "epoch 62, loss -0.5566670745611191\n",
      "epoch 63, iter 0, loss -0.16032886505126953\n",
      "epoch 63, iter 1, loss -0.16773435473442078\n",
      "epoch 63, iter 2, loss -0.1720527708530426\n",
      "epoch 63, loss -0.5001159906387329\n",
      "epoch 64, iter 0, loss -0.15663094818592072\n",
      "epoch 64, iter 1, loss -0.1992589831352234\n",
      "epoch 64, iter 2, loss -0.14266979694366455\n",
      "epoch 64, loss -0.49855972826480865\n",
      "epoch 65, iter 0, loss -0.16116563975811005\n",
      "epoch 65, iter 1, loss -0.15999506413936615\n",
      "epoch 65, iter 2, loss -0.1747472733259201\n",
      "epoch 65, loss -0.4959079772233963\n",
      "epoch 66, iter 0, loss -0.16992704570293427\n",
      "epoch 66, iter 1, loss -0.1612682044506073\n",
      "epoch 66, iter 2, loss -0.15499839186668396\n",
      "epoch 66, loss -0.4861936420202255\n",
      "epoch 67, iter 0, loss -0.17358751595020294\n",
      "epoch 67, iter 1, loss -0.1814802885055542\n",
      "epoch 67, iter 2, loss -0.15371929109096527\n",
      "epoch 67, loss -0.5087870955467224\n",
      "epoch 68, iter 0, loss -0.214854434132576\n",
      "epoch 68, iter 1, loss -0.18391092121601105\n",
      "epoch 68, iter 2, loss -0.15673869848251343\n",
      "epoch 68, loss -0.5555040538311005\n",
      "epoch 69, iter 0, loss -0.20430295169353485\n",
      "epoch 69, iter 1, loss -0.2051859200000763\n",
      "epoch 69, iter 2, loss -0.2086721807718277\n",
      "epoch 69, loss -0.6181610524654388\n",
      "epoch 70, iter 0, loss -0.20404036343097687\n",
      "epoch 70, iter 1, loss -0.23327654600143433\n",
      "epoch 70, iter 2, loss -0.24749425053596497\n",
      "epoch 70, loss -0.6848111599683762\n",
      "epoch 71, iter 0, loss -0.2425224930047989\n",
      "epoch 71, iter 1, loss -0.25695866346359253\n",
      "epoch 71, iter 2, loss -0.20417383313179016\n",
      "epoch 71, loss -0.7036549896001816\n",
      "epoch 72, iter 0, loss -0.2613377869129181\n",
      "epoch 72, iter 1, loss -0.23010721802711487\n",
      "epoch 72, iter 2, loss -0.22503618896007538\n",
      "epoch 72, loss -0.7164811939001083\n",
      "epoch 73, iter 0, loss -0.23188242316246033\n",
      "epoch 73, iter 1, loss -0.21050551533699036\n",
      "epoch 73, iter 2, loss -0.24158377945423126\n",
      "epoch 73, loss -0.683971717953682\n",
      "epoch 74, iter 0, loss -0.22919021546840668\n",
      "epoch 74, iter 1, loss -0.2276729941368103\n",
      "epoch 74, iter 2, loss -0.2290755957365036\n",
      "epoch 74, loss -0.6859388053417206\n",
      "epoch 75, iter 0, loss -0.2204025834798813\n",
      "epoch 75, iter 1, loss -0.2248978316783905\n",
      "epoch 75, iter 2, loss -0.21623797714710236\n",
      "epoch 75, loss -0.6615383923053741\n",
      "epoch 76, iter 0, loss -0.21151654422283173\n",
      "epoch 76, iter 1, loss -0.2077164351940155\n",
      "epoch 76, iter 2, loss -0.20956484973430634\n",
      "epoch 76, loss -0.6287978291511536\n",
      "epoch 77, iter 0, loss -0.23252609372138977\n",
      "epoch 77, iter 1, loss -0.20233209431171417\n",
      "epoch 77, iter 2, loss -0.19936950504779816\n",
      "epoch 77, loss -0.6342276930809021\n",
      "epoch 78, iter 0, loss -0.19503618776798248\n",
      "epoch 78, iter 1, loss -0.20612139999866486\n",
      "epoch 78, iter 2, loss -0.2287338376045227\n",
      "epoch 78, loss -0.62989142537117\n",
      "epoch 79, iter 0, loss -0.21326205134391785\n",
      "epoch 79, iter 1, loss -0.19399699568748474\n",
      "epoch 79, iter 2, loss -0.2245120406150818\n",
      "epoch 79, loss -0.6317710876464844\n",
      "epoch 80, iter 0, loss -0.22377575933933258\n",
      "epoch 80, iter 1, loss -0.19401872158050537\n",
      "epoch 80, iter 2, loss -0.2413294017314911\n",
      "epoch 80, loss -0.659123882651329\n",
      "epoch 81, iter 0, loss -0.224566251039505\n",
      "epoch 81, iter 1, loss -0.2385004609823227\n",
      "epoch 81, iter 2, loss -0.1666383147239685\n",
      "epoch 81, loss -0.6297050267457962\n",
      "epoch 82, iter 0, loss -0.22980737686157227\n",
      "epoch 82, iter 1, loss -0.23270194232463837\n",
      "epoch 82, iter 2, loss -0.2173367440700531\n",
      "epoch 82, loss -0.6798460632562637\n",
      "epoch 83, iter 0, loss -0.22878329455852509\n",
      "epoch 83, iter 1, loss -0.27092069387435913\n",
      "epoch 83, iter 2, loss -0.22465689480304718\n",
      "epoch 83, loss -0.7243608832359314\n",
      "epoch 84, iter 0, loss -0.253984659910202\n",
      "epoch 84, iter 1, loss -0.24666571617126465\n",
      "epoch 84, iter 2, loss -0.2355765551328659\n",
      "epoch 84, loss -0.7362269312143326\n",
      "epoch 85, iter 0, loss -0.24587444961071014\n",
      "epoch 85, iter 1, loss -0.2607688307762146\n",
      "epoch 85, iter 2, loss -0.23995529115200043\n",
      "epoch 85, loss -0.7465985715389252\n",
      "epoch 86, iter 0, loss -0.2256322056055069\n",
      "epoch 86, iter 1, loss -0.2713644504547119\n",
      "epoch 86, iter 2, loss -0.22764472663402557\n",
      "epoch 86, loss -0.7246413826942444\n",
      "epoch 87, iter 0, loss -0.2425966113805771\n",
      "epoch 87, iter 1, loss -0.22853364050388336\n",
      "epoch 87, iter 2, loss -0.26470136642456055\n",
      "epoch 87, loss -0.735831618309021\n",
      "epoch 88, iter 0, loss -0.21943221986293793\n",
      "epoch 88, iter 1, loss -0.2551320791244507\n",
      "epoch 88, iter 2, loss -0.28108012676239014\n",
      "epoch 88, loss -0.7556444257497787\n",
      "epoch 89, iter 0, loss -0.25659239292144775\n",
      "epoch 89, iter 1, loss -0.23337644338607788\n",
      "epoch 89, iter 2, loss -0.23283302783966064\n",
      "epoch 89, loss -0.7228018641471863\n",
      "epoch 90, iter 0, loss -0.2625157833099365\n",
      "epoch 90, iter 1, loss -0.23228111863136292\n",
      "epoch 90, iter 2, loss -0.2377578616142273\n",
      "epoch 90, loss -0.7325547635555267\n",
      "epoch 91, iter 0, loss -0.25931495428085327\n",
      "epoch 91, iter 1, loss -0.2373693883419037\n",
      "epoch 91, iter 2, loss -0.2339628040790558\n",
      "epoch 91, loss -0.7306471467018127\n",
      "epoch 92, iter 0, loss -0.2509354054927826\n",
      "epoch 92, iter 1, loss -0.22800643742084503\n",
      "epoch 92, iter 2, loss -0.2591427266597748\n",
      "epoch 92, loss -0.7380845695734024\n",
      "epoch 93, iter 0, loss -0.23489849269390106\n",
      "epoch 93, iter 1, loss -0.2495749443769455\n",
      "epoch 93, iter 2, loss -0.24539126455783844\n",
      "epoch 93, loss -0.729864701628685\n",
      "epoch 94, iter 0, loss -0.26259586215019226\n",
      "epoch 94, iter 1, loss -0.22532369196414948\n",
      "epoch 94, iter 2, loss -0.24774610996246338\n",
      "epoch 94, loss -0.7356656640768051\n",
      "epoch 95, iter 0, loss -0.2661063075065613\n",
      "epoch 95, iter 1, loss -0.23160597681999207\n",
      "epoch 95, iter 2, loss -0.2733965516090393\n",
      "epoch 95, loss -0.7711088359355927\n",
      "epoch 96, iter 0, loss -0.2780877947807312\n",
      "epoch 96, iter 1, loss -0.25582772493362427\n",
      "epoch 96, iter 2, loss -0.21164926886558533\n",
      "epoch 96, loss -0.7455647885799408\n",
      "epoch 97, iter 0, loss -0.2718311846256256\n",
      "epoch 97, iter 1, loss -0.25123268365859985\n",
      "epoch 97, iter 2, loss -0.2620951533317566\n",
      "epoch 97, loss -0.7851590216159821\n",
      "epoch 98, iter 0, loss -0.2760099172592163\n",
      "epoch 98, iter 1, loss -0.27400025725364685\n",
      "epoch 98, iter 2, loss -0.20131516456604004\n",
      "epoch 98, loss -0.7513253390789032\n",
      "epoch 99, iter 0, loss -0.2858313322067261\n",
      "epoch 99, iter 1, loss -0.25389406085014343\n",
      "epoch 99, iter 2, loss -0.24882641434669495\n",
      "epoch 99, loss -0.7885518074035645\n",
      "2019-03-31 00:53:37.311851, fold=1, rep=1, eta=0d 0h 20m 31s \n",
      "{'fold': 1, 'repeat': 1, 'n': 4137, 'd': 20, 'mse': 0.013896428979933262, 'train_time': 619.2448385150637, 'trained_epochs': 100, 'prior_train_nmll': -0.18421126902103424, 'train_nll': -1682.470947265625, 'test_nll': -846.2003173828125, 'train_mse': 0.014488105662167072, 'state_dict_file': 'model_state_dict_-499280153085256999.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, iter 0, loss 1.9681191444396973\n",
      "epoch 0, iter 1, loss 2.1041758060455322\n",
      "epoch 0, iter 2, loss 2.111330986022949\n",
      "epoch 0, loss 6.183625936508179\n",
      "epoch 1, iter 0, loss 1.989500880241394\n",
      "epoch 1, iter 1, loss 1.887448787689209\n",
      "epoch 1, iter 2, loss 1.860145092010498\n",
      "epoch 1, loss 5.737094759941101\n",
      "epoch 2, iter 0, loss 1.7258713245391846\n",
      "epoch 2, iter 1, loss 1.5685052871704102\n",
      "epoch 2, iter 2, loss 1.552805781364441\n",
      "epoch 2, loss 4.847182393074036\n",
      "epoch 3, iter 0, loss 1.5499801635742188\n",
      "epoch 3, iter 1, loss 1.4983233213424683\n",
      "epoch 3, iter 2, loss 1.4340288639068604\n",
      "epoch 3, loss 4.482332348823547\n",
      "epoch 4, iter 0, loss 1.4069904088974\n",
      "epoch 4, iter 1, loss 1.39859139919281\n",
      "epoch 4, iter 2, loss 1.3661518096923828\n",
      "epoch 4, loss 4.171733617782593\n",
      "epoch 5, iter 0, loss 1.3360850811004639\n",
      "epoch 5, iter 1, loss 1.3137747049331665\n",
      "epoch 5, iter 2, loss 1.298441767692566\n",
      "epoch 5, loss 3.9483015537261963\n",
      "epoch 6, iter 0, loss 1.2664880752563477\n",
      "epoch 6, iter 1, loss 1.2418396472930908\n",
      "epoch 6, iter 2, loss 1.2157648801803589\n",
      "epoch 6, loss 3.7240926027297974\n",
      "epoch 7, iter 0, loss 1.192413330078125\n",
      "epoch 7, iter 1, loss 1.163102626800537\n",
      "epoch 7, iter 2, loss 1.1439481973648071\n",
      "epoch 7, loss 3.4994641542434692\n",
      "epoch 8, iter 0, loss 1.1078503131866455\n",
      "epoch 8, iter 1, loss 1.0874296426773071\n",
      "epoch 8, iter 2, loss 1.0595346689224243\n",
      "epoch 8, loss 3.254814624786377\n",
      "epoch 9, iter 0, loss 1.0278962850570679\n",
      "epoch 9, iter 1, loss 0.9980248212814331\n",
      "epoch 9, iter 2, loss 0.967606246471405\n",
      "epoch 9, loss 2.993527352809906\n",
      "epoch 10, iter 0, loss 0.9325243234634399\n",
      "epoch 10, iter 1, loss 0.9038532972335815\n",
      "epoch 10, iter 2, loss 0.8699495196342468\n",
      "epoch 10, loss 2.7063271403312683\n",
      "epoch 11, iter 0, loss 0.8326188325881958\n",
      "epoch 11, iter 1, loss 0.8021082282066345\n",
      "epoch 11, iter 2, loss 0.7600268125534058\n",
      "epoch 11, loss 2.394753873348236\n",
      "epoch 12, iter 0, loss 0.7268199324607849\n",
      "epoch 12, iter 1, loss 0.6886755228042603\n",
      "epoch 12, iter 2, loss 0.6651816964149475\n",
      "epoch 12, loss 2.0806771516799927\n",
      "epoch 13, iter 0, loss 0.6261534094810486\n",
      "epoch 13, iter 1, loss 0.5852411985397339\n",
      "epoch 13, iter 2, loss 0.5573936104774475\n",
      "epoch 13, loss 1.76878821849823\n",
      "epoch 14, iter 0, loss 0.5252834558486938\n",
      "epoch 14, iter 1, loss 0.5006111264228821\n",
      "epoch 14, iter 2, loss 0.4800705909729004\n",
      "epoch 14, loss 1.5059651732444763\n",
      "epoch 15, iter 0, loss 0.4592839479446411\n",
      "epoch 15, iter 1, loss 0.4423290491104126\n",
      "epoch 15, iter 2, loss 0.424085795879364\n",
      "epoch 15, loss 1.3256987929344177\n",
      "epoch 16, iter 0, loss 0.42627888917922974\n",
      "epoch 16, iter 1, loss 0.41321587562561035\n",
      "epoch 16, iter 2, loss 0.4178865849971771\n",
      "epoch 16, loss 1.2573813498020172\n",
      "epoch 17, iter 0, loss 0.41533350944519043\n",
      "epoch 17, iter 1, loss 0.3775429129600525\n",
      "epoch 17, iter 2, loss 0.39585080742836\n",
      "epoch 17, loss 1.188727229833603\n",
      "epoch 18, iter 0, loss 0.407151997089386\n",
      "epoch 18, iter 1, loss 0.3770115375518799\n",
      "epoch 18, iter 2, loss 0.380919486284256\n",
      "epoch 18, loss 1.1650830209255219\n",
      "epoch 19, iter 0, loss 0.37346893548965454\n",
      "epoch 19, iter 1, loss 0.37850409746170044\n",
      "epoch 19, iter 2, loss 0.3953707218170166\n",
      "epoch 19, loss 1.1473437547683716\n",
      "epoch 20, iter 0, loss 0.3923685848712921\n",
      "epoch 20, iter 1, loss 0.3406791090965271\n",
      "epoch 20, iter 2, loss 0.3654813766479492\n",
      "epoch 20, loss 1.0985290706157684\n",
      "epoch 21, iter 0, loss 0.35120150446891785\n",
      "epoch 21, iter 1, loss 0.3187727630138397\n",
      "epoch 21, iter 2, loss 0.32759493589401245\n",
      "epoch 21, loss 0.99756920337677\n",
      "epoch 22, iter 0, loss 0.320945680141449\n",
      "epoch 22, iter 1, loss 0.27024170756340027\n",
      "epoch 22, iter 2, loss 0.29441410303115845\n",
      "epoch 22, loss 0.8856014907360077\n",
      "epoch 23, iter 0, loss 0.25726866722106934\n",
      "epoch 23, iter 1, loss 0.2268800139427185\n",
      "epoch 23, iter 2, loss 0.21049320697784424\n",
      "epoch 23, loss 0.6946418881416321\n",
      "epoch 24, iter 0, loss 0.20509499311447144\n",
      "epoch 24, iter 1, loss 0.19924555718898773\n",
      "epoch 24, iter 2, loss 0.17612913250923157\n",
      "epoch 24, loss 0.5804696828126907\n",
      "epoch 25, iter 0, loss 0.16094449162483215\n",
      "epoch 25, iter 1, loss 0.15061251819133759\n",
      "epoch 25, iter 2, loss 0.11919417977333069\n",
      "epoch 25, loss 0.4307511895895004\n",
      "epoch 26, iter 0, loss 0.11541546881198883\n",
      "epoch 26, iter 1, loss 0.0969998836517334\n",
      "epoch 26, iter 2, loss 0.09218102693557739\n",
      "epoch 26, loss 0.3045963793992996\n",
      "epoch 27, iter 0, loss 0.06580743193626404\n",
      "epoch 27, iter 1, loss 0.0678710788488388\n",
      "epoch 27, iter 2, loss 0.07278299331665039\n",
      "epoch 27, loss 0.20646150410175323\n",
      "epoch 28, iter 0, loss 0.05463261902332306\n",
      "epoch 28, iter 1, loss -0.0015391558408737183\n",
      "epoch 28, iter 2, loss 0.039077773690223694\n",
      "epoch 28, loss 0.09217123687267303\n",
      "epoch 29, iter 0, loss -0.0007349103689193726\n",
      "epoch 29, iter 1, loss 0.0016502141952514648\n",
      "epoch 29, iter 2, loss -0.02966490387916565\n",
      "epoch 29, loss -0.028749600052833557\n",
      "epoch 30, iter 0, loss -0.011748969554901123\n",
      "epoch 30, iter 1, loss -0.04028661549091339\n",
      "epoch 30, iter 2, loss -0.0420796275138855\n",
      "epoch 30, loss -0.09411521255970001\n",
      "epoch 31, iter 0, loss -0.0569131076335907\n",
      "epoch 31, iter 1, loss -0.01957535743713379\n",
      "epoch 31, iter 2, loss -0.0799424946308136\n",
      "epoch 31, loss -0.15643095970153809\n",
      "epoch 32, iter 0, loss -0.06226041913032532\n",
      "epoch 32, iter 1, loss -0.05870375037193298\n",
      "epoch 32, iter 2, loss -0.05355967581272125\n",
      "epoch 32, loss -0.17452384531497955\n",
      "epoch 33, iter 0, loss -0.021584242582321167\n",
      "epoch 33, iter 1, loss -0.04893532395362854\n",
      "epoch 33, iter 2, loss -0.06819382309913635\n",
      "epoch 33, loss -0.13871338963508606\n",
      "epoch 34, iter 0, loss -0.04383033514022827\n",
      "epoch 34, iter 1, loss -0.0403023362159729\n",
      "epoch 34, iter 2, loss -0.01087421178817749\n",
      "epoch 34, loss -0.09500688314437866\n",
      "epoch 35, iter 0, loss -0.034869223833084106\n",
      "epoch 35, iter 1, loss -0.0507064163684845\n",
      "epoch 35, iter 2, loss -0.050848305225372314\n",
      "epoch 35, loss -0.13642394542694092\n",
      "epoch 36, iter 0, loss -0.045825451612472534\n",
      "epoch 36, iter 1, loss -0.025267064571380615\n",
      "epoch 36, iter 2, loss -0.06026625633239746\n",
      "epoch 36, loss -0.1313587725162506\n",
      "epoch 37, iter 0, loss -0.058478325605392456\n",
      "epoch 37, iter 1, loss -0.0640602707862854\n",
      "epoch 37, iter 2, loss -0.06177671253681183\n",
      "epoch 37, loss -0.18431530892848969\n",
      "epoch 38, iter 0, loss -0.07762165367603302\n",
      "epoch 38, iter 1, loss -0.061173632740974426\n",
      "epoch 38, iter 2, loss -0.06296917796134949\n",
      "epoch 38, loss -0.20176446437835693\n",
      "epoch 39, iter 0, loss -0.06462907791137695\n",
      "epoch 39, iter 1, loss -0.0850820392370224\n",
      "epoch 39, iter 2, loss -0.08724328875541687\n",
      "epoch 39, loss -0.23695440590381622\n",
      "epoch 40, iter 0, loss -0.10014238953590393\n",
      "epoch 40, iter 1, loss -0.09406064450740814\n",
      "epoch 40, iter 2, loss -0.06814166903495789\n",
      "epoch 40, loss -0.26234470307826996\n",
      "epoch 41, iter 0, loss -0.09148941934108734\n",
      "epoch 41, iter 1, loss -0.11943599581718445\n",
      "epoch 41, iter 2, loss -0.07568195462226868\n",
      "epoch 41, loss -0.28660736978054047\n",
      "epoch 42, iter 0, loss -0.10740822553634644\n",
      "epoch 42, iter 1, loss -0.11540019512176514\n",
      "epoch 42, iter 2, loss -0.12733303010463715\n",
      "epoch 42, loss -0.3501414507627487\n",
      "epoch 43, iter 0, loss -0.10113805532455444\n",
      "epoch 43, iter 1, loss -0.13686397671699524\n",
      "epoch 43, iter 2, loss -0.13589008152484894\n",
      "epoch 43, loss -0.3738921135663986\n",
      "epoch 44, iter 0, loss -0.1482103019952774\n",
      "epoch 44, iter 1, loss -0.15856076776981354\n",
      "epoch 44, iter 2, loss -0.11014775931835175\n",
      "epoch 44, loss -0.4169188290834427\n",
      "epoch 45, iter 0, loss -0.13509319722652435\n",
      "epoch 45, iter 1, loss -0.1774318665266037\n",
      "epoch 45, iter 2, loss -0.14404498040676117\n",
      "epoch 45, loss -0.4565700441598892\n",
      "epoch 46, iter 0, loss -0.15295138955116272\n",
      "epoch 46, iter 1, loss -0.17645202577114105\n",
      "epoch 46, iter 2, loss -0.10567514598369598\n",
      "epoch 46, loss -0.43507856130599976\n",
      "epoch 47, iter 0, loss -0.16079284250736237\n",
      "epoch 47, iter 1, loss -0.15049193799495697\n",
      "epoch 47, iter 2, loss -0.16784963011741638\n",
      "epoch 47, loss -0.4791344106197357\n",
      "epoch 48, iter 0, loss -0.15228118002414703\n",
      "epoch 48, iter 1, loss -0.14927099645137787\n",
      "epoch 48, iter 2, loss -0.15663889050483704\n",
      "epoch 48, loss -0.45819106698036194\n",
      "epoch 49, iter 0, loss -0.14216038584709167\n",
      "epoch 49, iter 1, loss -0.127662792801857\n",
      "epoch 49, iter 2, loss -0.14956727623939514\n",
      "epoch 49, loss -0.4193904548883438\n",
      "epoch 50, iter 0, loss -0.1411636769771576\n",
      "epoch 50, iter 1, loss -0.12936072051525116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, iter 2, loss -0.14935614168643951\n",
      "epoch 50, loss -0.41988053917884827\n",
      "epoch 51, iter 0, loss -0.16482673585414886\n",
      "epoch 51, iter 1, loss -0.1547383964061737\n",
      "epoch 51, iter 2, loss -0.1342144012451172\n",
      "epoch 51, loss -0.45377953350543976\n",
      "epoch 52, iter 0, loss -0.11752855777740479\n",
      "epoch 52, iter 1, loss -0.1466139256954193\n",
      "epoch 52, iter 2, loss -0.16182361543178558\n",
      "epoch 52, loss -0.4259660989046097\n",
      "epoch 53, iter 0, loss -0.12046848237514496\n",
      "epoch 53, iter 1, loss -0.1463409811258316\n",
      "epoch 53, iter 2, loss -0.15568511188030243\n",
      "epoch 53, loss -0.422494575381279\n",
      "epoch 54, iter 0, loss -0.1290154904127121\n",
      "epoch 54, iter 1, loss -0.14263544976711273\n",
      "epoch 54, iter 2, loss -0.17394201457500458\n",
      "epoch 54, loss -0.4455929547548294\n",
      "epoch 55, iter 0, loss -0.15004058182239532\n",
      "epoch 55, iter 1, loss -0.1695978343486786\n",
      "epoch 55, iter 2, loss -0.14381547272205353\n",
      "epoch 55, loss -0.46345388889312744\n",
      "epoch 56, iter 0, loss -0.16185159981250763\n",
      "epoch 56, iter 1, loss -0.18532265722751617\n",
      "epoch 56, iter 2, loss -0.19678334891796112\n",
      "epoch 56, loss -0.5439576059579849\n",
      "epoch 57, iter 0, loss -0.15923191606998444\n",
      "epoch 57, iter 1, loss -0.18465875089168549\n",
      "epoch 57, iter 2, loss -0.19219672679901123\n",
      "epoch 57, loss -0.5360873937606812\n",
      "epoch 58, iter 0, loss -0.20998036861419678\n",
      "epoch 58, iter 1, loss -0.16627974808216095\n",
      "epoch 58, iter 2, loss -0.19575148820877075\n",
      "epoch 58, loss -0.5720116049051285\n",
      "epoch 59, iter 0, loss -0.19713346660137177\n",
      "epoch 59, iter 1, loss -0.22617316246032715\n",
      "epoch 59, iter 2, loss -0.18362797796726227\n",
      "epoch 59, loss -0.6069346070289612\n",
      "epoch 60, iter 0, loss -0.21349234879016876\n",
      "epoch 60, iter 1, loss -0.19690614938735962\n",
      "epoch 60, iter 2, loss -0.2196088582277298\n",
      "epoch 60, loss -0.6300073564052582\n",
      "epoch 61, iter 0, loss -0.2333856225013733\n",
      "epoch 61, iter 1, loss -0.20035681128501892\n",
      "epoch 61, iter 2, loss -0.19861797988414764\n",
      "epoch 61, loss -0.6323604136705399\n",
      "epoch 62, iter 0, loss -0.21050666272640228\n",
      "epoch 62, iter 1, loss -0.20850634574890137\n",
      "epoch 62, iter 2, loss -0.21450801193714142\n",
      "epoch 62, loss -0.6335210204124451\n",
      "epoch 63, iter 0, loss -0.19694814085960388\n",
      "epoch 63, iter 1, loss -0.2245733141899109\n",
      "epoch 63, iter 2, loss -0.20903800427913666\n",
      "epoch 63, loss -0.6305594593286514\n",
      "epoch 64, iter 0, loss -0.2095116674900055\n",
      "epoch 64, iter 1, loss -0.20832142233848572\n",
      "epoch 64, iter 2, loss -0.22256027162075043\n",
      "epoch 64, loss -0.6403933614492416\n",
      "epoch 65, iter 0, loss -0.23574425280094147\n",
      "epoch 65, iter 1, loss -0.13572661578655243\n",
      "epoch 65, iter 2, loss -0.17941856384277344\n",
      "epoch 65, loss -0.5508894324302673\n",
      "epoch 66, iter 0, loss -0.18606390058994293\n",
      "epoch 66, iter 1, loss -0.16108353435993195\n",
      "epoch 66, iter 2, loss -0.2007722407579422\n",
      "epoch 66, loss -0.5479196757078171\n",
      "epoch 67, iter 0, loss -0.19243068993091583\n",
      "epoch 67, iter 1, loss -0.18601574003696442\n",
      "epoch 67, iter 2, loss -0.1296118199825287\n",
      "epoch 67, loss -0.5080582499504089\n",
      "epoch 68, iter 0, loss -0.202494278550148\n",
      "epoch 68, iter 1, loss -0.16907063126564026\n",
      "epoch 68, iter 2, loss -0.19541719555854797\n",
      "epoch 68, loss -0.5669821053743362\n",
      "epoch 69, iter 0, loss -0.20307768881320953\n",
      "epoch 69, iter 1, loss -0.1933366358280182\n",
      "epoch 69, iter 2, loss -0.1947053074836731\n",
      "epoch 69, loss -0.5911196321249008\n",
      "epoch 70, iter 0, loss -0.2229493260383606\n",
      "epoch 70, iter 1, loss -0.19791603088378906\n",
      "epoch 70, iter 2, loss -0.22974859178066254\n",
      "epoch 70, loss -0.6506139487028122\n",
      "epoch 71, iter 0, loss -0.24187272787094116\n",
      "epoch 71, iter 1, loss -0.24640457332134247\n",
      "epoch 71, iter 2, loss -0.20881852507591248\n",
      "epoch 71, loss -0.6970958262681961\n",
      "epoch 72, iter 0, loss -0.25580430030822754\n",
      "epoch 72, iter 1, loss -0.22743913531303406\n",
      "epoch 72, iter 2, loss -0.22892777621746063\n",
      "epoch 72, loss -0.7121712118387222\n",
      "epoch 73, iter 0, loss -0.2211127132177353\n",
      "epoch 73, iter 1, loss -0.25187909603118896\n",
      "epoch 73, iter 2, loss -0.22595255076885223\n",
      "epoch 73, loss -0.6989443600177765\n",
      "epoch 74, iter 0, loss -0.2348012775182724\n",
      "epoch 74, iter 1, loss -0.21775031089782715\n",
      "epoch 74, iter 2, loss -0.2264450639486313\n",
      "epoch 74, loss -0.6789966523647308\n",
      "epoch 75, iter 0, loss -0.19840359687805176\n",
      "epoch 75, iter 1, loss -0.2479916214942932\n",
      "epoch 75, iter 2, loss -0.22607745230197906\n",
      "epoch 75, loss -0.672472670674324\n",
      "epoch 76, iter 0, loss -0.22455887496471405\n",
      "epoch 76, iter 1, loss -0.22151948511600494\n",
      "epoch 76, iter 2, loss -0.21422885358333588\n",
      "epoch 76, loss -0.6603072136640549\n",
      "epoch 77, iter 0, loss -0.2313690483570099\n",
      "epoch 77, iter 1, loss -0.2140810340642929\n",
      "epoch 77, iter 2, loss -0.19736972451210022\n",
      "epoch 77, loss -0.642819806933403\n",
      "epoch 78, iter 0, loss -0.22974450886249542\n",
      "epoch 78, iter 1, loss -0.22306916117668152\n",
      "epoch 78, iter 2, loss -0.19978190958499908\n",
      "epoch 78, loss -0.652595579624176\n",
      "epoch 79, iter 0, loss -0.2156105488538742\n",
      "epoch 79, iter 1, loss -0.22782261669635773\n",
      "epoch 79, iter 2, loss -0.21974089741706848\n",
      "epoch 79, loss -0.6631740629673004\n",
      "epoch 80, iter 0, loss -0.2230319380760193\n",
      "epoch 80, iter 1, loss -0.2199227660894394\n",
      "epoch 80, iter 2, loss -0.21768449246883392\n",
      "epoch 80, loss -0.6606391966342926\n",
      "epoch 81, iter 0, loss -0.25712066888809204\n",
      "epoch 81, iter 1, loss -0.21007061004638672\n",
      "epoch 81, iter 2, loss -0.191581591963768\n",
      "epoch 81, loss -0.6587728708982468\n",
      "epoch 82, iter 0, loss -0.233005091547966\n",
      "epoch 82, iter 1, loss -0.22521650791168213\n",
      "epoch 82, iter 2, loss -0.254238486289978\n",
      "epoch 82, loss -0.7124600857496262\n",
      "epoch 83, iter 0, loss -0.2479332536458969\n",
      "epoch 83, iter 1, loss -0.24567493796348572\n",
      "epoch 83, iter 2, loss -0.23833435773849487\n",
      "epoch 83, loss -0.7319425493478775\n",
      "epoch 84, iter 0, loss -0.23380640149116516\n",
      "epoch 84, iter 1, loss -0.2728310227394104\n",
      "epoch 84, iter 2, loss -0.21881575882434845\n",
      "epoch 84, loss -0.725453183054924\n",
      "epoch 85, iter 0, loss -0.21812254190444946\n",
      "epoch 85, iter 1, loss -0.25645729899406433\n",
      "epoch 85, iter 2, loss -0.2470604032278061\n",
      "epoch 85, loss -0.7216402441263199\n",
      "epoch 86, iter 0, loss -0.2506974935531616\n",
      "epoch 86, iter 1, loss -0.23692640662193298\n",
      "epoch 86, iter 2, loss -0.23954565823078156\n",
      "epoch 86, loss -0.7271695584058762\n",
      "epoch 87, iter 0, loss -0.2509237229824066\n",
      "epoch 87, iter 1, loss -0.2599092721939087\n",
      "epoch 87, iter 2, loss -0.22352495789527893\n",
      "epoch 87, loss -0.7343579530715942\n",
      "epoch 88, iter 0, loss -0.24308902025222778\n",
      "epoch 88, iter 1, loss -0.2525089979171753\n",
      "epoch 88, iter 2, loss -0.2371080070734024\n",
      "epoch 88, loss -0.7327060252428055\n",
      "epoch 89, iter 0, loss -0.26538872718811035\n",
      "epoch 89, iter 1, loss -0.22967609763145447\n",
      "epoch 89, iter 2, loss -0.26651036739349365\n",
      "epoch 89, loss -0.7615751922130585\n",
      "epoch 90, iter 0, loss -0.2770354747772217\n",
      "epoch 90, iter 1, loss -0.2258206456899643\n",
      "epoch 90, iter 2, loss -0.22511306405067444\n",
      "epoch 90, loss -0.7279691845178604\n",
      "epoch 91, iter 0, loss -0.2515733242034912\n",
      "epoch 91, iter 1, loss -0.26393789052963257\n",
      "epoch 91, iter 2, loss -0.23819100856781006\n",
      "epoch 91, loss -0.7537022233009338\n",
      "epoch 92, iter 0, loss -0.2512570023536682\n",
      "epoch 92, iter 1, loss -0.2504020035266876\n",
      "epoch 92, iter 2, loss -0.2581265866756439\n",
      "epoch 92, loss -0.7597855925559998\n",
      "epoch 93, iter 0, loss -0.25348830223083496\n",
      "epoch 93, iter 1, loss -0.24469955265522003\n",
      "epoch 93, iter 2, loss -0.24285855889320374\n",
      "epoch 93, loss -0.7410464137792587\n",
      "epoch 94, iter 0, loss -0.2646164298057556\n",
      "epoch 94, iter 1, loss -0.23309460282325745\n",
      "epoch 94, iter 2, loss -0.2382759004831314\n",
      "epoch 94, loss -0.7359869331121445\n",
      "epoch 95, iter 0, loss -0.25896337628364563\n",
      "epoch 95, iter 1, loss -0.25648993253707886\n",
      "epoch 95, iter 2, loss -0.2199528068304062\n",
      "epoch 95, loss -0.7354061156511307\n",
      "epoch 96, iter 0, loss -0.2899624705314636\n",
      "epoch 96, iter 1, loss -0.26197290420532227\n",
      "epoch 96, iter 2, loss -0.20744964480400085\n",
      "epoch 96, loss -0.7593850195407867\n",
      "epoch 97, iter 0, loss -0.278483510017395\n",
      "epoch 97, iter 1, loss -0.24828308820724487\n",
      "epoch 97, iter 2, loss -0.19503745436668396\n",
      "epoch 97, loss -0.7218040525913239\n",
      "epoch 98, iter 0, loss -0.2559700608253479\n",
      "epoch 98, iter 1, loss -0.2764970660209656\n",
      "epoch 98, iter 2, loss -0.268962562084198\n",
      "epoch 98, loss -0.8014296889305115\n",
      "epoch 99, iter 0, loss -0.2762376666069031\n",
      "epoch 99, iter 1, loss -0.2393082082271576\n",
      "epoch 99, iter 2, loss -0.28383129835128784\n",
      "epoch 99, loss -0.7993771731853485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-31 01:03:36.676314, fold=2, rep=0, eta=0d 0h 10m 12s \n",
      "{'fold': 2, 'repeat': 0, 'n': 4137, 'd': 20, 'mse': 0.01617894135415554, 'train_time': 597.9514264320023, 'trained_epochs': 100, 'prior_train_nmll': -0.22759974002838135, 'train_nll': -1698.525634765625, 'test_nll': -802.0628662109375, 'train_mse': 0.013315312564373016, 'state_dict_file': 'model_state_dict_-7904439601581500385.pkl'}\n",
      "epoch 0, iter 0, loss 1.9372121095657349\n",
      "epoch 0, iter 1, loss 2.1120176315307617\n",
      "epoch 0, iter 2, loss 2.169618606567383\n",
      "epoch 0, loss 6.218848347663879\n",
      "epoch 1, iter 0, loss 1.974071741104126\n",
      "epoch 1, iter 1, loss 1.902721881866455\n",
      "epoch 1, iter 2, loss 1.8754980564117432\n",
      "epoch 1, loss 5.752291679382324\n",
      "epoch 2, iter 0, loss 1.734567642211914\n",
      "epoch 2, iter 1, loss 1.5694891214370728\n",
      "epoch 2, iter 2, loss 1.5546938180923462\n",
      "epoch 2, loss 4.858750581741333\n",
      "epoch 3, iter 0, loss 1.5575683116912842\n",
      "epoch 3, iter 1, loss 1.506858229637146\n",
      "epoch 3, iter 2, loss 1.443507194519043\n",
      "epoch 3, loss 4.507933735847473\n",
      "epoch 4, iter 0, loss 1.4174308776855469\n",
      "epoch 4, iter 1, loss 1.3967725038528442\n",
      "epoch 4, iter 2, loss 1.3685535192489624\n",
      "epoch 4, loss 4.1827569007873535\n",
      "epoch 5, iter 0, loss 1.3394649028778076\n",
      "epoch 5, iter 1, loss 1.3174116611480713\n",
      "epoch 5, iter 2, loss 1.2996900081634521\n",
      "epoch 5, loss 3.956566572189331\n",
      "epoch 6, iter 0, loss 1.2684581279754639\n",
      "epoch 6, iter 1, loss 1.2461774349212646\n",
      "epoch 6, iter 2, loss 1.2180655002593994\n",
      "epoch 6, loss 3.732701063156128\n",
      "epoch 7, iter 0, loss 1.1914783716201782\n",
      "epoch 7, iter 1, loss 1.1673303842544556\n",
      "epoch 7, iter 2, loss 1.1448322534561157\n",
      "epoch 7, loss 3.5036410093307495\n",
      "epoch 8, iter 0, loss 1.1167194843292236\n",
      "epoch 8, iter 1, loss 1.082580804824829\n",
      "epoch 8, iter 2, loss 1.0586457252502441\n",
      "epoch 8, loss 3.257946014404297\n",
      "epoch 9, iter 0, loss 1.028363585472107\n",
      "epoch 9, iter 1, loss 0.9967470169067383\n",
      "epoch 9, iter 2, loss 0.9753872156143188\n",
      "epoch 9, loss 3.000497817993164\n",
      "epoch 10, iter 0, loss 0.9366308450698853\n",
      "epoch 10, iter 1, loss 0.9024011492729187\n",
      "epoch 10, iter 2, loss 0.8737902641296387\n",
      "epoch 10, loss 2.7128222584724426\n",
      "epoch 11, iter 0, loss 0.8323872089385986\n",
      "epoch 11, iter 1, loss 0.79648357629776\n",
      "epoch 11, iter 2, loss 0.7742791771888733\n",
      "epoch 11, loss 2.403149962425232\n",
      "epoch 12, iter 0, loss 0.7317689061164856\n",
      "epoch 12, iter 1, loss 0.691429078578949\n",
      "epoch 12, iter 2, loss 0.6652754545211792\n",
      "epoch 12, loss 2.0884734392166138\n",
      "epoch 13, iter 0, loss 0.6263927817344666\n",
      "epoch 13, iter 1, loss 0.5953021049499512\n",
      "epoch 13, iter 2, loss 0.5608786344528198\n",
      "epoch 13, loss 1.7825735211372375\n",
      "epoch 14, iter 0, loss 0.5325862169265747\n",
      "epoch 14, iter 1, loss 0.4995856285095215\n",
      "epoch 14, iter 2, loss 0.4839506149291992\n",
      "epoch 14, loss 1.5161224603652954\n",
      "epoch 15, iter 0, loss 0.45994889736175537\n",
      "epoch 15, iter 1, loss 0.44925493001937866\n",
      "epoch 15, iter 2, loss 0.43310779333114624\n",
      "epoch 15, loss 1.3423116207122803\n",
      "epoch 16, iter 0, loss 0.4276130497455597\n",
      "epoch 16, iter 1, loss 0.4001733362674713\n",
      "epoch 16, iter 2, loss 0.4082137942314148\n",
      "epoch 16, loss 1.2360001802444458\n",
      "epoch 17, iter 0, loss 0.4091426134109497\n",
      "epoch 17, iter 1, loss 0.37082499265670776\n",
      "epoch 17, iter 2, loss 0.39244335889816284\n",
      "epoch 17, loss 1.1724109649658203\n",
      "epoch 18, iter 0, loss 0.38266655802726746\n",
      "epoch 18, iter 1, loss 0.3615269660949707\n",
      "epoch 18, iter 2, loss 0.3673321008682251\n",
      "epoch 18, loss 1.1115256249904633\n",
      "epoch 19, iter 0, loss 0.36886483430862427\n",
      "epoch 19, iter 1, loss 0.3671780228614807\n",
      "epoch 19, iter 2, loss 0.4056429862976074\n",
      "epoch 19, loss 1.1416858434677124\n",
      "epoch 20, iter 0, loss 0.3751525580883026\n",
      "epoch 20, iter 1, loss 0.3622235953807831\n",
      "epoch 20, iter 2, loss 0.35039791464805603\n",
      "epoch 20, loss 1.0877740681171417\n",
      "epoch 21, iter 0, loss 0.3553762435913086\n",
      "epoch 21, iter 1, loss 0.3318088948726654\n",
      "epoch 21, iter 2, loss 0.4853222668170929\n",
      "epoch 21, loss 1.172507405281067\n",
      "epoch 22, iter 0, loss 0.3326863944530487\n",
      "epoch 22, iter 1, loss 0.30002063512802124\n",
      "epoch 22, iter 2, loss 0.2753683030605316\n",
      "epoch 22, loss 0.9080753326416016\n",
      "epoch 23, iter 0, loss 0.26594531536102295\n",
      "epoch 23, iter 1, loss 0.2512963116168976\n",
      "epoch 23, iter 2, loss 0.21535509824752808\n",
      "epoch 23, loss 0.7325967252254486\n",
      "epoch 24, iter 0, loss 0.20441032946109772\n",
      "epoch 24, iter 1, loss 0.18045344948768616\n",
      "epoch 24, iter 2, loss 0.20252835750579834\n",
      "epoch 24, loss 0.5873921364545822\n",
      "epoch 25, iter 0, loss 0.15237455070018768\n",
      "epoch 25, iter 1, loss 0.1384628266096115\n",
      "epoch 25, iter 2, loss 0.11663374304771423\n",
      "epoch 25, loss 0.4074711203575134\n",
      "epoch 26, iter 0, loss 0.10359296202659607\n",
      "epoch 26, iter 1, loss 0.09216208755970001\n",
      "epoch 26, iter 2, loss 0.0684347152709961\n",
      "epoch 26, loss 0.2641897648572922\n",
      "epoch 27, iter 0, loss 0.0580751895904541\n",
      "epoch 27, iter 1, loss 0.04668945074081421\n",
      "epoch 27, iter 2, loss 0.0168742835521698\n",
      "epoch 27, loss 0.12163892388343811\n",
      "epoch 28, iter 0, loss 0.010006874799728394\n",
      "epoch 28, iter 1, loss -0.020757347345352173\n",
      "epoch 28, iter 2, loss -0.01076720654964447\n",
      "epoch 28, loss -0.02151767909526825\n",
      "epoch 29, iter 0, loss -0.036683619022369385\n",
      "epoch 29, iter 1, loss -0.034092143177986145\n",
      "epoch 29, iter 2, loss -0.022638633847236633\n",
      "epoch 29, loss -0.09341439604759216\n",
      "epoch 30, iter 0, loss -0.04883915185928345\n",
      "epoch 30, iter 1, loss -0.07582572102546692\n",
      "epoch 30, iter 2, loss -0.04692423343658447\n",
      "epoch 30, loss -0.17158910632133484\n",
      "epoch 31, iter 0, loss -0.08692830801010132\n",
      "epoch 31, iter 1, loss -0.060907840728759766\n",
      "epoch 31, iter 2, loss -0.05148807168006897\n",
      "epoch 31, loss -0.19932422041893005\n",
      "epoch 32, iter 0, loss -0.07405272126197815\n",
      "epoch 32, iter 1, loss -0.0782490074634552\n",
      "epoch 32, iter 2, loss -0.05451670289039612\n",
      "epoch 32, loss -0.20681843161582947\n",
      "epoch 33, iter 0, loss -0.07568374276161194\n",
      "epoch 33, iter 1, loss -0.057756274938583374\n",
      "epoch 33, iter 2, loss -0.06046286225318909\n",
      "epoch 33, loss -0.1939028799533844\n",
      "epoch 34, iter 0, loss -0.06655257940292358\n",
      "epoch 34, iter 1, loss -0.02150699496269226\n",
      "epoch 34, iter 2, loss -0.034708619117736816\n",
      "epoch 34, loss -0.12276819348335266\n",
      "epoch 35, iter 0, loss -0.027370959520339966\n",
      "epoch 35, iter 1, loss -0.02960929274559021\n",
      "epoch 35, iter 2, loss -0.015467852354049683\n",
      "epoch 35, loss -0.07244810461997986\n",
      "epoch 36, iter 0, loss -0.018485069274902344\n",
      "epoch 36, iter 1, loss -0.03828278183937073\n",
      "epoch 36, iter 2, loss -0.02375653386116028\n",
      "epoch 36, loss -0.08052438497543335\n",
      "epoch 37, iter 0, loss -0.05108872056007385\n",
      "epoch 37, iter 1, loss -0.042112648487091064\n",
      "epoch 37, iter 2, loss -0.0614284873008728\n",
      "epoch 37, loss -0.15462985634803772\n",
      "epoch 38, iter 0, loss -0.028852194547653198\n",
      "epoch 38, iter 1, loss -0.08957721292972565\n",
      "epoch 38, iter 2, loss -0.06003437936306\n",
      "epoch 38, loss -0.17846378684043884\n",
      "epoch 39, iter 0, loss -0.059211328625679016\n",
      "epoch 39, iter 1, loss -0.0775848925113678\n",
      "epoch 39, iter 2, loss -0.08131575584411621\n",
      "epoch 39, loss -0.21811197698116302\n",
      "epoch 40, iter 0, loss -0.11113961040973663\n",
      "epoch 40, iter 1, loss -0.10041964054107666\n",
      "epoch 40, iter 2, loss -0.10106009244918823\n",
      "epoch 40, loss -0.3126193434000015\n",
      "epoch 41, iter 0, loss -0.12003841996192932\n",
      "epoch 41, iter 1, loss -0.13241977989673615\n",
      "epoch 41, iter 2, loss -0.12288236618041992\n",
      "epoch 41, loss -0.3753405660390854\n",
      "epoch 42, iter 0, loss -0.13713251054286957\n",
      "epoch 42, iter 1, loss -0.12533557415008545\n",
      "epoch 42, iter 2, loss -0.1431858390569687\n",
      "epoch 42, loss -0.4056539237499237\n",
      "epoch 43, iter 0, loss -0.15701887011528015\n",
      "epoch 43, iter 1, loss -0.14961043000221252\n",
      "epoch 43, iter 2, loss -0.1605815887451172\n",
      "epoch 43, loss -0.46721088886260986\n",
      "epoch 44, iter 0, loss -0.17651666700839996\n",
      "epoch 44, iter 1, loss -0.15356315672397614\n",
      "epoch 44, iter 2, loss -0.17506127059459686\n",
      "epoch 44, loss -0.505141094326973\n",
      "epoch 45, iter 0, loss -0.18045778572559357\n",
      "epoch 45, iter 1, loss -0.17946133017539978\n",
      "epoch 45, iter 2, loss -0.16981616616249084\n",
      "epoch 45, loss -0.5297352820634842\n",
      "epoch 46, iter 0, loss -0.184650719165802\n",
      "epoch 46, iter 1, loss -0.1802656650543213\n",
      "epoch 46, iter 2, loss -0.1390475183725357\n",
      "epoch 46, loss -0.503963902592659\n",
      "epoch 47, iter 0, loss -0.14277949929237366\n",
      "epoch 47, iter 1, loss -0.15712839365005493\n",
      "epoch 47, iter 2, loss -0.1510133445262909\n",
      "epoch 47, loss -0.4509212374687195\n",
      "epoch 48, iter 0, loss -0.1476442515850067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48, iter 1, loss -0.13675491511821747\n",
      "epoch 48, iter 2, loss -0.11651317775249481\n",
      "epoch 48, loss -0.400912344455719\n",
      "epoch 49, iter 0, loss -0.13927611708641052\n",
      "epoch 49, iter 1, loss -0.166640505194664\n",
      "epoch 49, iter 2, loss -0.14163556694984436\n",
      "epoch 49, loss -0.4475521892309189\n",
      "epoch 50, iter 0, loss -0.1546022891998291\n",
      "epoch 50, iter 1, loss -0.1404818743467331\n",
      "epoch 50, iter 2, loss -0.1095585972070694\n",
      "epoch 50, loss -0.4046427607536316\n",
      "epoch 51, iter 0, loss -0.150705486536026\n",
      "epoch 51, iter 1, loss -0.1403505653142929\n",
      "epoch 51, iter 2, loss -0.12045718729496002\n",
      "epoch 51, loss -0.41151323914527893\n",
      "epoch 52, iter 0, loss -0.1504351794719696\n",
      "epoch 52, iter 1, loss -0.10450318455696106\n",
      "epoch 52, iter 2, loss -0.17100773751735687\n",
      "epoch 52, loss -0.42594610154628754\n",
      "epoch 53, iter 0, loss -0.14783655107021332\n",
      "epoch 53, iter 1, loss -0.16582091152668\n",
      "epoch 53, iter 2, loss -0.12842977046966553\n",
      "epoch 53, loss -0.44208723306655884\n",
      "epoch 54, iter 0, loss -0.15442423522472382\n",
      "epoch 54, iter 1, loss -0.15652333199977875\n",
      "epoch 54, iter 2, loss -0.20136605203151703\n",
      "epoch 54, loss -0.5123136192560196\n",
      "epoch 55, iter 0, loss -0.17476576566696167\n",
      "epoch 55, iter 1, loss -0.18211588263511658\n",
      "epoch 55, iter 2, loss -0.16980114579200745\n",
      "epoch 55, loss -0.5266827940940857\n",
      "epoch 56, iter 0, loss -0.20107366144657135\n",
      "epoch 56, iter 1, loss -0.19435502588748932\n",
      "epoch 56, iter 2, loss -0.18788787722587585\n",
      "epoch 56, loss -0.5833165645599365\n",
      "epoch 57, iter 0, loss -0.19869989156723022\n",
      "epoch 57, iter 1, loss -0.17071221768856049\n",
      "epoch 57, iter 2, loss -0.21648378670215607\n",
      "epoch 57, loss -0.5858958959579468\n",
      "epoch 58, iter 0, loss -0.1934906244277954\n",
      "epoch 58, iter 1, loss -0.21206820011138916\n",
      "epoch 58, iter 2, loss -0.18389566242694855\n",
      "epoch 58, loss -0.5894544869661331\n",
      "epoch 59, iter 0, loss -0.22661086916923523\n",
      "epoch 59, iter 1, loss -0.2014751136302948\n",
      "epoch 59, iter 2, loss -0.19071531295776367\n",
      "epoch 59, loss -0.6188012957572937\n",
      "epoch 60, iter 0, loss -0.19690276682376862\n",
      "epoch 60, iter 1, loss -0.2106839120388031\n",
      "epoch 60, iter 2, loss -0.20199298858642578\n",
      "epoch 60, loss -0.6095796674489975\n",
      "epoch 61, iter 0, loss -0.18186232447624207\n",
      "epoch 61, iter 1, loss -0.2123611569404602\n",
      "epoch 61, iter 2, loss -0.20233182609081268\n",
      "epoch 61, loss -0.596555307507515\n",
      "epoch 62, iter 0, loss -0.2189980149269104\n",
      "epoch 62, iter 1, loss -0.1687176376581192\n",
      "epoch 62, iter 2, loss -0.20298923552036285\n",
      "epoch 62, loss -0.5907048881053925\n",
      "epoch 63, iter 0, loss -0.21693740785121918\n",
      "epoch 63, iter 1, loss -0.18997210264205933\n",
      "epoch 63, iter 2, loss -0.17639176547527313\n",
      "epoch 63, loss -0.5833012759685516\n",
      "epoch 64, iter 0, loss -0.16483958065509796\n",
      "epoch 64, iter 1, loss -0.22324731945991516\n",
      "epoch 64, iter 2, loss -0.1971292495727539\n",
      "epoch 64, loss -0.585216149687767\n",
      "epoch 65, iter 0, loss -0.19764219224452972\n",
      "epoch 65, iter 1, loss -0.18793711066246033\n",
      "epoch 65, iter 2, loss -0.2101205289363861\n",
      "epoch 65, loss -0.5956998318433762\n",
      "epoch 66, iter 0, loss -0.2258581668138504\n",
      "epoch 66, iter 1, loss -0.20796997845172882\n",
      "epoch 66, iter 2, loss -0.20365002751350403\n",
      "epoch 66, loss -0.6374781727790833\n",
      "epoch 67, iter 0, loss -0.22496084868907928\n",
      "epoch 67, iter 1, loss -0.21539713442325592\n",
      "epoch 67, iter 2, loss -0.22662338614463806\n",
      "epoch 67, loss -0.6669813692569733\n",
      "epoch 68, iter 0, loss -0.23057574033737183\n",
      "epoch 68, iter 1, loss -0.21485327184200287\n",
      "epoch 68, iter 2, loss -0.25281304121017456\n",
      "epoch 68, loss -0.6982420533895493\n",
      "epoch 69, iter 0, loss -0.23663055896759033\n",
      "epoch 69, iter 1, loss -0.20692722499370575\n",
      "epoch 69, iter 2, loss -0.2561609447002411\n",
      "epoch 69, loss -0.6997187286615372\n",
      "epoch 70, iter 0, loss -0.25805947184562683\n",
      "epoch 70, iter 1, loss -0.2212640345096588\n",
      "epoch 70, iter 2, loss -0.21623721718788147\n",
      "epoch 70, loss -0.6955607235431671\n",
      "epoch 71, iter 0, loss -0.2315550297498703\n",
      "epoch 71, iter 1, loss -0.22895832359790802\n",
      "epoch 71, iter 2, loss -0.21503715217113495\n",
      "epoch 71, loss -0.6755505055189133\n",
      "epoch 72, iter 0, loss -0.23255936801433563\n",
      "epoch 72, iter 1, loss -0.2608436942100525\n",
      "epoch 72, iter 2, loss -0.19859403371810913\n",
      "epoch 72, loss -0.6919970959424973\n",
      "epoch 73, iter 0, loss -0.23946310579776764\n",
      "epoch 73, iter 1, loss -0.24946318566799164\n",
      "epoch 73, iter 2, loss -0.22273123264312744\n",
      "epoch 73, loss -0.7116575241088867\n",
      "epoch 74, iter 0, loss -0.2533085346221924\n",
      "epoch 74, iter 1, loss -0.21871651709079742\n",
      "epoch 74, iter 2, loss -0.23289744555950165\n",
      "epoch 74, loss -0.7049224972724915\n",
      "epoch 75, iter 0, loss -0.2496025115251541\n",
      "epoch 75, iter 1, loss -0.20249758660793304\n",
      "epoch 75, iter 2, loss -0.22911742329597473\n",
      "epoch 75, loss -0.6812175214290619\n",
      "epoch 76, iter 0, loss -0.23229748010635376\n",
      "epoch 76, iter 1, loss -0.21831199526786804\n",
      "epoch 76, iter 2, loss -0.22562310099601746\n",
      "epoch 76, loss -0.6762325763702393\n",
      "epoch 77, iter 0, loss -0.2434539645910263\n",
      "epoch 77, iter 1, loss -0.23582711815834045\n",
      "epoch 77, iter 2, loss -0.2205459624528885\n",
      "epoch 77, loss -0.6998270452022552\n",
      "epoch 78, iter 0, loss -0.23957476019859314\n",
      "epoch 78, iter 1, loss -0.23831726610660553\n",
      "epoch 78, iter 2, loss -0.24679239094257355\n",
      "epoch 78, loss -0.7246844172477722\n",
      "epoch 79, iter 0, loss -0.23118956387043\n",
      "epoch 79, iter 1, loss -0.23447641730308533\n",
      "epoch 79, iter 2, loss -0.2276158481836319\n",
      "epoch 79, loss -0.6932818293571472\n",
      "epoch 80, iter 0, loss -0.24787406623363495\n",
      "epoch 80, iter 1, loss -0.212564155459404\n",
      "epoch 80, iter 2, loss -0.2587072253227234\n",
      "epoch 80, loss -0.7191454470157623\n",
      "epoch 81, iter 0, loss -0.2655935287475586\n",
      "epoch 81, iter 1, loss -0.24430570006370544\n",
      "epoch 81, iter 2, loss -0.22266380488872528\n",
      "epoch 81, loss -0.7325630336999893\n",
      "epoch 82, iter 0, loss -0.26065224409103394\n",
      "epoch 82, iter 1, loss -0.20704172551631927\n",
      "epoch 82, iter 2, loss -0.257871150970459\n",
      "epoch 82, loss -0.7255651205778122\n",
      "epoch 83, iter 0, loss -0.247096985578537\n",
      "epoch 83, iter 1, loss -0.22654516994953156\n",
      "epoch 83, iter 2, loss -0.2563930153846741\n",
      "epoch 83, loss -0.7300351709127426\n",
      "epoch 84, iter 0, loss -0.2652805745601654\n",
      "epoch 84, iter 1, loss -0.24846519529819489\n",
      "epoch 84, iter 2, loss -0.228786900639534\n",
      "epoch 84, loss -0.7425326704978943\n",
      "epoch 85, iter 0, loss -0.2582741975784302\n",
      "epoch 85, iter 1, loss -0.21156816184520721\n",
      "epoch 85, iter 2, loss -0.22683127224445343\n",
      "epoch 85, loss -0.6966736316680908\n",
      "epoch 86, iter 0, loss -0.24201464653015137\n",
      "epoch 86, iter 1, loss -0.23595795035362244\n",
      "epoch 86, iter 2, loss -0.19965873658657074\n",
      "epoch 86, loss -0.6776313334703445\n",
      "epoch 87, iter 0, loss -0.25683915615081787\n",
      "epoch 87, iter 1, loss -0.21400056779384613\n",
      "epoch 87, iter 2, loss -0.24045278131961823\n",
      "epoch 87, loss -0.7112925052642822\n",
      "epoch 88, iter 0, loss -0.2618728280067444\n",
      "epoch 88, iter 1, loss -0.25308918952941895\n",
      "epoch 88, iter 2, loss -0.24214506149291992\n",
      "epoch 88, loss -0.7571070790290833\n",
      "epoch 89, iter 0, loss -0.26908913254737854\n",
      "epoch 89, iter 1, loss -0.2466984987258911\n",
      "epoch 89, iter 2, loss -0.24122260510921478\n",
      "epoch 89, loss -0.7570102363824844\n",
      "epoch 90, iter 0, loss -0.2408764809370041\n",
      "epoch 90, iter 1, loss -0.2632044553756714\n",
      "epoch 90, iter 2, loss -0.24738246202468872\n",
      "epoch 90, loss -0.7514633983373642\n",
      "epoch 91, iter 0, loss -0.24827101826667786\n",
      "epoch 91, iter 1, loss -0.2671111226081848\n",
      "epoch 91, iter 2, loss -0.26408469676971436\n",
      "epoch 91, loss -0.779466837644577\n",
      "epoch 92, iter 0, loss -0.25953084230422974\n",
      "epoch 92, iter 1, loss -0.25043946504592896\n",
      "epoch 92, iter 2, loss -0.2680152356624603\n",
      "epoch 92, loss -0.777985543012619\n",
      "epoch 93, iter 0, loss -0.2547428607940674\n",
      "epoch 93, iter 1, loss -0.23625591397285461\n",
      "epoch 93, iter 2, loss -0.29945939779281616\n",
      "epoch 93, loss -0.7904581725597382\n",
      "epoch 94, iter 0, loss -0.2573152780532837\n",
      "epoch 94, iter 1, loss -0.24236968159675598\n",
      "epoch 94, iter 2, loss -0.2701427936553955\n",
      "epoch 94, loss -0.7698277533054352\n",
      "epoch 95, iter 0, loss -0.27750930190086365\n",
      "epoch 95, iter 1, loss -0.22494544088840485\n",
      "epoch 95, iter 2, loss -0.22493886947631836\n",
      "epoch 95, loss -0.7273936122655869\n",
      "epoch 96, iter 0, loss -0.24891869723796844\n",
      "epoch 96, iter 1, loss -0.2477790117263794\n",
      "epoch 96, iter 2, loss -0.18934720754623413\n",
      "epoch 96, loss -0.686044916510582\n",
      "epoch 97, iter 0, loss -0.24712201952934265\n",
      "epoch 97, iter 1, loss -0.23975296318531036\n",
      "epoch 97, iter 2, loss -0.2233535200357437\n",
      "epoch 97, loss -0.7102285027503967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98, iter 0, loss -0.26036337018013\n",
      "epoch 98, iter 1, loss -0.23444406688213348\n",
      "epoch 98, iter 2, loss -0.24644726514816284\n",
      "epoch 98, loss -0.7412547022104263\n",
      "epoch 99, iter 0, loss -0.26489031314849854\n",
      "epoch 99, iter 1, loss -0.2520592212677002\n",
      "epoch 99, iter 2, loss -0.23879952728748322\n",
      "epoch 99, loss -0.755749061703682\n",
      "2019-03-31 01:13:52.733929, fold=2, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 2, 'repeat': 1, 'n': 4137, 'd': 20, 'mse': 0.015191111713647842, 'train_time': 616.0573963429779, 'trained_epochs': 100, 'prior_train_nmll': -0.19992251694202423, 'train_nll': -1663.516845703125, 'test_nll': -797.4967041015625, 'train_mse': 0.01363111101090908, 'state_dict_file': 'model_state_dict_-2135077639786178646.pkl'}\n",
      "Begin dataset  parkinsons\n",
      "Begin with SKI\n",
      "epoch 0, loss 1.2019771337509155\n",
      "epoch 1, loss 1.181835412979126\n",
      "epoch 2, loss 1.1725802421569824\n",
      "epoch 3, loss 1.1588599681854248\n",
      "epoch 4, loss 1.152018427848816\n",
      "epoch 5, loss 1.1429669857025146\n",
      "epoch 6, loss 1.138390302658081\n",
      "epoch 7, loss 1.1403392553329468\n",
      "epoch 8, loss 1.1351649761199951\n",
      "epoch 9, loss 1.1350699663162231\n",
      "epoch 10, loss 1.128770351409912\n",
      "epoch 11, loss 1.1275303363800049\n",
      "epoch 12, loss 1.1227585077285767\n",
      "epoch 13, loss 1.121131181716919\n",
      "epoch 14, loss 1.1153497695922852\n",
      "epoch 15, loss 1.1151354312896729\n",
      "epoch 16, loss 1.1121957302093506\n",
      "epoch 17, loss 1.1121078729629517\n",
      "epoch 18, loss 1.1153446435928345\n",
      "epoch 19, loss 1.1091316938400269\n",
      "epoch 20, loss 1.1124167442321777\n",
      "epoch 21, loss 1.1063729524612427\n",
      "epoch 22, loss 1.1054672002792358\n",
      "epoch 23, loss 1.106015682220459\n",
      "epoch 24, loss 1.1038848161697388\n",
      "epoch 25, loss 1.1012089252471924\n",
      "epoch 26, loss 1.1034377813339233\n",
      "epoch 27, loss 1.10252046585083\n",
      "epoch 28, loss 1.10167396068573\n",
      "epoch 29, loss 1.1034448146820068\n",
      "epoch 30, loss 1.0986499786376953\n",
      "epoch 31, loss 1.098373532295227\n",
      "epoch 32, loss 1.0990264415740967\n",
      "epoch 33, loss 1.0994038581848145\n",
      "epoch 34, loss 1.0970447063446045\n",
      "epoch 35, loss 1.0969105958938599\n",
      "epoch 36, loss 1.0934834480285645\n",
      "epoch 37, loss 1.0974173545837402\n",
      "epoch 38, loss 1.0956926345825195\n",
      "epoch 39, loss 1.0930925607681274\n",
      "epoch 40, loss 1.0899641513824463\n",
      "epoch 41, loss 1.0936081409454346\n",
      "epoch 42, loss 1.095915675163269\n",
      "epoch 43, loss 1.0931862592697144\n",
      "epoch 44, loss 1.0926923751831055\n",
      "epoch 45, loss 1.0957950353622437\n",
      "epoch 46, loss 1.0950024127960205\n",
      "epoch 47, loss 1.0917797088623047\n",
      "epoch 48, loss 1.0856741666793823\n",
      "epoch 49, loss 1.0927700996398926\n",
      "epoch 50, loss 1.0919163227081299\n",
      "epoch 51, loss 1.093934178352356\n",
      "epoch 52, loss 1.0928312540054321\n",
      "epoch 53, loss 1.0905845165252686\n",
      "epoch 54, loss 1.0949664115905762\n",
      "epoch 55, loss 1.092857003211975\n",
      "epoch 56, loss 1.0934821367263794\n",
      "epoch 57, loss 1.0912470817565918\n",
      "epoch 58, loss 1.0897722244262695\n",
      "epoch 59, loss 1.0899240970611572\n",
      "epoch 60, loss 1.0916811227798462\n",
      "epoch 61, loss 1.0908292531967163\n",
      "epoch 62, loss 1.08895742893219\n",
      "epoch 63, loss 1.0908458232879639\n",
      "epoch 64, loss 1.0888774394989014\n",
      "epoch 65, loss 1.0882649421691895\n",
      "epoch 66, loss 1.090052843093872\n",
      "epoch 67, loss 1.0881073474884033\n",
      "epoch 68, loss 1.089789628982544\n",
      "epoch 69, loss 1.086653709411621\n",
      "epoch 70, loss 1.0873634815216064\n",
      "epoch 71, loss 1.0886043310165405\n",
      "epoch 72, loss 1.0871340036392212\n",
      "epoch 73, loss 1.0870853662490845\n",
      "epoch 74, loss 1.087110996246338\n",
      "epoch 75, loss 1.085129737854004\n",
      "epoch 76, loss 1.08675217628479\n",
      "epoch 77, loss 1.0865392684936523\n",
      "epoch 78, loss 1.0870352983474731\n",
      "epoch 79, loss 1.0872368812561035\n",
      "epoch 80, loss 1.0889840126037598\n",
      "epoch 81, loss 1.0885472297668457\n",
      "epoch 82, loss 1.0854214429855347\n",
      "epoch 83, loss 1.085842251777649\n",
      "epoch 84, loss 1.0854523181915283\n",
      "epoch 85, loss 1.0867301225662231\n",
      "epoch 86, loss 1.08513343334198\n",
      "epoch 87, loss 1.0870141983032227\n",
      "epoch 88, loss 1.0866878032684326\n",
      "epoch 89, loss 1.0849305391311646\n",
      "epoch 90, loss 1.08683180809021\n",
      "epoch 91, loss 1.0860249996185303\n",
      "epoch 92, loss 1.0846149921417236\n",
      "epoch 93, loss 1.087349534034729\n",
      "epoch 94, loss 1.0858736038208008\n",
      "epoch 95, loss 1.0873501300811768\n",
      "epoch 96, loss 1.08651864528656\n",
      "epoch 97, loss 1.0847357511520386\n",
      "epoch 98, loss 1.0836803913116455\n",
      "epoch 99, loss 1.0859029293060303\n",
      "2019-03-31 01:21:28.430571, fold=0, rep=0, eta=0d 0h 37m 58s \n",
      "{'fold': 0, 'repeat': 0, 'n': 5875, 'd': 18, 'mse': 1.8472484350204468, 'train_time': 454.393191176001, 'trained_epochs': 100, 'prior_train_nmll': 1.0862375497817993, 'train_nll': 4003.208251953125, 'test_nll': 2520.82861328125, 'train_mse': 0.43342846632003784, 'state_dict_file': 'model_state_dict_5020224551962167705.pkl'}\n",
      "epoch 0, loss 1.087519645690918\n",
      "epoch 1, loss 1.0635977983474731\n",
      "epoch 2, loss 1.0432883501052856\n",
      "epoch 3, loss 1.0212385654449463\n",
      "epoch 4, loss 1.0033133029937744\n",
      "epoch 5, loss 0.9858787655830383\n",
      "epoch 6, loss 0.9739030003547668\n",
      "epoch 7, loss 0.960972011089325\n",
      "epoch 8, loss 0.953213095664978\n",
      "epoch 9, loss 0.9427346587181091\n",
      "epoch 10, loss 0.9370262026786804\n",
      "epoch 11, loss 0.9357077479362488\n",
      "epoch 12, loss 0.9305136799812317\n",
      "epoch 13, loss 0.932788610458374\n",
      "epoch 14, loss 0.9312742948532104\n",
      "epoch 15, loss 0.9338545203208923\n",
      "epoch 16, loss 0.9328760504722595\n",
      "epoch 17, loss 0.9357954859733582\n",
      "epoch 18, loss 0.9306290149688721\n",
      "epoch 19, loss 0.9292129278182983\n",
      "epoch 20, loss 0.9257681965827942\n",
      "epoch 21, loss 0.9231888651847839\n",
      "epoch 22, loss 0.9201948046684265\n",
      "epoch 23, loss 0.9185853004455566\n",
      "epoch 24, loss 0.9172990918159485\n",
      "epoch 25, loss 0.9226145148277283\n",
      "epoch 26, loss 0.9153395891189575\n",
      "epoch 27, loss 0.9113677740097046\n",
      "epoch 28, loss 0.9115305542945862\n",
      "epoch 29, loss 0.913210928440094\n",
      "epoch 30, loss 0.9141420722007751\n",
      "epoch 31, loss 0.9126636385917664\n",
      "epoch 32, loss 0.9141204357147217\n",
      "epoch 33, loss 0.9121623635292053\n",
      "epoch 34, loss 0.9106423854827881\n",
      "epoch 35, loss 0.9118438959121704\n",
      "epoch 36, loss 0.9109066128730774\n",
      "epoch 37, loss 0.9108872413635254\n",
      "epoch 38, loss 0.9113020300865173\n",
      "epoch 39, loss 0.9081069827079773\n",
      "epoch 40, loss 0.9093698263168335\n",
      "epoch 41, loss 0.910481333732605\n",
      "epoch 42, loss 0.9048610925674438\n",
      "epoch 43, loss 0.9085399508476257\n",
      "epoch 44, loss 0.9088561534881592\n",
      "epoch 45, loss 0.9082843065261841\n",
      "epoch 46, loss 0.9094020128250122\n",
      "epoch 47, loss 0.9067478179931641\n",
      "epoch 48, loss 0.908287763595581\n",
      "epoch 49, loss 0.9077790379524231\n",
      "epoch 50, loss 0.9082610607147217\n",
      "epoch 51, loss 0.9071104526519775\n",
      "epoch 52, loss 0.9083361625671387\n",
      "epoch 53, loss 0.9067921042442322\n",
      "epoch 54, loss 0.9120165109634399\n",
      "epoch 55, loss 0.9051622152328491\n",
      "epoch 56, loss 0.9048905372619629\n",
      "epoch 57, loss 0.90599125623703\n",
      "epoch 58, loss 0.9078471660614014\n",
      "epoch 59, loss 0.9032120704650879\n",
      "epoch 60, loss 0.9071909189224243\n",
      "epoch 61, loss 0.9027559161186218\n",
      "epoch 62, loss 0.9089395403862\n",
      "epoch 63, loss 0.9104122519493103\n",
      "epoch 64, loss 0.9080004692077637\n",
      "epoch 65, loss 0.9027755856513977\n",
      "epoch 66, loss 0.9045460820198059\n",
      "epoch 67, loss 0.908484697341919\n",
      "epoch 68, loss 0.9084868431091309\n",
      "epoch 69, loss 0.9069885611534119\n",
      "epoch 70, loss 0.9061222076416016\n",
      "epoch 71, loss 0.9053081274032593\n",
      "epoch 72, loss 0.9055503606796265\n",
      "epoch 73, loss 0.904117226600647\n",
      "epoch 74, loss 0.9070017337799072\n",
      "epoch 75, loss 0.9071188569068909\n",
      "epoch 76, loss 0.9065375328063965\n",
      "epoch 77, loss 0.9083633422851562\n",
      "epoch 78, loss 0.9067948460578918\n",
      "epoch 79, loss 0.9061987996101379\n",
      "epoch 80, loss 0.9041638374328613\n",
      "epoch 81, loss 0.9036338925361633\n",
      "epoch 82, loss 0.9036827683448792\n",
      "epoch 83, loss 0.9037046432495117\n",
      "epoch 84, loss 0.9082093834877014\n",
      "epoch 85, loss 0.9064908027648926\n",
      "epoch 86, loss 0.9052538871765137\n",
      "epoch 87, loss 0.9073243141174316\n",
      "epoch 88, loss 0.905315101146698\n",
      "epoch 89, loss 0.9068364500999451\n",
      "epoch 90, loss 0.9092732071876526\n",
      "epoch 91, loss 0.9047542214393616\n",
      "epoch 92, loss 0.9046836495399475\n",
      "epoch 93, loss 0.908112108707428\n",
      "epoch 94, loss 0.9046832919120789\n",
      "epoch 95, loss 0.9043378829956055\n",
      "epoch 96, loss 0.9022500514984131\n",
      "epoch 97, loss 0.9054169654846191\n",
      "epoch 98, loss 0.9040105938911438\n",
      "epoch 99, loss 0.9058024287223816\n",
      "2019-03-31 01:29:04.350785, fold=0, rep=1, eta=0d 0h 30m 23s \n",
      "{'fold': 0, 'repeat': 1, 'n': 5875, 'd': 18, 'mse': 1.1143299341201782, 'train_time': 455.9198642729316, 'trained_epochs': 100, 'prior_train_nmll': 0.9065032005310059, 'train_nll': 3261.4892578125, 'test_nll': 1918.875, 'train_mse': 0.2935431897640228, 'state_dict_file': 'model_state_dict_7897793979244569063.pkl'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1.2367311716079712\n",
      "epoch 1, loss 1.2217899560928345\n",
      "epoch 2, loss 1.2175263166427612\n",
      "epoch 3, loss 1.2108724117279053\n",
      "epoch 4, loss 1.2071526050567627\n",
      "epoch 5, loss 1.1994558572769165\n",
      "epoch 6, loss 1.1996501684188843\n",
      "epoch 7, loss 1.1997908353805542\n",
      "epoch 8, loss 1.192511796951294\n",
      "epoch 9, loss 1.1906832456588745\n",
      "epoch 10, loss 1.1854914426803589\n",
      "epoch 11, loss 1.184314250946045\n",
      "epoch 12, loss 1.1831672191619873\n",
      "epoch 13, loss 1.1821914911270142\n",
      "epoch 14, loss 1.172856330871582\n",
      "epoch 15, loss 1.1752678155899048\n",
      "epoch 16, loss 1.171358346939087\n",
      "epoch 17, loss 1.1724212169647217\n",
      "epoch 18, loss 1.1713050603866577\n",
      "epoch 19, loss 1.1682931184768677\n",
      "epoch 20, loss 1.1664392948150635\n",
      "epoch 21, loss 1.1666531562805176\n",
      "epoch 22, loss 1.163072943687439\n",
      "epoch 23, loss 1.1628286838531494\n",
      "epoch 24, loss 1.1627064943313599\n",
      "epoch 25, loss 1.1657519340515137\n",
      "epoch 26, loss 1.1637556552886963\n",
      "epoch 27, loss 1.1623551845550537\n",
      "epoch 28, loss 1.1589337587356567\n",
      "epoch 29, loss 1.159360408782959\n",
      "epoch 30, loss 1.1588120460510254\n",
      "epoch 31, loss 1.1570225954055786\n",
      "epoch 32, loss 1.1572246551513672\n",
      "epoch 33, loss 1.1590700149536133\n",
      "epoch 34, loss 1.1538920402526855\n",
      "epoch 35, loss 1.1547260284423828\n",
      "epoch 36, loss 1.1549310684204102\n",
      "epoch 37, loss 1.1550137996673584\n",
      "epoch 38, loss 1.1540404558181763\n",
      "epoch 39, loss 1.1501290798187256\n",
      "epoch 40, loss 1.1542671918869019\n",
      "epoch 41, loss 1.1529765129089355\n",
      "epoch 42, loss 1.1523951292037964\n",
      "epoch 43, loss 1.1534143686294556\n",
      "epoch 44, loss 1.1496697664260864\n",
      "epoch 45, loss 1.1506967544555664\n",
      "epoch 46, loss 1.1491533517837524\n",
      "epoch 47, loss 1.149191975593567\n",
      "epoch 48, loss 1.1501283645629883\n",
      "epoch 49, loss 1.1495823860168457\n",
      "epoch 50, loss 1.1483105421066284\n",
      "epoch 51, loss 1.1486603021621704\n",
      "epoch 52, loss 1.1488481760025024\n",
      "epoch 53, loss 1.1455001831054688\n",
      "epoch 54, loss 1.1478333473205566\n",
      "epoch 55, loss 1.1448606252670288\n",
      "epoch 56, loss 1.1497722864151\n",
      "epoch 57, loss 1.1452964544296265\n",
      "epoch 58, loss 1.1490381956100464\n",
      "epoch 59, loss 1.1465215682983398\n",
      "epoch 60, loss 1.1438214778900146\n",
      "epoch 61, loss 1.144481897354126\n",
      "epoch 62, loss 1.1435641050338745\n",
      "epoch 63, loss 1.1460086107254028\n",
      "epoch 64, loss 1.148335337638855\n",
      "epoch 65, loss 1.1437554359436035\n",
      "epoch 66, loss 1.1450281143188477\n",
      "epoch 67, loss 1.1436195373535156\n",
      "epoch 68, loss 1.1438941955566406\n",
      "epoch 69, loss 1.1453207731246948\n",
      "epoch 70, loss 1.1433262825012207\n",
      "epoch 71, loss 1.1439871788024902\n",
      "epoch 72, loss 1.1465377807617188\n",
      "epoch 73, loss 1.144788146018982\n",
      "epoch 74, loss 1.1418983936309814\n",
      "epoch 75, loss 1.1428709030151367\n",
      "epoch 76, loss 1.1429078578948975\n",
      "epoch 77, loss 1.1416369676589966\n",
      "epoch 78, loss 1.1441878080368042\n",
      "epoch 79, loss 1.1422725915908813\n",
      "epoch 80, loss 1.1441864967346191\n",
      "epoch 81, loss 1.1436651945114136\n",
      "epoch 82, loss 1.142240285873413\n",
      "epoch 83, loss 1.141194462776184\n",
      "epoch 84, loss 1.143606185913086\n",
      "epoch 85, loss 1.1432372331619263\n",
      "epoch 86, loss 1.142101526260376\n",
      "epoch 87, loss 1.1422086954116821\n",
      "epoch 88, loss 1.1390035152435303\n",
      "epoch 89, loss 1.1396340131759644\n",
      "epoch 90, loss 1.1411372423171997\n",
      "epoch 91, loss 1.141232967376709\n",
      "epoch 92, loss 1.1396055221557617\n",
      "epoch 93, loss 1.1397444009780884\n",
      "epoch 94, loss 1.1404844522476196\n",
      "epoch 95, loss 1.1437337398529053\n",
      "epoch 96, loss 1.1421637535095215\n",
      "epoch 97, loss 1.1412590742111206\n",
      "epoch 98, loss 1.1419721841812134\n",
      "epoch 99, loss 1.143087387084961\n",
      "2019-03-31 01:36:36.491194, fold=1, rep=0, eta=0d 0h 22m 43s \n",
      "{'fold': 1, 'repeat': 0, 'n': 5875, 'd': 18, 'mse': 2.655792236328125, 'train_time': 450.8351298470516, 'trained_epochs': 100, 'prior_train_nmll': 1.1429154872894287, 'train_nll': 4213.02294921875, 'test_nll': 3557.33203125, 'train_mse': 0.4804924726486206, 'state_dict_file': 'model_state_dict_-5805794230116865467.pkl'}\n",
      "epoch 0, loss 1.202331304550171\n",
      "epoch 1, loss 1.189578890800476\n",
      "epoch 2, loss 1.1771490573883057\n",
      "epoch 3, loss 1.1660590171813965\n",
      "epoch 4, loss 1.1589444875717163\n",
      "epoch 5, loss 1.1512271165847778\n",
      "epoch 6, loss 1.1488823890686035\n",
      "epoch 7, loss 1.144424319267273\n",
      "epoch 8, loss 1.141048550605774\n",
      "epoch 9, loss 1.1470402479171753\n",
      "epoch 10, loss 1.1417795419692993\n",
      "epoch 11, loss 1.1399621963500977\n",
      "epoch 12, loss 1.136411428451538\n",
      "epoch 13, loss 1.1291075944900513\n",
      "epoch 14, loss 1.1263720989227295\n",
      "epoch 15, loss 1.1281195878982544\n",
      "epoch 16, loss 1.1251276731491089\n",
      "epoch 17, loss 1.1242790222167969\n",
      "epoch 18, loss 1.123887300491333\n",
      "epoch 19, loss 1.1223936080932617\n",
      "epoch 20, loss 1.119471788406372\n",
      "epoch 21, loss 1.1192673444747925\n",
      "epoch 22, loss 1.118526816368103\n",
      "epoch 23, loss 1.1208055019378662\n",
      "epoch 24, loss 1.115505337715149\n",
      "epoch 25, loss 1.1166640520095825\n",
      "epoch 26, loss 1.111785650253296\n",
      "epoch 27, loss 1.112606167793274\n",
      "epoch 28, loss 1.112748384475708\n",
      "epoch 29, loss 1.108973503112793\n",
      "epoch 30, loss 1.1114683151245117\n",
      "epoch 31, loss 1.111358642578125\n",
      "epoch 32, loss 1.1106163263320923\n",
      "epoch 33, loss 1.1093281507492065\n",
      "epoch 34, loss 1.1120685338974\n",
      "epoch 35, loss 1.1074734926223755\n",
      "epoch 36, loss 1.1083687543869019\n",
      "epoch 37, loss 1.1105709075927734\n",
      "epoch 38, loss 1.1075764894485474\n",
      "epoch 39, loss 1.1098785400390625\n",
      "epoch 40, loss 1.1089388132095337\n",
      "epoch 41, loss 1.1063909530639648\n",
      "epoch 42, loss 1.1068917512893677\n",
      "epoch 43, loss 1.1080548763275146\n",
      "epoch 44, loss 1.1067575216293335\n",
      "epoch 45, loss 1.1054569482803345\n",
      "epoch 46, loss 1.1046150922775269\n",
      "epoch 47, loss 1.1056171655654907\n",
      "epoch 48, loss 1.1043771505355835\n",
      "epoch 49, loss 1.1047799587249756\n",
      "epoch 50, loss 1.1034098863601685\n",
      "epoch 51, loss 1.1026077270507812\n",
      "epoch 52, loss 1.1027270555496216\n",
      "epoch 53, loss 1.1048063039779663\n",
      "epoch 54, loss 1.1004436016082764\n",
      "epoch 55, loss 1.1013263463974\n",
      "epoch 56, loss 1.100420594215393\n",
      "epoch 57, loss 1.1014443635940552\n",
      "epoch 58, loss 1.101685643196106\n",
      "epoch 59, loss 1.101804256439209\n",
      "epoch 60, loss 1.1024152040481567\n",
      "epoch 61, loss 1.1017683744430542\n",
      "epoch 62, loss 1.1045713424682617\n",
      "epoch 63, loss 1.1020700931549072\n",
      "epoch 64, loss 1.1031315326690674\n",
      "epoch 65, loss 1.0997501611709595\n",
      "epoch 66, loss 1.1000574827194214\n",
      "epoch 67, loss 1.0986354351043701\n",
      "epoch 68, loss 1.0991312265396118\n",
      "epoch 69, loss 1.1019487380981445\n",
      "epoch 70, loss 1.0999996662139893\n",
      "epoch 71, loss 1.1020821332931519\n",
      "epoch 72, loss 1.0971673727035522\n",
      "epoch 73, loss 1.1019068956375122\n",
      "epoch 74, loss 1.099365234375\n",
      "epoch 75, loss 1.0990995168685913\n",
      "epoch 76, loss 1.0993268489837646\n",
      "epoch 77, loss 1.1004283428192139\n",
      "epoch 78, loss 1.0965354442596436\n",
      "epoch 79, loss 1.1009753942489624\n",
      "epoch 80, loss 1.1019282341003418\n",
      "epoch 81, loss 1.0983102321624756\n",
      "epoch 82, loss 1.0995166301727295\n",
      "epoch 83, loss 1.098448634147644\n",
      "epoch 84, loss 1.10049307346344\n",
      "epoch 85, loss 1.1008038520812988\n",
      "epoch 86, loss 1.096757411956787\n",
      "epoch 87, loss 1.0970507860183716\n",
      "epoch 88, loss 1.1009693145751953\n",
      "epoch 89, loss 1.0980666875839233\n",
      "epoch 90, loss 1.10035240650177\n",
      "epoch 91, loss 1.0994279384613037\n",
      "epoch 92, loss 1.1019929647445679\n",
      "epoch 93, loss 1.095705270767212\n",
      "epoch 94, loss 1.098778247833252\n",
      "epoch 95, loss 1.1009896993637085\n",
      "epoch 96, loss 1.0971851348876953\n",
      "epoch 97, loss 1.1011536121368408\n",
      "epoch 98, loss 1.0981910228729248\n",
      "epoch 99, loss 1.0980254411697388\n",
      "2019-03-31 01:44:24.738216, fold=1, rep=1, eta=0d 0h 15m 15s \n",
      "{'fold': 1, 'repeat': 1, 'n': 5875, 'd': 18, 'mse': 2.4299566745758057, 'train_time': 468.24659274704754, 'trained_epochs': 100, 'prior_train_nmll': 1.0990303754806519, 'train_nll': 4037.28662109375, 'test_nll': 3515.2275390625, 'train_mse': 0.43853384256362915, 'state_dict_file': 'model_state_dict_-8079270238822093527.pkl'}\n",
      "epoch 0, loss 1.1902741193771362\n",
      "epoch 1, loss 1.1785681247711182\n",
      "epoch 2, loss 1.1659252643585205\n",
      "epoch 3, loss 1.1543654203414917\n",
      "epoch 4, loss 1.1494990587234497\n",
      "epoch 5, loss 1.1475948095321655\n",
      "epoch 6, loss 1.1469192504882812\n",
      "epoch 7, loss 1.1418176889419556\n",
      "epoch 8, loss 1.1398062705993652\n",
      "epoch 9, loss 1.1356630325317383\n",
      "epoch 10, loss 1.136264443397522\n",
      "epoch 11, loss 1.1294456720352173\n",
      "epoch 12, loss 1.1258788108825684\n",
      "epoch 13, loss 1.125151515007019\n",
      "epoch 14, loss 1.1232383251190186\n",
      "epoch 15, loss 1.1208040714263916\n",
      "epoch 16, loss 1.1212342977523804\n",
      "epoch 17, loss 1.1187721490859985\n",
      "epoch 18, loss 1.1181156635284424\n",
      "epoch 19, loss 1.1163896322250366\n",
      "epoch 20, loss 1.1133358478546143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, loss 1.1098803281784058\n",
      "epoch 22, loss 1.1135120391845703\n",
      "epoch 23, loss 1.1132211685180664\n",
      "epoch 24, loss 1.1113260984420776\n",
      "epoch 25, loss 1.1076340675354004\n",
      "epoch 26, loss 1.108557105064392\n",
      "epoch 27, loss 1.10544753074646\n",
      "epoch 28, loss 1.1057469844818115\n",
      "epoch 29, loss 1.1055601835250854\n",
      "epoch 30, loss 1.10491144657135\n",
      "epoch 31, loss 1.1058783531188965\n",
      "epoch 32, loss 1.106256365776062\n",
      "epoch 33, loss 1.1017961502075195\n",
      "epoch 34, loss 1.106795310974121\n",
      "epoch 35, loss 1.1047511100769043\n",
      "epoch 36, loss 1.1028201580047607\n",
      "epoch 37, loss 1.1002851724624634\n",
      "epoch 38, loss 1.1014132499694824\n",
      "epoch 39, loss 1.102147102355957\n",
      "epoch 40, loss 1.1015377044677734\n",
      "epoch 41, loss 1.0998120307922363\n",
      "epoch 42, loss 1.100467562675476\n",
      "epoch 43, loss 1.0981006622314453\n",
      "epoch 44, loss 1.0975637435913086\n",
      "epoch 45, loss 1.0982521772384644\n",
      "epoch 46, loss 1.095771312713623\n",
      "epoch 47, loss 1.0967042446136475\n",
      "epoch 48, loss 1.0970313549041748\n",
      "epoch 49, loss 1.0989550352096558\n",
      "epoch 50, loss 1.0957835912704468\n",
      "epoch 51, loss 1.0980714559555054\n",
      "epoch 52, loss 1.0960872173309326\n",
      "epoch 53, loss 1.096862554550171\n",
      "epoch 54, loss 1.0972362756729126\n",
      "epoch 55, loss 1.0982048511505127\n",
      "epoch 56, loss 1.095382809638977\n",
      "epoch 57, loss 1.0948961973190308\n",
      "epoch 58, loss 1.0964711904525757\n",
      "epoch 59, loss 1.0945239067077637\n",
      "epoch 60, loss 1.094699501991272\n",
      "epoch 61, loss 1.0972192287445068\n",
      "epoch 62, loss 1.0955828428268433\n",
      "epoch 63, loss 1.0948121547698975\n",
      "epoch 64, loss 1.0918097496032715\n",
      "epoch 65, loss 1.0964380502700806\n",
      "epoch 66, loss 1.0957812070846558\n",
      "epoch 67, loss 1.0972684621810913\n",
      "epoch 68, loss 1.093103051185608\n",
      "epoch 69, loss 1.0935909748077393\n",
      "epoch 70, loss 1.0944135189056396\n",
      "epoch 71, loss 1.09274160861969\n",
      "epoch 72, loss 1.0944634675979614\n",
      "epoch 73, loss 1.0920456647872925\n",
      "epoch 74, loss 1.0944914817810059\n",
      "epoch 75, loss 1.0926977396011353\n",
      "epoch 76, loss 1.0962965488433838\n",
      "epoch 77, loss 1.0933778285980225\n",
      "epoch 78, loss 1.0918316841125488\n",
      "epoch 79, loss 1.0919965505599976\n",
      "epoch 80, loss 1.09335458278656\n",
      "epoch 81, loss 1.0902972221374512\n",
      "epoch 82, loss 1.0922116041183472\n",
      "epoch 83, loss 1.0904935598373413\n",
      "epoch 84, loss 1.0933685302734375\n",
      "epoch 85, loss 1.0917326211929321\n",
      "epoch 86, loss 1.0916862487792969\n",
      "epoch 87, loss 1.0926517248153687\n",
      "epoch 88, loss 1.0905417203903198\n",
      "epoch 89, loss 1.0917205810546875\n",
      "epoch 90, loss 1.09176504611969\n",
      "epoch 91, loss 1.0920209884643555\n",
      "epoch 92, loss 1.090420126914978\n",
      "epoch 93, loss 1.0932263135910034\n",
      "epoch 94, loss 1.0908715724945068\n",
      "epoch 95, loss 1.0904734134674072\n",
      "epoch 96, loss 1.0910067558288574\n",
      "epoch 97, loss 1.0903841257095337\n",
      "epoch 98, loss 1.0922931432724\n",
      "epoch 99, loss 1.0939615964889526\n",
      "2019-03-31 01:51:43.444919, fold=2, rep=0, eta=0d 0h 7m 34s \n",
      "{'fold': 2, 'repeat': 0, 'n': 5875, 'd': 18, 'mse': 2.569410562515259, 'train_time': 437.3882728777826, 'trained_epochs': 100, 'prior_train_nmll': 1.0912744998931885, 'train_nll': 4073.75830078125, 'test_nll': 3349.693115234375, 'train_mse': 0.45341774821281433, 'state_dict_file': 'model_state_dict_7240171016298957146.pkl'}\n",
      "epoch 0, loss 1.1949670314788818\n",
      "epoch 1, loss 1.1810410022735596\n",
      "epoch 2, loss 1.1743829250335693\n",
      "epoch 3, loss 1.158651351928711\n",
      "epoch 4, loss 1.155224084854126\n",
      "epoch 5, loss 1.1490790843963623\n",
      "epoch 6, loss 1.1479846239089966\n",
      "epoch 7, loss 1.1413147449493408\n",
      "epoch 8, loss 1.1440634727478027\n",
      "epoch 9, loss 1.1411187648773193\n",
      "epoch 10, loss 1.138504147529602\n",
      "epoch 11, loss 1.1416252851486206\n",
      "epoch 12, loss 1.1372488737106323\n",
      "epoch 13, loss 1.1305077075958252\n",
      "epoch 14, loss 1.1283355951309204\n",
      "epoch 15, loss 1.125694990158081\n",
      "epoch 16, loss 1.126331090927124\n",
      "epoch 17, loss 1.1250561475753784\n",
      "epoch 18, loss 1.122456669807434\n",
      "epoch 19, loss 1.1217283010482788\n",
      "epoch 20, loss 1.117051601409912\n",
      "epoch 21, loss 1.118274211883545\n",
      "epoch 22, loss 1.1178839206695557\n",
      "epoch 23, loss 1.115269422531128\n",
      "epoch 24, loss 1.1143256425857544\n",
      "epoch 25, loss 1.1151018142700195\n",
      "epoch 26, loss 1.1141241788864136\n",
      "epoch 27, loss 1.1110514402389526\n",
      "epoch 28, loss 1.1118502616882324\n",
      "epoch 29, loss 1.1109462976455688\n",
      "epoch 30, loss 1.1113343238830566\n",
      "epoch 31, loss 1.1103079319000244\n",
      "epoch 32, loss 1.109227180480957\n",
      "epoch 33, loss 1.1074408292770386\n",
      "epoch 34, loss 1.1064924001693726\n",
      "epoch 35, loss 1.1100090742111206\n",
      "epoch 36, loss 1.105833888053894\n",
      "epoch 37, loss 1.106082558631897\n",
      "epoch 38, loss 1.1035795211791992\n",
      "epoch 39, loss 1.1046831607818604\n",
      "epoch 40, loss 1.1061171293258667\n",
      "epoch 41, loss 1.1022902727127075\n",
      "epoch 42, loss 1.1029471158981323\n",
      "epoch 43, loss 1.103842854499817\n",
      "epoch 44, loss 1.1076654195785522\n",
      "epoch 45, loss 1.1027476787567139\n",
      "epoch 46, loss 1.1034274101257324\n",
      "epoch 47, loss 1.1070647239685059\n",
      "epoch 48, loss 1.0995893478393555\n",
      "epoch 49, loss 1.102950096130371\n",
      "epoch 50, loss 1.0996688604354858\n",
      "epoch 51, loss 1.1031591892242432\n",
      "epoch 52, loss 1.1020475625991821\n",
      "epoch 53, loss 1.1020052433013916\n",
      "epoch 54, loss 1.1008681058883667\n",
      "epoch 55, loss 1.1018035411834717\n",
      "epoch 56, loss 1.100948452949524\n",
      "epoch 57, loss 1.1018798351287842\n",
      "epoch 58, loss 1.1020952463150024\n",
      "epoch 59, loss 1.1009677648544312\n",
      "epoch 60, loss 1.1003327369689941\n",
      "epoch 61, loss 1.1004877090454102\n",
      "epoch 62, loss 1.0997720956802368\n",
      "epoch 63, loss 1.1023268699645996\n",
      "epoch 64, loss 1.1011266708374023\n",
      "epoch 65, loss 1.0995837450027466\n",
      "epoch 66, loss 1.0981264114379883\n",
      "epoch 67, loss 1.0984729528427124\n",
      "epoch 68, loss 1.0973089933395386\n",
      "epoch 69, loss 1.0992302894592285\n",
      "epoch 70, loss 1.0998138189315796\n",
      "epoch 71, loss 1.0951472520828247\n",
      "epoch 72, loss 1.1005834341049194\n",
      "epoch 73, loss 1.0983836650848389\n",
      "epoch 74, loss 1.0947918891906738\n",
      "epoch 75, loss 1.098886489868164\n",
      "epoch 76, loss 1.0969759225845337\n",
      "epoch 77, loss 1.0965086221694946\n",
      "epoch 78, loss 1.096497654914856\n",
      "epoch 79, loss 1.0968247652053833\n",
      "epoch 80, loss 1.0957205295562744\n",
      "epoch 81, loss 1.1002328395843506\n",
      "epoch 82, loss 1.0974818468093872\n",
      "epoch 83, loss 1.0964716672897339\n",
      "epoch 84, loss 1.094893217086792\n",
      "epoch 85, loss 1.097375512123108\n",
      "epoch 86, loss 1.095785140991211\n",
      "epoch 87, loss 1.0947990417480469\n",
      "epoch 88, loss 1.0974092483520508\n",
      "epoch 89, loss 1.0924965143203735\n",
      "epoch 90, loss 1.0942325592041016\n",
      "epoch 91, loss 1.096992015838623\n",
      "epoch 92, loss 1.0940155982971191\n",
      "epoch 93, loss 1.092765212059021\n",
      "epoch 94, loss 1.092769742012024\n",
      "epoch 95, loss 1.0937438011169434\n",
      "epoch 96, loss 1.092047095298767\n",
      "epoch 97, loss 1.0963749885559082\n",
      "epoch 98, loss 1.0958963632583618\n",
      "epoch 99, loss 1.0944815874099731\n",
      "2019-03-31 01:59:03.889716, fold=2, rep=1, eta=0d 0h 0m 0s \n",
      "{'fold': 2, 'repeat': 1, 'n': 5875, 'd': 18, 'mse': 2.287264108657837, 'train_time': 440.44375113910064, 'trained_epochs': 100, 'prior_train_nmll': 1.0959151983261108, 'train_nll': 4057.79833984375, 'test_nll': 3214.161865234375, 'train_mse': 0.4486323595046997, 'state_dict_file': 'model_state_dict_8521842607609087868.pkl'}\n",
      "Begin with SVI\n",
      "epoch 0, iter 0, loss 1.9310814142227173\n",
      "epoch 0, iter 1, loss 2.2110438346862793\n",
      "epoch 0, iter 2, loss 2.1074628829956055\n",
      "epoch 0, iter 3, loss 1.8818016052246094\n",
      "epoch 0, loss 8.131389737129211\n",
      "epoch 1, iter 0, loss 1.8895423412322998\n",
      "epoch 1, iter 1, loss 1.8522377014160156\n",
      "epoch 1, iter 2, loss 1.7110426425933838\n",
      "epoch 1, iter 3, loss 1.5774118900299072\n",
      "epoch 1, loss 7.0302345752716064\n",
      "epoch 2, iter 0, loss 1.549217939376831\n",
      "epoch 2, iter 1, loss 1.5519928932189941\n",
      "epoch 2, iter 2, loss 1.5129880905151367\n",
      "epoch 2, iter 3, loss 1.4669103622436523\n",
      "epoch 2, loss 6.081109285354614\n",
      "epoch 3, iter 0, loss 1.4380236864089966\n",
      "epoch 3, iter 1, loss 1.4244709014892578\n",
      "epoch 3, iter 2, loss 1.4003567695617676\n",
      "epoch 3, iter 3, loss 1.3759156465530396\n",
      "epoch 3, loss 5.6387670040130615\n",
      "epoch 4, iter 0, loss 1.3490519523620605\n",
      "epoch 4, iter 1, loss 1.3376314640045166\n",
      "epoch 4, iter 2, loss 1.3195587396621704\n",
      "epoch 4, iter 3, loss 1.2974674701690674\n",
      "epoch 4, loss 5.303709626197815\n",
      "epoch 5, iter 0, loss 1.26944899559021\n",
      "epoch 5, iter 1, loss 1.2569243907928467\n",
      "epoch 5, iter 2, loss 1.2420696020126343\n",
      "epoch 5, iter 3, loss 1.2233024835586548\n",
      "epoch 5, loss 4.991745471954346\n",
      "epoch 6, iter 0, loss 1.1864997148513794\n",
      "epoch 6, iter 1, loss 1.1753482818603516\n",
      "epoch 6, iter 2, loss 1.1589828729629517\n",
      "epoch 6, iter 3, loss 1.1421692371368408\n",
      "epoch 6, loss 4.663000106811523\n",
      "epoch 7, iter 0, loss 1.1116424798965454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, iter 1, loss 1.0945875644683838\n",
      "epoch 7, iter 2, loss 1.0685728788375854\n",
      "epoch 7, iter 3, loss 1.0467969179153442\n",
      "epoch 7, loss 4.321599841117859\n",
      "epoch 8, iter 0, loss 1.0258758068084717\n",
      "epoch 8, iter 1, loss 0.9884729385375977\n",
      "epoch 8, iter 2, loss 0.9867119193077087\n",
      "epoch 8, iter 3, loss 0.9614036083221436\n",
      "epoch 8, loss 3.9624642729759216\n",
      "epoch 9, iter 0, loss 0.9250364303588867\n",
      "epoch 9, iter 1, loss 0.9041310548782349\n",
      "epoch 9, iter 2, loss 0.9128954410552979\n",
      "epoch 9, iter 3, loss 0.896567165851593\n",
      "epoch 9, loss 3.6386300921440125\n",
      "epoch 10, iter 0, loss 0.8466252684593201\n",
      "epoch 10, iter 1, loss 0.846791684627533\n",
      "epoch 10, iter 2, loss 0.8310194611549377\n",
      "epoch 10, iter 3, loss 0.840671181678772\n",
      "epoch 10, loss 3.3651075959205627\n",
      "epoch 11, iter 0, loss 0.7936264872550964\n",
      "epoch 11, iter 1, loss 0.7899634838104248\n",
      "epoch 11, iter 2, loss 0.8324898481369019\n",
      "epoch 11, iter 3, loss 0.8236232399940491\n",
      "epoch 11, loss 3.239703059196472\n",
      "epoch 12, iter 0, loss 0.8084617257118225\n",
      "epoch 12, iter 1, loss 0.7831733822822571\n",
      "epoch 12, iter 2, loss 0.8464862108230591\n",
      "epoch 12, iter 3, loss 0.8911266326904297\n",
      "epoch 12, loss 3.3292479515075684\n",
      "epoch 13, iter 0, loss 0.8370037078857422\n",
      "epoch 13, iter 1, loss 0.7963070869445801\n",
      "epoch 13, iter 2, loss 0.891600489616394\n",
      "epoch 13, iter 3, loss 0.9485608339309692\n",
      "epoch 13, loss 3.4734721183776855\n",
      "epoch 14, iter 0, loss 0.8589769601821899\n",
      "epoch 14, iter 1, loss 0.9024127721786499\n",
      "epoch 14, iter 2, loss 0.9091322422027588\n",
      "epoch 14, iter 3, loss 0.8746131658554077\n",
      "epoch 14, loss 3.5451351404190063\n",
      "epoch 15, iter 0, loss 0.8494730591773987\n",
      "epoch 15, iter 1, loss 0.885969877243042\n",
      "epoch 15, iter 2, loss 0.9184399247169495\n",
      "epoch 15, iter 3, loss 0.8637732267379761\n",
      "epoch 15, loss 3.517656087875366\n",
      "epoch 16, iter 0, loss 0.8189499378204346\n",
      "epoch 16, iter 1, loss 0.8541488647460938\n",
      "epoch 16, iter 2, loss 0.8405938148498535\n",
      "epoch 16, iter 3, loss 0.8653583526611328\n",
      "epoch 16, loss 3.3790509700775146\n",
      "epoch 17, iter 0, loss 0.8134853839874268\n",
      "epoch 17, iter 1, loss 0.802670955657959\n",
      "epoch 17, iter 2, loss 0.8558542132377625\n",
      "epoch 17, iter 3, loss 0.8280910849571228\n",
      "epoch 17, loss 3.300101637840271\n",
      "epoch 18, iter 0, loss 0.7965226769447327\n",
      "epoch 18, iter 1, loss 0.8044723868370056\n",
      "epoch 18, iter 2, loss 0.7998343110084534\n",
      "epoch 18, iter 3, loss 0.8149840831756592\n",
      "epoch 18, loss 3.215813457965851\n",
      "epoch 19, iter 0, loss 0.7890706062316895\n",
      "epoch 19, iter 1, loss 0.7976628541946411\n",
      "epoch 19, iter 2, loss 0.7661172151565552\n",
      "epoch 19, iter 3, loss 0.7875379920005798\n",
      "epoch 19, loss 3.1403886675834656\n",
      "epoch 20, iter 0, loss 0.7335253953933716\n",
      "epoch 20, iter 1, loss 0.7781127095222473\n",
      "epoch 20, iter 2, loss 0.7604771256446838\n",
      "epoch 20, iter 3, loss 0.7799667716026306\n",
      "epoch 20, loss 3.0520820021629333\n",
      "epoch 21, iter 0, loss 0.7424589395523071\n",
      "epoch 21, iter 1, loss 0.7586747407913208\n",
      "epoch 21, iter 2, loss 0.7342296838760376\n",
      "epoch 21, iter 3, loss 0.7520186305046082\n",
      "epoch 21, loss 2.9873819947242737\n",
      "epoch 22, iter 0, loss 0.7070541977882385\n",
      "epoch 22, iter 1, loss 0.7305542230606079\n",
      "epoch 22, iter 2, loss 0.7381737232208252\n",
      "epoch 22, iter 3, loss 0.7817249298095703\n",
      "epoch 22, loss 2.957507073879242\n",
      "epoch 23, iter 0, loss 0.7349635362625122\n",
      "epoch 23, iter 1, loss 0.7111612558364868\n",
      "epoch 23, iter 2, loss 0.749941885471344\n",
      "epoch 23, iter 3, loss 0.7431732416152954\n",
      "epoch 23, loss 2.9392399191856384\n",
      "epoch 24, iter 0, loss 0.7436354756355286\n",
      "epoch 24, iter 1, loss 0.7097293138504028\n",
      "epoch 24, iter 2, loss 0.7407755851745605\n",
      "epoch 24, iter 3, loss 0.7748286724090576\n",
      "epoch 24, loss 2.9689690470695496\n",
      "epoch 25, iter 0, loss 0.7139849662780762\n",
      "epoch 25, iter 1, loss 0.7586613297462463\n",
      "epoch 25, iter 2, loss 0.762341320514679\n",
      "epoch 25, iter 3, loss 0.7447205185890198\n",
      "epoch 25, loss 2.9797081351280212\n",
      "epoch 26, iter 0, loss 0.7502554655075073\n",
      "epoch 26, iter 1, loss 0.7517848610877991\n",
      "epoch 26, iter 2, loss 0.746654748916626\n",
      "epoch 26, iter 3, loss 0.7795131206512451\n",
      "epoch 26, loss 3.0282081961631775\n",
      "epoch 27, iter 0, loss 0.7145256996154785\n",
      "epoch 27, iter 1, loss 0.7613945007324219\n",
      "epoch 27, iter 2, loss 0.7573085427284241\n",
      "epoch 27, iter 3, loss 0.8137709498405457\n",
      "epoch 27, loss 3.04699969291687\n",
      "epoch 28, iter 0, loss 0.7431600689888\n",
      "epoch 28, iter 1, loss 0.7623841166496277\n",
      "epoch 28, iter 2, loss 0.7381837964057922\n",
      "epoch 28, iter 3, loss 0.8069688677787781\n",
      "epoch 28, loss 3.050696849822998\n",
      "epoch 29, iter 0, loss 0.7283363342285156\n",
      "epoch 29, iter 1, loss 0.7729144096374512\n",
      "epoch 29, iter 2, loss 0.7528128027915955\n",
      "epoch 29, iter 3, loss 0.798432469367981\n",
      "epoch 29, loss 3.052496016025543\n",
      "epoch 30, iter 0, loss 0.7364575862884521\n",
      "epoch 30, iter 1, loss 0.7527034878730774\n",
      "epoch 30, iter 2, loss 0.7370048761367798\n",
      "epoch 30, iter 3, loss 0.7719950675964355\n",
      "epoch 30, loss 2.998161017894745\n",
      "epoch 31, iter 0, loss 0.7465660572052002\n",
      "epoch 31, iter 1, loss 0.7315687537193298\n",
      "epoch 31, iter 2, loss 0.7730992436408997\n",
      "epoch 31, iter 3, loss 0.7405907511711121\n",
      "epoch 31, loss 2.9918248057365417\n",
      "epoch 32, iter 0, loss 0.7329419851303101\n",
      "epoch 32, iter 1, loss 0.7319063544273376\n",
      "epoch 32, iter 2, loss 0.7454706430435181\n",
      "epoch 32, iter 3, loss 0.763828456401825\n",
      "epoch 32, loss 2.9741474390029907\n",
      "epoch 33, iter 0, loss 0.7166785001754761\n",
      "epoch 33, iter 1, loss 0.7360824346542358\n",
      "epoch 33, iter 2, loss 0.7599935531616211\n",
      "epoch 33, iter 3, loss 0.7706907391548157\n",
      "epoch 33, loss 2.9834452271461487\n",
      "epoch 34, iter 0, loss 0.729856550693512\n",
      "epoch 34, iter 1, loss 0.7283985018730164\n",
      "epoch 34, iter 2, loss 0.7690170407295227\n",
      "epoch 34, iter 3, loss 0.7607082724571228\n",
      "epoch 34, loss 2.987980365753174\n",
      "epoch 35, iter 0, loss 0.7029764652252197\n",
      "epoch 35, iter 1, loss 0.7905519604682922\n",
      "epoch 35, iter 2, loss 0.7547233700752258\n",
      "epoch 35, iter 3, loss 0.7680360078811646\n",
      "epoch 35, loss 3.0162878036499023\n",
      "epoch 36, iter 0, loss 0.71937096118927\n",
      "epoch 36, iter 1, loss 0.7327134609222412\n",
      "epoch 36, iter 2, loss 0.7915304899215698\n",
      "epoch 36, iter 3, loss 0.7440580129623413\n",
      "epoch 36, loss 2.9876729249954224\n",
      "epoch 37, iter 0, loss 0.75167316198349\n",
      "epoch 37, iter 1, loss 0.7583447694778442\n",
      "epoch 37, iter 2, loss 0.7219733595848083\n",
      "epoch 37, iter 3, loss 0.7522984743118286\n",
      "epoch 37, loss 2.984289765357971\n",
      "epoch 38, iter 0, loss 0.7148757576942444\n",
      "epoch 38, iter 1, loss 0.7122967839241028\n",
      "epoch 38, iter 2, loss 0.7832539081573486\n",
      "epoch 38, iter 3, loss 0.7805755138397217\n",
      "epoch 38, loss 2.9910019636154175\n",
      "epoch 39, iter 0, loss 0.7225708961486816\n",
      "epoch 39, iter 1, loss 0.7364844679832458\n",
      "epoch 39, iter 2, loss 0.7517992258071899\n",
      "epoch 39, iter 3, loss 0.791337788105011\n",
      "epoch 39, loss 3.0021923780441284\n",
      "epoch 40, iter 0, loss 0.7466994524002075\n",
      "epoch 40, iter 1, loss 0.7551471590995789\n",
      "epoch 40, iter 2, loss 0.7289009094238281\n",
      "epoch 40, iter 3, loss 0.781362771987915\n",
      "epoch 40, loss 3.0121102929115295\n",
      "epoch 41, iter 0, loss 0.7238906025886536\n",
      "epoch 41, iter 1, loss 0.7507745027542114\n",
      "epoch 41, iter 2, loss 0.7352546453475952\n",
      "epoch 41, iter 3, loss 0.7870887517929077\n",
      "epoch 41, loss 2.997008502483368\n",
      "epoch 42, iter 0, loss 0.7255504131317139\n",
      "epoch 42, iter 1, loss 0.7478231191635132\n",
      "epoch 42, iter 2, loss 0.7628976106643677\n",
      "epoch 42, iter 3, loss 0.7599429488182068\n",
      "epoch 42, loss 2.9962140917778015\n",
      "epoch 43, iter 0, loss 0.7226154208183289\n",
      "epoch 43, iter 1, loss 0.7259798645973206\n",
      "epoch 43, iter 2, loss 0.7888140678405762\n",
      "epoch 43, iter 3, loss 0.7220988273620605\n",
      "epoch 43, loss 2.959508180618286\n",
      "epoch 44, iter 0, loss 0.7337442636489868\n",
      "epoch 44, iter 1, loss 0.746070921421051\n",
      "epoch 44, iter 2, loss 0.7299899458885193\n",
      "epoch 44, iter 3, loss 0.7476887702941895\n",
      "epoch 44, loss 2.9574939012527466\n",
      "epoch 45, iter 0, loss 0.7090703845024109\n",
      "epoch 45, iter 1, loss 0.7501564621925354\n",
      "epoch 45, iter 2, loss 0.7622063755989075\n",
      "epoch 45, iter 3, loss 0.7471148371696472\n",
      "epoch 45, loss 2.968548059463501\n",
      "epoch 46, iter 0, loss 0.7412921190261841\n",
      "epoch 46, iter 1, loss 0.7645992040634155\n",
      "epoch 46, iter 2, loss 0.7220617532730103\n",
      "epoch 46, iter 3, loss 0.7448977828025818\n",
      "epoch 46, loss 2.9728508591651917\n",
      "epoch 47, iter 0, loss 0.7017189264297485\n",
      "epoch 47, iter 1, loss 0.7371634244918823\n",
      "epoch 47, iter 2, loss 0.7677348852157593\n",
      "epoch 47, iter 3, loss 0.7940949201583862\n",
      "epoch 47, loss 3.0007121562957764\n",
      "epoch 48, iter 0, loss 0.7226043343544006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48, iter 1, loss 0.7407475113868713\n",
      "epoch 48, iter 2, loss 0.7480698227882385\n",
      "epoch 48, iter 3, loss 0.7638691663742065\n",
      "epoch 48, loss 2.975290834903717\n",
      "epoch 49, iter 0, loss 0.7427536249160767\n",
      "epoch 49, iter 1, loss 0.7760637402534485\n",
      "epoch 49, iter 2, loss 0.7362222075462341\n",
      "epoch 49, iter 3, loss 0.7474825382232666\n",
      "epoch 49, loss 3.002522110939026\n",
      "epoch 50, iter 0, loss 0.7378407120704651\n",
      "epoch 50, iter 1, loss 0.7477651834487915\n",
      "epoch 50, iter 2, loss 0.7606167793273926\n",
      "epoch 50, iter 3, loss 0.743740439414978\n",
      "epoch 50, loss 2.989963114261627\n",
      "epoch 51, iter 0, loss 0.7240806818008423\n",
      "epoch 51, iter 1, loss 0.7415705919265747\n",
      "epoch 51, iter 2, loss 0.7797731161117554\n",
      "epoch 51, iter 3, loss 0.7397047281265259\n",
      "epoch 51, loss 2.9851291179656982\n",
      "epoch 52, iter 0, loss 0.7542206645011902\n",
      "epoch 52, iter 1, loss 0.7212855815887451\n",
      "epoch 52, iter 2, loss 0.7362681031227112\n",
      "epoch 52, iter 3, loss 0.7518080472946167\n",
      "epoch 52, loss 2.963582396507263\n",
      "epoch 53, iter 0, loss 0.7035138607025146\n",
      "epoch 53, iter 1, loss 0.7622166872024536\n",
      "epoch 53, iter 2, loss 0.7626977562904358\n",
      "epoch 53, iter 3, loss 0.7697683572769165\n",
      "epoch 53, loss 2.9981966614723206\n",
      "epoch 54, iter 0, loss 0.70841383934021\n",
      "epoch 54, iter 1, loss 0.7203468084335327\n",
      "epoch 54, iter 2, loss 0.7741208076477051\n",
      "epoch 54, iter 3, loss 0.7658299207687378\n",
      "epoch 54, loss 2.9687113761901855\n",
      "epoch 55, iter 0, loss 0.7011297345161438\n",
      "epoch 55, iter 1, loss 0.726219654083252\n",
      "epoch 55, iter 2, loss 0.7783664464950562\n",
      "epoch 55, iter 3, loss 0.7662163376808167\n",
      "epoch 55, loss 2.9719321727752686\n",
      "epoch 56, iter 0, loss 0.7482379078865051\n",
      "epoch 56, iter 1, loss 0.7363390922546387\n",
      "epoch 56, iter 2, loss 0.7582757472991943\n",
      "epoch 56, iter 3, loss 0.7471336722373962\n",
      "epoch 56, loss 2.9899864196777344\n",
      "epoch 57, iter 0, loss 0.7325972318649292\n",
      "epoch 57, iter 1, loss 0.750594437122345\n",
      "epoch 57, iter 2, loss 0.7283268570899963\n",
      "epoch 57, iter 3, loss 0.7749675512313843\n",
      "epoch 57, loss 2.986486077308655\n",
      "epoch 58, iter 0, loss 0.7181777954101562\n",
      "epoch 58, iter 1, loss 0.7588419914245605\n",
      "epoch 58, iter 2, loss 0.7486099004745483\n",
      "epoch 58, iter 3, loss 0.7495453953742981\n",
      "epoch 58, loss 2.9751750826835632\n",
      "epoch 59, iter 0, loss 0.729406476020813\n",
      "epoch 59, iter 1, loss 0.747067928314209\n",
      "epoch 59, iter 2, loss 0.7560922503471375\n",
      "epoch 59, iter 3, loss 0.741492509841919\n",
      "epoch 59, loss 2.9740591645240784\n",
      "epoch 60, iter 0, loss 0.7370944023132324\n",
      "epoch 60, iter 1, loss 0.7526410222053528\n",
      "epoch 60, iter 2, loss 0.7734604477882385\n",
      "epoch 60, iter 3, loss 0.7316535115242004\n",
      "epoch 60, loss 2.994849383831024\n",
      "epoch 61, iter 0, loss 0.732351541519165\n",
      "epoch 61, iter 1, loss 0.7611097097396851\n",
      "epoch 61, iter 2, loss 0.7258589267730713\n",
      "epoch 61, iter 3, loss 0.7475033402442932\n",
      "epoch 61, loss 2.9668235182762146\n",
      "epoch 62, iter 0, loss 0.7604718208312988\n",
      "epoch 62, iter 1, loss 0.7366123795509338\n",
      "epoch 62, iter 2, loss 0.7349929809570312\n",
      "epoch 62, iter 3, loss 0.737043023109436\n",
      "epoch 62, loss 2.9691202044487\n",
      "epoch 63, iter 0, loss 0.7371648550033569\n",
      "epoch 63, iter 1, loss 0.7384950518608093\n",
      "epoch 63, iter 2, loss 0.7320964932441711\n",
      "epoch 63, iter 3, loss 0.7649382948875427\n",
      "epoch 63, loss 2.97269469499588\n",
      "epoch 64, iter 0, loss 0.7344414591789246\n",
      "epoch 64, iter 1, loss 0.7166810035705566\n",
      "epoch 64, iter 2, loss 0.7380471229553223\n",
      "epoch 64, iter 3, loss 0.7693639397621155\n",
      "epoch 64, loss 2.958533525466919\n",
      "epoch 65, iter 0, loss 0.714394748210907\n",
      "epoch 65, iter 1, loss 0.7494477033615112\n",
      "epoch 65, iter 2, loss 0.7555262446403503\n",
      "epoch 65, iter 3, loss 0.7279364466667175\n",
      "epoch 65, loss 2.947305142879486\n",
      "epoch 66, iter 0, loss 0.6928113102912903\n",
      "epoch 66, iter 1, loss 0.735160231590271\n",
      "epoch 66, iter 2, loss 0.7491430640220642\n",
      "epoch 66, iter 3, loss 0.754743754863739\n",
      "epoch 66, loss 2.9318583607673645\n",
      "epoch 67, iter 0, loss 0.714282214641571\n",
      "epoch 67, iter 1, loss 0.7445428371429443\n",
      "epoch 67, iter 2, loss 0.7738920450210571\n",
      "epoch 67, iter 3, loss 0.7221269011497498\n",
      "epoch 67, loss 2.9548439979553223\n",
      "epoch 68, iter 0, loss 0.6873822212219238\n",
      "epoch 68, iter 1, loss 0.7517690658569336\n",
      "epoch 68, iter 2, loss 0.7524077892303467\n",
      "epoch 68, iter 3, loss 0.7758558988571167\n",
      "epoch 68, loss 2.967414975166321\n",
      "epoch 69, iter 0, loss 0.7357306480407715\n",
      "epoch 69, iter 1, loss 0.7212698459625244\n",
      "epoch 69, iter 2, loss 0.7569774985313416\n",
      "epoch 69, iter 3, loss 0.7399362921714783\n",
      "epoch 69, loss 2.9539142847061157\n",
      "epoch 70, iter 0, loss 0.7020542621612549\n",
      "epoch 70, iter 1, loss 0.7474786639213562\n",
      "epoch 70, iter 2, loss 0.7507124543190002\n",
      "epoch 70, iter 3, loss 0.7545595169067383\n",
      "epoch 70, loss 2.9548048973083496\n",
      "epoch 71, iter 0, loss 0.7239857912063599\n",
      "epoch 71, iter 1, loss 0.749576985836029\n",
      "epoch 71, iter 2, loss 0.7236769199371338\n",
      "epoch 71, iter 3, loss 0.7680987119674683\n",
      "epoch 71, loss 2.965338408946991\n",
      "epoch 72, iter 0, loss 0.7401573061943054\n",
      "epoch 72, iter 1, loss 0.7666183710098267\n",
      "epoch 72, iter 2, loss 0.7287384867668152\n",
      "epoch 72, iter 3, loss 0.7317496538162231\n",
      "epoch 72, loss 2.9672638177871704\n",
      "epoch 73, iter 0, loss 0.7443982362747192\n",
      "epoch 73, iter 1, loss 0.7390562891960144\n",
      "epoch 73, iter 2, loss 0.7385895252227783\n",
      "epoch 73, iter 3, loss 0.7539476156234741\n",
      "epoch 73, loss 2.975991666316986\n",
      "epoch 74, iter 0, loss 0.7209574580192566\n",
      "epoch 74, iter 1, loss 0.7660624980926514\n",
      "epoch 74, iter 2, loss 0.7634448409080505\n",
      "epoch 74, iter 3, loss 0.7278760075569153\n",
      "epoch 74, loss 2.978340804576874\n",
      "epoch 75, iter 0, loss 0.7495805025100708\n",
      "epoch 75, iter 1, loss 0.7489588260650635\n",
      "epoch 75, iter 2, loss 0.7530530691146851\n",
      "epoch 75, iter 3, loss 0.7359074950218201\n",
      "epoch 75, loss 2.9874998927116394\n",
      "epoch 76, iter 0, loss 0.7265686988830566\n",
      "epoch 76, iter 1, loss 0.7474855184555054\n",
      "epoch 76, iter 2, loss 0.7644023895263672\n",
      "epoch 76, iter 3, loss 0.7460744380950928\n",
      "epoch 76, loss 2.984531044960022\n",
      "epoch 77, iter 0, loss 0.7376481890678406\n",
      "epoch 77, iter 1, loss 0.728872537612915\n",
      "epoch 77, iter 2, loss 0.7428830862045288\n",
      "epoch 77, iter 3, loss 0.7418811917304993\n",
      "epoch 77, loss 2.9512850046157837\n",
      "epoch 78, iter 0, loss 0.7272833585739136\n",
      "epoch 78, iter 1, loss 0.696570634841919\n",
      "epoch 78, iter 2, loss 0.7531571984291077\n",
      "epoch 78, iter 3, loss 0.7493351101875305\n",
      "epoch 78, loss 2.9263463020324707\n",
      "epoch 79, iter 0, loss 0.753004789352417\n",
      "epoch 79, iter 1, loss 0.7133324146270752\n",
      "epoch 79, iter 2, loss 0.7372885942459106\n",
      "epoch 79, iter 3, loss 0.7393743395805359\n",
      "epoch 79, loss 2.9430001378059387\n",
      "epoch 80, iter 0, loss 0.7119141817092896\n",
      "epoch 80, iter 1, loss 0.7318085432052612\n",
      "epoch 80, iter 2, loss 0.7311697006225586\n",
      "epoch 80, iter 3, loss 0.7744309902191162\n",
      "epoch 80, loss 2.9493234157562256\n",
      "epoch 81, iter 0, loss 0.7076192498207092\n",
      "epoch 81, iter 1, loss 0.7404143810272217\n",
      "epoch 81, iter 2, loss 0.7576872110366821\n",
      "epoch 81, iter 3, loss 0.735785186290741\n",
      "epoch 81, loss 2.941506028175354\n",
      "epoch 82, iter 0, loss 0.7207530736923218\n",
      "epoch 82, iter 1, loss 0.7400404214859009\n",
      "epoch 82, iter 2, loss 0.7388408184051514\n",
      "epoch 82, iter 3, loss 0.7765538692474365\n",
      "epoch 82, loss 2.9761881828308105\n",
      "epoch 83, iter 0, loss 0.7287006378173828\n",
      "epoch 83, iter 1, loss 0.7517637014389038\n",
      "epoch 83, iter 2, loss 0.7442954182624817\n",
      "epoch 83, iter 3, loss 0.7631818652153015\n",
      "epoch 83, loss 2.98794162273407\n",
      "epoch 84, iter 0, loss 0.7234801650047302\n",
      "epoch 84, iter 1, loss 0.7575566172599792\n",
      "epoch 84, iter 2, loss 0.7508297562599182\n",
      "epoch 84, iter 3, loss 0.7183608412742615\n",
      "epoch 84, loss 2.950227379798889\n",
      "epoch 85, iter 0, loss 0.7202494740486145\n",
      "epoch 85, iter 1, loss 0.7635701298713684\n",
      "epoch 85, iter 2, loss 0.71945720911026\n",
      "epoch 85, iter 3, loss 0.733654260635376\n",
      "epoch 85, loss 2.936931073665619\n",
      "epoch 86, iter 0, loss 0.7102572321891785\n",
      "epoch 86, iter 1, loss 0.7246019244194031\n",
      "epoch 86, iter 2, loss 0.7559534311294556\n",
      "epoch 86, iter 3, loss 0.729801595211029\n",
      "epoch 86, loss 2.920614182949066\n",
      "epoch 87, iter 0, loss 0.7234411239624023\n",
      "epoch 87, iter 1, loss 0.7315487861633301\n",
      "epoch 87, iter 2, loss 0.7352660894393921\n",
      "epoch 87, iter 3, loss 0.7280457019805908\n",
      "epoch 87, loss 2.9183017015457153\n",
      "epoch 88, iter 0, loss 0.7456644773483276\n",
      "epoch 88, iter 1, loss 0.722894549369812\n",
      "epoch 88, iter 2, loss 0.7323557734489441\n",
      "epoch 88, iter 3, loss 0.7320998311042786\n",
      "epoch 88, loss 2.9330146312713623\n",
      "epoch 89, iter 0, loss 0.7051872611045837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89, iter 1, loss 0.7274793982505798\n",
      "epoch 89, iter 2, loss 0.7488800883293152\n",
      "epoch 89, iter 3, loss 0.7752314209938049\n",
      "epoch 89, loss 2.9567781686782837\n",
      "epoch 90, iter 0, loss 0.7069275975227356\n",
      "epoch 90, iter 1, loss 0.7435600161552429\n",
      "epoch 90, iter 2, loss 0.738888680934906\n",
      "epoch 90, iter 3, loss 0.7693560123443604\n",
      "epoch 90, loss 2.958732306957245\n",
      "epoch 91, iter 0, loss 0.7230823636054993\n",
      "epoch 91, iter 1, loss 0.734477698802948\n",
      "epoch 91, iter 2, loss 0.7597634196281433\n",
      "epoch 91, iter 3, loss 0.7194293737411499\n",
      "epoch 91, loss 2.9367528557777405\n",
      "epoch 92, iter 0, loss 0.6724341511726379\n",
      "epoch 92, iter 1, loss 0.7591251134872437\n",
      "epoch 92, iter 2, loss 0.7728227376937866\n",
      "epoch 92, iter 3, loss 0.7648789882659912\n",
      "epoch 92, loss 2.9692609906196594\n",
      "epoch 93, iter 0, loss 0.7263979315757751\n",
      "epoch 93, iter 1, loss 0.7168295383453369\n",
      "epoch 93, iter 2, loss 0.7605579495429993\n",
      "epoch 93, iter 3, loss 0.750385046005249\n",
      "epoch 93, loss 2.9541704654693604\n",
      "epoch 94, iter 0, loss 0.724637508392334\n",
      "epoch 94, iter 1, loss 0.7291768789291382\n",
      "epoch 94, iter 2, loss 0.7638888955116272\n",
      "epoch 94, iter 3, loss 0.7268560528755188\n",
      "epoch 94, loss 2.944559335708618\n",
      "epoch 95, iter 0, loss 0.7175556421279907\n",
      "epoch 95, iter 1, loss 0.7470830678939819\n",
      "epoch 95, iter 2, loss 0.716738224029541\n",
      "epoch 95, iter 3, loss 0.7754948735237122\n",
      "epoch 95, loss 2.956871807575226\n",
      "epoch 96, iter 0, loss 0.720911979675293\n",
      "epoch 96, iter 1, loss 0.7594374418258667\n",
      "epoch 96, iter 2, loss 0.7378586530685425\n",
      "epoch 96, iter 3, loss 0.7223626375198364\n",
      "epoch 96, loss 2.9405707120895386\n",
      "epoch 97, iter 0, loss 0.7639681100845337\n",
      "epoch 97, iter 1, loss 0.7300495505332947\n",
      "epoch 97, iter 2, loss 0.7202252149581909\n",
      "epoch 97, iter 3, loss 0.7415981888771057\n",
      "epoch 97, loss 2.955841064453125\n",
      "epoch 98, iter 0, loss 0.7276444435119629\n",
      "epoch 98, iter 1, loss 0.7606098055839539\n",
      "epoch 98, iter 2, loss 0.7430209517478943\n",
      "epoch 98, iter 3, loss 0.7280782461166382\n",
      "epoch 98, loss 2.959353446960449\n",
      "epoch 99, iter 0, loss 0.7099076509475708\n",
      "epoch 99, iter 1, loss 0.7206398248672485\n",
      "epoch 99, iter 2, loss 0.7473724484443665\n",
      "epoch 99, iter 3, loss 0.7747712135314941\n",
      "epoch 99, loss 2.95269113779068\n",
      "2019-03-31 02:12:13.139820, fold=0, rep=0, eta=0d 1h 5m 46s \n",
      "{'fold': 0, 'repeat': 0, 'n': 5875, 'd': 18, 'mse': 1.526193380355835, 'train_time': 787.8546244390309, 'trained_epochs': 100, 'prior_train_nmll': 0.7793449759483337, 'train_nll': 1681.1932373046875, 'test_nll': 2040.5482177734375, 'train_mse': 0.12961754202842712, 'state_dict_file': 'model_state_dict_4141854275351682.pkl'}\n",
      "epoch 0, iter 0, loss 1.9291349649429321\n",
      "epoch 0, iter 1, loss 2.2390658855438232\n",
      "epoch 0, iter 2, loss 2.0799381732940674\n",
      "epoch 0, iter 3, loss 1.8744261264801025\n",
      "epoch 0, loss 8.122565150260925\n",
      "epoch 1, iter 0, loss 1.905570149421692\n",
      "epoch 1, iter 1, loss 1.8489910364151\n",
      "epoch 1, iter 2, loss 1.6696689128875732\n",
      "epoch 1, iter 3, loss 1.6024678945541382\n",
      "epoch 1, loss 7.026697993278503\n",
      "epoch 2, iter 0, loss 1.5474605560302734\n",
      "epoch 2, iter 1, loss 1.54445481300354\n",
      "epoch 2, iter 2, loss 1.5030195713043213\n",
      "epoch 2, iter 3, loss 1.4718334674835205\n",
      "epoch 2, loss 6.066768407821655\n",
      "epoch 3, iter 0, loss 1.4301255941390991\n",
      "epoch 3, iter 1, loss 1.4254285097122192\n",
      "epoch 3, iter 2, loss 1.3974125385284424\n",
      "epoch 3, iter 3, loss 1.3748514652252197\n",
      "epoch 3, loss 5.6278181076049805\n",
      "epoch 4, iter 0, loss 1.3471280336380005\n",
      "epoch 4, iter 1, loss 1.3428388833999634\n",
      "epoch 4, iter 2, loss 1.3105942010879517\n",
      "epoch 4, iter 3, loss 1.2990115880966187\n",
      "epoch 4, loss 5.299572706222534\n",
      "epoch 5, iter 0, loss 1.270313024520874\n",
      "epoch 5, iter 1, loss 1.2640514373779297\n",
      "epoch 5, iter 2, loss 1.2279099225997925\n",
      "epoch 5, iter 3, loss 1.2207889556884766\n",
      "epoch 5, loss 4.983063340187073\n",
      "epoch 6, iter 0, loss 1.1918811798095703\n",
      "epoch 6, iter 1, loss 1.1722041368484497\n",
      "epoch 6, iter 2, loss 1.1506327390670776\n",
      "epoch 6, iter 3, loss 1.1402784585952759\n",
      "epoch 6, loss 4.6549965143203735\n",
      "epoch 7, iter 0, loss 1.097045660018921\n",
      "epoch 7, iter 1, loss 1.0856420993804932\n",
      "epoch 7, iter 2, loss 1.0801235437393188\n",
      "epoch 7, iter 3, loss 1.0615215301513672\n",
      "epoch 7, loss 4.3243328332901\n",
      "epoch 8, iter 0, loss 1.0165507793426514\n",
      "epoch 8, iter 1, loss 1.0101819038391113\n",
      "epoch 8, iter 2, loss 0.9873857498168945\n",
      "epoch 8, iter 3, loss 0.9597271680831909\n",
      "epoch 8, loss 3.973845601081848\n",
      "epoch 9, iter 0, loss 0.9311275482177734\n",
      "epoch 9, iter 1, loss 0.9192684292793274\n",
      "epoch 9, iter 2, loss 0.8971725106239319\n",
      "epoch 9, iter 3, loss 0.901151180267334\n",
      "epoch 9, loss 3.6487196683883667\n",
      "epoch 10, iter 0, loss 0.8553462624549866\n",
      "epoch 10, iter 1, loss 0.8491380214691162\n",
      "epoch 10, iter 2, loss 0.8453565835952759\n",
      "epoch 10, iter 3, loss 0.847407877445221\n",
      "epoch 10, loss 3.3972487449645996\n",
      "epoch 11, iter 0, loss 0.8062723278999329\n",
      "epoch 11, iter 1, loss 0.8263005018234253\n",
      "epoch 11, iter 2, loss 0.8108632564544678\n",
      "epoch 11, iter 3, loss 0.8588671684265137\n",
      "epoch 11, loss 3.3023032546043396\n",
      "epoch 12, iter 0, loss 0.8014568090438843\n",
      "epoch 12, iter 1, loss 0.8548921346664429\n",
      "epoch 12, iter 2, loss 0.8504670858383179\n",
      "epoch 12, iter 3, loss 0.835273265838623\n",
      "epoch 12, loss 3.342089295387268\n",
      "epoch 13, iter 0, loss 0.8343368768692017\n",
      "epoch 13, iter 1, loss 0.8617237210273743\n",
      "epoch 13, iter 2, loss 0.8635467290878296\n",
      "epoch 13, iter 3, loss 0.9165983200073242\n",
      "epoch 13, loss 3.4762056469917297\n",
      "epoch 14, iter 0, loss 0.8336374759674072\n",
      "epoch 14, iter 1, loss 0.8721212148666382\n",
      "epoch 14, iter 2, loss 0.8847960829734802\n",
      "epoch 14, iter 3, loss 0.9585680961608887\n",
      "epoch 14, loss 3.5491228699684143\n",
      "epoch 15, iter 0, loss 0.8348681926727295\n",
      "epoch 15, iter 1, loss 0.8610731959342957\n",
      "epoch 15, iter 2, loss 0.8963683843612671\n",
      "epoch 15, iter 3, loss 0.9401218891143799\n",
      "epoch 15, loss 3.532431662082672\n",
      "epoch 16, iter 0, loss 0.8566954135894775\n",
      "epoch 16, iter 1, loss 0.8471015095710754\n",
      "epoch 16, iter 2, loss 0.8407379984855652\n",
      "epoch 16, iter 3, loss 0.8747801184654236\n",
      "epoch 16, loss 3.4193150401115417\n",
      "epoch 17, iter 0, loss 0.8380100727081299\n",
      "epoch 17, iter 1, loss 0.8244088888168335\n",
      "epoch 17, iter 2, loss 0.8313246369361877\n",
      "epoch 17, iter 3, loss 0.8509356379508972\n",
      "epoch 17, loss 3.3446792364120483\n",
      "epoch 18, iter 0, loss 0.819066047668457\n",
      "epoch 18, iter 1, loss 0.7993558049201965\n",
      "epoch 18, iter 2, loss 0.8128450512886047\n",
      "epoch 18, iter 3, loss 0.8193007111549377\n",
      "epoch 18, loss 3.250567615032196\n",
      "epoch 19, iter 0, loss 0.8024436235427856\n",
      "epoch 19, iter 1, loss 0.7869943380355835\n",
      "epoch 19, iter 2, loss 0.787090003490448\n",
      "epoch 19, iter 3, loss 0.7800700664520264\n",
      "epoch 19, loss 3.1565980315208435\n",
      "epoch 20, iter 0, loss 0.7469081878662109\n",
      "epoch 20, iter 1, loss 0.76894211769104\n",
      "epoch 20, iter 2, loss 0.8004134893417358\n",
      "epoch 20, iter 3, loss 0.7987946271896362\n",
      "epoch 20, loss 3.115058422088623\n",
      "epoch 21, iter 0, loss 0.7719636559486389\n",
      "epoch 21, iter 1, loss 0.7705160975456238\n",
      "epoch 21, iter 2, loss 0.7584330439567566\n",
      "epoch 21, iter 3, loss 0.7529720067977905\n",
      "epoch 21, loss 3.05388480424881\n",
      "epoch 22, iter 0, loss 0.7334243655204773\n",
      "epoch 22, iter 1, loss 0.7185894846916199\n",
      "epoch 22, iter 2, loss 0.7630998492240906\n",
      "epoch 22, iter 3, loss 0.7916127443313599\n",
      "epoch 22, loss 3.0067264437675476\n",
      "epoch 23, iter 0, loss 0.735880970954895\n",
      "epoch 23, iter 1, loss 0.714285135269165\n",
      "epoch 23, iter 2, loss 0.7877713441848755\n",
      "epoch 23, iter 3, loss 0.7651164531707764\n",
      "epoch 23, loss 3.003053903579712\n",
      "epoch 24, iter 0, loss 0.7248154878616333\n",
      "epoch 24, iter 1, loss 0.7235101461410522\n",
      "epoch 24, iter 2, loss 0.7798757553100586\n",
      "epoch 24, iter 3, loss 0.7668612003326416\n",
      "epoch 24, loss 2.9950625896453857\n",
      "epoch 25, iter 0, loss 0.7195546627044678\n",
      "epoch 25, iter 1, loss 0.7542498111724854\n",
      "epoch 25, iter 2, loss 0.7707472443580627\n",
      "epoch 25, iter 3, loss 0.7659692764282227\n",
      "epoch 25, loss 3.0105209946632385\n",
      "epoch 26, iter 0, loss 0.7888959050178528\n",
      "epoch 26, iter 1, loss 0.7219090461730957\n",
      "epoch 26, iter 2, loss 0.7866270542144775\n",
      "epoch 26, iter 3, loss 0.7391173243522644\n",
      "epoch 26, loss 3.0365493297576904\n",
      "epoch 27, iter 0, loss 0.756212055683136\n",
      "epoch 27, iter 1, loss 0.7335286140441895\n",
      "epoch 27, iter 2, loss 0.7614200115203857\n",
      "epoch 27, iter 3, loss 0.7805430293083191\n",
      "epoch 27, loss 3.0317037105560303\n",
      "epoch 28, iter 0, loss 0.7550523281097412\n",
      "epoch 28, iter 1, loss 0.7346052527427673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28, iter 2, loss 0.7676372528076172\n",
      "epoch 28, iter 3, loss 0.8114074468612671\n",
      "epoch 28, loss 3.068702280521393\n",
      "epoch 29, iter 0, loss 0.7613396644592285\n",
      "epoch 29, iter 1, loss 0.7745689749717712\n",
      "epoch 29, iter 2, loss 0.7263979315757751\n",
      "epoch 29, iter 3, loss 0.7704756259918213\n",
      "epoch 29, loss 3.032782196998596\n",
      "epoch 30, iter 0, loss 0.7643622756004333\n",
      "epoch 30, iter 1, loss 0.7526320219039917\n",
      "epoch 30, iter 2, loss 0.7207263708114624\n",
      "epoch 30, iter 3, loss 0.7635706067085266\n",
      "epoch 30, loss 3.001291275024414\n",
      "epoch 31, iter 0, loss 0.7442386150360107\n",
      "epoch 31, iter 1, loss 0.7272129654884338\n",
      "epoch 31, iter 2, loss 0.7732995748519897\n",
      "epoch 31, iter 3, loss 0.7657022476196289\n",
      "epoch 31, loss 3.0104534029960632\n",
      "epoch 32, iter 0, loss 0.7276281118392944\n",
      "epoch 32, iter 1, loss 0.7272147536277771\n",
      "epoch 32, iter 2, loss 0.7632547616958618\n",
      "epoch 32, iter 3, loss 0.7592079043388367\n",
      "epoch 32, loss 2.97730553150177\n",
      "epoch 33, iter 0, loss 0.7301281690597534\n",
      "epoch 33, iter 1, loss 0.726714551448822\n",
      "epoch 33, iter 2, loss 0.7260114550590515\n",
      "epoch 33, iter 3, loss 0.7872704267501831\n",
      "epoch 33, loss 2.97012460231781\n",
      "epoch 34, iter 0, loss 0.7337437272071838\n",
      "epoch 34, iter 1, loss 0.7240136861801147\n",
      "epoch 34, iter 2, loss 0.7484131455421448\n",
      "epoch 34, iter 3, loss 0.7616281509399414\n",
      "epoch 34, loss 2.9677987098693848\n",
      "epoch 35, iter 0, loss 0.7301685810089111\n",
      "epoch 35, iter 1, loss 0.7457945346832275\n",
      "epoch 35, iter 2, loss 0.7692184448242188\n",
      "epoch 35, iter 3, loss 0.7327941060066223\n",
      "epoch 35, loss 2.9779756665229797\n",
      "epoch 36, iter 0, loss 0.7527304887771606\n",
      "epoch 36, iter 1, loss 0.7345036864280701\n",
      "epoch 36, iter 2, loss 0.7401812076568604\n",
      "epoch 36, iter 3, loss 0.7530630230903625\n",
      "epoch 36, loss 2.9804784059524536\n",
      "epoch 37, iter 0, loss 0.7194232940673828\n",
      "epoch 37, iter 1, loss 0.7328383326530457\n",
      "epoch 37, iter 2, loss 0.776867151260376\n",
      "epoch 37, iter 3, loss 0.743347704410553\n",
      "epoch 37, loss 2.9724764823913574\n",
      "epoch 38, iter 0, loss 0.7296349406242371\n",
      "epoch 38, iter 1, loss 0.736963152885437\n",
      "epoch 38, iter 2, loss 0.7646355032920837\n",
      "epoch 38, iter 3, loss 0.7600074410438538\n",
      "epoch 38, loss 2.9912410378456116\n",
      "epoch 39, iter 0, loss 0.7167030572891235\n",
      "epoch 39, iter 1, loss 0.7273320555686951\n",
      "epoch 39, iter 2, loss 0.7878057360649109\n",
      "epoch 39, iter 3, loss 0.7504331469535828\n",
      "epoch 39, loss 2.9822739958763123\n",
      "epoch 40, iter 0, loss 0.7378257513046265\n",
      "epoch 40, iter 1, loss 0.7225083112716675\n",
      "epoch 40, iter 2, loss 0.7564212083816528\n",
      "epoch 40, iter 3, loss 0.7842684984207153\n",
      "epoch 40, loss 3.001023769378662\n",
      "epoch 41, iter 0, loss 0.7145167589187622\n",
      "epoch 41, iter 1, loss 0.7469872236251831\n",
      "epoch 41, iter 2, loss 0.7577875852584839\n",
      "epoch 41, iter 3, loss 0.7797292470932007\n",
      "epoch 41, loss 2.99902081489563\n",
      "epoch 42, iter 0, loss 0.7487189173698425\n",
      "epoch 42, iter 1, loss 0.7579360604286194\n",
      "epoch 42, iter 2, loss 0.7105952501296997\n",
      "epoch 42, iter 3, loss 0.7729136347770691\n",
      "epoch 42, loss 2.9901638627052307\n",
      "epoch 43, iter 0, loss 0.7272558212280273\n",
      "epoch 43, iter 1, loss 0.7347291707992554\n",
      "epoch 43, iter 2, loss 0.7602904438972473\n",
      "epoch 43, iter 3, loss 0.7577781677246094\n",
      "epoch 43, loss 2.9800536036491394\n",
      "epoch 44, iter 0, loss 0.7485092282295227\n",
      "epoch 44, iter 1, loss 0.7050100564956665\n",
      "epoch 44, iter 2, loss 0.748519778251648\n",
      "epoch 44, iter 3, loss 0.7627350091934204\n",
      "epoch 44, loss 2.9647740721702576\n",
      "epoch 45, iter 0, loss 0.7338466048240662\n",
      "epoch 45, iter 1, loss 0.7347694039344788\n",
      "epoch 45, iter 2, loss 0.7560127973556519\n",
      "epoch 45, iter 3, loss 0.7370429635047913\n",
      "epoch 45, loss 2.961671769618988\n",
      "epoch 46, iter 0, loss 0.7307906746864319\n",
      "epoch 46, iter 1, loss 0.7073155045509338\n",
      "epoch 46, iter 2, loss 0.7660952806472778\n",
      "epoch 46, iter 3, loss 0.736466109752655\n",
      "epoch 46, loss 2.9406675696372986\n",
      "epoch 47, iter 0, loss 0.7162041664123535\n",
      "epoch 47, iter 1, loss 0.7338544130325317\n",
      "epoch 47, iter 2, loss 0.7595051527023315\n",
      "epoch 47, iter 3, loss 0.7619615197181702\n",
      "epoch 47, loss 2.971525251865387\n",
      "epoch 48, iter 0, loss 0.6920282244682312\n",
      "epoch 48, iter 1, loss 0.7570638656616211\n",
      "epoch 48, iter 2, loss 0.7306290864944458\n",
      "epoch 48, iter 3, loss 0.7727645635604858\n",
      "epoch 48, loss 2.952485740184784\n",
      "epoch 49, iter 0, loss 0.7406017184257507\n",
      "epoch 49, iter 1, loss 0.731616735458374\n",
      "epoch 49, iter 2, loss 0.7519936561584473\n",
      "epoch 49, iter 3, loss 0.7789644002914429\n",
      "epoch 49, loss 3.003176510334015\n",
      "epoch 50, iter 0, loss 0.7055702805519104\n",
      "epoch 50, iter 1, loss 0.7592321038246155\n",
      "epoch 50, iter 2, loss 0.7537342309951782\n",
      "epoch 50, iter 3, loss 0.7743527889251709\n",
      "epoch 50, loss 2.992889404296875\n",
      "epoch 51, iter 0, loss 0.758111834526062\n",
      "epoch 51, iter 1, loss 0.713718593120575\n",
      "epoch 51, iter 2, loss 0.77567058801651\n",
      "epoch 51, iter 3, loss 0.7655994892120361\n",
      "epoch 51, loss 3.013100504875183\n",
      "epoch 52, iter 0, loss 0.7342397570610046\n",
      "epoch 52, iter 1, loss 0.7656568884849548\n",
      "epoch 52, iter 2, loss 0.748321533203125\n",
      "epoch 52, iter 3, loss 0.767392635345459\n",
      "epoch 52, loss 3.0156108140945435\n",
      "epoch 53, iter 0, loss 0.7651439905166626\n",
      "epoch 53, iter 1, loss 0.7361528277397156\n",
      "epoch 53, iter 2, loss 0.7315860986709595\n",
      "epoch 53, iter 3, loss 0.766158401966095\n",
      "epoch 53, loss 2.9990413188934326\n",
      "epoch 54, iter 0, loss 0.7171750664710999\n",
      "epoch 54, iter 1, loss 0.7735493779182434\n",
      "epoch 54, iter 2, loss 0.7376325130462646\n",
      "epoch 54, iter 3, loss 0.7481380701065063\n",
      "epoch 54, loss 2.9764950275421143\n",
      "epoch 55, iter 0, loss 0.708006739616394\n",
      "epoch 55, iter 1, loss 0.7524400949478149\n",
      "epoch 55, iter 2, loss 0.7484156489372253\n",
      "epoch 55, iter 3, loss 0.7637181282043457\n",
      "epoch 55, loss 2.97258061170578\n",
      "epoch 56, iter 0, loss 0.7008191347122192\n",
      "epoch 56, iter 1, loss 0.7452928423881531\n",
      "epoch 56, iter 2, loss 0.7505167722702026\n",
      "epoch 56, iter 3, loss 0.7652977108955383\n",
      "epoch 56, loss 2.9619264602661133\n",
      "epoch 57, iter 0, loss 0.6819404363632202\n",
      "epoch 57, iter 1, loss 0.8058102130889893\n",
      "epoch 57, iter 2, loss 0.7578604221343994\n",
      "epoch 57, iter 3, loss 0.7425425052642822\n",
      "epoch 57, loss 2.988153576850891\n",
      "epoch 58, iter 0, loss 0.742006242275238\n",
      "epoch 58, iter 1, loss 0.7418469190597534\n",
      "epoch 58, iter 2, loss 0.7366077899932861\n",
      "epoch 58, iter 3, loss 0.7686785459518433\n",
      "epoch 58, loss 2.989139497280121\n",
      "epoch 59, iter 0, loss 0.7406665086746216\n",
      "epoch 59, iter 1, loss 0.7374367117881775\n",
      "epoch 59, iter 2, loss 0.7450461387634277\n",
      "epoch 59, iter 3, loss 0.769292950630188\n",
      "epoch 59, loss 2.992442309856415\n",
      "epoch 60, iter 0, loss 0.7237499356269836\n",
      "epoch 60, iter 1, loss 0.7842838168144226\n",
      "epoch 60, iter 2, loss 0.7422419786453247\n",
      "epoch 60, iter 3, loss 0.7376165986061096\n",
      "epoch 60, loss 2.9878923296928406\n",
      "epoch 61, iter 0, loss 0.7334197759628296\n",
      "epoch 61, iter 1, loss 0.7307636737823486\n",
      "epoch 61, iter 2, loss 0.7432400584220886\n",
      "epoch 61, iter 3, loss 0.7802910804748535\n",
      "epoch 61, loss 2.9877145886421204\n",
      "epoch 62, iter 0, loss 0.730368971824646\n",
      "epoch 62, iter 1, loss 0.7218554615974426\n",
      "epoch 62, iter 2, loss 0.7731645107269287\n",
      "epoch 62, iter 3, loss 0.7380382418632507\n",
      "epoch 62, loss 2.963427186012268\n",
      "epoch 63, iter 0, loss 0.734474778175354\n",
      "epoch 63, iter 1, loss 0.7395235896110535\n",
      "epoch 63, iter 2, loss 0.7460598945617676\n",
      "epoch 63, iter 3, loss 0.7580210566520691\n",
      "epoch 63, loss 2.978079319000244\n",
      "epoch 64, iter 0, loss 0.6970452666282654\n",
      "epoch 64, iter 1, loss 0.7438191175460815\n",
      "epoch 64, iter 2, loss 0.7583356499671936\n",
      "epoch 64, iter 3, loss 0.7650374174118042\n",
      "epoch 64, loss 2.9642374515533447\n",
      "epoch 65, iter 0, loss 0.7187665700912476\n",
      "epoch 65, iter 1, loss 0.7602696418762207\n",
      "epoch 65, iter 2, loss 0.7458881139755249\n",
      "epoch 65, iter 3, loss 0.7743938565254211\n",
      "epoch 65, loss 2.9993181824684143\n",
      "epoch 66, iter 0, loss 0.715872049331665\n",
      "epoch 66, iter 1, loss 0.7535554766654968\n",
      "epoch 66, iter 2, loss 0.7532640695571899\n",
      "epoch 66, iter 3, loss 0.7606494426727295\n",
      "epoch 66, loss 2.9833410382270813\n",
      "epoch 67, iter 0, loss 0.7319748401641846\n",
      "epoch 67, iter 1, loss 0.7535833716392517\n",
      "epoch 67, iter 2, loss 0.7445986866950989\n",
      "epoch 67, iter 3, loss 0.7546207308769226\n",
      "epoch 67, loss 2.9847776293754578\n",
      "epoch 68, iter 0, loss 0.7297978401184082\n",
      "epoch 68, iter 1, loss 0.7716630697250366\n",
      "epoch 68, iter 2, loss 0.773483395576477\n",
      "epoch 68, iter 3, loss 0.703180193901062\n",
      "epoch 68, loss 2.978124499320984\n",
      "epoch 69, iter 0, loss 0.746610701084137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69, iter 1, loss 0.7312167882919312\n",
      "epoch 69, iter 2, loss 0.7402260303497314\n",
      "epoch 69, iter 3, loss 0.7406338453292847\n",
      "epoch 69, loss 2.9586873650550842\n",
      "epoch 70, iter 0, loss 0.7251712083816528\n",
      "epoch 70, iter 1, loss 0.7454059720039368\n",
      "epoch 70, iter 2, loss 0.7448795437812805\n",
      "epoch 70, iter 3, loss 0.7273654341697693\n",
      "epoch 70, loss 2.9428221583366394\n",
      "epoch 71, iter 0, loss 0.691913366317749\n",
      "epoch 71, iter 1, loss 0.7454283237457275\n",
      "epoch 71, iter 2, loss 0.7597646713256836\n",
      "epoch 71, iter 3, loss 0.731824517250061\n",
      "epoch 71, loss 2.928930878639221\n",
      "epoch 72, iter 0, loss 0.7085208296775818\n",
      "epoch 72, iter 1, loss 0.726367712020874\n",
      "epoch 72, iter 2, loss 0.7454773187637329\n",
      "epoch 72, iter 3, loss 0.7344590425491333\n",
      "epoch 72, loss 2.914824903011322\n",
      "epoch 73, iter 0, loss 0.7286888360977173\n",
      "epoch 73, iter 1, loss 0.7205510139465332\n",
      "epoch 73, iter 2, loss 0.7036298513412476\n",
      "epoch 73, iter 3, loss 0.770922064781189\n",
      "epoch 73, loss 2.923791766166687\n",
      "epoch 74, iter 0, loss 0.7218276262283325\n",
      "epoch 74, iter 1, loss 0.7186901569366455\n",
      "epoch 74, iter 2, loss 0.7177346348762512\n",
      "epoch 74, iter 3, loss 0.7616857290267944\n",
      "epoch 74, loss 2.9199381470680237\n",
      "epoch 75, iter 0, loss 0.705380380153656\n",
      "epoch 75, iter 1, loss 0.7563537955284119\n",
      "epoch 75, iter 2, loss 0.7388445138931274\n",
      "epoch 75, iter 3, loss 0.761424720287323\n",
      "epoch 75, loss 2.9620034098625183\n",
      "epoch 76, iter 0, loss 0.7257946729660034\n",
      "epoch 76, iter 1, loss 0.727401852607727\n",
      "epoch 76, iter 2, loss 0.7590198516845703\n",
      "epoch 76, iter 3, loss 0.7280353307723999\n",
      "epoch 76, loss 2.9402517080307007\n",
      "epoch 77, iter 0, loss 0.7478360533714294\n",
      "epoch 77, iter 1, loss 0.7195382118225098\n",
      "epoch 77, iter 2, loss 0.764985978603363\n",
      "epoch 77, iter 3, loss 0.7355527281761169\n",
      "epoch 77, loss 2.967912971973419\n",
      "epoch 78, iter 0, loss 0.7400228381156921\n",
      "epoch 78, iter 1, loss 0.7358133792877197\n",
      "epoch 78, iter 2, loss 0.7308006882667542\n",
      "epoch 78, iter 3, loss 0.7539294958114624\n",
      "epoch 78, loss 2.9605664014816284\n",
      "epoch 79, iter 0, loss 0.7199164628982544\n",
      "epoch 79, iter 1, loss 0.7083883285522461\n",
      "epoch 79, iter 2, loss 0.7516969442367554\n",
      "epoch 79, iter 3, loss 0.7708760499954224\n",
      "epoch 79, loss 2.9508777856826782\n",
      "epoch 80, iter 0, loss 0.6971593499183655\n",
      "epoch 80, iter 1, loss 0.7152562737464905\n",
      "epoch 80, iter 2, loss 0.7442764639854431\n",
      "epoch 80, iter 3, loss 0.7825440168380737\n",
      "epoch 80, loss 2.939236104488373\n",
      "epoch 81, iter 0, loss 0.7361550331115723\n",
      "epoch 81, iter 1, loss 0.73389732837677\n",
      "epoch 81, iter 2, loss 0.7270734906196594\n",
      "epoch 81, iter 3, loss 0.7320685386657715\n",
      "epoch 81, loss 2.929194390773773\n",
      "epoch 82, iter 0, loss 0.7224509119987488\n",
      "epoch 82, iter 1, loss 0.714192807674408\n",
      "epoch 82, iter 2, loss 0.7579115629196167\n",
      "epoch 82, iter 3, loss 0.7303721904754639\n",
      "epoch 82, loss 2.9249274730682373\n",
      "epoch 83, iter 0, loss 0.7304406762123108\n",
      "epoch 83, iter 1, loss 0.7333308458328247\n",
      "epoch 83, iter 2, loss 0.7354152798652649\n",
      "epoch 83, iter 3, loss 0.7387219667434692\n",
      "epoch 83, loss 2.9379087686538696\n",
      "epoch 84, iter 0, loss 0.7239972949028015\n",
      "epoch 84, iter 1, loss 0.7129464149475098\n",
      "epoch 84, iter 2, loss 0.7392129898071289\n",
      "epoch 84, iter 3, loss 0.7682021856307983\n",
      "epoch 84, loss 2.9443588852882385\n",
      "epoch 85, iter 0, loss 0.7305561900138855\n",
      "epoch 85, iter 1, loss 0.7431180477142334\n",
      "epoch 85, iter 2, loss 0.7351858615875244\n",
      "epoch 85, iter 3, loss 0.7298458814620972\n",
      "epoch 85, loss 2.9387059807777405\n",
      "epoch 86, iter 0, loss 0.7323086857795715\n",
      "epoch 86, iter 1, loss 0.7362271547317505\n",
      "epoch 86, iter 2, loss 0.7354335784912109\n",
      "epoch 86, iter 3, loss 0.7160571217536926\n",
      "epoch 86, loss 2.9200265407562256\n",
      "epoch 87, iter 0, loss 0.7010157108306885\n",
      "epoch 87, iter 1, loss 0.7268909811973572\n",
      "epoch 87, iter 2, loss 0.7741813659667969\n",
      "epoch 87, iter 3, loss 0.7442505359649658\n",
      "epoch 87, loss 2.9463385939598083\n",
      "epoch 88, iter 0, loss 0.7297409772872925\n",
      "epoch 88, iter 1, loss 0.6967875361442566\n",
      "epoch 88, iter 2, loss 0.7745039463043213\n",
      "epoch 88, iter 3, loss 0.7552739381790161\n",
      "epoch 88, loss 2.9563063979148865\n",
      "epoch 89, iter 0, loss 0.7017278075218201\n",
      "epoch 89, iter 1, loss 0.7577704191207886\n",
      "epoch 89, iter 2, loss 0.7385866045951843\n",
      "epoch 89, iter 3, loss 0.7656745910644531\n",
      "epoch 89, loss 2.963759422302246\n",
      "epoch 90, iter 0, loss 0.7233052253723145\n",
      "epoch 90, iter 1, loss 0.7070091366767883\n",
      "epoch 90, iter 2, loss 0.787630558013916\n",
      "epoch 90, iter 3, loss 0.7466377019882202\n",
      "epoch 90, loss 2.964582622051239\n",
      "epoch 91, iter 0, loss 0.7077855467796326\n",
      "epoch 91, iter 1, loss 0.7526596188545227\n",
      "epoch 91, iter 2, loss 0.7414273619651794\n",
      "epoch 91, iter 3, loss 0.7496978640556335\n",
      "epoch 91, loss 2.9515703916549683\n",
      "epoch 92, iter 0, loss 0.7157232761383057\n",
      "epoch 92, iter 1, loss 0.7471991181373596\n",
      "epoch 92, iter 2, loss 0.7153037190437317\n",
      "epoch 92, iter 3, loss 0.768713653087616\n",
      "epoch 92, loss 2.946939766407013\n",
      "epoch 93, iter 0, loss 0.7053310871124268\n",
      "epoch 93, iter 1, loss 0.7554421424865723\n",
      "epoch 93, iter 2, loss 0.7379146814346313\n",
      "epoch 93, iter 3, loss 0.751194417476654\n",
      "epoch 93, loss 2.9498823285102844\n",
      "epoch 94, iter 0, loss 0.7210088968276978\n",
      "epoch 94, iter 1, loss 0.7760189771652222\n",
      "epoch 94, iter 2, loss 0.722906768321991\n",
      "epoch 94, iter 3, loss 0.7365278601646423\n",
      "epoch 94, loss 2.9564625024795532\n",
      "epoch 95, iter 0, loss 0.7485687136650085\n",
      "epoch 95, iter 1, loss 0.750568687915802\n",
      "epoch 95, iter 2, loss 0.7633448839187622\n",
      "epoch 95, iter 3, loss 0.7327171564102173\n",
      "epoch 95, loss 2.99519944190979\n",
      "epoch 96, iter 0, loss 0.7507350444793701\n",
      "epoch 96, iter 1, loss 0.7265575528144836\n",
      "epoch 96, iter 2, loss 0.7565981149673462\n",
      "epoch 96, iter 3, loss 0.7410638928413391\n",
      "epoch 96, loss 2.974954605102539\n",
      "epoch 97, iter 0, loss 0.7306492328643799\n",
      "epoch 97, iter 1, loss 0.7613552808761597\n",
      "epoch 97, iter 2, loss 0.7609281539916992\n",
      "epoch 97, iter 3, loss 0.7514204978942871\n",
      "epoch 97, loss 3.004353165626526\n",
      "epoch 98, iter 0, loss 0.7145034074783325\n",
      "epoch 98, iter 1, loss 0.739512026309967\n",
      "epoch 98, iter 2, loss 0.7495914101600647\n",
      "epoch 98, iter 3, loss 0.8041501045227051\n",
      "epoch 98, loss 3.0077569484710693\n",
      "epoch 99, iter 0, loss 0.721158504486084\n",
      "epoch 99, iter 1, loss 0.7446367144584656\n",
      "epoch 99, iter 2, loss 0.7338727712631226\n",
      "epoch 99, iter 3, loss 0.7587829828262329\n",
      "epoch 99, loss 2.958450973033905\n",
      "2019-03-31 02:25:24.638342, fold=0, rep=1, eta=0d 0h 52m 41s \n",
      "{'fold': 0, 'repeat': 1, 'n': 5875, 'd': 18, 'mse': 1.4919077157974243, 'train_time': 791.4982339902781, 'trained_epochs': 100, 'prior_train_nmll': 0.7924450635910034, 'train_nll': 1650.4111328125, 'test_nll': 2053.700927734375, 'train_mse': 0.11922474205493927, 'state_dict_file': 'model_state_dict_-7916676624832147446.pkl'}\n",
      "epoch 0, iter 0, loss 1.9321799278259277\n",
      "epoch 0, iter 1, loss 2.302400588989258\n",
      "epoch 0, iter 2, loss 2.096590995788574\n",
      "epoch 0, iter 3, loss 1.8813966512680054\n",
      "epoch 0, loss 8.212568163871765\n",
      "epoch 1, iter 0, loss 1.9224315881729126\n",
      "epoch 1, iter 1, loss 1.8939173221588135\n",
      "epoch 1, iter 2, loss 1.7653783559799194\n",
      "epoch 1, iter 3, loss 1.6259971857070923\n",
      "epoch 1, loss 7.207724452018738\n",
      "epoch 2, iter 0, loss 1.5858217477798462\n",
      "epoch 2, iter 1, loss 1.587400197982788\n",
      "epoch 2, iter 2, loss 1.563905119895935\n",
      "epoch 2, iter 3, loss 1.5247340202331543\n",
      "epoch 2, loss 6.261861085891724\n",
      "epoch 3, iter 0, loss 1.4816207885742188\n",
      "epoch 3, iter 1, loss 1.4572519063949585\n",
      "epoch 3, iter 2, loss 1.455068826675415\n",
      "epoch 3, iter 3, loss 1.4218165874481201\n",
      "epoch 3, loss 5.815758109092712\n",
      "epoch 4, iter 0, loss 1.3972909450531006\n",
      "epoch 4, iter 1, loss 1.3806250095367432\n",
      "epoch 4, iter 2, loss 1.3668372631072998\n",
      "epoch 4, iter 3, loss 1.3491551876068115\n",
      "epoch 4, loss 5.493908405303955\n",
      "epoch 5, iter 0, loss 1.3243273496627808\n",
      "epoch 5, iter 1, loss 1.314064860343933\n",
      "epoch 5, iter 2, loss 1.2900735139846802\n",
      "epoch 5, iter 3, loss 1.284296989440918\n",
      "epoch 5, loss 5.212762713432312\n",
      "epoch 6, iter 0, loss 1.2556941509246826\n",
      "epoch 6, iter 1, loss 1.2345784902572632\n",
      "epoch 6, iter 2, loss 1.2296154499053955\n",
      "epoch 6, iter 3, loss 1.2123273611068726\n",
      "epoch 6, loss 4.932215452194214\n",
      "epoch 7, iter 0, loss 1.1745367050170898\n",
      "epoch 7, iter 1, loss 1.1641438007354736\n",
      "epoch 7, iter 2, loss 1.1541402339935303\n",
      "epoch 7, iter 3, loss 1.143883466720581\n",
      "epoch 7, loss 4.636704206466675\n",
      "epoch 8, iter 0, loss 1.1131149530410767\n",
      "epoch 8, iter 1, loss 1.0869507789611816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, iter 2, loss 1.0823025703430176\n",
      "epoch 8, iter 3, loss 1.0659523010253906\n",
      "epoch 8, loss 4.3483206033706665\n",
      "epoch 9, iter 0, loss 1.0310267210006714\n",
      "epoch 9, iter 1, loss 1.0142086744308472\n",
      "epoch 9, iter 2, loss 1.0336040258407593\n",
      "epoch 9, iter 3, loss 1.0120173692703247\n",
      "epoch 9, loss 4.0908567905426025\n",
      "epoch 10, iter 0, loss 0.9724346399307251\n",
      "epoch 10, iter 1, loss 0.9769082069396973\n",
      "epoch 10, iter 2, loss 0.9538922309875488\n",
      "epoch 10, iter 3, loss 0.9769539833068848\n",
      "epoch 10, loss 3.880189061164856\n",
      "epoch 11, iter 0, loss 0.9181087017059326\n",
      "epoch 11, iter 1, loss 0.9342275857925415\n",
      "epoch 11, iter 2, loss 0.9390037059783936\n",
      "epoch 11, iter 3, loss 0.9859229326248169\n",
      "epoch 11, loss 3.7772629261016846\n",
      "epoch 12, iter 0, loss 0.9252021312713623\n",
      "epoch 12, iter 1, loss 0.9459865689277649\n",
      "epoch 12, iter 2, loss 0.9581905603408813\n",
      "epoch 12, iter 3, loss 0.9820340275764465\n",
      "epoch 12, loss 3.811413288116455\n",
      "epoch 13, iter 0, loss 0.9356904029846191\n",
      "epoch 13, iter 1, loss 0.9571789503097534\n",
      "epoch 13, iter 2, loss 0.9714017510414124\n",
      "epoch 13, iter 3, loss 1.0240057706832886\n",
      "epoch 13, loss 3.8882768750190735\n",
      "epoch 14, iter 0, loss 0.9768523573875427\n",
      "epoch 14, iter 1, loss 0.9825472831726074\n",
      "epoch 14, iter 2, loss 0.9750708341598511\n",
      "epoch 14, iter 3, loss 1.0066463947296143\n",
      "epoch 14, loss 3.9411168694496155\n",
      "epoch 15, iter 0, loss 0.9493103623390198\n",
      "epoch 15, iter 1, loss 0.9760942459106445\n",
      "epoch 15, iter 2, loss 1.0267236232757568\n",
      "epoch 15, iter 3, loss 1.0151582956314087\n",
      "epoch 15, loss 3.96728652715683\n",
      "epoch 16, iter 0, loss 0.9362640380859375\n",
      "epoch 16, iter 1, loss 0.9576906561851501\n",
      "epoch 16, iter 2, loss 0.9778106212615967\n",
      "epoch 16, iter 3, loss 0.9879123568534851\n",
      "epoch 16, loss 3.8596776723861694\n",
      "epoch 17, iter 0, loss 0.9354601502418518\n",
      "epoch 17, iter 1, loss 0.9654799699783325\n",
      "epoch 17, iter 2, loss 0.9551853537559509\n",
      "epoch 17, iter 3, loss 0.9391151070594788\n",
      "epoch 17, loss 3.795240581035614\n",
      "epoch 18, iter 0, loss 0.9380890130996704\n",
      "epoch 18, iter 1, loss 0.9055174589157104\n",
      "epoch 18, iter 2, loss 0.9329596757888794\n",
      "epoch 18, iter 3, loss 0.9750262498855591\n",
      "epoch 18, loss 3.7515923976898193\n",
      "epoch 19, iter 0, loss 0.9271507263183594\n",
      "epoch 19, iter 1, loss 0.9040980935096741\n",
      "epoch 19, iter 2, loss 0.9139477610588074\n",
      "epoch 19, iter 3, loss 0.9198776483535767\n",
      "epoch 19, loss 3.6650742292404175\n",
      "epoch 20, iter 0, loss 0.9082912802696228\n",
      "epoch 20, iter 1, loss 0.8839608430862427\n",
      "epoch 20, iter 2, loss 0.9191988110542297\n",
      "epoch 20, iter 3, loss 0.9340203404426575\n",
      "epoch 20, loss 3.6454712748527527\n",
      "epoch 21, iter 0, loss 0.8821844458580017\n",
      "epoch 21, iter 1, loss 0.8807690143585205\n",
      "epoch 21, iter 2, loss 0.9118796586990356\n",
      "epoch 21, iter 3, loss 0.9416592121124268\n",
      "epoch 21, loss 3.6164923310279846\n",
      "epoch 22, iter 0, loss 0.8754976391792297\n",
      "epoch 22, iter 1, loss 0.8997787833213806\n",
      "epoch 22, iter 2, loss 0.9095711708068848\n",
      "epoch 22, iter 3, loss 0.894865870475769\n",
      "epoch 22, loss 3.579713463783264\n",
      "epoch 23, iter 0, loss 0.8576269149780273\n",
      "epoch 23, iter 1, loss 0.8815438747406006\n",
      "epoch 23, iter 2, loss 0.9247397780418396\n",
      "epoch 23, iter 3, loss 0.9345680475234985\n",
      "epoch 23, loss 3.598478615283966\n",
      "epoch 24, iter 0, loss 0.8564018607139587\n",
      "epoch 24, iter 1, loss 0.8835752010345459\n",
      "epoch 24, iter 2, loss 0.9351253509521484\n",
      "epoch 24, iter 3, loss 0.9208188056945801\n",
      "epoch 24, loss 3.595921218395233\n",
      "epoch 25, iter 0, loss 0.9006557464599609\n",
      "epoch 25, iter 1, loss 0.8626670837402344\n",
      "epoch 25, iter 2, loss 0.933283269405365\n",
      "epoch 25, iter 3, loss 0.9235955476760864\n",
      "epoch 25, loss 3.6202016472816467\n",
      "epoch 26, iter 0, loss 0.8866674304008484\n",
      "epoch 26, iter 1, loss 0.8990508317947388\n",
      "epoch 26, iter 2, loss 0.9102485179901123\n",
      "epoch 26, iter 3, loss 0.9137535691261292\n",
      "epoch 26, loss 3.6097203493118286\n",
      "epoch 27, iter 0, loss 0.8879474401473999\n",
      "epoch 27, iter 1, loss 0.8942972421646118\n",
      "epoch 27, iter 2, loss 0.9229707717895508\n",
      "epoch 27, iter 3, loss 0.9327138066291809\n",
      "epoch 27, loss 3.6379292607307434\n",
      "epoch 28, iter 0, loss 0.9031685590744019\n",
      "epoch 28, iter 1, loss 0.9525728821754456\n",
      "epoch 28, iter 2, loss 0.8736206293106079\n",
      "epoch 28, iter 3, loss 0.9217298030853271\n",
      "epoch 28, loss 3.6510918736457825\n",
      "epoch 29, iter 0, loss 0.8946439027786255\n",
      "epoch 29, iter 1, loss 0.8910965919494629\n",
      "epoch 29, iter 2, loss 0.9473172426223755\n",
      "epoch 29, iter 3, loss 0.8870581388473511\n",
      "epoch 29, loss 3.620115876197815\n",
      "epoch 30, iter 0, loss 0.9006613492965698\n",
      "epoch 30, iter 1, loss 0.8616018295288086\n",
      "epoch 30, iter 2, loss 0.9226825833320618\n",
      "epoch 30, iter 3, loss 0.9214996099472046\n",
      "epoch 30, loss 3.6064453721046448\n",
      "epoch 31, iter 0, loss 0.8695923089981079\n",
      "epoch 31, iter 1, loss 0.8958898782730103\n",
      "epoch 31, iter 2, loss 0.9108220338821411\n",
      "epoch 31, iter 3, loss 0.8972126841545105\n",
      "epoch 31, loss 3.5735169053077698\n",
      "epoch 32, iter 0, loss 0.8848527073860168\n",
      "epoch 32, iter 1, loss 0.9045668840408325\n",
      "epoch 32, iter 2, loss 0.8845579028129578\n",
      "epoch 32, iter 3, loss 0.9293472170829773\n",
      "epoch 32, loss 3.6033247113227844\n",
      "epoch 33, iter 0, loss 0.8785687685012817\n",
      "epoch 33, iter 1, loss 0.8973144888877869\n",
      "epoch 33, iter 2, loss 0.9074298739433289\n",
      "epoch 33, iter 3, loss 0.9063912034034729\n",
      "epoch 33, loss 3.5897043347358704\n",
      "epoch 34, iter 0, loss 0.8770977854728699\n",
      "epoch 34, iter 1, loss 0.9113677144050598\n",
      "epoch 34, iter 2, loss 0.8827131390571594\n",
      "epoch 34, iter 3, loss 0.9016903638839722\n",
      "epoch 34, loss 3.5728690028190613\n",
      "epoch 35, iter 0, loss 0.8969573974609375\n",
      "epoch 35, iter 1, loss 0.8739272356033325\n",
      "epoch 35, iter 2, loss 0.909546971321106\n",
      "epoch 35, iter 3, loss 0.9202675223350525\n",
      "epoch 35, loss 3.6006991267204285\n",
      "epoch 36, iter 0, loss 0.8499877452850342\n",
      "epoch 36, iter 1, loss 0.9043786525726318\n",
      "epoch 36, iter 2, loss 0.9333434700965881\n",
      "epoch 36, iter 3, loss 0.9088121652603149\n",
      "epoch 36, loss 3.596522033214569\n",
      "epoch 37, iter 0, loss 0.8990951180458069\n",
      "epoch 37, iter 1, loss 0.906396210193634\n",
      "epoch 37, iter 2, loss 0.8857417702674866\n",
      "epoch 37, iter 3, loss 0.8852109909057617\n",
      "epoch 37, loss 3.576444089412689\n",
      "epoch 38, iter 0, loss 0.8650400638580322\n",
      "epoch 38, iter 1, loss 0.8998266458511353\n",
      "epoch 38, iter 2, loss 0.9018733501434326\n",
      "epoch 38, iter 3, loss 0.9193935990333557\n",
      "epoch 38, loss 3.586133658885956\n",
      "epoch 39, iter 0, loss 0.8994570970535278\n",
      "epoch 39, iter 1, loss 0.8881599307060242\n",
      "epoch 39, iter 2, loss 0.9107146859169006\n",
      "epoch 39, iter 3, loss 0.9028483629226685\n",
      "epoch 39, loss 3.601180076599121\n",
      "epoch 40, iter 0, loss 0.8765055537223816\n",
      "epoch 40, iter 1, loss 0.8860443830490112\n",
      "epoch 40, iter 2, loss 0.9074532985687256\n",
      "epoch 40, iter 3, loss 0.9374617338180542\n",
      "epoch 40, loss 3.6074649691581726\n",
      "epoch 41, iter 0, loss 0.928161084651947\n",
      "epoch 41, iter 1, loss 0.8853380680084229\n",
      "epoch 41, iter 2, loss 0.9062595963478088\n",
      "epoch 41, iter 3, loss 0.8820968866348267\n",
      "epoch 41, loss 3.6018556356430054\n",
      "epoch 42, iter 0, loss 0.8758492469787598\n",
      "epoch 42, iter 1, loss 0.8708938360214233\n",
      "epoch 42, iter 2, loss 0.9196294546127319\n",
      "epoch 42, iter 3, loss 0.9420669078826904\n",
      "epoch 42, loss 3.6084394454956055\n",
      "epoch 43, iter 0, loss 0.8917090892791748\n",
      "epoch 43, iter 1, loss 0.8890979886054993\n",
      "epoch 43, iter 2, loss 0.8893876075744629\n",
      "epoch 43, iter 3, loss 0.9496586322784424\n",
      "epoch 43, loss 3.6198533177375793\n",
      "epoch 44, iter 0, loss 0.9165807962417603\n",
      "epoch 44, iter 1, loss 0.8880734443664551\n",
      "epoch 44, iter 2, loss 0.9021406173706055\n",
      "epoch 44, iter 3, loss 0.9154577851295471\n",
      "epoch 44, loss 3.622252643108368\n",
      "epoch 45, iter 0, loss 0.9011563062667847\n",
      "epoch 45, iter 1, loss 0.8969438076019287\n",
      "epoch 45, iter 2, loss 0.894955575466156\n",
      "epoch 45, iter 3, loss 0.9150363206863403\n",
      "epoch 45, loss 3.6080920100212097\n",
      "epoch 46, iter 0, loss 0.8903448581695557\n",
      "epoch 46, iter 1, loss 0.9153220057487488\n",
      "epoch 46, iter 2, loss 0.8917195796966553\n",
      "epoch 46, iter 3, loss 0.9303790330886841\n",
      "epoch 46, loss 3.627765476703644\n",
      "epoch 47, iter 0, loss 0.887061595916748\n",
      "epoch 47, iter 1, loss 0.8886034488677979\n",
      "epoch 47, iter 2, loss 0.9056081771850586\n",
      "epoch 47, iter 3, loss 0.9179204702377319\n",
      "epoch 47, loss 3.5991936922073364\n",
      "epoch 48, iter 0, loss 0.8650763630867004\n",
      "epoch 48, iter 1, loss 0.9140670299530029\n",
      "epoch 48, iter 2, loss 0.899044930934906\n",
      "epoch 48, iter 3, loss 0.9250079393386841\n",
      "epoch 48, loss 3.6031962633132935\n",
      "epoch 49, iter 0, loss 0.9076589941978455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49, iter 1, loss 0.8990870714187622\n",
      "epoch 49, iter 2, loss 0.8837229013442993\n",
      "epoch 49, iter 3, loss 0.9142467379570007\n",
      "epoch 49, loss 3.6047157049179077\n",
      "epoch 50, iter 0, loss 0.8882521986961365\n",
      "epoch 50, iter 1, loss 0.9122630953788757\n",
      "epoch 50, iter 2, loss 0.882603108882904\n",
      "epoch 50, iter 3, loss 0.9142090678215027\n",
      "epoch 50, loss 3.597327470779419\n",
      "epoch 51, iter 0, loss 0.8891664743423462\n",
      "epoch 51, iter 1, loss 0.89658522605896\n",
      "epoch 51, iter 2, loss 0.9074223637580872\n",
      "epoch 51, iter 3, loss 0.9042773246765137\n",
      "epoch 51, loss 3.597451388835907\n",
      "epoch 52, iter 0, loss 0.9043841361999512\n",
      "epoch 52, iter 1, loss 0.88758784532547\n",
      "epoch 52, iter 2, loss 0.8711705207824707\n",
      "epoch 52, iter 3, loss 0.942577600479126\n",
      "epoch 52, loss 3.605720102787018\n",
      "epoch 53, iter 0, loss 0.8898921608924866\n",
      "epoch 53, iter 1, loss 0.8787022829055786\n",
      "epoch 53, iter 2, loss 0.8975670337677002\n",
      "epoch 53, iter 3, loss 0.9132896065711975\n",
      "epoch 53, loss 3.579451084136963\n",
      "epoch 54, iter 0, loss 0.8862184286117554\n",
      "epoch 54, iter 1, loss 0.8786347508430481\n",
      "epoch 54, iter 2, loss 0.9257979393005371\n",
      "epoch 54, iter 3, loss 0.9158780574798584\n",
      "epoch 54, loss 3.606529176235199\n",
      "epoch 55, iter 0, loss 0.8588641881942749\n",
      "epoch 55, iter 1, loss 0.8911125659942627\n",
      "epoch 55, iter 2, loss 0.9270490407943726\n",
      "epoch 55, iter 3, loss 0.9252748489379883\n",
      "epoch 55, loss 3.6023006439208984\n",
      "epoch 56, iter 0, loss 0.875518798828125\n",
      "epoch 56, iter 1, loss 0.8790158629417419\n",
      "epoch 56, iter 2, loss 0.9273838996887207\n",
      "epoch 56, iter 3, loss 0.9090749621391296\n",
      "epoch 56, loss 3.5909935235977173\n",
      "epoch 57, iter 0, loss 0.9177387952804565\n",
      "epoch 57, iter 1, loss 0.8877536058425903\n",
      "epoch 57, iter 2, loss 0.8993422985076904\n",
      "epoch 57, iter 3, loss 0.9053309559822083\n",
      "epoch 57, loss 3.6101656556129456\n",
      "epoch 58, iter 0, loss 0.8755892515182495\n",
      "epoch 58, iter 1, loss 0.9019955396652222\n",
      "epoch 58, iter 2, loss 0.9104008674621582\n",
      "epoch 58, iter 3, loss 0.9269062280654907\n",
      "epoch 58, loss 3.6148918867111206\n",
      "epoch 59, iter 0, loss 0.8856993913650513\n",
      "epoch 59, iter 1, loss 0.8894094228744507\n",
      "epoch 59, iter 2, loss 0.8981406688690186\n",
      "epoch 59, iter 3, loss 0.9199390411376953\n",
      "epoch 59, loss 3.593188524246216\n",
      "epoch 60, iter 0, loss 0.8849999904632568\n",
      "epoch 60, iter 1, loss 0.896594762802124\n",
      "epoch 60, iter 2, loss 0.9296759963035583\n",
      "epoch 60, iter 3, loss 0.8718193769454956\n",
      "epoch 60, loss 3.583090126514435\n",
      "epoch 61, iter 0, loss 0.8668478727340698\n",
      "epoch 61, iter 1, loss 0.9135825037956238\n",
      "epoch 61, iter 2, loss 0.9414623975753784\n",
      "epoch 61, iter 3, loss 0.8736475110054016\n",
      "epoch 61, loss 3.5955402851104736\n",
      "epoch 62, iter 0, loss 0.8833155632019043\n",
      "epoch 62, iter 1, loss 0.8968997001647949\n",
      "epoch 62, iter 2, loss 0.8985321521759033\n",
      "epoch 62, iter 3, loss 0.9239982962608337\n",
      "epoch 62, loss 3.6027457118034363\n",
      "epoch 63, iter 0, loss 0.8645370006561279\n",
      "epoch 63, iter 1, loss 0.8798142671585083\n",
      "epoch 63, iter 2, loss 0.9712026715278625\n",
      "epoch 63, iter 3, loss 0.8900163769721985\n",
      "epoch 63, loss 3.6055703163146973\n",
      "epoch 64, iter 0, loss 0.9033527970314026\n",
      "epoch 64, iter 1, loss 0.8924526572227478\n",
      "epoch 64, iter 2, loss 0.8967922329902649\n",
      "epoch 64, iter 3, loss 0.9173786640167236\n",
      "epoch 64, loss 3.609976351261139\n",
      "epoch 65, iter 0, loss 0.8863611817359924\n",
      "epoch 65, iter 1, loss 0.9276650547981262\n",
      "epoch 65, iter 2, loss 0.9093843102455139\n",
      "epoch 65, iter 3, loss 0.9157205820083618\n",
      "epoch 65, loss 3.6391311287879944\n",
      "epoch 66, iter 0, loss 0.9048631191253662\n",
      "epoch 66, iter 1, loss 0.8784582614898682\n",
      "epoch 66, iter 2, loss 0.9161815643310547\n",
      "epoch 66, iter 3, loss 0.9309241771697998\n",
      "epoch 66, loss 3.630427122116089\n",
      "epoch 67, iter 0, loss 0.8698773980140686\n",
      "epoch 67, iter 1, loss 0.9136791229248047\n",
      "epoch 67, iter 2, loss 0.9055430293083191\n",
      "epoch 67, iter 3, loss 0.9242318868637085\n",
      "epoch 67, loss 3.613331437110901\n",
      "epoch 68, iter 0, loss 0.884229302406311\n",
      "epoch 68, iter 1, loss 0.8974639773368835\n",
      "epoch 68, iter 2, loss 0.9003175497055054\n",
      "epoch 68, iter 3, loss 0.9250304102897644\n",
      "epoch 68, loss 3.6070412397384644\n",
      "epoch 69, iter 0, loss 0.9056028127670288\n",
      "epoch 69, iter 1, loss 0.9095679521560669\n",
      "epoch 69, iter 2, loss 0.8852288722991943\n",
      "epoch 69, iter 3, loss 0.8904152512550354\n",
      "epoch 69, loss 3.5908148884773254\n",
      "epoch 70, iter 0, loss 0.8742014169692993\n",
      "epoch 70, iter 1, loss 0.9042498469352722\n",
      "epoch 70, iter 2, loss 0.9336345195770264\n",
      "epoch 70, iter 3, loss 0.8763699531555176\n",
      "epoch 70, loss 3.5884557366371155\n",
      "epoch 71, iter 0, loss 0.8811120986938477\n",
      "epoch 71, iter 1, loss 0.913779079914093\n",
      "epoch 71, iter 2, loss 0.9163438677787781\n",
      "epoch 71, iter 3, loss 0.890118420124054\n",
      "epoch 71, loss 3.6013534665107727\n",
      "epoch 72, iter 0, loss 0.8691948652267456\n",
      "epoch 72, iter 1, loss 0.9184616804122925\n",
      "epoch 72, iter 2, loss 0.8890265226364136\n",
      "epoch 72, iter 3, loss 0.9180306196212769\n",
      "epoch 72, loss 3.5947136878967285\n",
      "epoch 73, iter 0, loss 0.8811604976654053\n",
      "epoch 73, iter 1, loss 0.8999010324478149\n",
      "epoch 73, iter 2, loss 0.9364356398582458\n",
      "epoch 73, iter 3, loss 0.8787593841552734\n",
      "epoch 73, loss 3.5962565541267395\n",
      "epoch 74, iter 0, loss 0.894348680973053\n",
      "epoch 74, iter 1, loss 0.8972192406654358\n",
      "epoch 74, iter 2, loss 0.8806357979774475\n",
      "epoch 74, iter 3, loss 0.9347575902938843\n",
      "epoch 74, loss 3.6069613099098206\n",
      "epoch 75, iter 0, loss 0.8527289628982544\n",
      "epoch 75, iter 1, loss 0.9109762907028198\n",
      "epoch 75, iter 2, loss 0.8832011222839355\n",
      "epoch 75, iter 3, loss 0.9553076028823853\n",
      "epoch 75, loss 3.602213978767395\n",
      "epoch 76, iter 0, loss 0.8705028295516968\n",
      "epoch 76, iter 1, loss 0.9024785757064819\n",
      "epoch 76, iter 2, loss 0.937317967414856\n",
      "epoch 76, iter 3, loss 0.909205973148346\n",
      "epoch 76, loss 3.6195053458213806\n",
      "epoch 77, iter 0, loss 0.8598403930664062\n",
      "epoch 77, iter 1, loss 0.8945363759994507\n",
      "epoch 77, iter 2, loss 0.9091624617576599\n",
      "epoch 77, iter 3, loss 0.9090635776519775\n",
      "epoch 77, loss 3.5726028084754944\n",
      "epoch 78, iter 0, loss 0.8768723011016846\n",
      "epoch 78, iter 1, loss 0.8832896947860718\n",
      "epoch 78, iter 2, loss 0.8876587748527527\n",
      "epoch 78, iter 3, loss 0.9271795749664307\n",
      "epoch 78, loss 3.5750003457069397\n",
      "epoch 79, iter 0, loss 0.8916208148002625\n",
      "epoch 79, iter 1, loss 0.8996371030807495\n",
      "epoch 79, iter 2, loss 0.9147065877914429\n",
      "epoch 79, iter 3, loss 0.9023221135139465\n",
      "epoch 79, loss 3.6082866191864014\n",
      "epoch 80, iter 0, loss 0.8723892569541931\n",
      "epoch 80, iter 1, loss 0.91444993019104\n",
      "epoch 80, iter 2, loss 0.9004623889923096\n",
      "epoch 80, iter 3, loss 0.9129464626312256\n",
      "epoch 80, loss 3.6002480387687683\n",
      "epoch 81, iter 0, loss 0.8912305235862732\n",
      "epoch 81, iter 1, loss 0.8835532069206238\n",
      "epoch 81, iter 2, loss 0.8938212990760803\n",
      "epoch 81, iter 3, loss 0.9229668378829956\n",
      "epoch 81, loss 3.591571867465973\n",
      "epoch 82, iter 0, loss 0.8905144333839417\n",
      "epoch 82, iter 1, loss 0.8775556087493896\n",
      "epoch 82, iter 2, loss 0.9401713013648987\n",
      "epoch 82, iter 3, loss 0.9110506772994995\n",
      "epoch 82, loss 3.6192920207977295\n",
      "epoch 83, iter 0, loss 0.8900009989738464\n",
      "epoch 83, iter 1, loss 0.8988034725189209\n",
      "epoch 83, iter 2, loss 0.891175389289856\n",
      "epoch 83, iter 3, loss 0.9195655584335327\n",
      "epoch 83, loss 3.599545419216156\n",
      "epoch 84, iter 0, loss 0.935491681098938\n",
      "epoch 84, iter 1, loss 0.8712306618690491\n",
      "epoch 84, iter 2, loss 0.8968786597251892\n",
      "epoch 84, iter 3, loss 0.9027739763259888\n",
      "epoch 84, loss 3.606374979019165\n",
      "epoch 85, iter 0, loss 0.8783730864524841\n",
      "epoch 85, iter 1, loss 0.9017302393913269\n",
      "epoch 85, iter 2, loss 0.9250831604003906\n",
      "epoch 85, iter 3, loss 0.9462023973464966\n",
      "epoch 85, loss 3.6513888835906982\n",
      "epoch 86, iter 0, loss 0.8809081315994263\n",
      "epoch 86, iter 1, loss 0.9035273194313049\n",
      "epoch 86, iter 2, loss 0.9292439818382263\n",
      "epoch 86, iter 3, loss 0.9093678593635559\n",
      "epoch 86, loss 3.6230472922325134\n",
      "epoch 87, iter 0, loss 0.9176483154296875\n",
      "epoch 87, iter 1, loss 0.8669605851173401\n",
      "epoch 87, iter 2, loss 0.9415909051895142\n",
      "epoch 87, iter 3, loss 0.9292434453964233\n",
      "epoch 87, loss 3.655443251132965\n",
      "epoch 88, iter 0, loss 0.8835858106613159\n",
      "epoch 88, iter 1, loss 0.9279083609580994\n",
      "epoch 88, iter 2, loss 0.9106207489967346\n",
      "epoch 88, iter 3, loss 0.9016914367675781\n",
      "epoch 88, loss 3.623806357383728\n",
      "epoch 89, iter 0, loss 0.8433781862258911\n",
      "epoch 89, iter 1, loss 0.9306606650352478\n",
      "epoch 89, iter 2, loss 0.8914045095443726\n",
      "epoch 89, iter 3, loss 0.9619239568710327\n",
      "epoch 89, loss 3.627367317676544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90, iter 0, loss 0.9021950364112854\n",
      "epoch 90, iter 1, loss 0.897331953048706\n",
      "epoch 90, iter 2, loss 0.8983500599861145\n",
      "epoch 90, iter 3, loss 0.8967030048370361\n",
      "epoch 90, loss 3.594580054283142\n",
      "epoch 91, iter 0, loss 0.8866302967071533\n",
      "epoch 91, iter 1, loss 0.8641763925552368\n",
      "epoch 91, iter 2, loss 0.9117059111595154\n",
      "epoch 91, iter 3, loss 0.9180312156677246\n",
      "epoch 91, loss 3.58054381608963\n",
      "epoch 92, iter 0, loss 0.8845195174217224\n",
      "epoch 92, iter 1, loss 0.9055966138839722\n",
      "epoch 92, iter 2, loss 0.8851483464241028\n",
      "epoch 92, iter 3, loss 0.9207807183265686\n",
      "epoch 92, loss 3.596045196056366\n",
      "epoch 93, iter 0, loss 0.8732120394706726\n",
      "epoch 93, iter 1, loss 0.9075348973274231\n",
      "epoch 93, iter 2, loss 0.90217125415802\n",
      "epoch 93, iter 3, loss 0.9136922359466553\n",
      "epoch 93, loss 3.596610426902771\n",
      "epoch 94, iter 0, loss 0.8648943901062012\n",
      "epoch 94, iter 1, loss 0.9108179807662964\n",
      "epoch 94, iter 2, loss 0.9134995341300964\n",
      "epoch 94, iter 3, loss 0.9326533079147339\n",
      "epoch 94, loss 3.621865212917328\n",
      "epoch 95, iter 0, loss 0.8744034171104431\n",
      "epoch 95, iter 1, loss 0.892277717590332\n",
      "epoch 95, iter 2, loss 0.8852866888046265\n",
      "epoch 95, iter 3, loss 0.9635005593299866\n",
      "epoch 95, loss 3.615468382835388\n",
      "epoch 96, iter 0, loss 0.8986095190048218\n",
      "epoch 96, iter 1, loss 0.9043724536895752\n",
      "epoch 96, iter 2, loss 0.8843786716461182\n",
      "epoch 96, iter 3, loss 0.9038265347480774\n",
      "epoch 96, loss 3.5911871790885925\n",
      "epoch 97, iter 0, loss 0.870598316192627\n",
      "epoch 97, iter 1, loss 0.8815620541572571\n",
      "epoch 97, iter 2, loss 0.9252788424491882\n",
      "epoch 97, iter 3, loss 0.8997262120246887\n",
      "epoch 97, loss 3.577165424823761\n",
      "epoch 98, iter 0, loss 0.8660947680473328\n",
      "epoch 98, iter 1, loss 0.8896273374557495\n",
      "epoch 98, iter 2, loss 0.9303362369537354\n",
      "epoch 98, iter 3, loss 0.8906453251838684\n",
      "epoch 98, loss 3.576703667640686\n",
      "epoch 99, iter 0, loss 0.8884807825088501\n",
      "epoch 99, iter 1, loss 0.8772692084312439\n",
      "epoch 99, iter 2, loss 0.9088912010192871\n",
      "epoch 99, iter 3, loss 0.9253155589103699\n",
      "epoch 99, loss 3.599956750869751\n",
      "2019-03-31 02:38:06.888969, fold=1, rep=0, eta=0d 0h 39m 2s \n",
      "{'fold': 1, 'repeat': 0, 'n': 5875, 'd': 18, 'mse': 2.2664735317230225, 'train_time': 760.9639031612314, 'trained_epochs': 100, 'prior_train_nmll': 0.9496549367904663, 'train_nll': 2465.58251953125, 'test_nll': 3092.461181640625, 'train_mse': 0.19319455325603485, 'state_dict_file': 'model_state_dict_2502523686176575464.pkl'}\n",
      "epoch 0, iter 0, loss 1.9283337593078613\n",
      "epoch 0, iter 1, loss 2.2532105445861816\n",
      "epoch 0, iter 2, loss 2.118119478225708\n",
      "epoch 0, iter 3, loss 1.8750780820846558\n",
      "epoch 0, loss 8.174741864204407\n",
      "epoch 1, iter 0, loss 1.9252910614013672\n",
      "epoch 1, iter 1, loss 1.8963369131088257\n",
      "epoch 1, iter 2, loss 1.738220453262329\n",
      "epoch 1, iter 3, loss 1.5971976518630981\n",
      "epoch 1, loss 7.15704607963562\n",
      "epoch 2, iter 0, loss 1.5966378450393677\n",
      "epoch 2, iter 1, loss 1.5828224420547485\n",
      "epoch 2, iter 2, loss 1.5353002548217773\n",
      "epoch 2, iter 3, loss 1.5198974609375\n",
      "epoch 2, loss 6.2346580028533936\n",
      "epoch 3, iter 0, loss 1.462471842765808\n",
      "epoch 3, iter 1, loss 1.4580721855163574\n",
      "epoch 3, iter 2, loss 1.4479700326919556\n",
      "epoch 3, iter 3, loss 1.4174296855926514\n",
      "epoch 3, loss 5.7859437465667725\n",
      "epoch 4, iter 0, loss 1.38919198513031\n",
      "epoch 4, iter 1, loss 1.3711305856704712\n",
      "epoch 4, iter 2, loss 1.3596582412719727\n",
      "epoch 4, iter 3, loss 1.3460091352462769\n",
      "epoch 4, loss 5.465989947319031\n",
      "epoch 5, iter 0, loss 1.3313591480255127\n",
      "epoch 5, iter 1, loss 1.2955936193466187\n",
      "epoch 5, iter 2, loss 1.2846579551696777\n",
      "epoch 5, iter 3, loss 1.2738252878189087\n",
      "epoch 5, loss 5.185436010360718\n",
      "epoch 6, iter 0, loss 1.2467682361602783\n",
      "epoch 6, iter 1, loss 1.221847414970398\n",
      "epoch 6, iter 2, loss 1.2166216373443604\n",
      "epoch 6, iter 3, loss 1.21880304813385\n",
      "epoch 6, loss 4.904040336608887\n",
      "epoch 7, iter 0, loss 1.1755800247192383\n",
      "epoch 7, iter 1, loss 1.1545600891113281\n",
      "epoch 7, iter 2, loss 1.140748381614685\n",
      "epoch 7, iter 3, loss 1.141829013824463\n",
      "epoch 7, loss 4.612717509269714\n",
      "epoch 8, iter 0, loss 1.0985914468765259\n",
      "epoch 8, iter 1, loss 1.0881829261779785\n",
      "epoch 8, iter 2, loss 1.0758535861968994\n",
      "epoch 8, iter 3, loss 1.0519859790802002\n",
      "epoch 8, loss 4.314613938331604\n",
      "epoch 9, iter 0, loss 1.0339726209640503\n",
      "epoch 9, iter 1, loss 1.0175063610076904\n",
      "epoch 9, iter 2, loss 1.0146369934082031\n",
      "epoch 9, iter 3, loss 0.9836996793746948\n",
      "epoch 9, loss 4.049815654754639\n",
      "epoch 10, iter 0, loss 0.9533743858337402\n",
      "epoch 10, iter 1, loss 0.9665990471839905\n",
      "epoch 10, iter 2, loss 0.9854364395141602\n",
      "epoch 10, iter 3, loss 0.9572112560272217\n",
      "epoch 10, loss 3.8626211285591125\n",
      "epoch 11, iter 0, loss 0.9053235054016113\n",
      "epoch 11, iter 1, loss 0.9491428136825562\n",
      "epoch 11, iter 2, loss 0.9682097434997559\n",
      "epoch 11, iter 3, loss 0.9600064754486084\n",
      "epoch 11, loss 3.7826825380325317\n",
      "epoch 12, iter 0, loss 0.9337256550788879\n",
      "epoch 12, iter 1, loss 0.9098332524299622\n",
      "epoch 12, iter 2, loss 1.0114428997039795\n",
      "epoch 12, iter 3, loss 0.9809600710868835\n",
      "epoch 12, loss 3.835961878299713\n",
      "epoch 13, iter 0, loss 0.9690570831298828\n",
      "epoch 13, iter 1, loss 1.009029507637024\n",
      "epoch 13, iter 2, loss 0.96847003698349\n",
      "epoch 13, iter 3, loss 1.0152133703231812\n",
      "epoch 13, loss 3.961769998073578\n",
      "epoch 14, iter 0, loss 0.9760396480560303\n",
      "epoch 14, iter 1, loss 1.0127131938934326\n",
      "epoch 14, iter 2, loss 1.0040724277496338\n",
      "epoch 14, iter 3, loss 0.9895654320716858\n",
      "epoch 14, loss 3.9823907017707825\n",
      "epoch 15, iter 0, loss 0.9236494302749634\n",
      "epoch 15, iter 1, loss 1.0003654956817627\n",
      "epoch 15, iter 2, loss 0.9958537817001343\n",
      "epoch 15, iter 3, loss 1.0488791465759277\n",
      "epoch 15, loss 3.968747854232788\n",
      "epoch 16, iter 0, loss 0.9467471837997437\n",
      "epoch 16, iter 1, loss 0.9576209187507629\n",
      "epoch 16, iter 2, loss 1.0087838172912598\n",
      "epoch 16, iter 3, loss 0.979325532913208\n",
      "epoch 16, loss 3.8924774527549744\n",
      "epoch 17, iter 0, loss 0.9213550090789795\n",
      "epoch 17, iter 1, loss 0.9741466045379639\n",
      "epoch 17, iter 2, loss 0.9442702531814575\n",
      "epoch 17, iter 3, loss 0.9697991609573364\n",
      "epoch 17, loss 3.8095710277557373\n",
      "epoch 18, iter 0, loss 0.9316357374191284\n",
      "epoch 18, iter 1, loss 0.9338736534118652\n",
      "epoch 18, iter 2, loss 0.9455491304397583\n",
      "epoch 18, iter 3, loss 0.9476715922355652\n",
      "epoch 18, loss 3.758730113506317\n",
      "epoch 19, iter 0, loss 0.9196455478668213\n",
      "epoch 19, iter 1, loss 0.9162006974220276\n",
      "epoch 19, iter 2, loss 0.9391461610794067\n",
      "epoch 19, iter 3, loss 0.9053640365600586\n",
      "epoch 19, loss 3.680356442928314\n",
      "epoch 20, iter 0, loss 0.898615837097168\n",
      "epoch 20, iter 1, loss 0.9077039957046509\n",
      "epoch 20, iter 2, loss 0.9472819566726685\n",
      "epoch 20, iter 3, loss 0.8836669921875\n",
      "epoch 20, loss 3.6372687816619873\n",
      "epoch 21, iter 0, loss 0.8994423747062683\n",
      "epoch 21, iter 1, loss 0.8768671154975891\n",
      "epoch 21, iter 2, loss 0.951820969581604\n",
      "epoch 21, iter 3, loss 0.8681174516677856\n",
      "epoch 21, loss 3.596247911453247\n",
      "epoch 22, iter 0, loss 0.8724374175071716\n",
      "epoch 22, iter 1, loss 0.8977227807044983\n",
      "epoch 22, iter 2, loss 0.9134706854820251\n",
      "epoch 22, iter 3, loss 0.9163322448730469\n",
      "epoch 22, loss 3.599963128566742\n",
      "epoch 23, iter 0, loss 0.8601933717727661\n",
      "epoch 23, iter 1, loss 0.9024449586868286\n",
      "epoch 23, iter 2, loss 0.9587734341621399\n",
      "epoch 23, iter 3, loss 0.8769118785858154\n",
      "epoch 23, loss 3.59832364320755\n",
      "epoch 24, iter 0, loss 0.860845685005188\n",
      "epoch 24, iter 1, loss 0.9220584630966187\n",
      "epoch 24, iter 2, loss 0.9339424967765808\n",
      "epoch 24, iter 3, loss 0.9197596907615662\n",
      "epoch 24, loss 3.6366063356399536\n",
      "epoch 25, iter 0, loss 0.8801350593566895\n",
      "epoch 25, iter 1, loss 0.9576950669288635\n",
      "epoch 25, iter 2, loss 0.8892804980278015\n",
      "epoch 25, iter 3, loss 0.9325746893882751\n",
      "epoch 25, loss 3.6596853137016296\n",
      "epoch 26, iter 0, loss 0.9134173393249512\n",
      "epoch 26, iter 1, loss 0.9072849154472351\n",
      "epoch 26, iter 2, loss 0.8986557722091675\n",
      "epoch 26, iter 3, loss 0.921762228012085\n",
      "epoch 26, loss 3.6411202549934387\n",
      "epoch 27, iter 0, loss 0.8777784109115601\n",
      "epoch 27, iter 1, loss 0.8899669051170349\n",
      "epoch 27, iter 2, loss 0.933995246887207\n",
      "epoch 27, iter 3, loss 0.930674135684967\n",
      "epoch 27, loss 3.632414698600769\n",
      "epoch 28, iter 0, loss 0.9207069873809814\n",
      "epoch 28, iter 1, loss 0.9053856134414673\n",
      "epoch 28, iter 2, loss 0.9044764637947083\n",
      "epoch 28, iter 3, loss 0.8916977047920227\n",
      "epoch 28, loss 3.6222667694091797\n",
      "epoch 29, iter 0, loss 0.8543611764907837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29, iter 1, loss 0.8819228410720825\n",
      "epoch 29, iter 2, loss 0.9678045511245728\n",
      "epoch 29, iter 3, loss 0.9230091571807861\n",
      "epoch 29, loss 3.627097725868225\n",
      "epoch 30, iter 0, loss 0.8768991231918335\n",
      "epoch 30, iter 1, loss 0.8810592889785767\n",
      "epoch 30, iter 2, loss 0.9179627895355225\n",
      "epoch 30, iter 3, loss 0.9684433937072754\n",
      "epoch 30, loss 3.644364595413208\n",
      "epoch 31, iter 0, loss 0.8790592551231384\n",
      "epoch 31, iter 1, loss 0.879610002040863\n",
      "epoch 31, iter 2, loss 0.9370157122612\n",
      "epoch 31, iter 3, loss 0.9132258892059326\n",
      "epoch 31, loss 3.608910858631134\n",
      "epoch 32, iter 0, loss 0.8694071769714355\n",
      "epoch 32, iter 1, loss 0.897335946559906\n",
      "epoch 32, iter 2, loss 0.9333135485649109\n",
      "epoch 32, iter 3, loss 0.9196609854698181\n",
      "epoch 32, loss 3.6197176575660706\n",
      "epoch 33, iter 0, loss 0.8961111903190613\n",
      "epoch 33, iter 1, loss 0.9215742349624634\n",
      "epoch 33, iter 2, loss 0.8808410167694092\n",
      "epoch 33, iter 3, loss 0.8988609910011292\n",
      "epoch 33, loss 3.597387433052063\n",
      "epoch 34, iter 0, loss 0.8905931115150452\n",
      "epoch 34, iter 1, loss 0.8747207522392273\n",
      "epoch 34, iter 2, loss 0.9345757961273193\n",
      "epoch 34, iter 3, loss 0.9149038791656494\n",
      "epoch 34, loss 3.614793539047241\n",
      "epoch 35, iter 0, loss 0.8824902772903442\n",
      "epoch 35, iter 1, loss 0.8823455572128296\n",
      "epoch 35, iter 2, loss 0.9371646046638489\n",
      "epoch 35, iter 3, loss 0.9017305374145508\n",
      "epoch 35, loss 3.6037309765815735\n",
      "epoch 36, iter 0, loss 0.8819119930267334\n",
      "epoch 36, iter 1, loss 0.8890513181686401\n",
      "epoch 36, iter 2, loss 0.9308180212974548\n",
      "epoch 36, iter 3, loss 0.883473813533783\n",
      "epoch 36, loss 3.5852551460266113\n",
      "epoch 37, iter 0, loss 0.8736802935600281\n",
      "epoch 37, iter 1, loss 0.8658221364021301\n",
      "epoch 37, iter 2, loss 0.9357235431671143\n",
      "epoch 37, iter 3, loss 0.9245513677597046\n",
      "epoch 37, loss 3.599777340888977\n",
      "epoch 38, iter 0, loss 0.8665410876274109\n",
      "epoch 38, iter 1, loss 0.9006415009498596\n",
      "epoch 38, iter 2, loss 0.9379096031188965\n",
      "epoch 38, iter 3, loss 0.9163188934326172\n",
      "epoch 38, loss 3.621411085128784\n",
      "epoch 39, iter 0, loss 0.885585606098175\n",
      "epoch 39, iter 1, loss 0.9020868539810181\n",
      "epoch 39, iter 2, loss 0.9090355634689331\n",
      "epoch 39, iter 3, loss 0.9378913044929504\n",
      "epoch 39, loss 3.6345993280410767\n",
      "epoch 40, iter 0, loss 0.9022299647331238\n",
      "epoch 40, iter 1, loss 0.8670093417167664\n",
      "epoch 40, iter 2, loss 0.9257885217666626\n",
      "epoch 40, iter 3, loss 0.9469045400619507\n",
      "epoch 40, loss 3.6419323682785034\n",
      "epoch 41, iter 0, loss 0.927086353302002\n",
      "epoch 41, iter 1, loss 0.8920400142669678\n",
      "epoch 41, iter 2, loss 0.8889238834381104\n",
      "epoch 41, iter 3, loss 0.9280686378479004\n",
      "epoch 41, loss 3.6361188888549805\n",
      "epoch 42, iter 0, loss 0.8996909856796265\n",
      "epoch 42, iter 1, loss 0.9094105362892151\n",
      "epoch 42, iter 2, loss 0.9098621010780334\n",
      "epoch 42, iter 3, loss 0.9010375738143921\n",
      "epoch 42, loss 3.620001196861267\n",
      "epoch 43, iter 0, loss 0.9036250710487366\n",
      "epoch 43, iter 1, loss 0.8802873492240906\n",
      "epoch 43, iter 2, loss 0.9343200922012329\n",
      "epoch 43, iter 3, loss 0.8944190144538879\n",
      "epoch 43, loss 3.612651526927948\n",
      "epoch 44, iter 0, loss 0.8693045973777771\n",
      "epoch 44, iter 1, loss 0.8774799108505249\n",
      "epoch 44, iter 2, loss 0.9270548820495605\n",
      "epoch 44, iter 3, loss 0.9268186092376709\n",
      "epoch 44, loss 3.6006579995155334\n",
      "epoch 45, iter 0, loss 0.8964933156967163\n",
      "epoch 45, iter 1, loss 0.9115274548530579\n",
      "epoch 45, iter 2, loss 0.9019416570663452\n",
      "epoch 45, iter 3, loss 0.9062561392784119\n",
      "epoch 45, loss 3.6162185668945312\n",
      "epoch 46, iter 0, loss 0.9064406752586365\n",
      "epoch 46, iter 1, loss 0.8829353451728821\n",
      "epoch 46, iter 2, loss 0.890724778175354\n",
      "epoch 46, iter 3, loss 0.9209282398223877\n",
      "epoch 46, loss 3.6010290384292603\n",
      "epoch 47, iter 0, loss 0.8992922306060791\n",
      "epoch 47, iter 1, loss 0.8951106071472168\n",
      "epoch 47, iter 2, loss 0.9197263717651367\n",
      "epoch 47, iter 3, loss 0.9159159064292908\n",
      "epoch 47, loss 3.6300451159477234\n",
      "epoch 48, iter 0, loss 0.8954147100448608\n",
      "epoch 48, iter 1, loss 0.8700966238975525\n",
      "epoch 48, iter 2, loss 0.8900316953659058\n",
      "epoch 48, iter 3, loss 0.9527416229248047\n",
      "epoch 48, loss 3.608284652233124\n",
      "epoch 49, iter 0, loss 0.8555625677108765\n",
      "epoch 49, iter 1, loss 0.90616774559021\n",
      "epoch 49, iter 2, loss 0.9214733242988586\n",
      "epoch 49, iter 3, loss 0.9236303567886353\n",
      "epoch 49, loss 3.6068339943885803\n",
      "epoch 50, iter 0, loss 0.8997259140014648\n",
      "epoch 50, iter 1, loss 0.8859257698059082\n",
      "epoch 50, iter 2, loss 0.9005539417266846\n",
      "epoch 50, iter 3, loss 0.8909868597984314\n",
      "epoch 50, loss 3.577192485332489\n",
      "epoch 51, iter 0, loss 0.8761770725250244\n",
      "epoch 51, iter 1, loss 0.8860296607017517\n",
      "epoch 51, iter 2, loss 0.8914040327072144\n",
      "epoch 51, iter 3, loss 0.9479753375053406\n",
      "epoch 51, loss 3.601586103439331\n",
      "epoch 52, iter 0, loss 0.877944827079773\n",
      "epoch 52, iter 1, loss 0.8997492790222168\n",
      "epoch 52, iter 2, loss 0.8889787793159485\n",
      "epoch 52, iter 3, loss 0.9468544721603394\n",
      "epoch 52, loss 3.6135273575782776\n",
      "epoch 53, iter 0, loss 0.9167382121086121\n",
      "epoch 53, iter 1, loss 0.8867088556289673\n",
      "epoch 53, iter 2, loss 0.8698716163635254\n",
      "epoch 53, iter 3, loss 0.9663184881210327\n",
      "epoch 53, loss 3.6396371722221375\n",
      "epoch 54, iter 0, loss 0.8960338234901428\n",
      "epoch 54, iter 1, loss 0.8958522081375122\n",
      "epoch 54, iter 2, loss 0.9343851804733276\n",
      "epoch 54, iter 3, loss 0.912939190864563\n",
      "epoch 54, loss 3.6392104029655457\n",
      "epoch 55, iter 0, loss 0.8916112780570984\n",
      "epoch 55, iter 1, loss 0.9015554189682007\n",
      "epoch 55, iter 2, loss 0.9040273427963257\n",
      "epoch 55, iter 3, loss 0.9318823218345642\n",
      "epoch 55, loss 3.629076361656189\n",
      "epoch 56, iter 0, loss 0.8927595615386963\n",
      "epoch 56, iter 1, loss 0.8836079835891724\n",
      "epoch 56, iter 2, loss 0.9284844398498535\n",
      "epoch 56, iter 3, loss 0.9108242392539978\n",
      "epoch 56, loss 3.61567622423172\n",
      "epoch 57, iter 0, loss 0.8715270757675171\n",
      "epoch 57, iter 1, loss 0.9279714226722717\n",
      "epoch 57, iter 2, loss 0.9060486555099487\n",
      "epoch 57, iter 3, loss 0.9184059500694275\n",
      "epoch 57, loss 3.623953104019165\n",
      "epoch 58, iter 0, loss 0.8814866542816162\n",
      "epoch 58, iter 1, loss 0.9044857621192932\n",
      "epoch 58, iter 2, loss 0.8923068046569824\n",
      "epoch 58, iter 3, loss 0.94405198097229\n",
      "epoch 58, loss 3.622331202030182\n",
      "epoch 59, iter 0, loss 0.9002115726470947\n",
      "epoch 59, iter 1, loss 0.8934026956558228\n",
      "epoch 59, iter 2, loss 0.9572754502296448\n",
      "epoch 59, iter 3, loss 0.8634306192398071\n",
      "epoch 59, loss 3.6143203377723694\n",
      "epoch 60, iter 0, loss 0.9009389281272888\n",
      "epoch 60, iter 1, loss 0.8963333368301392\n",
      "epoch 60, iter 2, loss 0.9111585021018982\n",
      "epoch 60, iter 3, loss 0.9056669473648071\n",
      "epoch 60, loss 3.6140977144241333\n",
      "epoch 61, iter 0, loss 0.8968483209609985\n",
      "epoch 61, iter 1, loss 0.88084876537323\n",
      "epoch 61, iter 2, loss 0.8846350312232971\n",
      "epoch 61, iter 3, loss 0.9586220979690552\n",
      "epoch 61, loss 3.620954215526581\n",
      "epoch 62, iter 0, loss 0.8515006303787231\n",
      "epoch 62, iter 1, loss 0.9066762328147888\n",
      "epoch 62, iter 2, loss 0.9096786379814148\n",
      "epoch 62, iter 3, loss 0.9271725416183472\n",
      "epoch 62, loss 3.595028042793274\n",
      "epoch 63, iter 0, loss 0.8953384160995483\n",
      "epoch 63, iter 1, loss 0.8965359926223755\n",
      "epoch 63, iter 2, loss 0.878910481929779\n",
      "epoch 63, iter 3, loss 0.941422164440155\n",
      "epoch 63, loss 3.612207055091858\n",
      "epoch 64, iter 0, loss 0.8752990961074829\n",
      "epoch 64, iter 1, loss 0.9051499962806702\n",
      "epoch 64, iter 2, loss 0.8589009046554565\n",
      "epoch 64, iter 3, loss 0.9376495480537415\n",
      "epoch 64, loss 3.576999545097351\n",
      "epoch 65, iter 0, loss 0.8911014199256897\n",
      "epoch 65, iter 1, loss 0.9079495668411255\n",
      "epoch 65, iter 2, loss 0.8865699768066406\n",
      "epoch 65, iter 3, loss 0.8890577554702759\n",
      "epoch 65, loss 3.5746787190437317\n",
      "epoch 66, iter 0, loss 0.849500298500061\n",
      "epoch 66, iter 1, loss 0.8937566876411438\n",
      "epoch 66, iter 2, loss 0.895814061164856\n",
      "epoch 66, iter 3, loss 0.8992136716842651\n",
      "epoch 66, loss 3.538284718990326\n",
      "epoch 67, iter 0, loss 0.9006527662277222\n",
      "epoch 67, iter 1, loss 0.8820921778678894\n",
      "epoch 67, iter 2, loss 0.9009958505630493\n",
      "epoch 67, iter 3, loss 0.864799439907074\n",
      "epoch 67, loss 3.548540234565735\n",
      "epoch 68, iter 0, loss 0.8480252027511597\n",
      "epoch 68, iter 1, loss 0.9157453775405884\n",
      "epoch 68, iter 2, loss 0.8929716348648071\n",
      "epoch 68, iter 3, loss 0.9177089333534241\n",
      "epoch 68, loss 3.5744511485099792\n",
      "epoch 69, iter 0, loss 0.8872666954994202\n",
      "epoch 69, iter 1, loss 0.8996402025222778\n",
      "epoch 69, iter 2, loss 0.8944262862205505\n",
      "epoch 69, iter 3, loss 0.9316137433052063\n",
      "epoch 69, loss 3.612946927547455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70, iter 0, loss 0.8710489869117737\n",
      "epoch 70, iter 1, loss 0.8911514282226562\n",
      "epoch 70, iter 2, loss 0.919543981552124\n",
      "epoch 70, iter 3, loss 0.9135905504226685\n",
      "epoch 70, loss 3.5953349471092224\n",
      "epoch 71, iter 0, loss 0.8835558891296387\n",
      "epoch 71, iter 1, loss 0.9138543009757996\n",
      "epoch 71, iter 2, loss 0.8783167600631714\n",
      "epoch 71, iter 3, loss 0.9325937032699585\n",
      "epoch 71, loss 3.608320653438568\n",
      "epoch 72, iter 0, loss 0.8979671597480774\n",
      "epoch 72, iter 1, loss 0.9075038433074951\n",
      "epoch 72, iter 2, loss 0.8899964690208435\n",
      "epoch 72, iter 3, loss 0.9140840172767639\n",
      "epoch 72, loss 3.60955148935318\n",
      "epoch 73, iter 0, loss 0.8724365234375\n",
      "epoch 73, iter 1, loss 0.9052136540412903\n",
      "epoch 73, iter 2, loss 0.8991721868515015\n",
      "epoch 73, iter 3, loss 0.9545024633407593\n",
      "epoch 73, loss 3.631324827671051\n",
      "epoch 74, iter 0, loss 0.8952099084854126\n",
      "epoch 74, iter 1, loss 0.918859601020813\n",
      "epoch 74, iter 2, loss 0.921717643737793\n",
      "epoch 74, iter 3, loss 0.8979117274284363\n",
      "epoch 74, loss 3.633698880672455\n",
      "epoch 75, iter 0, loss 0.8807200193405151\n",
      "epoch 75, iter 1, loss 0.9206899404525757\n",
      "epoch 75, iter 2, loss 0.8963356018066406\n",
      "epoch 75, iter 3, loss 0.9310430884361267\n",
      "epoch 75, loss 3.628788650035858\n",
      "epoch 76, iter 0, loss 0.8735597729682922\n",
      "epoch 76, iter 1, loss 0.9015130996704102\n",
      "epoch 76, iter 2, loss 0.9206109642982483\n",
      "epoch 76, iter 3, loss 0.9130394458770752\n",
      "epoch 76, loss 3.608723282814026\n",
      "epoch 77, iter 0, loss 0.8778539299964905\n",
      "epoch 77, iter 1, loss 0.8993974924087524\n",
      "epoch 77, iter 2, loss 0.9251307249069214\n",
      "epoch 77, iter 3, loss 0.9193286895751953\n",
      "epoch 77, loss 3.6217108368873596\n",
      "epoch 78, iter 0, loss 0.9391340017318726\n",
      "epoch 78, iter 1, loss 0.8534896969795227\n",
      "epoch 78, iter 2, loss 0.9151843786239624\n",
      "epoch 78, iter 3, loss 0.9225855469703674\n",
      "epoch 78, loss 3.630393624305725\n",
      "epoch 79, iter 0, loss 0.905342698097229\n",
      "epoch 79, iter 1, loss 0.8884564638137817\n",
      "epoch 79, iter 2, loss 0.9172579050064087\n",
      "epoch 79, iter 3, loss 0.9246396422386169\n",
      "epoch 79, loss 3.6356967091560364\n",
      "epoch 80, iter 0, loss 0.9080518484115601\n",
      "epoch 80, iter 1, loss 0.8703280687332153\n",
      "epoch 80, iter 2, loss 0.9496669769287109\n",
      "epoch 80, iter 3, loss 0.891441285610199\n",
      "epoch 80, loss 3.6194881796836853\n",
      "epoch 81, iter 0, loss 0.8877918124198914\n",
      "epoch 81, iter 1, loss 0.8876804709434509\n",
      "epoch 81, iter 2, loss 0.9276654720306396\n",
      "epoch 81, iter 3, loss 0.9154377579689026\n",
      "epoch 81, loss 3.6185755133628845\n",
      "epoch 82, iter 0, loss 0.9113696217536926\n",
      "epoch 82, iter 1, loss 0.9063488841056824\n",
      "epoch 82, iter 2, loss 0.8736981153488159\n",
      "epoch 82, iter 3, loss 0.9158355593681335\n",
      "epoch 82, loss 3.6072521805763245\n",
      "epoch 83, iter 0, loss 0.8904063105583191\n",
      "epoch 83, iter 1, loss 0.8820607662200928\n",
      "epoch 83, iter 2, loss 0.9306245446205139\n",
      "epoch 83, iter 3, loss 0.9126712083816528\n",
      "epoch 83, loss 3.6157628297805786\n",
      "epoch 84, iter 0, loss 0.875927746295929\n",
      "epoch 84, iter 1, loss 0.8990039825439453\n",
      "epoch 84, iter 2, loss 0.8762259483337402\n",
      "epoch 84, iter 3, loss 0.9598088264465332\n",
      "epoch 84, loss 3.6109665036201477\n",
      "epoch 85, iter 0, loss 0.8866081833839417\n",
      "epoch 85, iter 1, loss 0.9224705696105957\n",
      "epoch 85, iter 2, loss 0.8901404738426208\n",
      "epoch 85, iter 3, loss 0.9014932513237\n",
      "epoch 85, loss 3.600712478160858\n",
      "epoch 86, iter 0, loss 0.8828786611557007\n",
      "epoch 86, iter 1, loss 0.8865633010864258\n",
      "epoch 86, iter 2, loss 0.907827615737915\n",
      "epoch 86, iter 3, loss 0.9297908544540405\n",
      "epoch 86, loss 3.607060432434082\n",
      "epoch 87, iter 0, loss 0.8912482261657715\n",
      "epoch 87, iter 1, loss 0.8895413279533386\n",
      "epoch 87, iter 2, loss 0.934009850025177\n",
      "epoch 87, iter 3, loss 0.8918614387512207\n",
      "epoch 87, loss 3.606660842895508\n",
      "epoch 88, iter 0, loss 0.90635085105896\n",
      "epoch 88, iter 1, loss 0.9178341031074524\n",
      "epoch 88, iter 2, loss 0.8948572278022766\n",
      "epoch 88, iter 3, loss 0.8857909440994263\n",
      "epoch 88, loss 3.6048331260681152\n",
      "epoch 89, iter 0, loss 0.9047988057136536\n",
      "epoch 89, iter 1, loss 0.8917697668075562\n",
      "epoch 89, iter 2, loss 0.9166958332061768\n",
      "epoch 89, iter 3, loss 0.9149010181427002\n",
      "epoch 89, loss 3.6281654238700867\n",
      "epoch 90, iter 0, loss 0.8819307088851929\n",
      "epoch 90, iter 1, loss 0.9096341133117676\n",
      "epoch 90, iter 2, loss 0.897348165512085\n",
      "epoch 90, iter 3, loss 0.9160774350166321\n",
      "epoch 90, loss 3.6049904227256775\n",
      "epoch 91, iter 0, loss 0.8848130702972412\n",
      "epoch 91, iter 1, loss 0.8779454827308655\n",
      "epoch 91, iter 2, loss 0.9021163582801819\n",
      "epoch 91, iter 3, loss 0.9285050630569458\n",
      "epoch 91, loss 3.5933799743652344\n",
      "epoch 92, iter 0, loss 0.8562358021736145\n",
      "epoch 92, iter 1, loss 0.8904154300689697\n",
      "epoch 92, iter 2, loss 0.9117867350578308\n",
      "epoch 92, iter 3, loss 0.9424008131027222\n",
      "epoch 92, loss 3.600838780403137\n",
      "epoch 93, iter 0, loss 0.9005982279777527\n",
      "epoch 93, iter 1, loss 0.8872950673103333\n",
      "epoch 93, iter 2, loss 0.8859722018241882\n",
      "epoch 93, iter 3, loss 0.9213610887527466\n",
      "epoch 93, loss 3.5952265858650208\n",
      "epoch 94, iter 0, loss 0.8902555108070374\n",
      "epoch 94, iter 1, loss 0.9115513563156128\n",
      "epoch 94, iter 2, loss 0.8876508474349976\n",
      "epoch 94, iter 3, loss 0.8933638334274292\n",
      "epoch 94, loss 3.582821547985077\n",
      "epoch 95, iter 0, loss 0.8835278749465942\n",
      "epoch 95, iter 1, loss 0.8981136083602905\n",
      "epoch 95, iter 2, loss 0.9153946042060852\n",
      "epoch 95, iter 3, loss 0.9321631193161011\n",
      "epoch 95, loss 3.629199206829071\n",
      "epoch 96, iter 0, loss 0.9052419066429138\n",
      "epoch 96, iter 1, loss 0.898848295211792\n",
      "epoch 96, iter 2, loss 0.9023429751396179\n",
      "epoch 96, iter 3, loss 0.9086186289787292\n",
      "epoch 96, loss 3.615051805973053\n",
      "epoch 97, iter 0, loss 0.8742165565490723\n",
      "epoch 97, iter 1, loss 0.9116965532302856\n",
      "epoch 97, iter 2, loss 0.9214758276939392\n",
      "epoch 97, iter 3, loss 0.9365954995155334\n",
      "epoch 97, loss 3.6439844369888306\n",
      "epoch 98, iter 0, loss 0.919553279876709\n",
      "epoch 98, iter 1, loss 0.8956167101860046\n",
      "epoch 98, iter 2, loss 0.8975956439971924\n",
      "epoch 98, iter 3, loss 0.934516191482544\n",
      "epoch 98, loss 3.64728182554245\n",
      "epoch 99, iter 0, loss 0.8900614380836487\n",
      "epoch 99, iter 1, loss 0.9059215784072876\n",
      "epoch 99, iter 2, loss 0.9125975966453552\n",
      "epoch 99, iter 3, loss 0.9017454981803894\n",
      "epoch 99, loss 3.610326111316681\n",
      "2019-03-31 02:51:34.975230, fold=1, rep=1, eta=0d 0h 26m 15s \n",
      "{'fold': 1, 'repeat': 1, 'n': 5875, 'd': 18, 'mse': 2.235564708709717, 'train_time': 808.0859856638126, 'trained_epochs': 100, 'prior_train_nmll': 0.942348301410675, 'train_nll': 2547.209228515625, 'test_nll': 2957.67236328125, 'train_mse': 0.19781887531280518, 'state_dict_file': 'model_state_dict_-7231275440747032823.pkl'}\n",
      "epoch 0, iter 0, loss 1.9776806831359863\n",
      "epoch 0, iter 1, loss 2.1442980766296387\n",
      "epoch 0, iter 2, loss 2.093533515930176\n",
      "epoch 0, iter 3, loss 1.854379653930664\n",
      "epoch 0, loss 8.069891929626465\n",
      "epoch 1, iter 0, loss 1.897700309753418\n",
      "epoch 1, iter 1, loss 1.8479807376861572\n",
      "epoch 1, iter 2, loss 1.7288874387741089\n",
      "epoch 1, iter 3, loss 1.6019781827926636\n",
      "epoch 1, loss 7.076546669006348\n",
      "epoch 2, iter 0, loss 1.5709457397460938\n",
      "epoch 2, iter 1, loss 1.564355492591858\n",
      "epoch 2, iter 2, loss 1.5454113483428955\n",
      "epoch 2, iter 3, loss 1.482137680053711\n",
      "epoch 2, loss 6.162850260734558\n",
      "epoch 3, iter 0, loss 1.46062433719635\n",
      "epoch 3, iter 1, loss 1.4499386548995972\n",
      "epoch 3, iter 2, loss 1.4121286869049072\n",
      "epoch 3, iter 3, loss 1.410120964050293\n",
      "epoch 3, loss 5.7328126430511475\n",
      "epoch 4, iter 0, loss 1.3834954500198364\n",
      "epoch 4, iter 1, loss 1.367355465888977\n",
      "epoch 4, iter 2, loss 1.3477940559387207\n",
      "epoch 4, iter 3, loss 1.3241074085235596\n",
      "epoch 4, loss 5.422752380371094\n",
      "epoch 5, iter 0, loss 1.2957642078399658\n",
      "epoch 5, iter 1, loss 1.2954857349395752\n",
      "epoch 5, iter 2, loss 1.2747000455856323\n",
      "epoch 5, iter 3, loss 1.2611531019210815\n",
      "epoch 5, loss 5.127103090286255\n",
      "epoch 6, iter 0, loss 1.2307707071304321\n",
      "epoch 6, iter 1, loss 1.2080628871917725\n",
      "epoch 6, iter 2, loss 1.203444242477417\n",
      "epoch 6, iter 3, loss 1.1948988437652588\n",
      "epoch 6, loss 4.83717668056488\n",
      "epoch 7, iter 0, loss 1.1430144309997559\n",
      "epoch 7, iter 1, loss 1.142297625541687\n",
      "epoch 7, iter 2, loss 1.128746509552002\n",
      "epoch 7, iter 3, loss 1.1182284355163574\n",
      "epoch 7, loss 4.532287001609802\n",
      "epoch 8, iter 0, loss 1.0685714483261108\n",
      "epoch 8, iter 1, loss 1.064451813697815\n",
      "epoch 8, iter 2, loss 1.0424134731292725\n",
      "epoch 8, iter 3, loss 1.0440820455551147\n",
      "epoch 8, loss 4.219518780708313\n",
      "epoch 9, iter 0, loss 0.9846334457397461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, iter 1, loss 0.9865041375160217\n",
      "epoch 9, iter 2, loss 0.9883910417556763\n",
      "epoch 9, iter 3, loss 0.973347008228302\n",
      "epoch 9, loss 3.932875633239746\n",
      "epoch 10, iter 0, loss 0.9370540976524353\n",
      "epoch 10, iter 1, loss 0.929253876209259\n",
      "epoch 10, iter 2, loss 0.9344065189361572\n",
      "epoch 10, iter 3, loss 0.9135437607765198\n",
      "epoch 10, loss 3.7142582535743713\n",
      "epoch 11, iter 0, loss 0.8904018402099609\n",
      "epoch 11, iter 1, loss 0.8919016718864441\n",
      "epoch 11, iter 2, loss 0.919784426689148\n",
      "epoch 11, iter 3, loss 0.9176257848739624\n",
      "epoch 11, loss 3.6197137236595154\n",
      "epoch 12, iter 0, loss 0.9075560569763184\n",
      "epoch 12, iter 1, loss 0.9071532487869263\n",
      "epoch 12, iter 2, loss 0.9062916040420532\n",
      "epoch 12, iter 3, loss 0.9505898952484131\n",
      "epoch 12, loss 3.671590805053711\n",
      "epoch 13, iter 0, loss 0.9293815493583679\n",
      "epoch 13, iter 1, loss 0.8978323340415955\n",
      "epoch 13, iter 2, loss 0.9242189526557922\n",
      "epoch 13, iter 3, loss 1.0130671262741089\n",
      "epoch 13, loss 3.7644999623298645\n",
      "epoch 14, iter 0, loss 0.9685847759246826\n",
      "epoch 14, iter 1, loss 0.9467967748641968\n",
      "epoch 14, iter 2, loss 0.9470685124397278\n",
      "epoch 14, iter 3, loss 0.9587107300758362\n",
      "epoch 14, loss 3.8211607933044434\n",
      "epoch 15, iter 0, loss 0.8895465731620789\n",
      "epoch 15, iter 1, loss 0.9357993006706238\n",
      "epoch 15, iter 2, loss 0.9770129919052124\n",
      "epoch 15, iter 3, loss 1.0021692514419556\n",
      "epoch 15, loss 3.8045281171798706\n",
      "epoch 16, iter 0, loss 0.9264984130859375\n",
      "epoch 16, iter 1, loss 0.9228987097740173\n",
      "epoch 16, iter 2, loss 0.9199336171150208\n",
      "epoch 16, iter 3, loss 0.9668908715248108\n",
      "epoch 16, loss 3.7362216114997864\n",
      "epoch 17, iter 0, loss 0.8885341286659241\n",
      "epoch 17, iter 1, loss 0.9085001349449158\n",
      "epoch 17, iter 2, loss 0.9223911762237549\n",
      "epoch 17, iter 3, loss 0.8899376392364502\n",
      "epoch 17, loss 3.609363079071045\n",
      "epoch 18, iter 0, loss 0.8775477409362793\n",
      "epoch 18, iter 1, loss 0.8838186860084534\n",
      "epoch 18, iter 2, loss 0.9065316319465637\n",
      "epoch 18, iter 3, loss 0.9096707105636597\n",
      "epoch 18, loss 3.577568769454956\n",
      "epoch 19, iter 0, loss 0.8880243897438049\n",
      "epoch 19, iter 1, loss 0.8875765800476074\n",
      "epoch 19, iter 2, loss 0.8549307584762573\n",
      "epoch 19, iter 3, loss 0.8555071353912354\n",
      "epoch 19, loss 3.486038863658905\n",
      "epoch 20, iter 0, loss 0.8466718196868896\n",
      "epoch 20, iter 1, loss 0.8587151765823364\n",
      "epoch 20, iter 2, loss 0.8713024854660034\n",
      "epoch 20, iter 3, loss 0.8692182302474976\n",
      "epoch 20, loss 3.445907711982727\n",
      "epoch 21, iter 0, loss 0.8734018206596375\n",
      "epoch 21, iter 1, loss 0.8483392000198364\n",
      "epoch 21, iter 2, loss 0.8228455781936646\n",
      "epoch 21, iter 3, loss 0.8523524403572083\n",
      "epoch 21, loss 3.3969390392303467\n",
      "epoch 22, iter 0, loss 0.8388756513595581\n",
      "epoch 22, iter 1, loss 0.8066059350967407\n",
      "epoch 22, iter 2, loss 0.8640584349632263\n",
      "epoch 22, iter 3, loss 0.8612727522850037\n",
      "epoch 22, loss 3.370812773704529\n",
      "epoch 23, iter 0, loss 0.8487557768821716\n",
      "epoch 23, iter 1, loss 0.8497825860977173\n",
      "epoch 23, iter 2, loss 0.8166462182998657\n",
      "epoch 23, iter 3, loss 0.8843331336975098\n",
      "epoch 23, loss 3.3995177149772644\n",
      "epoch 24, iter 0, loss 0.8019902110099792\n",
      "epoch 24, iter 1, loss 0.8275710344314575\n",
      "epoch 24, iter 2, loss 0.8725181818008423\n",
      "epoch 24, iter 3, loss 0.8989354968070984\n",
      "epoch 24, loss 3.4010149240493774\n",
      "epoch 25, iter 0, loss 0.8594945669174194\n",
      "epoch 25, iter 1, loss 0.8361186385154724\n",
      "epoch 25, iter 2, loss 0.8412589430809021\n",
      "epoch 25, iter 3, loss 0.864802896976471\n",
      "epoch 25, loss 3.401675045490265\n",
      "epoch 26, iter 0, loss 0.82780921459198\n",
      "epoch 26, iter 1, loss 0.8425946235656738\n",
      "epoch 26, iter 2, loss 0.8589587211608887\n",
      "epoch 26, iter 3, loss 0.8687030076980591\n",
      "epoch 26, loss 3.3980655670166016\n",
      "epoch 27, iter 0, loss 0.8136345744132996\n",
      "epoch 27, iter 1, loss 0.8351480960845947\n",
      "epoch 27, iter 2, loss 0.8497644066810608\n",
      "epoch 27, iter 3, loss 0.8841769099235535\n",
      "epoch 27, loss 3.3827239871025085\n",
      "epoch 28, iter 0, loss 0.7974876165390015\n",
      "epoch 28, iter 1, loss 0.8415759801864624\n",
      "epoch 28, iter 2, loss 0.8527892231941223\n",
      "epoch 28, iter 3, loss 0.8837913870811462\n",
      "epoch 28, loss 3.3756442070007324\n",
      "epoch 29, iter 0, loss 0.8434973359107971\n",
      "epoch 29, iter 1, loss 0.8142116665840149\n",
      "epoch 29, iter 2, loss 0.8604883551597595\n",
      "epoch 29, iter 3, loss 0.8560545444488525\n",
      "epoch 29, loss 3.374251902103424\n",
      "epoch 30, iter 0, loss 0.8085988163948059\n",
      "epoch 30, iter 1, loss 0.8370175361633301\n",
      "epoch 30, iter 2, loss 0.859377384185791\n",
      "epoch 30, iter 3, loss 0.8646891117095947\n",
      "epoch 30, loss 3.3696828484535217\n",
      "epoch 31, iter 0, loss 0.8046327829360962\n",
      "epoch 31, iter 1, loss 0.8306019306182861\n",
      "epoch 31, iter 2, loss 0.8433277606964111\n",
      "epoch 31, iter 3, loss 0.8878941535949707\n",
      "epoch 31, loss 3.366456627845764\n",
      "epoch 32, iter 0, loss 0.8270612955093384\n",
      "epoch 32, iter 1, loss 0.8103365898132324\n",
      "epoch 32, iter 2, loss 0.8487231135368347\n",
      "epoch 32, iter 3, loss 0.8766552805900574\n",
      "epoch 32, loss 3.362776279449463\n",
      "epoch 33, iter 0, loss 0.8046943545341492\n",
      "epoch 33, iter 1, loss 0.8369446992874146\n",
      "epoch 33, iter 2, loss 0.8612098693847656\n",
      "epoch 33, iter 3, loss 0.8660789132118225\n",
      "epoch 33, loss 3.368927836418152\n",
      "epoch 34, iter 0, loss 0.8325064778327942\n",
      "epoch 34, iter 1, loss 0.8284966349601746\n",
      "epoch 34, iter 2, loss 0.8536184430122375\n",
      "epoch 34, iter 3, loss 0.8531206250190735\n",
      "epoch 34, loss 3.36774218082428\n",
      "epoch 35, iter 0, loss 0.7853107452392578\n",
      "epoch 35, iter 1, loss 0.8152602910995483\n",
      "epoch 35, iter 2, loss 0.8353077173233032\n",
      "epoch 35, iter 3, loss 0.916479766368866\n",
      "epoch 35, loss 3.3523585200309753\n",
      "epoch 36, iter 0, loss 0.8094248175621033\n",
      "epoch 36, iter 1, loss 0.8324679136276245\n",
      "epoch 36, iter 2, loss 0.8591597676277161\n",
      "epoch 36, iter 3, loss 0.8359532356262207\n",
      "epoch 36, loss 3.3370057344436646\n",
      "epoch 37, iter 0, loss 0.8255641460418701\n",
      "epoch 37, iter 1, loss 0.8366273641586304\n",
      "epoch 37, iter 2, loss 0.8388121724128723\n",
      "epoch 37, iter 3, loss 0.827014148235321\n",
      "epoch 37, loss 3.328017830848694\n",
      "epoch 38, iter 0, loss 0.7974823117256165\n",
      "epoch 38, iter 1, loss 0.84605872631073\n",
      "epoch 38, iter 2, loss 0.8397719264030457\n",
      "epoch 38, iter 3, loss 0.8406325578689575\n",
      "epoch 38, loss 3.3239455223083496\n",
      "epoch 39, iter 0, loss 0.8123995065689087\n",
      "epoch 39, iter 1, loss 0.8318573832511902\n",
      "epoch 39, iter 2, loss 0.8426555395126343\n",
      "epoch 39, iter 3, loss 0.820775032043457\n",
      "epoch 39, loss 3.30768746137619\n",
      "epoch 40, iter 0, loss 0.8428218364715576\n",
      "epoch 40, iter 1, loss 0.8151220679283142\n",
      "epoch 40, iter 2, loss 0.8324215412139893\n",
      "epoch 40, iter 3, loss 0.8000020384788513\n",
      "epoch 40, loss 3.2903674840927124\n",
      "epoch 41, iter 0, loss 0.8337515592575073\n",
      "epoch 41, iter 1, loss 0.8422368764877319\n",
      "epoch 41, iter 2, loss 0.8101210594177246\n",
      "epoch 41, iter 3, loss 0.8235428333282471\n",
      "epoch 41, loss 3.309652328491211\n",
      "epoch 42, iter 0, loss 0.7732328176498413\n",
      "epoch 42, iter 1, loss 0.8509960174560547\n",
      "epoch 42, iter 2, loss 0.8119149804115295\n",
      "epoch 42, iter 3, loss 0.8657507300376892\n",
      "epoch 42, loss 3.3018945455551147\n",
      "epoch 43, iter 0, loss 0.8221873044967651\n",
      "epoch 43, iter 1, loss 0.8280483484268188\n",
      "epoch 43, iter 2, loss 0.8502956628799438\n",
      "epoch 43, iter 3, loss 0.8228551149368286\n",
      "epoch 43, loss 3.3233864307403564\n",
      "epoch 44, iter 0, loss 0.813130795955658\n",
      "epoch 44, iter 1, loss 0.8432623147964478\n",
      "epoch 44, iter 2, loss 0.8570274710655212\n",
      "epoch 44, iter 3, loss 0.8053353428840637\n",
      "epoch 44, loss 3.3187559247016907\n",
      "epoch 45, iter 0, loss 0.7946996688842773\n",
      "epoch 45, iter 1, loss 0.8441033959388733\n",
      "epoch 45, iter 2, loss 0.8181890249252319\n",
      "epoch 45, iter 3, loss 0.8688533902168274\n",
      "epoch 45, loss 3.32584547996521\n",
      "epoch 46, iter 0, loss 0.8008603453636169\n",
      "epoch 46, iter 1, loss 0.8404020667076111\n",
      "epoch 46, iter 2, loss 0.8550825119018555\n",
      "epoch 46, iter 3, loss 0.8296744227409363\n",
      "epoch 46, loss 3.3260193467140198\n",
      "epoch 47, iter 0, loss 0.8247290253639221\n",
      "epoch 47, iter 1, loss 0.8367100954055786\n",
      "epoch 47, iter 2, loss 0.8291809558868408\n",
      "epoch 47, iter 3, loss 0.8569895625114441\n",
      "epoch 47, loss 3.3476096391677856\n",
      "epoch 48, iter 0, loss 0.8049880266189575\n",
      "epoch 48, iter 1, loss 0.8390650153160095\n",
      "epoch 48, iter 2, loss 0.8376554250717163\n",
      "epoch 48, iter 3, loss 0.8203552961349487\n",
      "epoch 48, loss 3.302063763141632\n",
      "epoch 49, iter 0, loss 0.8012680411338806\n",
      "epoch 49, iter 1, loss 0.8046380281448364\n",
      "epoch 49, iter 2, loss 0.8587138056755066\n",
      "epoch 49, iter 3, loss 0.8546987175941467\n",
      "epoch 49, loss 3.3193185925483704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, iter 0, loss 0.8220482468605042\n",
      "epoch 50, iter 1, loss 0.8344200849533081\n",
      "epoch 50, iter 2, loss 0.8374106884002686\n",
      "epoch 50, iter 3, loss 0.8342491388320923\n",
      "epoch 50, loss 3.328128159046173\n",
      "epoch 51, iter 0, loss 0.8315590620040894\n",
      "epoch 51, iter 1, loss 0.8020864725112915\n",
      "epoch 51, iter 2, loss 0.8249562382698059\n",
      "epoch 51, iter 3, loss 0.8721407651901245\n",
      "epoch 51, loss 3.3307425379753113\n",
      "epoch 52, iter 0, loss 0.8006390333175659\n",
      "epoch 52, iter 1, loss 0.8218421936035156\n",
      "epoch 52, iter 2, loss 0.8447006344795227\n",
      "epoch 52, iter 3, loss 0.8640663623809814\n",
      "epoch 52, loss 3.3312482237815857\n",
      "epoch 53, iter 0, loss 0.7937759160995483\n",
      "epoch 53, iter 1, loss 0.8233456611633301\n",
      "epoch 53, iter 2, loss 0.8250586986541748\n",
      "epoch 53, iter 3, loss 0.8789135813713074\n",
      "epoch 53, loss 3.3210938572883606\n",
      "epoch 54, iter 0, loss 0.7928758263587952\n",
      "epoch 54, iter 1, loss 0.81251060962677\n",
      "epoch 54, iter 2, loss 0.8648585677146912\n",
      "epoch 54, iter 3, loss 0.8622747659683228\n",
      "epoch 54, loss 3.332519769668579\n",
      "epoch 55, iter 0, loss 0.8282712697982788\n",
      "epoch 55, iter 1, loss 0.807296872138977\n",
      "epoch 55, iter 2, loss 0.82912677526474\n",
      "epoch 55, iter 3, loss 0.8470828533172607\n",
      "epoch 55, loss 3.3117777705192566\n",
      "epoch 56, iter 0, loss 0.8033641576766968\n",
      "epoch 56, iter 1, loss 0.799394428730011\n",
      "epoch 56, iter 2, loss 0.8341618776321411\n",
      "epoch 56, iter 3, loss 0.8636023998260498\n",
      "epoch 56, loss 3.3005228638648987\n",
      "epoch 57, iter 0, loss 0.826257050037384\n",
      "epoch 57, iter 1, loss 0.8312324285507202\n",
      "epoch 57, iter 2, loss 0.8134975433349609\n",
      "epoch 57, iter 3, loss 0.8250472545623779\n",
      "epoch 57, loss 3.296034276485443\n",
      "epoch 58, iter 0, loss 0.7870618104934692\n",
      "epoch 58, iter 1, loss 0.8069084286689758\n",
      "epoch 58, iter 2, loss 0.8453113436698914\n",
      "epoch 58, iter 3, loss 0.8769910931587219\n",
      "epoch 58, loss 3.3162726759910583\n",
      "epoch 59, iter 0, loss 0.8002803325653076\n",
      "epoch 59, iter 1, loss 0.8653296232223511\n",
      "epoch 59, iter 2, loss 0.7998048067092896\n",
      "epoch 59, iter 3, loss 0.8587965369224548\n",
      "epoch 59, loss 3.324211299419403\n",
      "epoch 60, iter 0, loss 0.8180032968521118\n",
      "epoch 60, iter 1, loss 0.8306347131729126\n",
      "epoch 60, iter 2, loss 0.8286210298538208\n",
      "epoch 60, iter 3, loss 0.8215681910514832\n",
      "epoch 60, loss 3.2988272309303284\n",
      "epoch 61, iter 0, loss 0.8129268884658813\n",
      "epoch 61, iter 1, loss 0.8342832326889038\n",
      "epoch 61, iter 2, loss 0.8104950189590454\n",
      "epoch 61, iter 3, loss 0.8576575517654419\n",
      "epoch 61, loss 3.3153626918792725\n",
      "epoch 62, iter 0, loss 0.7870895862579346\n",
      "epoch 62, iter 1, loss 0.8478156924247742\n",
      "epoch 62, iter 2, loss 0.8271868228912354\n",
      "epoch 62, iter 3, loss 0.8540118932723999\n",
      "epoch 62, loss 3.316103994846344\n",
      "epoch 63, iter 0, loss 0.7991507053375244\n",
      "epoch 63, iter 1, loss 0.8202632665634155\n",
      "epoch 63, iter 2, loss 0.8364056348800659\n",
      "epoch 63, iter 3, loss 0.8415538668632507\n",
      "epoch 63, loss 3.2973734736442566\n",
      "epoch 64, iter 0, loss 0.8217229247093201\n",
      "epoch 64, iter 1, loss 0.8159621953964233\n",
      "epoch 64, iter 2, loss 0.8239082098007202\n",
      "epoch 64, iter 3, loss 0.8369654417037964\n",
      "epoch 64, loss 3.29855877161026\n",
      "epoch 65, iter 0, loss 0.8293585777282715\n",
      "epoch 65, iter 1, loss 0.8221266269683838\n",
      "epoch 65, iter 2, loss 0.8155181407928467\n",
      "epoch 65, iter 3, loss 0.8372617959976196\n",
      "epoch 65, loss 3.3042651414871216\n",
      "epoch 66, iter 0, loss 0.8344196081161499\n",
      "epoch 66, iter 1, loss 0.8068557977676392\n",
      "epoch 66, iter 2, loss 0.8462356925010681\n",
      "epoch 66, iter 3, loss 0.8369393944740295\n",
      "epoch 66, loss 3.3244504928588867\n",
      "epoch 67, iter 0, loss 0.8095943927764893\n",
      "epoch 67, iter 1, loss 0.8346725106239319\n",
      "epoch 67, iter 2, loss 0.8401769995689392\n",
      "epoch 67, iter 3, loss 0.8398374915122986\n",
      "epoch 67, loss 3.324281394481659\n",
      "epoch 68, iter 0, loss 0.8142234086990356\n",
      "epoch 68, iter 1, loss 0.8456158638000488\n",
      "epoch 68, iter 2, loss 0.8374402523040771\n",
      "epoch 68, iter 3, loss 0.8041585087776184\n",
      "epoch 68, loss 3.30143803358078\n",
      "epoch 69, iter 0, loss 0.8075849413871765\n",
      "epoch 69, iter 1, loss 0.8399726152420044\n",
      "epoch 69, iter 2, loss 0.8064419627189636\n",
      "epoch 69, iter 3, loss 0.8556540012359619\n",
      "epoch 69, loss 3.3096535205841064\n",
      "epoch 70, iter 0, loss 0.8034893274307251\n",
      "epoch 70, iter 1, loss 0.821590006351471\n",
      "epoch 70, iter 2, loss 0.8243647813796997\n",
      "epoch 70, iter 3, loss 0.8216026425361633\n",
      "epoch 70, loss 3.271046757698059\n",
      "epoch 71, iter 0, loss 0.7882134914398193\n",
      "epoch 71, iter 1, loss 0.8085049390792847\n",
      "epoch 71, iter 2, loss 0.8409630060195923\n",
      "epoch 71, iter 3, loss 0.8566598892211914\n",
      "epoch 71, loss 3.2943413257598877\n",
      "epoch 72, iter 0, loss 0.7932800650596619\n",
      "epoch 72, iter 1, loss 0.8115403056144714\n",
      "epoch 72, iter 2, loss 0.8313603401184082\n",
      "epoch 72, iter 3, loss 0.8548859357833862\n",
      "epoch 72, loss 3.2910666465759277\n",
      "epoch 73, iter 0, loss 0.7820520401000977\n",
      "epoch 73, iter 1, loss 0.8452016115188599\n",
      "epoch 73, iter 2, loss 0.8210688829421997\n",
      "epoch 73, iter 3, loss 0.8464675545692444\n",
      "epoch 73, loss 3.2947900891304016\n",
      "epoch 74, iter 0, loss 0.8091211915016174\n",
      "epoch 74, iter 1, loss 0.8302698731422424\n",
      "epoch 74, iter 2, loss 0.8443188667297363\n",
      "epoch 74, iter 3, loss 0.8109079003334045\n",
      "epoch 74, loss 3.2946178317070007\n",
      "epoch 75, iter 0, loss 0.8100839257240295\n",
      "epoch 75, iter 1, loss 0.8143883347511292\n",
      "epoch 75, iter 2, loss 0.8468674421310425\n",
      "epoch 75, iter 3, loss 0.8357399702072144\n",
      "epoch 75, loss 3.3070796728134155\n",
      "epoch 76, iter 0, loss 0.8013313412666321\n",
      "epoch 76, iter 1, loss 0.8021042346954346\n",
      "epoch 76, iter 2, loss 0.8584509491920471\n",
      "epoch 76, iter 3, loss 0.8451318144798279\n",
      "epoch 76, loss 3.3070183396339417\n",
      "epoch 77, iter 0, loss 0.7930251359939575\n",
      "epoch 77, iter 1, loss 0.8094733953475952\n",
      "epoch 77, iter 2, loss 0.8452692031860352\n",
      "epoch 77, iter 3, loss 0.8770294189453125\n",
      "epoch 77, loss 3.3247971534729004\n",
      "epoch 78, iter 0, loss 0.8245166540145874\n",
      "epoch 78, iter 1, loss 0.7850368022918701\n",
      "epoch 78, iter 2, loss 0.8706194758415222\n",
      "epoch 78, iter 3, loss 0.8534291982650757\n",
      "epoch 78, loss 3.3336021304130554\n",
      "epoch 79, iter 0, loss 0.79429692029953\n",
      "epoch 79, iter 1, loss 0.8535929322242737\n",
      "epoch 79, iter 2, loss 0.8362941145896912\n",
      "epoch 79, iter 3, loss 0.8486819863319397\n",
      "epoch 79, loss 3.3328659534454346\n",
      "epoch 80, iter 0, loss 0.7693498134613037\n",
      "epoch 80, iter 1, loss 0.8228001594543457\n",
      "epoch 80, iter 2, loss 0.8401146531105042\n",
      "epoch 80, iter 3, loss 0.8810321092605591\n",
      "epoch 80, loss 3.3132967352867126\n",
      "epoch 81, iter 0, loss 0.8000670671463013\n",
      "epoch 81, iter 1, loss 0.7992506623268127\n",
      "epoch 81, iter 2, loss 0.8797866702079773\n",
      "epoch 81, iter 3, loss 0.8326143622398376\n",
      "epoch 81, loss 3.311718761920929\n",
      "epoch 82, iter 0, loss 0.8019037246704102\n",
      "epoch 82, iter 1, loss 0.85164874792099\n",
      "epoch 82, iter 2, loss 0.8274410367012024\n",
      "epoch 82, iter 3, loss 0.848694384098053\n",
      "epoch 82, loss 3.3296878933906555\n",
      "epoch 83, iter 0, loss 0.839509129524231\n",
      "epoch 83, iter 1, loss 0.8122731447219849\n",
      "epoch 83, iter 2, loss 0.8235224485397339\n",
      "epoch 83, iter 3, loss 0.8279032111167908\n",
      "epoch 83, loss 3.3032079339027405\n",
      "epoch 84, iter 0, loss 0.7955530881881714\n",
      "epoch 84, iter 1, loss 0.8254901766777039\n",
      "epoch 84, iter 2, loss 0.8403124809265137\n",
      "epoch 84, iter 3, loss 0.8647864460945129\n",
      "epoch 84, loss 3.326142191886902\n",
      "epoch 85, iter 0, loss 0.8312844634056091\n",
      "epoch 85, iter 1, loss 0.8305678367614746\n",
      "epoch 85, iter 2, loss 0.8367857336997986\n",
      "epoch 85, iter 3, loss 0.8221918344497681\n",
      "epoch 85, loss 3.3208298683166504\n",
      "epoch 86, iter 0, loss 0.8293399214744568\n",
      "epoch 86, iter 1, loss 0.8350492119789124\n",
      "epoch 86, iter 2, loss 0.8481895327568054\n",
      "epoch 86, iter 3, loss 0.8121054768562317\n",
      "epoch 86, loss 3.3246841430664062\n",
      "epoch 87, iter 0, loss 0.8130635023117065\n",
      "epoch 87, iter 1, loss 0.800316333770752\n",
      "epoch 87, iter 2, loss 0.8628698587417603\n",
      "epoch 87, iter 3, loss 0.845048189163208\n",
      "epoch 87, loss 3.3212978839874268\n",
      "epoch 88, iter 0, loss 0.7921912670135498\n",
      "epoch 88, iter 1, loss 0.8143003582954407\n",
      "epoch 88, iter 2, loss 0.8540281057357788\n",
      "epoch 88, iter 3, loss 0.8727331757545471\n",
      "epoch 88, loss 3.3332529067993164\n",
      "epoch 89, iter 0, loss 0.8127565979957581\n",
      "epoch 89, iter 1, loss 0.8297116756439209\n",
      "epoch 89, iter 2, loss 0.8090063333511353\n",
      "epoch 89, iter 3, loss 0.8643386363983154\n",
      "epoch 89, loss 3.3158132433891296\n",
      "epoch 90, iter 0, loss 0.8357654809951782\n",
      "epoch 90, iter 1, loss 0.8151677250862122\n",
      "epoch 90, iter 2, loss 0.8424115180969238\n",
      "epoch 90, iter 3, loss 0.798851490020752\n",
      "epoch 90, loss 3.292196214199066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91, iter 0, loss 0.7872314453125\n",
      "epoch 91, iter 1, loss 0.8131313323974609\n",
      "epoch 91, iter 2, loss 0.8509753346443176\n",
      "epoch 91, iter 3, loss 0.8424626588821411\n",
      "epoch 91, loss 3.2938007712364197\n",
      "epoch 92, iter 0, loss 0.7870073318481445\n",
      "epoch 92, iter 1, loss 0.8435889482498169\n",
      "epoch 92, iter 2, loss 0.8263660073280334\n",
      "epoch 92, iter 3, loss 0.8477187156677246\n",
      "epoch 92, loss 3.3046810030937195\n",
      "epoch 93, iter 0, loss 0.7877396941184998\n",
      "epoch 93, iter 1, loss 0.8357800245285034\n",
      "epoch 93, iter 2, loss 0.8291416764259338\n",
      "epoch 93, iter 3, loss 0.84406578540802\n",
      "epoch 93, loss 3.296727180480957\n",
      "epoch 94, iter 0, loss 0.8038527965545654\n",
      "epoch 94, iter 1, loss 0.7963953018188477\n",
      "epoch 94, iter 2, loss 0.836181640625\n",
      "epoch 94, iter 3, loss 0.8979637622833252\n",
      "epoch 94, loss 3.3343935012817383\n",
      "epoch 95, iter 0, loss 0.8116386532783508\n",
      "epoch 95, iter 1, loss 0.8148860931396484\n",
      "epoch 95, iter 2, loss 0.8311755657196045\n",
      "epoch 95, iter 3, loss 0.8619812726974487\n",
      "epoch 95, loss 3.3196815848350525\n",
      "epoch 96, iter 0, loss 0.7937315106391907\n",
      "epoch 96, iter 1, loss 0.8153647184371948\n",
      "epoch 96, iter 2, loss 0.8599210977554321\n",
      "epoch 96, iter 3, loss 0.8496416211128235\n",
      "epoch 96, loss 3.318658947944641\n",
      "epoch 97, iter 0, loss 0.8084803223609924\n",
      "epoch 97, iter 1, loss 0.8568052053451538\n",
      "epoch 97, iter 2, loss 0.8372862339019775\n",
      "epoch 97, iter 3, loss 0.8320617079734802\n",
      "epoch 97, loss 3.334633469581604\n",
      "epoch 98, iter 0, loss 0.8280736207962036\n",
      "epoch 98, iter 1, loss 0.8065952658653259\n",
      "epoch 98, iter 2, loss 0.8381019830703735\n",
      "epoch 98, iter 3, loss 0.8737228512763977\n",
      "epoch 98, loss 3.346493721008301\n",
      "epoch 99, iter 0, loss 0.8218687772750854\n",
      "epoch 99, iter 1, loss 0.8205282092094421\n",
      "epoch 99, iter 2, loss 0.8771401047706604\n",
      "epoch 99, iter 3, loss 0.8182629346847534\n",
      "epoch 99, loss 3.3378000259399414\n",
      "2019-03-31 03:05:57.477545, fold=2, rep=0, eta=0d 0h 13m 22s \n",
      "{'fold': 2, 'repeat': 0, 'n': 5875, 'd': 18, 'mse': 2.9236505031585693, 'train_time': 861.2120567872189, 'trained_epochs': 100, 'prior_train_nmll': 0.871090829372406, 'train_nll': 2103.016357421875, 'test_nll': 2948.4482421875, 'train_mse': 0.15239088237285614, 'state_dict_file': 'model_state_dict_-5722107228132557951.pkl'}\n",
      "epoch 0, iter 0, loss 1.9255646467208862\n",
      "epoch 0, iter 1, loss 2.1808531284332275\n",
      "epoch 0, iter 2, loss 2.063401222229004\n",
      "epoch 0, iter 3, loss 1.9070639610290527\n",
      "epoch 0, loss 8.07688295841217\n",
      "epoch 1, iter 0, loss 1.9090369939804077\n",
      "epoch 1, iter 1, loss 1.8609728813171387\n",
      "epoch 1, iter 2, loss 1.7052849531173706\n",
      "epoch 1, iter 3, loss 1.5972782373428345\n",
      "epoch 1, loss 7.0725730657577515\n",
      "epoch 2, iter 0, loss 1.5896409749984741\n",
      "epoch 2, iter 1, loss 1.5408986806869507\n",
      "epoch 2, iter 2, loss 1.5316325426101685\n",
      "epoch 2, iter 3, loss 1.4956711530685425\n",
      "epoch 2, loss 6.157843351364136\n",
      "epoch 3, iter 0, loss 1.4604287147521973\n",
      "epoch 3, iter 1, loss 1.4450727701187134\n",
      "epoch 3, iter 2, loss 1.4184855222702026\n",
      "epoch 3, iter 3, loss 1.3965007066726685\n",
      "epoch 3, loss 5.720487713813782\n",
      "epoch 4, iter 0, loss 1.3813345432281494\n",
      "epoch 4, iter 1, loss 1.3527895212173462\n",
      "epoch 4, iter 2, loss 1.3447731733322144\n",
      "epoch 4, iter 3, loss 1.3254656791687012\n",
      "epoch 4, loss 5.404362916946411\n",
      "epoch 5, iter 0, loss 1.3062944412231445\n",
      "epoch 5, iter 1, loss 1.2859371900558472\n",
      "epoch 5, iter 2, loss 1.2688287496566772\n",
      "epoch 5, iter 3, loss 1.2551393508911133\n",
      "epoch 5, loss 5.116199731826782\n",
      "epoch 6, iter 0, loss 1.2326996326446533\n",
      "epoch 6, iter 1, loss 1.2017649412155151\n",
      "epoch 6, iter 2, loss 1.1988028287887573\n",
      "epoch 6, iter 3, loss 1.1866281032562256\n",
      "epoch 6, loss 4.819895505905151\n",
      "epoch 7, iter 0, loss 1.143351674079895\n",
      "epoch 7, iter 1, loss 1.140667200088501\n",
      "epoch 7, iter 2, loss 1.1181936264038086\n",
      "epoch 7, iter 3, loss 1.1117504835128784\n",
      "epoch 7, loss 4.513962984085083\n",
      "epoch 8, iter 0, loss 1.0708661079406738\n",
      "epoch 8, iter 1, loss 1.0509694814682007\n",
      "epoch 8, iter 2, loss 1.052838921546936\n",
      "epoch 8, iter 3, loss 1.03067147731781\n",
      "epoch 8, loss 4.205345988273621\n",
      "epoch 9, iter 0, loss 0.9987608790397644\n",
      "epoch 9, iter 1, loss 0.9679462313652039\n",
      "epoch 9, iter 2, loss 1.006077527999878\n",
      "epoch 9, iter 3, loss 0.9467447996139526\n",
      "epoch 9, loss 3.919529438018799\n",
      "epoch 10, iter 0, loss 0.9607521891593933\n",
      "epoch 10, iter 1, loss 0.8963192701339722\n",
      "epoch 10, iter 2, loss 0.9126781225204468\n",
      "epoch 10, iter 3, loss 0.9284552335739136\n",
      "epoch 10, loss 3.698204815387726\n",
      "epoch 11, iter 0, loss 0.8912796974182129\n",
      "epoch 11, iter 1, loss 0.8758928775787354\n",
      "epoch 11, iter 2, loss 0.8782820701599121\n",
      "epoch 11, iter 3, loss 0.9462172985076904\n",
      "epoch 11, loss 3.591671943664551\n",
      "epoch 12, iter 0, loss 0.8761163949966431\n",
      "epoch 12, iter 1, loss 0.9144234657287598\n",
      "epoch 12, iter 2, loss 0.8975366353988647\n",
      "epoch 12, iter 3, loss 0.9512675404548645\n",
      "epoch 12, loss 3.639344036579132\n",
      "epoch 13, iter 0, loss 0.9087662100791931\n",
      "epoch 13, iter 1, loss 0.9133743643760681\n",
      "epoch 13, iter 2, loss 0.9336261749267578\n",
      "epoch 13, iter 3, loss 0.9887406826019287\n",
      "epoch 13, loss 3.7445074319839478\n",
      "epoch 14, iter 0, loss 0.9253062009811401\n",
      "epoch 14, iter 1, loss 0.9575085639953613\n",
      "epoch 14, iter 2, loss 0.991867184638977\n",
      "epoch 14, iter 3, loss 0.9690171480178833\n",
      "epoch 14, loss 3.843699097633362\n",
      "epoch 15, iter 0, loss 0.9388151168823242\n",
      "epoch 15, iter 1, loss 0.9669692516326904\n",
      "epoch 15, iter 2, loss 0.9278037548065186\n",
      "epoch 15, iter 3, loss 0.943636417388916\n",
      "epoch 15, loss 3.777224540710449\n",
      "epoch 16, iter 0, loss 0.9292876124382019\n",
      "epoch 16, iter 1, loss 0.9331474304199219\n",
      "epoch 16, iter 2, loss 0.9182750582695007\n",
      "epoch 16, iter 3, loss 0.9349986910820007\n",
      "epoch 16, loss 3.7157087922096252\n",
      "epoch 17, iter 0, loss 0.8947866559028625\n",
      "epoch 17, iter 1, loss 0.9247684478759766\n",
      "epoch 17, iter 2, loss 0.905216634273529\n",
      "epoch 17, iter 3, loss 0.9136804342269897\n",
      "epoch 17, loss 3.638452172279358\n",
      "epoch 18, iter 0, loss 0.8805640339851379\n",
      "epoch 18, iter 1, loss 0.8989068269729614\n",
      "epoch 18, iter 2, loss 0.8786882162094116\n",
      "epoch 18, iter 3, loss 0.9114291667938232\n",
      "epoch 18, loss 3.5695882439613342\n",
      "epoch 19, iter 0, loss 0.8509868383407593\n",
      "epoch 19, iter 1, loss 0.8948172330856323\n",
      "epoch 19, iter 2, loss 0.8596397042274475\n",
      "epoch 19, iter 3, loss 0.8849812746047974\n",
      "epoch 19, loss 3.4904250502586365\n",
      "epoch 20, iter 0, loss 0.8417198657989502\n",
      "epoch 20, iter 1, loss 0.839644193649292\n",
      "epoch 88, loss 0.6177770495414734\n",
      "epoch 89, loss 0.6181411743164062\n",
      "epoch 90, loss 0.6174643039703369\n",
      "epoch 91, loss 0.6159466505050659\n",
      "epoch 92, loss 0.6183368563652039\n",
      "epoch 93, loss 0.6188696026802063\n",
      "epoch 94, loss 0.6154713034629822\n",
      "epoch 95, loss 0.6137619614601135\n",
      "epoch 96, loss 0.6167951226234436\n",
      "epoch 97, loss 0.6152039170265198\n",
      "epoch 98, loss 0.6129924654960632\n",
      "epoch 99, loss 0.6152641773223877\n",
      "2019-03-31 10:45:04.345639, fold=1, rep=0, eta=0d 1h 17m 12s \n",
      "{'fold': 1, 'repeat': 0, 'n': 16599, 'd': 16, 'mse': 0.19457818567752838, 'train_time': 1587.87648059614, 'trained_epochs': 100, 'prior_train_nmll': 0.6142532229423523, 'train_nll': 6298.73046875, 'test_nll': 3293.162353515625, 'train_mse': 0.1765214502811432, 'state_dict_file': 'model_state_dict_-799587309204930405.pkl'}\n",
      "epoch 0, loss 1.09308660030365\n",
      "epoch 1, loss 1.0701111555099487\n",
      "epoch 2, loss 1.0447051525115967\n",
      "epoch 3, loss 1.021725058555603\n",
      "epoch 4, loss 0.9951517581939697\n",
      "epoch 5, loss 0.9706814885139465\n",
      "epoch 6, loss 0.9458746314048767\n",
      "epoch 7, loss 0.9236696362495422\n",
      "epoch 8, loss 0.8962932229042053\n",
      "epoch 9, loss 0.872664213180542\n",
      "epoch 10, loss 0.8496725559234619\n",
      "epoch 11, loss 0.8275094628334045\n",
      "epoch 12, loss 0.8085939884185791\n",
      "epoch 13, loss 0.7907108068466187\n",
      "epoch 14, loss 0.7728333473205566\n",
      "epoch 15, loss 0.761957049369812\n",
      "epoch 16, loss 0.7518845796585083\n",
      "epoch 17, loss 0.7450446486473083\n",
      "epoch 18, loss 0.7402569055557251\n",
      "epoch 19, loss 0.7352587580680847\n",
      "epoch 20, loss 0.7287518978118896\n",
      "epoch 21, loss 0.7244469523429871\n",
      "epoch 22, loss 0.7248440384864807\n",
      "epoch 23, loss 0.7206905484199524\n",
      "epoch 24, loss 0.7160167694091797\n",
      "epoch 25, loss 0.7078503370285034\n",
      "epoch 26, loss 0.7017858028411865\n",
      "epoch 27, loss 0.6960836052894592\n",
      "epoch 28, loss 0.6904062628746033\n",
      "epoch 29, loss 0.6887742877006531\n",
      "epoch 30, loss 0.6830310225486755\n",
      "epoch 31, loss 0.6827348470687866\n",
      "epoch 32, loss 0.6782040596008301\n",
      "epoch 33, loss 0.6810462474822998\n",
      "epoch 34, loss 0.6780418157577515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35, loss 0.6735873818397522\n",
      "epoch 36, loss 0.6701214909553528\n",
      "epoch 37, loss 0.6714113354682922\n",
      "epoch 38, loss 0.6706355810165405\n",
      "epoch 39, loss 0.6706628203392029\n",
      "epoch 40, loss 0.6667630672454834\n",
      "epoch 41, loss 0.6681023836135864\n",
      "epoch 42, loss 0.6686618328094482\n",
      "epoch 43, loss 0.6627583503723145\n",
      "epoch 44, loss 0.6613948941230774\n",
      "epoch 45, loss 0.6627941727638245\n",
      "epoch 46, loss 0.6609769463539124\n",
      "epoch 47, loss 0.6570680737495422\n",
      "epoch 48, loss 0.6600012183189392\n",
      "epoch 49, loss 0.6583755016326904\n",
      "epoch 50, loss 0.6586036682128906\n",
      "epoch 51, loss 0.6587193608283997\n",
      "epoch 52, loss 0.6571688652038574\n",
      "epoch 53, loss 0.6541221141815186\n",
      "epoch 54, loss 0.6562290191650391\n",
      "epoch 55, loss 0.6560450196266174\n",
      "epoch 56, loss 0.659205436706543\n",
      "epoch 57, loss 0.6535946130752563\n",
      "epoch 58, loss 0.652499794960022\n",
      "epoch 59, loss 0.6535585522651672\n",
      "epoch 60, loss 0.6541609764099121\n",
      "epoch 61, loss 0.6520264744758606\n",
      "epoch 62, loss 0.6520591974258423\n",
      "epoch 63, loss 0.6510164737701416\n",
      "epoch 64, loss 0.6497384309768677\n",
      "epoch 65, loss 0.648223340511322\n",
      "epoch 66, loss 0.651069700717926\n",
      "epoch 67, loss 0.6484352946281433\n",
      "epoch 68, loss 0.6504601836204529\n",
      "epoch 69, loss 0.6504614353179932\n",
      "epoch 70, loss 0.6506339907646179\n",
      "epoch 71, loss 0.6473419666290283\n",
      "epoch 72, loss 0.646224319934845\n",
      "epoch 73, loss 0.6460558176040649\n",
      "epoch 74, loss 0.6486266851425171\n",
      "epoch 75, loss 0.6471290588378906\n",
      "epoch 76, loss 0.6462433934211731\n",
      "epoch 77, loss 0.6465824842453003\n",
      "epoch 78, loss 0.6482791900634766\n",
      "epoch 79, loss 0.646239697933197\n",
      "epoch 80, loss 0.6434094309806824\n",
      "epoch 81, loss 0.6451409459114075\n",
      "epoch 82, loss 0.6441932320594788\n",
      "epoch 83, loss 0.6459677815437317\n",
      "epoch 84, loss 0.6453960537910461\n",
      "epoch 85, loss 0.647467315196991\n",
      "epoch 86, loss 0.6454660892486572\n",
      "epoch 87, loss 0.6428388953208923\n",
      "epoch 88, loss 0.6431787610054016\n",
      "epoch 89, loss 0.6445140242576599\n",
      "epoch 90, loss 0.6446425914764404\n",
      "epoch 91, loss 0.6414234042167664\n",
      "epoch 92, loss 0.6433220505714417\n",
      "epoch 93, loss 0.6432586312294006\n",
      "epoch 94, loss 0.6421453356742859\n",
      "epoch 95, loss 0.6392868161201477\n",
      "epoch 96, loss 0.6407126784324646\n",
      "epoch 97, loss 0.640998363494873\n",
      "epoch 98, loss 0.6418710947036743\n",
      "epoch 99, loss 0.6399028897285461\n",
      "2019-03-31 11:11:17.140000, fold=1, rep=1, eta=0d 0h 51m 42s \n",
      "{'fold': 1, 'repeat': 1, 'n': 16599, 'd': 16, 'mse': 0.2000483274459839, 'train_time': 1572.7939022970386, 'trained_epochs': 100, 'prior_train_nmll': 0.6402878761291504, 'train_nll': 6497.3232421875, 'test_nll': 3407.481201171875, 'train_mse': 0.18370114266872406, 'state_dict_file': 'model_state_dict_-5449895748849750553.pkl'}\n",
      "epoch 0, loss 1.0898116827011108\n",
      "epoch 1, loss 1.0670920610427856\n",
      "epoch 2, loss 1.0435611009597778\n",
      "epoch 3, loss 1.0194339752197266\n",
      "epoch 4, loss 0.997704267501831\n",
      "epoch 5, loss 0.9739041924476624\n",
      "epoch 6, loss 0.9503032565116882\n",
      "epoch 7, loss 0.929800271987915\n",
      "epoch 8, loss 0.9090702533721924\n",
      "epoch 9, loss 0.88608318567276\n",
      "epoch 10, loss 0.8681709170341492\n",
      "epoch 11, loss 0.8534102439880371\n",
      "epoch 12, loss 0.8369878530502319\n",
      "epoch 13, loss 0.8205626010894775\n",
      "epoch 14, loss 0.807766854763031\n",
      "epoch 15, loss 0.7979627847671509\n",
      "epoch 16, loss 0.7888787388801575\n",
      "epoch 17, loss 0.779945433139801\n",
      "epoch 18, loss 0.7722039818763733\n",
      "epoch 19, loss 0.7664681077003479\n",
      "epoch 20, loss 0.765960693359375\n",
      "epoch 21, loss 0.7606792449951172\n",
      "epoch 22, loss 0.7547539472579956\n",
      "epoch 23, loss 0.7495684623718262\n",
      "epoch 24, loss 0.7421043515205383\n",
      "epoch 25, loss 0.7378398776054382\n",
      "epoch 26, loss 0.7322692275047302\n",
      "epoch 27, loss 0.7295399904251099\n",
      "epoch 28, loss 0.7231903076171875\n",
      "epoch 29, loss 0.7172706723213196\n",
      "epoch 30, loss 0.7171699404716492\n",
      "epoch 31, loss 0.7097041010856628\n",
      "epoch 32, loss 0.7077917456626892\n",
      "epoch 33, loss 0.7035444378852844\n",
      "epoch 34, loss 0.7050277590751648\n",
      "epoch 35, loss 0.7001613974571228\n",
      "epoch 36, loss 0.6992111206054688\n",
      "epoch 37, loss 0.6966134905815125\n",
      "epoch 38, loss 0.696677029132843\n",
      "epoch 39, loss 0.6952694058418274\n",
      "epoch 40, loss 0.6946789026260376\n",
      "epoch 41, loss 0.6925892233848572\n",
      "epoch 42, loss 0.6926085352897644\n",
      "epoch 43, loss 0.6945966482162476\n",
      "epoch 44, loss 0.690813422203064\n",
      "epoch 45, loss 0.6872919797897339\n",
      "epoch 46, loss 0.689956784248352\n",
      "epoch 47, loss 0.6900469660758972\n",
      "epoch 48, loss 0.686722457408905\n",
      "epoch 49, loss 0.6842268109321594\n",
      "epoch 50, loss 0.6863893270492554\n",
      "epoch 51, loss 0.6847870349884033\n",
      "epoch 52, loss 0.6853654980659485\n",
      "epoch 53, loss 0.6830332279205322\n",
      "epoch 54, loss 0.6864659190177917\n",
      "epoch 55, loss 0.6867128014564514\n",
      "epoch 56, loss 0.6813642978668213\n",
      "epoch 57, loss 0.6783211827278137\n",
      "epoch 58, loss 0.6771959066390991\n",
      "epoch 59, loss 0.6778841614723206\n",
      "epoch 60, loss 0.6752774715423584\n",
      "epoch 61, loss 0.6750102043151855\n",
      "epoch 62, loss 0.6807623505592346\n",
      "epoch 63, loss 0.6740379333496094\n",
      "epoch 64, loss 0.6758244037628174\n",
      "epoch 65, loss 0.6732690930366516\n",
      "epoch 66, loss 0.6734071969985962\n",
      "epoch 67, loss 0.6721197366714478\n",
      "epoch 68, loss 0.6707801222801208\n",
      "epoch 69, loss 0.6744508147239685\n",
      "epoch 70, loss 0.6701180338859558\n",
      "epoch 71, loss 0.6709700226783752\n",
      "epoch 72, loss 0.6674426794052124\n",
      "epoch 73, loss 0.66219562292099\n",
      "epoch 74, loss 0.6683676838874817\n",
      "epoch 75, loss 0.670065701007843\n",
      "epoch 76, loss 0.6657363772392273\n",
      "epoch 77, loss 0.6661239266395569\n",
      "epoch 78, loss 0.6673062443733215\n",
      "epoch 79, loss 0.6669919490814209\n",
      "epoch 80, loss 0.6649271249771118\n",
      "epoch 81, loss 0.6626939177513123\n",
      "epoch 82, loss 0.6646896600723267\n",
      "epoch 83, loss 0.6625199317932129\n",
      "epoch 84, loss 0.6644255518913269\n",
      "epoch 85, loss 0.6609784364700317\n",
      "epoch 86, loss 0.6664803624153137\n",
      "epoch 87, loss 0.6656952500343323\n",
      "epoch 88, loss 0.6600322723388672\n",
      "epoch 89, loss 0.6613811254501343\n",
      "epoch 90, loss 0.664896547794342\n",
      "epoch 91, loss 0.6602360010147095\n",
      "epoch 92, loss 0.6603336930274963\n",
      "epoch 93, loss 0.6584214568138123\n",
      "epoch 94, loss 0.6587152481079102\n",
      "epoch 95, loss 0.6587893962860107\n"
     ]
    }
   ],
   "source": [
    "with gpytorch.settings.cg_tolerance(0.05), gpytorch.settings.fast_pred_var(True):\n",
    "    for dataset in datasets:\n",
    "        print(\"Begin dataset \", dataset)\n",
    "        # with SKI\n",
    "        if len(df[(df['dataset'] == dataset) & (df['scalable_method'] == 'ski')]) == 0:\n",
    "            print(\"Begin with SKI\")\n",
    "            result = rp_experiments.run_experiment(\n",
    "                    training_routines.train_exact_gp, ski_options, \n",
    "                    dataset=dataset, split=1/3, cv=True, repeats=2, normalize_using_train=True)\n",
    "            result['RP'] = True\n",
    "            result['k'] = 1\n",
    "            result['J'] = 20\n",
    "            result['dataset'] = dataset\n",
    "            result['scalable_method'] = 'ski'\n",
    "            result['options'] = json.dumps(ski_options)\n",
    "\n",
    "            df = pd.concat([df, result])\n",
    "            df.to_csv('./comparable_.csv')\n",
    "\n",
    "        print(\"Begin with SVI\")\n",
    "        result = rp_experiments.run_experiment(\n",
    "            training_routines.train_svi_gp, svi_rbf_options, \n",
    "            dataset=dataset, split=1/3, cv=True, repeats=2, normalize_using_train=True)\n",
    "        result['RP'] = False\n",
    "        result['k'] = 1\n",
    "        result['J'] = 20\n",
    "        result['dataset'] = dataset\n",
    "        result['scalable_method'] = 'SVI'\n",
    "        result['options'] = json.dumps(svi_rbf_options)\n",
    "        \n",
    "        df = pd.concat([df, result])\n",
    "#         df.to_csv('./scalable_methods_resultsv2.csv')\n",
    "#         clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HEYO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_performance = pd.read_csv('./3-24-ablation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_performance = rbf_performance[rbf_performance['RP'] == False]\n",
    "rbf_performance = rbf_performance.groupby('dataset')['mse'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dict(verbose=False, ard=False, activation=None, optimizer='adam',\n",
    "               n_epochs=1000, lr=0.1, patience=20, k=1, J=20, smooth=True, \n",
    "               noise_prior=True, learn_weights=False)\n",
    "datasets = rp_experiments.get_small_datasets() + rp_experiments.get_medium_datasets()\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Dataset={}\".format(dataset))\n",
    "    rbf_perf = rbf_performance.loc[dataset]\n",
    "    J_2 = 5\n",
    "    J_1 = 8\n",
    "    performance = np.inf\n",
    "    rel_perf = (performance - rbf_perf) / rbf_perf \n",
    "    while rel_perf > 1.05:\n",
    "        print(\"J={}\".format(J))\n",
    "        J = J_1 + J_2\n",
    "        J_2 = J_1\n",
    "        J_1 = J\n",
    "        options['J'] = J\n",
    "        with gpytorch.settings.cg_tolerance(0.01):\n",
    "            result = rp_experiments.run_experiment(training_routines.train_additive_rp_gp, options, \n",
    "                                         dataset=dataset, split=0.1, cv=True, repeats=1)\n",
    "        result['RP'] = True\n",
    "        result['k'] = 1\n",
    "        result['J'] = J\n",
    "        result['dataset'] = dataset\n",
    "        result['options'] = json.dumps(options)\n",
    "        result['relative_error'] = rel_perf\n",
    "        df = pd.concat([df, result])\n",
    "        df.to_csv('./required_J_study.csv')\n",
    "        \n",
    "        performance = result['mse'].median()\n",
    "        rel_perf = (performance - rbf_perf) / rbf_perf \n",
    "        print(\"relative error {}\".format(rel_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
