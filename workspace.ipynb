{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "The goal of this notebook is to practice using GPyTorch by replicating GP results on test datasets with \n",
    "1. standard SE GPs, \n",
    "2. SM GPs, \n",
    "3. SKI-GPs, and \n",
    "4. Additive GPs. \n",
    "\n",
    "Additive GPs require implementation of the Additive GP model, but this should not be too difficult.\n",
    "\n",
    "Test datasets will include \n",
    "1. Boston Housing (standard regression benchmark with small N=506 and d=13)\n",
    "2. Addition of Sines (synthetic dataset from Additive GP paper N=~20 and d=2)\n",
    "3. Breast (standard classification benchmark with small N=569 and d=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import torch\n",
    "import gpytorch\n",
    "from scipy.io import loadmat\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.kernels import Kernel, ScaleKernel, RBFKernel, GridInterpolationKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.datasets import  fetch_kddcup99, make_friedman1, make_regression, fetch_openml, load_boston, load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "bostonX = torch.FloatTensor(boston['data'])\n",
    "bostonY = torch.FloatTensor(boston['target'])\n",
    "bostonFeats = boston['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506, 13])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bostonX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinesX = np.hstack([\n",
    "    np.vstack([np.random.uniform(-2,-1.5, size=10),\n",
    "               np.random.uniform(-2, 2, size=10)]),\n",
    "    np.vstack([np.random.uniform(-2,2, size=10),\n",
    "               np.random.uniform(-2, -1.5, size=10)])\n",
    "]).T\n",
    "sinesY = np.sin(sinesX[:, 0]*2+.5) + np.sin(sinesX[:,1]*2+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f1cb7fddc50>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_x = _y = np.linspace(-2, 2, num=1000)\n",
    "sinesMG = np.meshgrid(_x, _y)\n",
    "z = np.sin(sinesMG[0]*2+0.5) + np.sin(sinesMG[1]*2.+0.5)\n",
    "plt.contourf(sinesMG[0], sinesMG[1], z, 50, cmap='jet')\n",
    "plt.scatter(sinesX[:, 0], sinesX[:,1], marker='x', c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinesX = torch.FloatTensor(sinesX)\n",
    "sinesY = torch.FloatTensor(sinesY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast = load_breast_cancer()\n",
    "breastX = torch.FloatTensor(breast['data'])\n",
    "breastY = torch.FloatTensor(breast['target'])\n",
    "breastFeats = breast['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569, 30])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlineM = loadmat('./airlinedata (1).mat')\n",
    "airlineX = airlineM['xtrain']\n",
    "airlineY = airlineM['ytrain']\n",
    "airlineXtest = airlineM['xtest']\n",
    "airlineYtest = airlineM['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel):\n",
    "        train_x = torch.FloatTensor(train_x)\n",
    "        train_y = torch.FloatTensor(train_y)\n",
    "        \n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.FloatTensor(x)\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "SE_model = ExactGPModel(bostonX[:400], bostonY[:400], likelihood, \n",
    "                       kernel=ScaleKernel(RBFKernel(ard_num_dims=13)))\n",
    "SE_model.eval()\n",
    "SE_unfit_pred = SE_model(torch.FloatTensor(bostonX[400:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianLikelihood()"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10648.9873046875"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvn = likelihood(SE_model(bostonX[400:]))\n",
    "-mvn.log_prob(bostonY[400:]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 2.924\n",
      "Iter 2/100 - Loss: 2.920\n",
      "Iter 3/100 - Loss: 2.903\n",
      "Iter 4/100 - Loss: 2.896\n",
      "Iter 5/100 - Loss: 2.885\n",
      "Iter 6/100 - Loss: 2.889\n",
      "Iter 7/100 - Loss: 2.872\n",
      "Iter 8/100 - Loss: 2.888\n",
      "Iter 9/100 - Loss: 2.873\n",
      "Iter 10/100 - Loss: 2.891\n",
      "Iter 11/100 - Loss: 2.865\n",
      "Iter 12/100 - Loss: 2.883\n",
      "Iter 13/100 - Loss: 2.878\n",
      "Iter 14/100 - Loss: 2.884\n",
      "Iter 15/100 - Loss: 2.861\n",
      "Iter 16/100 - Loss: 2.875\n",
      "Iter 17/100 - Loss: 2.876\n",
      "Iter 18/100 - Loss: 2.863\n",
      "Iter 19/100 - Loss: 2.859\n",
      "Iter 20/100 - Loss: 2.862\n",
      "Iter 21/100 - Loss: 2.844\n",
      "Iter 22/100 - Loss: 2.838\n",
      "Iter 23/100 - Loss: 2.831\n",
      "Iter 24/100 - Loss: 2.854\n",
      "Iter 25/100 - Loss: 2.835\n",
      "Iter 26/100 - Loss: 2.826\n",
      "Iter 27/100 - Loss: 2.846\n",
      "Iter 28/100 - Loss: 2.829\n",
      "Iter 29/100 - Loss: 2.828\n",
      "Iter 30/100 - Loss: 2.840\n",
      "Iter 31/100 - Loss: 2.839\n",
      "Iter 32/100 - Loss: 2.813\n",
      "Iter 33/100 - Loss: 2.807\n",
      "Iter 34/100 - Loss: 2.812\n",
      "Iter 35/100 - Loss: 2.821\n",
      "Iter 36/100 - Loss: 2.822\n",
      "Iter 37/100 - Loss: 2.803\n",
      "Iter 38/100 - Loss: 2.800\n",
      "Iter 39/100 - Loss: 2.789\n",
      "Iter 40/100 - Loss: 2.822\n",
      "Iter 41/100 - Loss: 2.795\n",
      "Iter 42/100 - Loss: 2.790\n",
      "Iter 43/100 - Loss: 2.805\n",
      "Iter 44/100 - Loss: 2.788\n",
      "Iter 45/100 - Loss: 2.810\n",
      "Iter 46/100 - Loss: 2.785\n",
      "Iter 47/100 - Loss: 2.799\n",
      "Iter 48/100 - Loss: 2.804\n",
      "Iter 49/100 - Loss: 2.795\n",
      "Iter 50/100 - Loss: 2.784\n",
      "Iter 51/100 - Loss: 2.790\n",
      "Iter 52/100 - Loss: 2.793\n",
      "Iter 53/100 - Loss: 2.787\n",
      "Iter 54/100 - Loss: 2.781\n",
      "Iter 55/100 - Loss: 2.772\n",
      "Iter 56/100 - Loss: 2.776\n",
      "Iter 57/100 - Loss: 2.782\n",
      "Iter 58/100 - Loss: 2.761\n",
      "Iter 59/100 - Loss: 2.740\n",
      "Iter 60/100 - Loss: 2.745\n",
      "Iter 61/100 - Loss: 2.733\n",
      "Iter 62/100 - Loss: 2.778\n",
      "Iter 63/100 - Loss: 2.782\n",
      "Iter 64/100 - Loss: 2.773\n",
      "Iter 65/100 - Loss: 2.774\n",
      "Iter 66/100 - Loss: 2.759\n",
      "Iter 67/100 - Loss: 2.741\n",
      "Iter 68/100 - Loss: 2.763\n",
      "Iter 69/100 - Loss: 2.761\n",
      "Iter 70/100 - Loss: 2.749\n",
      "Iter 71/100 - Loss: 2.743\n",
      "Iter 72/100 - Loss: 2.759\n",
      "Iter 73/100 - Loss: 2.716\n",
      "Iter 74/100 - Loss: 2.765\n",
      "Iter 75/100 - Loss: 2.774\n",
      "Iter 76/100 - Loss: 2.741\n",
      "Iter 77/100 - Loss: 2.756\n",
      "Iter 78/100 - Loss: 2.721\n",
      "Iter 79/100 - Loss: 2.746\n",
      "Iter 80/100 - Loss: 2.751\n",
      "Iter 81/100 - Loss: 2.755\n",
      "Iter 82/100 - Loss: 2.754\n",
      "Iter 83/100 - Loss: 2.718\n",
      "Iter 84/100 - Loss: 2.732\n",
      "Iter 85/100 - Loss: 2.738\n",
      "Iter 86/100 - Loss: 2.730\n",
      "Iter 87/100 - Loss: 2.721\n",
      "Iter 88/100 - Loss: 2.723\n",
      "Iter 89/100 - Loss: 2.730\n",
      "Iter 90/100 - Loss: 2.737\n",
      "Iter 91/100 - Loss: 2.700\n",
      "Iter 92/100 - Loss: 2.735\n",
      "Iter 93/100 - Loss: 2.739\n",
      "Iter 94/100 - Loss: 2.721\n",
      "Iter 95/100 - Loss: 2.731\n",
      "Iter 96/100 - Loss: 2.702\n",
      "Iter 97/100 - Loss: 2.724\n",
      "Iter 98/100 - Loss: 2.730\n",
      "Iter 99/100 - Loss: 2.733\n",
      "Iter 100/100 - Loss: 2.702\n"
     ]
    }
   ],
   "source": [
    "# Go into eval mode\n",
    "SE_model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(SE_model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# num_data refers to the amount of training data\n",
    "mll = gpytorch.ExactMarginalLogLikelihood(likelihood, SE_model)\n",
    "\n",
    "training_iter = 100\n",
    "for i in range(training_iter):\n",
    "    # Zero backpropped gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Get predictive output\n",
    "    output = SE_model(bostonX[:400])\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, bostonY[:400])\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.3794, 10.3302, 15.6194, 11.1963, 14.3626, 15.1207, 18.7652, 39.8639,\n",
       "        17.0207, 23.5774, 23.5876, 23.5872, 23.5874, 23.2875, 23.5859, 23.5875,\n",
       "        23.5876, 23.5392, 23.5876, 23.5872, 24.5492, 23.1149, 25.3862, 23.5876,\n",
       "        23.5875, 23.5876, 23.5872, 23.5875, 23.5690, 23.5851, 23.5734, 23.5824,\n",
       "        23.5688, 23.5690, 23.5569, 23.5794, 23.5874, 23.5876, 23.5869, 12.7545,\n",
       "        12.2414, 15.4389, 14.4746, 14.9961, 21.5751, 23.5872, 19.0639, 16.0394,\n",
       "        11.7378, 19.5277, 23.5876, 22.4306, 18.4567, 18.4905, 23.5876, 23.5862,\n",
       "        23.5875, 23.5876, 21.9679, 18.9077, 22.9584, 20.3460, 20.0563, 25.6732,\n",
       "        26.6917, 30.7687, 23.5872, 24.7637, 30.1992, 26.8935, 24.1644, 30.4813,\n",
       "        28.5684, 34.8837, 32.3795, 20.3706, 17.5227, 17.4660, 23.3973, 32.9636,\n",
       "        30.5663, 35.6017, 35.4547, 27.3007, 31.2105, 31.8276, 27.9354, 32.1201,\n",
       "        22.8805, 22.8368, 21.6571, 22.9101, 24.7735, 21.2188, 21.9450, 21.9951,\n",
       "        20.2227, 20.4257, 21.7092, 19.8427, 20.4678, 22.8891, 17.7921, 24.3467,\n",
       "        23.0442, 16.9239], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SE_model.eval()\n",
    "likelihood.eval()\n",
    "SE_fit_pred = SE_model(bostonX[400:])\n",
    "SE_fit_pred.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(77.5886, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((SE_fit_pred.mean - bostonY[400:])**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f1cc45889e8>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(SE_fit_pred.mean.detach().numpy(), bostonY[400:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f1cb6446400>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(bostonX[:400].numpy(),\n",
    "       bostonY[:400].numpy())\n",
    "rf_pred = rf.predict(bostonX[400:].numpy())\n",
    "plt.scatter(rf_pred, bostonY[400:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.36819978880272"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((rf_pred - bostonY[400:].numpy())**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f1cb641ea20>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(bostonX[:400].numpy(),\n",
    "       bostonY[:400].numpy())\n",
    "lr_pred = lr.predict(bostonX[400:].numpy())\n",
    "plt.scatter(lr_pred, bostonY[400:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.89387"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((lr_pred - bostonY[400:].numpy())**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import AdditiveKernel, AdditiveStructureKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "AGP_model = ExactGPModel(bostonX[:400], bostonY[:400], likelihood, \n",
    "                       kernel=ScaleKernel(AdditiveStructureKernel(RBFKernel(), 13)))\n",
    "AGP_model.eval()\n",
    "AGP_unfit_pred = AGP_model(torch.FloatTensor(bostonX[400:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 6.904\n",
      "Iter 2/100 - Loss: 6.506\n",
      "Iter 3/100 - Loss: 6.146\n",
      "Iter 4/100 - Loss: 5.863\n",
      "Iter 5/100 - Loss: 5.578\n",
      "Iter 6/100 - Loss: 5.354\n",
      "Iter 7/100 - Loss: 5.141\n",
      "Iter 8/100 - Loss: 4.946\n",
      "Iter 9/100 - Loss: 4.787\n",
      "Iter 10/100 - Loss: 4.636\n",
      "Iter 11/100 - Loss: 4.501\n",
      "Iter 12/100 - Loss: 4.384\n",
      "Iter 13/100 - Loss: 4.319\n",
      "Iter 14/100 - Loss: 4.221\n",
      "Iter 15/100 - Loss: 4.133\n",
      "Iter 16/100 - Loss: 4.079\n",
      "Iter 17/100 - Loss: 4.021\n",
      "Iter 18/100 - Loss: 3.962\n",
      "Iter 19/100 - Loss: 3.933\n",
      "Iter 20/100 - Loss: 3.888\n",
      "Iter 21/100 - Loss: 3.840\n",
      "Iter 22/100 - Loss: 3.804\n",
      "Iter 23/100 - Loss: 3.769\n",
      "Iter 24/100 - Loss: 3.748\n",
      "Iter 25/100 - Loss: 3.710\n",
      "Iter 26/100 - Loss: 3.682\n",
      "Iter 27/100 - Loss: 3.660\n",
      "Iter 28/100 - Loss: 3.632\n",
      "Iter 29/100 - Loss: 3.613\n",
      "Iter 30/100 - Loss: 3.601\n",
      "Iter 31/100 - Loss: 3.554\n",
      "Iter 32/100 - Loss: 3.556\n",
      "Iter 33/100 - Loss: 3.554\n",
      "Iter 34/100 - Loss: 3.526\n",
      "Iter 35/100 - Loss: 3.504\n",
      "Iter 36/100 - Loss: 3.475\n",
      "Iter 37/100 - Loss: 3.485\n",
      "Iter 38/100 - Loss: 3.473\n",
      "Iter 39/100 - Loss: 3.433\n",
      "Iter 40/100 - Loss: 3.432\n",
      "Iter 41/100 - Loss: 3.431\n",
      "Iter 42/100 - Loss: 3.419\n",
      "Iter 43/100 - Loss: 3.430\n",
      "Iter 44/100 - Loss: 3.405\n",
      "Iter 45/100 - Loss: 3.376\n",
      "Iter 46/100 - Loss: 3.394\n",
      "Iter 47/100 - Loss: 3.376\n",
      "Iter 48/100 - Loss: 3.352\n",
      "Iter 49/100 - Loss: 3.365\n",
      "Iter 50/100 - Loss: 3.367\n",
      "Iter 51/100 - Loss: 3.362\n",
      "Iter 52/100 - Loss: 3.336\n",
      "Iter 53/100 - Loss: 3.311\n",
      "Iter 54/100 - Loss: 3.336\n",
      "Iter 55/100 - Loss: 3.319\n",
      "Iter 56/100 - Loss: 3.315\n",
      "Iter 57/100 - Loss: 3.326\n",
      "Iter 58/100 - Loss: 3.320\n",
      "Iter 59/100 - Loss: 3.314\n",
      "Iter 60/100 - Loss: 3.281\n",
      "Iter 61/100 - Loss: 3.315\n",
      "Iter 62/100 - Loss: 3.305\n",
      "Iter 63/100 - Loss: 3.279\n",
      "Iter 64/100 - Loss: 3.279\n",
      "Iter 65/100 - Loss: 3.278\n",
      "Iter 66/100 - Loss: 3.272\n",
      "Iter 67/100 - Loss: 3.301\n",
      "Iter 68/100 - Loss: 3.280\n",
      "Iter 69/100 - Loss: 3.273\n",
      "Iter 70/100 - Loss: 3.226\n",
      "Iter 71/100 - Loss: 3.260\n",
      "Iter 72/100 - Loss: 3.240\n",
      "Iter 73/100 - Loss: 3.261\n",
      "Iter 74/100 - Loss: 3.244\n",
      "Iter 75/100 - Loss: 3.244\n",
      "Iter 76/100 - Loss: 3.239\n",
      "Iter 77/100 - Loss: 3.231\n",
      "Iter 78/100 - Loss: 3.233\n",
      "Iter 79/100 - Loss: 3.208\n",
      "Iter 80/100 - Loss: 3.226\n",
      "Iter 81/100 - Loss: 3.247\n",
      "Iter 82/100 - Loss: 3.226\n",
      "Iter 83/100 - Loss: 3.235\n",
      "Iter 84/100 - Loss: 3.190\n",
      "Iter 85/100 - Loss: 3.223\n",
      "Iter 86/100 - Loss: 3.236\n",
      "Iter 87/100 - Loss: 3.215\n",
      "Iter 88/100 - Loss: 3.209\n",
      "Iter 89/100 - Loss: 3.198\n",
      "Iter 90/100 - Loss: 3.224\n",
      "Iter 91/100 - Loss: 3.195\n",
      "Iter 92/100 - Loss: 3.209\n",
      "Iter 93/100 - Loss: 3.200\n",
      "Iter 94/100 - Loss: 3.203\n",
      "Iter 95/100 - Loss: 3.215\n",
      "Iter 96/100 - Loss: 3.186\n",
      "Iter 97/100 - Loss: 3.196\n",
      "Iter 98/100 - Loss: 3.185\n",
      "Iter 99/100 - Loss: 3.195\n",
      "Iter 100/100 - Loss: 3.181\n"
     ]
    }
   ],
   "source": [
    "# Go into eval mode\n",
    "AGP_model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(AGP_model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# num_data refers to the amount of training data\n",
    "mll = gpytorch.ExactMarginalLogLikelihood(likelihood, AGP_model)\n",
    "\n",
    "training_iter = 100\n",
    "for i in range(training_iter):\n",
    "    # Zero backpropped gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Get predictive output\n",
    "    output = AGP_model(bostonX[:400])\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, bostonY[:400])\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.8555,  8.7282, 12.9529,  9.6916, 11.0755, 14.5148, 18.0006, 18.0716,\n",
       "        15.2729, 18.0327, 19.5550, 15.8997, 10.0749, 12.4962, 13.9026,  8.9114,\n",
       "        17.1384, 12.8052, 10.0009, 13.1471, 10.6620, 17.3558, 14.1306, 16.6623,\n",
       "        19.2602, 11.7279, 15.8409, 13.2339, 12.1808, 16.5079, 17.6181, 13.9206,\n",
       "        23.9585, 20.1836, 16.0882,  9.2202, 12.2938,  8.5252, 12.2940, 11.2979,\n",
       "         6.0960, 13.5742, 15.2364, 13.3317, 14.3192,  7.6870, 21.6310, 15.2520,\n",
       "        16.5050, 15.2885, 20.2353, 21.0744, 23.4910, 31.7036, 19.8482, 23.6576,\n",
       "        22.6534, 21.6986, 16.2044, 16.6136, 28.2627, 12.8557, 18.2114, 22.6899,\n",
       "        17.8672, 13.4117, 15.1613, 18.4627, 16.7069, 14.4303, 18.1081, 23.9223,\n",
       "        16.9656, 33.1828, 22.4702, 20.3938, 24.8797, 14.4196, 16.3722, 14.8971,\n",
       "        21.0152, 27.3114, 28.7738, 13.3293, 14.9573, 16.6483, 20.1965, 27.4391,\n",
       "        12.9622, 10.9007, 13.4018, 13.0222, 14.0223, 16.2295, 16.3494, 15.5709,\n",
       "        14.4523, 16.8920, 18.7822, 15.7397, 16.9602, 20.1859, 16.2826, 26.7251,\n",
       "        25.6965, 20.6823], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGP_model.eval()\n",
    "likelihood.eval()\n",
    "AGP_fit_pred = AGP_model(bostonX[400:])\n",
    "AGP_fit_pred.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.249012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f1cb63afa90>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(((AGP_fit_pred.mean.detach().numpy() - bostonY[400:].numpy())**2).mean())\n",
    "plt.scatter(AGP_fit_pred.mean.detach().numpy(), bostonY[400:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New dataset - just working on the Sin functions now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gp_model(gp_model, gp_likelihood, xs, ys, lr=0.1, gp_mll=None, training_iter=100):\n",
    "    if gp_mll is None:\n",
    "        gp_mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
    "    \n",
    "    gp_model.train()\n",
    "    gp_likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(gp_model.parameters(), lr=lr)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    # num_data refers to the amount of training data\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero backpropped gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Get predictive output\n",
    "        output = gp_model(xs)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -gp_mll(output, ys)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = gpytorch.likelihoods.GaussianLikelihood()\n",
    "SE_model = ExactGPModel(sinesX, sinesY, likelihood=l, kernel=ScaleKernel(RBFKernel(ard_num_dims=2)))\n",
    "l2 = gpytorch.likelihoods.GaussianLikelihood()\n",
    "AGP_model = ExactGPModel(sinesX, sinesY, likelihood=l,\n",
    "                        kernel=ScaleKernel(AdditiveStructureKernel(RBFKernel(), 2)))\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(sinesX.numpy(), sinesY.numpy())\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(sinesX.numpy(), sinesY.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 1.643\n",
      "Iter 2/100 - Loss: 1.635\n",
      "Iter 3/100 - Loss: 1.628\n",
      "Iter 4/100 - Loss: 1.619\n",
      "Iter 5/100 - Loss: 1.610\n",
      "Iter 6/100 - Loss: 1.620\n",
      "Iter 7/100 - Loss: 1.614\n",
      "Iter 8/100 - Loss: 1.612\n",
      "Iter 9/100 - Loss: 1.606\n",
      "Iter 10/100 - Loss: 1.604\n",
      "Iter 11/100 - Loss: 1.604\n",
      "Iter 12/100 - Loss: 1.600\n",
      "Iter 13/100 - Loss: 1.598\n",
      "Iter 14/100 - Loss: 1.597\n",
      "Iter 15/100 - Loss: 1.594\n",
      "Iter 16/100 - Loss: 1.593\n",
      "Iter 17/100 - Loss: 1.592\n",
      "Iter 18/100 - Loss: 1.591\n",
      "Iter 19/100 - Loss: 1.591\n",
      "Iter 20/100 - Loss: 1.589\n",
      "Iter 21/100 - Loss: 1.590\n",
      "Iter 22/100 - Loss: 1.587\n",
      "Iter 23/100 - Loss: 1.586\n",
      "Iter 24/100 - Loss: 1.585\n",
      "Iter 25/100 - Loss: 1.585\n",
      "Iter 26/100 - Loss: 1.584\n",
      "Iter 27/100 - Loss: 1.584\n",
      "Iter 28/100 - Loss: 1.583\n",
      "Iter 29/100 - Loss: 1.582\n",
      "Iter 30/100 - Loss: 1.582\n",
      "Iter 31/100 - Loss: 1.581\n",
      "Iter 32/100 - Loss: 1.581\n",
      "Iter 33/100 - Loss: 1.580\n",
      "Iter 34/100 - Loss: 1.580\n",
      "Iter 35/100 - Loss: 1.580\n",
      "Iter 36/100 - Loss: 1.580\n",
      "Iter 37/100 - Loss: 1.579\n",
      "Iter 38/100 - Loss: 1.579\n",
      "Iter 39/100 - Loss: 1.579\n",
      "Iter 40/100 - Loss: 1.578\n",
      "Iter 41/100 - Loss: 1.578\n",
      "Iter 42/100 - Loss: 1.578\n",
      "Iter 43/100 - Loss: 1.578\n",
      "Iter 44/100 - Loss: 1.578\n",
      "Iter 45/100 - Loss: 1.577\n",
      "Iter 46/100 - Loss: 1.577\n",
      "Iter 47/100 - Loss: 1.577\n",
      "Iter 48/100 - Loss: 1.577\n",
      "Iter 49/100 - Loss: 1.577\n",
      "Iter 50/100 - Loss: 1.577\n",
      "Iter 51/100 - Loss: 1.577\n",
      "Iter 52/100 - Loss: 1.577\n",
      "Iter 53/100 - Loss: 1.576\n",
      "Iter 54/100 - Loss: 1.576\n",
      "Iter 55/100 - Loss: 1.576\n",
      "Iter 56/100 - Loss: 1.576\n",
      "Iter 57/100 - Loss: 1.576\n",
      "Iter 58/100 - Loss: 1.576\n",
      "Iter 59/100 - Loss: 1.576\n",
      "Iter 60/100 - Loss: 1.576\n",
      "Iter 61/100 - Loss: 1.576\n",
      "Iter 62/100 - Loss: 1.576\n",
      "Iter 63/100 - Loss: 1.576\n",
      "Iter 64/100 - Loss: 1.576\n",
      "Iter 65/100 - Loss: 1.575\n",
      "Iter 66/100 - Loss: 1.575\n",
      "Iter 67/100 - Loss: 1.576\n",
      "Iter 68/100 - Loss: 1.575\n",
      "Iter 69/100 - Loss: 1.575\n",
      "Iter 70/100 - Loss: 1.575\n",
      "Iter 71/100 - Loss: 1.575\n",
      "Iter 72/100 - Loss: 1.575\n",
      "Iter 73/100 - Loss: 1.575\n",
      "Iter 74/100 - Loss: 1.575\n",
      "Iter 75/100 - Loss: 1.575\n",
      "Iter 76/100 - Loss: 1.575\n",
      "Iter 77/100 - Loss: 1.575\n",
      "Iter 78/100 - Loss: 1.575\n",
      "Iter 79/100 - Loss: 1.575\n",
      "Iter 80/100 - Loss: 1.575\n",
      "Iter 81/100 - Loss: 1.575\n",
      "Iter 82/100 - Loss: 1.575\n",
      "Iter 83/100 - Loss: 1.575\n",
      "Iter 84/100 - Loss: 1.575\n",
      "Iter 85/100 - Loss: 1.575\n",
      "Iter 86/100 - Loss: 1.575\n",
      "Iter 87/100 - Loss: 1.575\n",
      "Iter 88/100 - Loss: 1.575\n",
      "Iter 89/100 - Loss: 1.575\n",
      "Iter 90/100 - Loss: 1.575\n",
      "Iter 91/100 - Loss: 1.575\n",
      "Iter 92/100 - Loss: 1.575\n",
      "Iter 93/100 - Loss: 1.575\n",
      "Iter 94/100 - Loss: 1.575\n",
      "Iter 95/100 - Loss: 1.575\n",
      "Iter 96/100 - Loss: 1.575\n",
      "Iter 97/100 - Loss: 1.575\n",
      "Iter 98/100 - Loss: 1.575\n",
      "Iter 99/100 - Loss: 1.574\n",
      "Iter 100/100 - Loss: 1.575\n",
      "Iter 1/100 - Loss: 1.667\n",
      "Iter 2/100 - Loss: 1.656\n",
      "Iter 3/100 - Loss: 1.647\n",
      "Iter 4/100 - Loss: 1.648\n",
      "Iter 5/100 - Loss: 1.645\n",
      "Iter 6/100 - Loss: 1.637\n",
      "Iter 7/100 - Loss: 1.635\n",
      "Iter 8/100 - Loss: 1.632\n",
      "Iter 9/100 - Loss: 1.634\n",
      "Iter 10/100 - Loss: 1.626\n",
      "Iter 11/100 - Loss: 1.623\n",
      "Iter 12/100 - Loss: 1.622\n",
      "Iter 13/100 - Loss: 1.618\n",
      "Iter 14/100 - Loss: 1.616\n",
      "Iter 15/100 - Loss: 1.614\n",
      "Iter 16/100 - Loss: 1.612\n",
      "Iter 17/100 - Loss: 1.609\n",
      "Iter 18/100 - Loss: 1.608\n",
      "Iter 19/100 - Loss: 1.606\n",
      "Iter 20/100 - Loss: 1.605\n",
      "Iter 21/100 - Loss: 1.602\n",
      "Iter 22/100 - Loss: 1.601\n",
      "Iter 23/100 - Loss: 1.599\n",
      "Iter 24/100 - Loss: 1.597\n",
      "Iter 25/100 - Loss: 1.596\n",
      "Iter 26/100 - Loss: 1.595\n",
      "Iter 27/100 - Loss: 1.594\n",
      "Iter 28/100 - Loss: 1.593\n",
      "Iter 29/100 - Loss: 1.591\n",
      "Iter 30/100 - Loss: 1.590\n",
      "Iter 31/100 - Loss: 1.590\n",
      "Iter 32/100 - Loss: 1.589\n",
      "Iter 33/100 - Loss: 1.588\n",
      "Iter 34/100 - Loss: 1.587\n",
      "Iter 35/100 - Loss: 1.587\n",
      "Iter 36/100 - Loss: 1.586\n",
      "Iter 37/100 - Loss: 1.585\n",
      "Iter 38/100 - Loss: 1.585\n",
      "Iter 39/100 - Loss: 1.584\n",
      "Iter 40/100 - Loss: 1.583\n",
      "Iter 41/100 - Loss: 1.583\n",
      "Iter 42/100 - Loss: 1.583\n",
      "Iter 43/100 - Loss: 1.582\n",
      "Iter 44/100 - Loss: 1.582\n",
      "Iter 45/100 - Loss: 1.582\n",
      "Iter 46/100 - Loss: 1.581\n",
      "Iter 47/100 - Loss: 1.581\n",
      "Iter 48/100 - Loss: 1.581\n",
      "Iter 49/100 - Loss: 1.580\n",
      "Iter 50/100 - Loss: 1.580\n",
      "Iter 51/100 - Loss: 1.580\n",
      "Iter 52/100 - Loss: 1.580\n",
      "Iter 53/100 - Loss: 1.579\n",
      "Iter 54/100 - Loss: 1.579\n",
      "Iter 55/100 - Loss: 1.579\n",
      "Iter 56/100 - Loss: 1.579\n",
      "Iter 57/100 - Loss: 1.579\n",
      "Iter 58/100 - Loss: 1.578\n",
      "Iter 59/100 - Loss: 1.578\n",
      "Iter 60/100 - Loss: 1.578\n",
      "Iter 61/100 - Loss: 1.578\n",
      "Iter 62/100 - Loss: 1.578\n",
      "Iter 63/100 - Loss: 1.578\n",
      "Iter 64/100 - Loss: 1.578\n",
      "Iter 65/100 - Loss: 1.578\n",
      "Iter 66/100 - Loss: 1.577\n",
      "Iter 67/100 - Loss: 1.577\n",
      "Iter 68/100 - Loss: 1.577\n",
      "Iter 69/100 - Loss: 1.577\n",
      "Iter 70/100 - Loss: 1.577\n",
      "Iter 71/100 - Loss: 1.577\n",
      "Iter 72/100 - Loss: 1.577\n",
      "Iter 73/100 - Loss: 1.577\n",
      "Iter 74/100 - Loss: 1.577\n",
      "Iter 75/100 - Loss: 1.577\n",
      "Iter 76/100 - Loss: 1.577\n",
      "Iter 77/100 - Loss: 1.576\n",
      "Iter 78/100 - Loss: 1.576\n",
      "Iter 79/100 - Loss: 1.576\n",
      "Iter 80/100 - Loss: 1.576\n",
      "Iter 81/100 - Loss: 1.576\n",
      "Iter 82/100 - Loss: 1.576\n",
      "Iter 83/100 - Loss: 1.576\n",
      "Iter 84/100 - Loss: 1.576\n",
      "Iter 85/100 - Loss: 1.576\n",
      "Iter 86/100 - Loss: 1.576\n",
      "Iter 87/100 - Loss: 1.576\n",
      "Iter 88/100 - Loss: 1.576\n",
      "Iter 89/100 - Loss: 1.576\n",
      "Iter 90/100 - Loss: 1.576\n",
      "Iter 91/100 - Loss: 1.576\n",
      "Iter 92/100 - Loss: 1.576\n",
      "Iter 93/100 - Loss: 1.576\n",
      "Iter 94/100 - Loss: 1.576\n",
      "Iter 95/100 - Loss: 1.576\n",
      "Iter 96/100 - Loss: 1.576\n",
      "Iter 97/100 - Loss: 1.576\n",
      "Iter 98/100 - Loss: 1.575\n",
      "Iter 99/100 - Loss: 1.575\n",
      "Iter 100/100 - Loss: 1.575\n"
     ]
    }
   ],
   "source": [
    "fit_gp_model(SE_model, l, sinesX, sinesY)\n",
    "fit_gp_model(AGP_model, l2, sinesX, sinesY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 2)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinesMGflat = np.vstack([sinesMG[0].flatten(), sinesMG[1].flatten()]).T\n",
    "sinesMGflat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = rf_model.predict(sinesMGflat)\n",
    "lr_pred = lr_model.predict(sinesMGflat)\n",
    "SE_model.eval()\n",
    "AGP_model.eval()\n",
    "SE_pred = SE_model(torch.FloatTensor(sinesMGflat)).mean.detach().numpy()\n",
    "AGP_pred = AGP_model(torch.FloatTensor(sinesMGflat)).mean.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(3, 2, sharex=True, sharey=True, figsize=[10,10])\n",
    "ax[0,0].contourf(sinesMG[0], sinesMG[1], z, 50, cmap='jet')\n",
    "ax[0,0].scatter(sinesX[:, 0], sinesX[:,1], marker='x', c='k')\n",
    "ax[0,0].set_title('True Function')\n",
    "\n",
    "ax[1,0].contourf(sinesMG[0], sinesMG[1], rf_pred.reshape(1000,1000), 50, cmap='jet')\n",
    "ax[1,0].scatter(sinesX[:, 0], sinesX[:,1], marker='x', c='k')\n",
    "ax[1,0].set_title('Random Forest Regression')\n",
    "\n",
    "ax[1,1].contourf(sinesMG[0], sinesMG[1], lr_pred.reshape(1000,1000), 50, cmap='jet')\n",
    "ax[1,1].scatter(sinesX[:, 0], sinesX[:,1], marker='x', c='k')\n",
    "ax[1,1].set_title('Linear Regression')\n",
    "\n",
    "ax[2,0].contourf(sinesMG[0], sinesMG[1], SE_pred.reshape(1000,1000), 50, cmap='jet')\n",
    "ax[2,0].scatter(sinesX[:, 0], sinesX[:,1], marker='x', c='k')\n",
    "ax[2,0].set_title('SE GP Regression')\n",
    "\n",
    "ax[2,1].contourf(sinesMG[0], sinesMG[1], AGP_pred.reshape(1000,1000), 50, cmap='jet')\n",
    "ax[2,1].scatter(sinesX[:, 0], sinesX[:,1], marker='x', c='k')\n",
    "ax[2,1].set_title('Additive GP Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New task: classification. Now using variational inference. The one where u is integrated out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPClassificationModel(AbstractVariationalGP,):\n",
    "    def __init__(self, train_x, kernel):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, train_x, variational_distribution)\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        return latent_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569, 30])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "SE_model = GPClassificationModel(breastX[:400], \n",
    "                                 kernel=ScaleKernel(RBFKernel(ard_num_dims=30)))\n",
    "l2 = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "AGP_model = GPClassificationModel(breastX[:400],\n",
    "                                  kernel=ScaleKernel(AdditiveStructureKernel(RBFKernel(), 2)))\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(breastX[:400].numpy(), breastY[:400].numpy())\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(breastX[:400], breastY[:400].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 0.829\n",
      "Iter 2/100 - Loss: 2.288\n",
      "Iter 3/100 - Loss: 2.699\n",
      "Iter 4/100 - Loss: 2.826\n",
      "Iter 5/100 - Loss: 3.181\n",
      "Iter 6/100 - Loss: 2.366\n",
      "Iter 7/100 - Loss: 2.588\n",
      "Iter 8/100 - Loss: 2.376\n",
      "Iter 9/100 - Loss: 2.555\n",
      "Iter 10/100 - Loss: 2.697\n",
      "Iter 11/100 - Loss: 2.231\n",
      "Iter 12/100 - Loss: 2.107\n",
      "Iter 13/100 - Loss: 1.948\n",
      "Iter 14/100 - Loss: 2.203\n",
      "Iter 15/100 - Loss: 2.000\n",
      "Iter 16/100 - Loss: 1.822\n",
      "Iter 17/100 - Loss: 1.819\n",
      "Iter 18/100 - Loss: 2.081\n",
      "Iter 19/100 - Loss: 1.636\n",
      "Iter 20/100 - Loss: 1.716\n",
      "Iter 21/100 - Loss: 1.887\n",
      "Iter 22/100 - Loss: 1.970\n",
      "Iter 23/100 - Loss: 1.590\n",
      "Iter 24/100 - Loss: 1.723\n",
      "Iter 25/100 - Loss: 1.817\n",
      "Iter 26/100 - Loss: 2.065\n",
      "Iter 27/100 - Loss: 1.638\n",
      "Iter 28/100 - Loss: 1.339\n",
      "Iter 29/100 - Loss: 1.305\n",
      "Iter 30/100 - Loss: 1.389\n",
      "Iter 31/100 - Loss: 1.213\n",
      "Iter 32/100 - Loss: 1.443\n",
      "Iter 33/100 - Loss: 1.731\n",
      "Iter 34/100 - Loss: 1.243\n",
      "Iter 35/100 - Loss: 1.294\n",
      "Iter 36/100 - Loss: 1.174\n",
      "Iter 37/100 - Loss: 1.329\n",
      "Iter 38/100 - Loss: 1.079\n",
      "Iter 39/100 - Loss: 1.302\n",
      "Iter 40/100 - Loss: 1.159\n",
      "Iter 41/100 - Loss: 1.008\n",
      "Iter 42/100 - Loss: 0.968\n",
      "Iter 43/100 - Loss: 1.138\n",
      "Iter 44/100 - Loss: 1.286\n",
      "Iter 45/100 - Loss: 0.968\n",
      "Iter 46/100 - Loss: 1.231\n",
      "Iter 47/100 - Loss: 0.988\n",
      "Iter 48/100 - Loss: 0.896\n",
      "Iter 49/100 - Loss: 1.007\n",
      "Iter 50/100 - Loss: 0.883\n",
      "Iter 51/100 - Loss: 0.826\n",
      "Iter 52/100 - Loss: 0.978\n",
      "Iter 53/100 - Loss: 0.847\n",
      "Iter 54/100 - Loss: 0.808\n",
      "Iter 55/100 - Loss: 0.908\n",
      "Iter 56/100 - Loss: 0.821\n",
      "Iter 57/100 - Loss: 0.834\n",
      "Iter 58/100 - Loss: 0.774\n",
      "Iter 59/100 - Loss: 0.772\n",
      "Iter 60/100 - Loss: 0.866\n",
      "Iter 61/100 - Loss: 0.762\n",
      "Iter 62/100 - Loss: 0.800\n",
      "Iter 63/100 - Loss: 0.732\n",
      "Iter 64/100 - Loss: 0.804\n",
      "Iter 65/100 - Loss: 0.678\n",
      "Iter 66/100 - Loss: 0.705\n",
      "Iter 67/100 - Loss: 0.666\n",
      "Iter 68/100 - Loss: 0.669\n",
      "Iter 69/100 - Loss: 0.665\n",
      "Iter 70/100 - Loss: 0.649\n",
      "Iter 71/100 - Loss: 0.633\n",
      "Iter 72/100 - Loss: 0.680\n",
      "Iter 73/100 - Loss: 0.708\n",
      "Iter 74/100 - Loss: 0.640\n",
      "Iter 75/100 - Loss: 0.603\n",
      "Iter 76/100 - Loss: 0.601\n",
      "Iter 77/100 - Loss: 0.584\n",
      "Iter 78/100 - Loss: 0.604\n",
      "Iter 79/100 - Loss: 0.576\n",
      "Iter 80/100 - Loss: 0.601\n",
      "Iter 81/100 - Loss: 0.572\n",
      "Iter 82/100 - Loss: 0.556\n",
      "Iter 83/100 - Loss: 0.556\n",
      "Iter 84/100 - Loss: 0.559\n",
      "Iter 85/100 - Loss: 0.530\n",
      "Iter 86/100 - Loss: 0.523\n",
      "Iter 87/100 - Loss: 0.531\n",
      "Iter 88/100 - Loss: 0.507\n",
      "Iter 89/100 - Loss: 0.499\n",
      "Iter 90/100 - Loss: 0.488\n",
      "Iter 91/100 - Loss: 0.492\n",
      "Iter 92/100 - Loss: 0.494\n",
      "Iter 93/100 - Loss: 0.483\n",
      "Iter 94/100 - Loss: 0.490\n",
      "Iter 95/100 - Loss: 0.472\n",
      "Iter 96/100 - Loss: 0.459\n",
      "Iter 97/100 - Loss: 0.466\n",
      "Iter 98/100 - Loss: 0.466\n",
      "Iter 99/100 - Loss: 0.460\n",
      "Iter 100/100 - Loss: 0.453\n",
      "Iter 1/100 - Loss: 1.340\n",
      "Iter 2/100 - Loss: 1.854\n",
      "Iter 3/100 - Loss: 3.776\n",
      "Iter 4/100 - Loss: 2.340\n",
      "Iter 5/100 - Loss: 3.688\n",
      "Iter 6/100 - Loss: 4.088\n",
      "Iter 7/100 - Loss: 3.366\n",
      "Iter 8/100 - Loss: 5.856\n",
      "Iter 9/100 - Loss: 4.963\n",
      "Iter 10/100 - Loss: 3.888\n",
      "Iter 11/100 - Loss: 5.491\n",
      "Iter 12/100 - Loss: 4.381\n",
      "Iter 13/100 - Loss: 4.470\n",
      "Iter 14/100 - Loss: 4.396\n",
      "Iter 15/100 - Loss: 1.953\n",
      "Iter 16/100 - Loss: 5.634\n",
      "Iter 17/100 - Loss: 3.525\n",
      "Iter 18/100 - Loss: 3.135\n",
      "Iter 19/100 - Loss: 1.814\n",
      "Iter 20/100 - Loss: 1.582\n",
      "Iter 21/100 - Loss: 6.575\n",
      "Iter 22/100 - Loss: 1.679\n",
      "Iter 23/100 - Loss: 8.478\n",
      "Iter 24/100 - Loss: 6.016\n",
      "Iter 25/100 - Loss: 4.827\n",
      "Iter 26/100 - Loss: 3.017\n",
      "Iter 27/100 - Loss: 5.527\n",
      "Iter 28/100 - Loss: 4.697\n",
      "Iter 29/100 - Loss: 5.196\n",
      "Iter 30/100 - Loss: 3.781\n",
      "Iter 31/100 - Loss: 2.958\n",
      "Iter 32/100 - Loss: 6.866\n",
      "Iter 33/100 - Loss: 3.192\n",
      "Iter 34/100 - Loss: 4.225\n",
      "Iter 35/100 - Loss: 2.533\n",
      "Iter 36/100 - Loss: 2.959\n",
      "Iter 37/100 - Loss: 2.752\n",
      "Iter 38/100 - Loss: 2.065\n",
      "Iter 39/100 - Loss: 7.827\n",
      "Iter 40/100 - Loss: 3.695\n",
      "Iter 41/100 - Loss: 4.774\n",
      "Iter 42/100 - Loss: 2.927\n",
      "Iter 43/100 - Loss: 2.886\n",
      "Iter 44/100 - Loss: 2.078\n",
      "Iter 45/100 - Loss: 3.374\n",
      "Iter 46/100 - Loss: 13.924\n",
      "Iter 47/100 - Loss: 2.838\n",
      "Iter 48/100 - Loss: 4.238\n",
      "Iter 49/100 - Loss: 2.333\n",
      "Iter 50/100 - Loss: 2.740\n",
      "Iter 51/100 - Loss: 3.629\n",
      "Iter 52/100 - Loss: 2.095\n",
      "Iter 53/100 - Loss: 1.787\n",
      "Iter 54/100 - Loss: 1.422\n",
      "Iter 55/100 - Loss: 3.314\n",
      "Iter 56/100 - Loss: 2.646\n",
      "Iter 57/100 - Loss: 4.115\n",
      "Iter 58/100 - Loss: 7.068\n",
      "Iter 59/100 - Loss: 2.094\n",
      "Iter 60/100 - Loss: 1.366\n",
      "Iter 61/100 - Loss: 1.431\n",
      "Iter 62/100 - Loss: 3.049\n",
      "Iter 63/100 - Loss: 2.203\n",
      "Iter 64/100 - Loss: 2.525\n",
      "Iter 65/100 - Loss: 1.810\n",
      "Iter 66/100 - Loss: 2.155\n",
      "Iter 67/100 - Loss: 1.404\n",
      "Iter 68/100 - Loss: 1.514\n",
      "Iter 69/100 - Loss: 1.793\n",
      "Iter 70/100 - Loss: 3.464\n",
      "Iter 71/100 - Loss: 2.413\n",
      "Iter 72/100 - Loss: 3.013\n",
      "Iter 73/100 - Loss: 1.612\n",
      "Iter 74/100 - Loss: 1.776\n",
      "Iter 75/100 - Loss: 1.544\n",
      "Iter 76/100 - Loss: 1.801\n",
      "Iter 77/100 - Loss: 2.521\n",
      "Iter 78/100 - Loss: 1.273\n",
      "Iter 79/100 - Loss: 1.677\n",
      "Iter 80/100 - Loss: 2.265\n",
      "Iter 81/100 - Loss: 4.968\n",
      "Iter 82/100 - Loss: 2.093\n",
      "Iter 83/100 - Loss: 2.113\n",
      "Iter 84/100 - Loss: 1.825\n",
      "Iter 85/100 - Loss: 1.423\n",
      "Iter 86/100 - Loss: 2.665\n",
      "Iter 87/100 - Loss: 3.773\n",
      "Iter 88/100 - Loss: 1.263\n",
      "Iter 89/100 - Loss: 1.450\n",
      "Iter 90/100 - Loss: 1.645\n",
      "Iter 91/100 - Loss: 1.695\n",
      "Iter 92/100 - Loss: 1.867\n",
      "Iter 93/100 - Loss: 2.993\n",
      "Iter 94/100 - Loss: 1.411\n",
      "Iter 95/100 - Loss: 2.358\n",
      "Iter 96/100 - Loss: 1.535\n",
      "Iter 97/100 - Loss: 1.809\n",
      "Iter 98/100 - Loss: 1.226\n",
      "Iter 99/100 - Loss: 1.196\n",
      "Iter 100/100 - Loss: 2.152\n"
     ]
    }
   ],
   "source": [
    "fit_gp_model(SE_model, l, breastX[:400], breastY[:400], gp_mll=gpytorch.mlls.VariationalELBO(l, SE_model, 400))\n",
    "fit_gp_model(AGP_model, l2, breastX[:400], breastY[:400], gp_mll=gpytorch.mlls.VariationalELBO(l2, AGP_model, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPClassificationModel(\n",
       "  (variational_strategy): VariationalStrategy(\n",
       "    (variational_distribution): CholeskyVariationalDistribution()\n",
       "  )\n",
       "  (mean_module): ConstantMean()\n",
       "  (covar_module): ScaleKernel(\n",
       "    (base_kernel): RBFKernel()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SE_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPClassificationModel(\n",
       "  (variational_strategy): VariationalStrategy(\n",
       "    (variational_distribution): CholeskyVariationalDistribution()\n",
       "  )\n",
       "  (mean_module): ConstantMean()\n",
       "  (covar_module): ScaleKernel(\n",
       "    (base_kernel): AdditiveStructureKernel(\n",
       "      (base_kernel): RBFKernel()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGP_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type      Accuracy       \n",
      "Random Forest   0.9527\n",
      "Log. Regression 0.9467\n",
      "RBF GP          0.7692\n"
     ]
    }
   ],
   "source": [
    "y = breastY[400:].numpy()\n",
    "rf_pred = rf_model.predict(breastX[400:].numpy())\n",
    "lr_pred = lr_model.predict(breastX[400:].numpy())\n",
    "SE_model.eval()\n",
    "l.eval()\n",
    "AGP_model.eval()\n",
    "l2.eval()\n",
    "SE_pred = l(SE_model(breastX[400:])).mean.detach().numpy() >= 0.5\n",
    "# AGP_pred = AGP_model(breastX[400:]).mean.detach().numpy()\n",
    "\n",
    "print('{:15s} {:15s}'.format('Model Type', 'Accuracy'))\n",
    "print('{:15s} {:1.4f}'.format('Random Forest', np.abs(y==rf_pred).mean()))\n",
    "print('{:15s} {:1.4f}'.format('Log. Regression', np.abs(y==lr_pred).mean()))\n",
    "print('{:15s} {:1.4f}'.format('RBF GP', np.abs(y==SE_pred).mean()))\n",
    "# print('{:15s} {:15s}'.format('Additive GP', np.abs(y-AGP_pred).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569, 30])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breastX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rp' from '/home/ian/Documents/Research/Scalable_GPs/rp.py'>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rp\n",
    "import imp\n",
    "imp.reload(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular RP\n",
    "W = rp.gen_rp(30, 5, dist='gaussian')\n",
    "breastRP = breastX.matmul(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rp.gen_rp(30, 200, dist='gaussian')\n",
    "b = rp.gen_rp(1, 200, dist='gaussian')\n",
    "breastELMsigmoid = rp.Sigmoid(A, b, breastX)\n",
    "breastELMgaussian = rp.Gaussian(A, b, breastX)\n",
    "breastELMfourier = rp.Fourier(A, b, breastX)\n",
    "breastELMhard_limit = rp.Hard_limit(A, b, breastX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "\n",
    "        super(LogisticRegressionModel, self).__init__() \n",
    "        # Calling Super Class's constructor\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Here the forward pass is simply a linear function\n",
    "        out = self.sigmoid(self.linear(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_lr(X, y, Xtest, epochs=30000, verbose=True, lr=0.1):\n",
    "    [n,d] = X.shape\n",
    "    model = LogisticRegressionModel(d,1)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=lr) #Stochastic Gradient Descent\n",
    "    epochs = epochs\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #clear grads as discussed in prev post\n",
    "        optimiser.zero_grad()\n",
    "        #forward to get predicted values\n",
    "        outputs = model.forward(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()# back props\n",
    "        optimiser.step()# update the parameters\n",
    "        if verbose or epoch==epochs-1:\n",
    "            print('epoch {}, loss {}'.format(epoch+1,loss.item()))\n",
    "    optimiser.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "        y_pred_test = model(Xtest)\n",
    "        \n",
    "    return model, y_pred, y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 11.829080581665039\n",
      "epoch 2, loss 11.829082489013672\n",
      "epoch 3, loss 11.829082489013672\n",
      "epoch 4, loss 11.829082489013672\n",
      "epoch 5, loss 11.829083442687988\n",
      "epoch 6, loss 11.828068733215332\n",
      "epoch 7, loss 11.828069686889648\n",
      "epoch 8, loss 11.827353477478027\n",
      "epoch 9, loss 11.827353477478027\n",
      "epoch 10, loss 11.827354431152344\n",
      "epoch 11, loss 11.827354431152344\n",
      "epoch 12, loss 11.827356338500977\n",
      "epoch 13, loss 11.827356338500977\n",
      "epoch 14, loss 11.826079368591309\n",
      "epoch 15, loss 11.824346542358398\n",
      "epoch 16, loss 11.824348449707031\n",
      "epoch 17, loss 11.824349403381348\n",
      "epoch 18, loss 11.822615623474121\n",
      "epoch 19, loss 11.822160720825195\n",
      "epoch 20, loss 11.821602821350098\n",
      "epoch 21, loss 11.821603775024414\n",
      "epoch 22, loss 11.821606636047363\n",
      "epoch 23, loss 11.821223258972168\n",
      "epoch 24, loss 11.821223258972168\n",
      "epoch 25, loss 11.791547775268555\n",
      "epoch 26, loss 11.790201187133789\n",
      "epoch 27, loss 11.78918743133545\n",
      "epoch 28, loss 11.788804054260254\n",
      "epoch 29, loss 11.7885103225708\n",
      "epoch 30, loss 11.788512229919434\n",
      "epoch 31, loss 11.787196159362793\n",
      "epoch 32, loss 11.787196159362793\n",
      "epoch 33, loss 11.785945892333984\n",
      "epoch 34, loss 11.785730361938477\n",
      "epoch 35, loss 11.785469055175781\n",
      "epoch 36, loss 11.784711837768555\n",
      "epoch 37, loss 11.783731460571289\n",
      "epoch 38, loss 11.783515930175781\n",
      "epoch 39, loss 11.782889366149902\n",
      "epoch 40, loss 11.782073974609375\n",
      "epoch 41, loss 11.78174114227295\n",
      "epoch 42, loss 11.779095649719238\n",
      "epoch 43, loss 11.778790473937988\n",
      "epoch 44, loss 11.777832984924316\n",
      "epoch 45, loss 11.777563095092773\n",
      "epoch 46, loss 11.776717185974121\n",
      "epoch 47, loss 11.776472091674805\n",
      "epoch 48, loss 11.775609016418457\n",
      "epoch 49, loss 11.775154113769531\n",
      "epoch 50, loss 11.773603439331055\n",
      "epoch 51, loss 11.772865295410156\n",
      "epoch 52, loss 11.772198677062988\n",
      "epoch 53, loss 11.772024154663086\n",
      "epoch 54, loss 11.771318435668945\n",
      "epoch 55, loss 11.770025253295898\n",
      "epoch 56, loss 11.769401550292969\n",
      "epoch 57, loss 11.768817901611328\n",
      "epoch 58, loss 11.738909721374512\n",
      "epoch 59, loss 11.737651824951172\n",
      "epoch 60, loss 11.7369384765625\n",
      "epoch 61, loss 11.736115455627441\n",
      "epoch 62, loss 11.705727577209473\n",
      "epoch 63, loss 11.704977989196777\n",
      "epoch 64, loss 11.703636169433594\n",
      "epoch 65, loss 11.644109725952148\n",
      "epoch 66, loss 11.642560958862305\n",
      "epoch 67, loss 11.582653045654297\n",
      "epoch 68, loss 11.578930854797363\n",
      "epoch 69, loss 11.546126365661621\n",
      "epoch 70, loss 11.542223930358887\n",
      "epoch 71, loss 11.478852272033691\n",
      "epoch 72, loss 11.411704063415527\n",
      "epoch 73, loss 11.345236778259277\n",
      "epoch 74, loss 11.18902587890625\n",
      "epoch 75, loss 11.050925254821777\n",
      "epoch 76, loss 10.81153678894043\n",
      "epoch 77, loss 10.634024620056152\n",
      "epoch 78, loss 10.313013076782227\n",
      "epoch 79, loss 9.815948486328125\n",
      "epoch 80, loss 8.817580223083496\n",
      "epoch 81, loss 6.847639560699463\n",
      "epoch 82, loss 2.4765143394470215\n",
      "epoch 83, loss 1.4592078924179077\n",
      "epoch 84, loss 0.5746015310287476\n",
      "epoch 85, loss 0.5812324285507202\n",
      "epoch 86, loss 0.5960275530815125\n",
      "epoch 87, loss 0.6474440097808838\n",
      "epoch 88, loss 0.6816388964653015\n",
      "epoch 89, loss 0.854855477809906\n",
      "epoch 90, loss 0.7151241898536682\n",
      "epoch 91, loss 0.920066237449646\n",
      "epoch 92, loss 0.6898921132087708\n",
      "epoch 93, loss 0.8674439787864685\n",
      "epoch 94, loss 0.6912776827812195\n",
      "epoch 95, loss 0.8672118186950684\n",
      "epoch 96, loss 0.6811966896057129\n",
      "epoch 97, loss 0.8448895215988159\n",
      "epoch 98, loss 0.6754779815673828\n",
      "epoch 99, loss 0.8312153816223145\n",
      "epoch 100, loss 0.6683303713798523\n",
      "epoch 101, loss 0.815106213092804\n",
      "epoch 102, loss 0.6616315245628357\n",
      "epoch 103, loss 0.8001235127449036\n",
      "epoch 104, loss 0.6548556685447693\n",
      "epoch 105, loss 0.7852831482887268\n",
      "epoch 106, loss 0.6481292247772217\n",
      "epoch 107, loss 0.7708154320716858\n",
      "epoch 108, loss 0.6414246559143066\n",
      "epoch 109, loss 0.7566664218902588\n",
      "epoch 110, loss 0.6347534656524658\n",
      "epoch 111, loss 0.7428441047668457\n",
      "epoch 112, loss 0.6281113624572754\n",
      "epoch 113, loss 0.7293323278427124\n",
      "epoch 114, loss 0.6215075850486755\n",
      "epoch 115, loss 0.7161331176757812\n",
      "epoch 116, loss 0.6149398684501648\n",
      "epoch 117, loss 0.7032303810119629\n",
      "epoch 118, loss 0.6084136366844177\n",
      "epoch 119, loss 0.6906177997589111\n",
      "epoch 120, loss 0.6019294857978821\n",
      "epoch 121, loss 0.678281307220459\n",
      "epoch 122, loss 0.5954898595809937\n",
      "epoch 123, loss 0.6662134528160095\n",
      "epoch 124, loss 0.5890941023826599\n",
      "epoch 125, loss 0.6544011831283569\n",
      "epoch 126, loss 0.5827463269233704\n",
      "epoch 127, loss 0.6428394913673401\n",
      "epoch 128, loss 0.576444685459137\n",
      "epoch 129, loss 0.63151615858078\n",
      "epoch 130, loss 0.5701910257339478\n",
      "epoch 131, loss 0.6204262375831604\n",
      "epoch 132, loss 0.5639800429344177\n",
      "epoch 133, loss 0.6095556020736694\n",
      "epoch 134, loss 0.5578153133392334\n",
      "epoch 135, loss 0.5989066362380981\n",
      "epoch 136, loss 0.5516926050186157\n",
      "epoch 137, loss 0.588474452495575\n",
      "epoch 138, loss 0.5456146597862244\n",
      "epoch 139, loss 0.5782604813575745\n",
      "epoch 140, loss 0.5395779609680176\n",
      "epoch 141, loss 0.5682670474052429\n",
      "epoch 142, loss 0.5335877537727356\n",
      "epoch 143, loss 0.5585060715675354\n",
      "epoch 144, loss 0.5276462435722351\n",
      "epoch 145, loss 0.5489908456802368\n",
      "epoch 146, loss 0.5217607617378235\n",
      "epoch 147, loss 0.5397428274154663\n",
      "epoch 148, loss 0.5159415602684021\n",
      "epoch 149, loss 0.5307908654212952\n",
      "epoch 150, loss 0.5102042555809021\n",
      "epoch 151, loss 0.5221688747406006\n",
      "epoch 152, loss 0.504572868347168\n",
      "epoch 153, loss 0.5139240026473999\n",
      "epoch 154, loss 0.4990750849246979\n",
      "epoch 155, loss 0.5061073899269104\n",
      "epoch 156, loss 0.4937485456466675\n",
      "epoch 157, loss 0.49877169728279114\n",
      "epoch 158, loss 0.4886358380317688\n",
      "epoch 159, loss 0.49197152256965637\n",
      "epoch 160, loss 0.4837823808193207\n",
      "epoch 161, loss 0.4857551157474518\n",
      "epoch 162, loss 0.4792364239692688\n",
      "epoch 163, loss 0.4801578223705292\n",
      "epoch 164, loss 0.47503843903541565\n",
      "epoch 165, loss 0.4751962721347809\n",
      "epoch 166, loss 0.47122082114219666\n",
      "epoch 167, loss 0.47086337208747864\n",
      "epoch 168, loss 0.4677993357181549\n",
      "epoch 169, loss 0.4671308994293213\n",
      "epoch 170, loss 0.46477317810058594\n",
      "epoch 171, loss 0.46394699811935425\n",
      "epoch 172, loss 0.46212223172187805\n",
      "epoch 173, loss 0.46124404668807983\n",
      "epoch 174, loss 0.4598117470741272\n",
      "epoch 175, loss 0.4589455723762512\n",
      "epoch 176, loss 0.4577954113483429\n",
      "epoch 177, loss 0.4569742977619171\n",
      "epoch 178, loss 0.45602166652679443\n",
      "epoch 179, loss 0.4552564322948456\n",
      "epoch 180, loss 0.4544404149055481\n",
      "epoch 181, loss 0.45372894406318665\n",
      "epoch 182, loss 0.45300453901290894\n",
      "epoch 183, loss 0.4523389935493469\n",
      "epoch 184, loss 0.451675683259964\n",
      "epoch 185, loss 0.45104601979255676\n",
      "epoch 186, loss 0.4504234790802002\n",
      "epoch 187, loss 0.44982096552848816\n",
      "epoch 188, loss 0.44922542572021484\n",
      "epoch 189, loss 0.4486428201198578\n",
      "epoch 190, loss 0.4480663239955902\n",
      "epoch 191, loss 0.4474976658821106\n",
      "epoch 192, loss 0.44693443179130554\n",
      "epoch 193, loss 0.44637706875801086\n",
      "epoch 194, loss 0.44582417607307434\n",
      "epoch 195, loss 0.44527602195739746\n",
      "epoch 196, loss 0.44473153352737427\n",
      "epoch 197, loss 0.44419044256210327\n",
      "epoch 198, loss 0.44365325570106506\n",
      "epoch 199, loss 0.4431193172931671\n",
      "epoch 200, loss 0.4425884187221527\n",
      "epoch 201, loss 0.44206055998802185\n",
      "epoch 202, loss 0.4415360689163208\n",
      "epoch 203, loss 0.4410145580768585\n",
      "epoch 204, loss 0.4404957592487335\n",
      "epoch 205, loss 0.4399802088737488\n",
      "epoch 206, loss 0.43946725130081177\n",
      "epoch 207, loss 0.4389571249485016\n",
      "epoch 208, loss 0.43845000863075256\n",
      "epoch 209, loss 0.43794527649879456\n",
      "epoch 210, loss 0.43744388222694397\n",
      "epoch 211, loss 0.4369446933269501\n",
      "epoch 212, loss 0.43644827604293823\n",
      "epoch 213, loss 0.4359545409679413\n",
      "epoch 214, loss 0.4354633688926697\n",
      "epoch 215, loss 0.4349750578403473\n",
      "epoch 216, loss 0.4344889521598816\n",
      "epoch 217, loss 0.43400582671165466\n",
      "epoch 218, loss 0.4335249364376068\n",
      "epoch 219, loss 0.4330466389656067\n",
      "epoch 220, loss 0.43257075548171997\n",
      "epoch 221, loss 0.43209725618362427\n",
      "epoch 222, loss 0.4316263198852539\n",
      "epoch 223, loss 0.4311579465866089\n",
      "epoch 224, loss 0.4306917190551758\n",
      "epoch 225, loss 0.43022772669792175\n",
      "epoch 226, loss 0.4297662377357483\n",
      "epoch 227, loss 0.42930734157562256\n",
      "epoch 228, loss 0.42885053157806396\n",
      "epoch 229, loss 0.4283958673477173\n",
      "epoch 230, loss 0.4279438257217407\n",
      "epoch 231, loss 0.4274933636188507\n",
      "epoch 232, loss 0.42704570293426514\n",
      "epoch 233, loss 0.42660030722618103\n",
      "epoch 234, loss 0.4261567294597626\n",
      "epoch 235, loss 0.42571547627449036\n",
      "epoch 236, loss 0.42527610063552856\n",
      "epoch 237, loss 0.4248391389846802\n",
      "epoch 238, loss 0.4244045913219452\n",
      "epoch 239, loss 0.42397162318229675\n",
      "epoch 240, loss 0.42354077100753784\n",
      "epoch 241, loss 0.42311209440231323\n",
      "epoch 242, loss 0.42268574237823486\n",
      "epoch 243, loss 0.42226096987724304\n",
      "epoch 244, loss 0.4218381643295288\n",
      "epoch 245, loss 0.4214174747467041\n",
      "epoch 246, loss 0.42099887132644653\n",
      "epoch 247, loss 0.4205823838710785\n",
      "epoch 248, loss 0.4201675057411194\n",
      "epoch 249, loss 0.41975441575050354\n",
      "epoch 250, loss 0.419343501329422\n",
      "epoch 251, loss 0.4189344346523285\n",
      "epoch 252, loss 0.4185272455215454\n",
      "epoch 253, loss 0.4181215167045593\n",
      "epoch 254, loss 0.4177181124687195\n",
      "epoch 255, loss 0.4173164665699005\n",
      "epoch 256, loss 0.41691669821739197\n",
      "epoch 257, loss 0.4165184497833252\n",
      "epoch 258, loss 0.41612204909324646\n",
      "epoch 259, loss 0.41572752594947815\n",
      "epoch 260, loss 0.41533440351486206\n",
      "epoch 261, loss 0.41494327783584595\n",
      "epoch 262, loss 0.4145541489124298\n",
      "epoch 263, loss 0.41416633129119873\n",
      "epoch 264, loss 0.4137803018093109\n",
      "epoch 265, loss 0.41339603066444397\n",
      "epoch 266, loss 0.4130135774612427\n",
      "epoch 267, loss 0.41263216733932495\n",
      "epoch 268, loss 0.412252813577652\n",
      "epoch 269, loss 0.4118753671646118\n",
      "epoch 270, loss 0.4114992916584015\n",
      "epoch 271, loss 0.4111248254776001\n",
      "epoch 272, loss 0.4107517600059509\n",
      "epoch 273, loss 0.41038039326667786\n",
      "epoch 274, loss 0.41001060605049133\n",
      "epoch 275, loss 0.40964260697364807\n",
      "epoch 276, loss 0.4092758893966675\n",
      "epoch 277, loss 0.4089106619358063\n",
      "epoch 278, loss 0.4085470139980316\n",
      "epoch 279, loss 0.40818488597869873\n",
      "epoch 280, loss 0.40782415866851807\n",
      "epoch 281, loss 0.40746501088142395\n",
      "epoch 282, loss 0.4071075916290283\n",
      "epoch 283, loss 0.4067513644695282\n",
      "epoch 284, loss 0.4063964784145355\n",
      "epoch 285, loss 0.4060431718826294\n",
      "epoch 286, loss 0.4056914150714874\n",
      "epoch 287, loss 0.4053407311439514\n",
      "epoch 288, loss 0.40499165654182434\n",
      "epoch 289, loss 0.40464404225349426\n",
      "epoch 290, loss 0.40429791808128357\n",
      "epoch 291, loss 0.403952956199646\n",
      "epoch 292, loss 0.4036094546318054\n",
      "epoch 293, loss 0.4032672643661499\n",
      "epoch 294, loss 0.4029265344142914\n",
      "epoch 295, loss 0.4025869071483612\n",
      "epoch 296, loss 0.40224888920783997\n",
      "epoch 297, loss 0.40191203355789185\n",
      "epoch 298, loss 0.40157631039619446\n",
      "epoch 299, loss 0.4012422263622284\n",
      "epoch 300, loss 0.40090954303741455\n"
     ]
    }
   ],
   "source": [
    "[_, rp_pred, rp_test_pred] = torch_lr(breastRP[:400], breastY[:400].unsqueeze(1), breastRP[400:], lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error 0.153, Test Error 0.195 (Random Projections + Logistic Regression)\n"
     ]
    }
   ],
   "source": [
    "train_err_rp = torch.mean(((rp_pred > 0.5).squeeze().float() - breastY[:400])**2)\n",
    "test_err_rp = torch.mean(((rp_test_pred > 0.5).squeeze().float() - breastY[400:])**2)\n",
    "print(\"Train Error {:2.3f}, Test Error {:2.3f} (Random Projections + Logistic Regression)\".format(train_err_rp, test_err_rp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_routine(X, Y, Xtest, Ytest, method):\n",
    "    [_, pred, test_pred] = torch_lr(X, Y.unsqueeze(1), Xtest, lr=0.0001, verbose=False)\n",
    "    train_err = torch.mean(((pred > 0.5).squeeze().float() - Y)**2)\n",
    "    test_err = torch.mean(((test_pred > 0.5).squeeze().float() - Ytest)**2)\n",
    "    print(\"Train Error {:2.3f}, Test Error {:2.3f} ({})\".format(train_err, test_err, method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30000, loss 11.950418472290039\n",
      "Train Error 0.433, Test Error 0.231 (RP + Logistic Regression)\n"
     ]
    }
   ],
   "source": [
    "train_routine(breastRP[:400], breastY[:400], breastRP[400:], breastY[400:], 'RP + Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30000, loss 0.22053208947181702\n",
      "Train Error 0.083, Test Error 0.083 (Sigmoid ELM + Logistic Regression)\n"
     ]
    }
   ],
   "source": [
    "train_routine(breastELMsigmoid[:400], breastY[:400], breastELMsigmoid[400:], breastY[400:], 'Sigmoid ELM + Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30000, loss 0.15825466811656952\n",
      "Train Error 0.000, Test Error 0.450 (Cosine ELM + Logistic Regression)\n"
     ]
    }
   ],
   "source": [
    "train_routine(breastELMfourier[:400], breastY[:400], breastELMfourier[400:], breastY[400:], 'Cosine ELM + Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "breastELMgaussian = breastELMgaussian.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30000, loss 11.950418472290039\n",
      "Train Error 0.433, Test Error 0.231 (Gaussian ELM + Logistic Regression)\n"
     ]
    }
   ],
   "source": [
    "train_routine(breastELMgaussian[:400], breastY[:400], breastELMgaussian[400:], breastY[400:], 'Gaussian ELM + Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30000, loss 0.21974597871303558\n",
      "Train Error 0.085, Test Error 0.083 (Hard Limit ELM + Logistic Regression)\n"
     ]
    }
   ],
   "source": [
    "train_routine(breastELMhard_limit[:400], breastY[:400], breastELMhard_limit[400:], breastY[400:], 'Hard Limit ELM + Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30000, loss 0.39064690470695496\n",
      "Train Error 0.093, Test Error 0.059 (Just Logistic Regression)\n"
     ]
    }
   ],
   "source": [
    "train_routine(breastX[:400], breastY[:400], breastX[400:], breastY[400:], 'Just Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_gp_model(AGP_model, l2, breastX[:400], breastY[:400], gp_mll=gpytorch.mlls.VariationalELBO(l2, AGP_model, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGP_model = GPClassificationModel(breastRP[:400],\n",
    "                                  kernel=ScaleKernel(AdditiveStructureKernel(RBFKernel(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpytorch.mlls.VariationalMarginalLogLikelihood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpytorch.mlls.VariationalELBOEmpirical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpytorch.mlls.VariationalELBO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/1000 - Loss: 4.257\n",
      "Iter 2/1000 - Loss: 2.636\n",
      "Iter 3/1000 - Loss: 2.592\n",
      "Iter 4/1000 - Loss: 1.799\n",
      "Iter 5/1000 - Loss: 2.488\n",
      "Iter 6/1000 - Loss: 2.473\n",
      "Iter 7/1000 - Loss: 2.528\n",
      "Iter 8/1000 - Loss: 2.109\n",
      "Iter 9/1000 - Loss: 2.886\n",
      "Iter 10/1000 - Loss: 3.688\n",
      "Iter 11/1000 - Loss: 1.798\n",
      "Iter 12/1000 - Loss: 1.873\n",
      "Iter 13/1000 - Loss: 2.915\n",
      "Iter 14/1000 - Loss: 3.314\n",
      "Iter 15/1000 - Loss: 3.303\n",
      "Iter 16/1000 - Loss: 2.989\n",
      "Iter 17/1000 - Loss: 2.349\n",
      "Iter 18/1000 - Loss: 1.798\n",
      "Iter 19/1000 - Loss: 3.360\n",
      "Iter 20/1000 - Loss: 2.353\n",
      "Iter 21/1000 - Loss: 2.846\n",
      "Iter 22/1000 - Loss: 3.636\n",
      "Iter 23/1000 - Loss: 3.204\n",
      "Iter 24/1000 - Loss: 3.115\n",
      "Iter 25/1000 - Loss: 2.422\n",
      "Iter 26/1000 - Loss: 2.889\n",
      "Iter 27/1000 - Loss: 2.075\n",
      "Iter 28/1000 - Loss: 2.055\n",
      "Iter 29/1000 - Loss: 2.234\n",
      "Iter 30/1000 - Loss: 1.820\n",
      "Iter 31/1000 - Loss: 2.371\n",
      "Iter 32/1000 - Loss: 3.995\n",
      "Iter 33/1000 - Loss: 2.281\n",
      "Iter 34/1000 - Loss: 3.645\n",
      "Iter 35/1000 - Loss: 1.826\n",
      "Iter 36/1000 - Loss: 1.949\n",
      "Iter 37/1000 - Loss: 1.782\n",
      "Iter 38/1000 - Loss: 2.035\n",
      "Iter 39/1000 - Loss: 1.881\n",
      "Iter 40/1000 - Loss: 2.288\n",
      "Iter 41/1000 - Loss: 2.108\n",
      "Iter 42/1000 - Loss: 2.077\n",
      "Iter 43/1000 - Loss: 1.850\n",
      "Iter 44/1000 - Loss: 1.891\n",
      "Iter 45/1000 - Loss: 2.000\n",
      "Iter 46/1000 - Loss: 1.862\n",
      "Iter 47/1000 - Loss: 1.381\n",
      "Iter 48/1000 - Loss: 2.003\n",
      "Iter 49/1000 - Loss: 2.086\n",
      "Iter 50/1000 - Loss: 1.315\n",
      "Iter 51/1000 - Loss: 1.686\n",
      "Iter 52/1000 - Loss: 1.561\n",
      "Iter 53/1000 - Loss: 1.381\n",
      "Iter 54/1000 - Loss: 1.951\n",
      "Iter 55/1000 - Loss: 1.254\n",
      "Iter 56/1000 - Loss: 2.501\n",
      "Iter 57/1000 - Loss: 2.334\n",
      "Iter 58/1000 - Loss: 1.962\n",
      "Iter 59/1000 - Loss: 1.449\n",
      "Iter 60/1000 - Loss: 1.675\n",
      "Iter 61/1000 - Loss: 1.067\n",
      "Iter 62/1000 - Loss: 1.124\n",
      "Iter 63/1000 - Loss: 1.703\n",
      "Iter 64/1000 - Loss: 1.209\n",
      "Iter 65/1000 - Loss: 1.896\n",
      "Iter 66/1000 - Loss: 1.323\n",
      "Iter 67/1000 - Loss: 1.245\n",
      "Iter 68/1000 - Loss: 1.187\n",
      "Iter 69/1000 - Loss: 1.282\n",
      "Iter 70/1000 - Loss: 1.399\n",
      "Iter 71/1000 - Loss: 1.443\n",
      "Iter 72/1000 - Loss: 0.984\n",
      "Iter 73/1000 - Loss: 1.592\n",
      "Iter 74/1000 - Loss: 1.009\n",
      "Iter 75/1000 - Loss: 1.250\n",
      "Iter 76/1000 - Loss: 2.197\n",
      "Iter 77/1000 - Loss: 2.088\n",
      "Iter 78/1000 - Loss: 1.621\n",
      "Iter 79/1000 - Loss: 1.659\n",
      "Iter 80/1000 - Loss: 1.081\n",
      "Iter 81/1000 - Loss: 1.723\n",
      "Iter 82/1000 - Loss: 1.319\n",
      "Iter 83/1000 - Loss: 1.238\n",
      "Iter 84/1000 - Loss: 1.391\n",
      "Iter 85/1000 - Loss: 1.069\n",
      "Iter 86/1000 - Loss: 1.167\n",
      "Iter 87/1000 - Loss: 1.204\n",
      "Iter 88/1000 - Loss: 1.148\n",
      "Iter 89/1000 - Loss: 1.482\n",
      "Iter 90/1000 - Loss: 1.494\n",
      "Iter 91/1000 - Loss: 1.144\n",
      "Iter 92/1000 - Loss: 1.304\n",
      "Iter 93/1000 - Loss: 1.500\n",
      "Iter 94/1000 - Loss: 1.592\n",
      "Iter 95/1000 - Loss: 1.606\n",
      "Iter 96/1000 - Loss: 1.545\n",
      "Iter 97/1000 - Loss: 1.015\n",
      "Iter 98/1000 - Loss: 1.557\n",
      "Iter 99/1000 - Loss: 1.037\n",
      "Iter 100/1000 - Loss: 1.272\n",
      "Iter 101/1000 - Loss: 1.680\n",
      "Iter 102/1000 - Loss: 1.045\n",
      "Iter 103/1000 - Loss: 1.462\n",
      "Iter 104/1000 - Loss: 1.275\n",
      "Iter 105/1000 - Loss: 1.191\n",
      "Iter 106/1000 - Loss: 1.181\n",
      "Iter 107/1000 - Loss: 1.318\n",
      "Iter 108/1000 - Loss: 1.244\n",
      "Iter 109/1000 - Loss: 1.200\n",
      "Iter 110/1000 - Loss: 1.056\n",
      "Iter 111/1000 - Loss: 1.266\n",
      "Iter 112/1000 - Loss: 0.991\n",
      "Iter 113/1000 - Loss: 1.391\n",
      "Iter 114/1000 - Loss: 1.073\n",
      "Iter 115/1000 - Loss: 1.058\n",
      "Iter 116/1000 - Loss: 0.869\n",
      "Iter 117/1000 - Loss: 0.861\n",
      "Iter 118/1000 - Loss: 0.883\n",
      "Iter 119/1000 - Loss: 0.902\n",
      "Iter 120/1000 - Loss: 0.992\n",
      "Iter 121/1000 - Loss: 1.039\n",
      "Iter 122/1000 - Loss: 0.977\n",
      "Iter 123/1000 - Loss: 1.529\n",
      "Iter 124/1000 - Loss: 0.833\n",
      "Iter 125/1000 - Loss: 1.173\n",
      "Iter 126/1000 - Loss: 0.879\n",
      "Iter 127/1000 - Loss: 1.308\n",
      "Iter 128/1000 - Loss: 0.859\n",
      "Iter 129/1000 - Loss: 0.855\n",
      "Iter 130/1000 - Loss: 1.090\n",
      "Iter 131/1000 - Loss: 1.007\n",
      "Iter 132/1000 - Loss: 1.057\n",
      "Iter 133/1000 - Loss: 0.932\n",
      "Iter 134/1000 - Loss: 0.816\n",
      "Iter 135/1000 - Loss: 0.854\n",
      "Iter 136/1000 - Loss: 0.764\n",
      "Iter 137/1000 - Loss: 0.845\n",
      "Iter 138/1000 - Loss: 0.885\n",
      "Iter 139/1000 - Loss: 0.896\n",
      "Iter 140/1000 - Loss: 1.185\n",
      "Iter 141/1000 - Loss: 0.837\n",
      "Iter 142/1000 - Loss: 0.926\n",
      "Iter 143/1000 - Loss: 1.056\n",
      "Iter 144/1000 - Loss: 0.924\n",
      "Iter 145/1000 - Loss: 0.947\n",
      "Iter 146/1000 - Loss: 0.888\n",
      "Iter 147/1000 - Loss: 0.905\n",
      "Iter 148/1000 - Loss: 0.994\n",
      "Iter 149/1000 - Loss: 0.805\n",
      "Iter 150/1000 - Loss: 0.958\n",
      "Iter 151/1000 - Loss: 0.704\n",
      "Iter 152/1000 - Loss: 0.924\n",
      "Iter 153/1000 - Loss: 0.789\n",
      "Iter 154/1000 - Loss: 0.838\n",
      "Iter 155/1000 - Loss: 0.814\n",
      "Iter 156/1000 - Loss: 0.899\n",
      "Iter 157/1000 - Loss: 0.782\n",
      "Iter 158/1000 - Loss: 0.778\n",
      "Iter 159/1000 - Loss: 0.748\n",
      "Iter 160/1000 - Loss: 0.833\n",
      "Iter 161/1000 - Loss: 0.761\n",
      "Iter 162/1000 - Loss: 0.760\n",
      "Iter 163/1000 - Loss: 0.775\n",
      "Iter 164/1000 - Loss: 0.687\n",
      "Iter 165/1000 - Loss: 0.766\n",
      "Iter 166/1000 - Loss: 0.772\n",
      "Iter 167/1000 - Loss: 0.741\n",
      "Iter 168/1000 - Loss: 0.987\n",
      "Iter 169/1000 - Loss: 1.140\n",
      "Iter 170/1000 - Loss: 0.708\n",
      "Iter 171/1000 - Loss: 0.748\n",
      "Iter 172/1000 - Loss: 0.685\n",
      "Iter 173/1000 - Loss: 0.789\n",
      "Iter 174/1000 - Loss: 0.840\n",
      "Iter 175/1000 - Loss: 0.696\n",
      "Iter 176/1000 - Loss: 0.701\n",
      "Iter 177/1000 - Loss: 0.636\n",
      "Iter 178/1000 - Loss: 0.725\n",
      "Iter 179/1000 - Loss: 0.777\n",
      "Iter 180/1000 - Loss: 0.734\n",
      "Iter 181/1000 - Loss: 0.829\n",
      "Iter 182/1000 - Loss: 0.978\n",
      "Iter 183/1000 - Loss: 0.752\n",
      "Iter 184/1000 - Loss: 0.670\n",
      "Iter 185/1000 - Loss: 0.905\n",
      "Iter 186/1000 - Loss: 0.638\n",
      "Iter 187/1000 - Loss: 0.642\n",
      "Iter 188/1000 - Loss: 0.598\n",
      "Iter 189/1000 - Loss: 0.786\n",
      "Iter 190/1000 - Loss: 0.695\n",
      "Iter 191/1000 - Loss: 0.666\n",
      "Iter 192/1000 - Loss: 0.744\n",
      "Iter 193/1000 - Loss: 0.692\n",
      "Iter 194/1000 - Loss: 0.613\n",
      "Iter 195/1000 - Loss: 0.712\n",
      "Iter 196/1000 - Loss: 0.719\n",
      "Iter 197/1000 - Loss: 0.755\n",
      "Iter 198/1000 - Loss: 0.708\n",
      "Iter 199/1000 - Loss: 0.694\n",
      "Iter 200/1000 - Loss: 0.655\n",
      "Iter 201/1000 - Loss: 0.600\n",
      "Iter 202/1000 - Loss: 0.563\n",
      "Iter 203/1000 - Loss: 0.662\n",
      "Iter 204/1000 - Loss: 0.689\n",
      "Iter 205/1000 - Loss: 0.598\n",
      "Iter 206/1000 - Loss: 0.660\n",
      "Iter 207/1000 - Loss: 0.592\n",
      "Iter 208/1000 - Loss: 0.594\n",
      "Iter 209/1000 - Loss: 0.668\n",
      "Iter 210/1000 - Loss: 0.724\n",
      "Iter 211/1000 - Loss: 0.615\n",
      "Iter 212/1000 - Loss: 0.600\n",
      "Iter 213/1000 - Loss: 0.630\n",
      "Iter 214/1000 - Loss: 0.584\n",
      "Iter 215/1000 - Loss: 0.578\n",
      "Iter 216/1000 - Loss: 0.536\n",
      "Iter 217/1000 - Loss: 0.747\n",
      "Iter 218/1000 - Loss: 0.594\n",
      "Iter 219/1000 - Loss: 0.578\n",
      "Iter 220/1000 - Loss: 0.655\n",
      "Iter 221/1000 - Loss: 0.596\n",
      "Iter 222/1000 - Loss: 0.705\n",
      "Iter 223/1000 - Loss: 0.634\n",
      "Iter 224/1000 - Loss: 0.622\n",
      "Iter 225/1000 - Loss: 0.621\n",
      "Iter 226/1000 - Loss: 0.614\n",
      "Iter 227/1000 - Loss: 0.659\n",
      "Iter 228/1000 - Loss: 0.590\n",
      "Iter 229/1000 - Loss: 0.595\n",
      "Iter 230/1000 - Loss: 0.603\n",
      "Iter 231/1000 - Loss: 0.557\n",
      "Iter 232/1000 - Loss: 0.538\n",
      "Iter 233/1000 - Loss: 0.655\n",
      "Iter 234/1000 - Loss: 0.626\n",
      "Iter 235/1000 - Loss: 0.638\n",
      "Iter 236/1000 - Loss: 0.586\n",
      "Iter 237/1000 - Loss: 0.642\n",
      "Iter 238/1000 - Loss: 0.563\n",
      "Iter 239/1000 - Loss: 0.585\n",
      "Iter 240/1000 - Loss: 0.608\n",
      "Iter 241/1000 - Loss: 0.577\n",
      "Iter 242/1000 - Loss: 0.565\n",
      "Iter 243/1000 - Loss: 0.639\n",
      "Iter 244/1000 - Loss: 0.566\n",
      "Iter 245/1000 - Loss: 0.529\n",
      "Iter 246/1000 - Loss: 0.634\n",
      "Iter 247/1000 - Loss: 0.570\n",
      "Iter 248/1000 - Loss: 0.542\n",
      "Iter 249/1000 - Loss: 0.557\n",
      "Iter 250/1000 - Loss: 0.511\n",
      "Iter 251/1000 - Loss: 0.584\n",
      "Iter 252/1000 - Loss: 0.514\n",
      "Iter 253/1000 - Loss: 0.681\n",
      "Iter 254/1000 - Loss: 0.548\n",
      "Iter 255/1000 - Loss: 0.566\n",
      "Iter 256/1000 - Loss: 0.543\n",
      "Iter 257/1000 - Loss: 0.507\n",
      "Iter 258/1000 - Loss: 0.535\n",
      "Iter 259/1000 - Loss: 0.543\n",
      "Iter 260/1000 - Loss: 0.523\n",
      "Iter 261/1000 - Loss: 0.530\n",
      "Iter 262/1000 - Loss: 0.524\n",
      "Iter 263/1000 - Loss: 0.558\n",
      "Iter 264/1000 - Loss: 0.497\n",
      "Iter 265/1000 - Loss: 0.567\n",
      "Iter 266/1000 - Loss: 0.542\n",
      "Iter 267/1000 - Loss: 0.500\n",
      "Iter 268/1000 - Loss: 0.514\n",
      "Iter 269/1000 - Loss: 0.566\n",
      "Iter 270/1000 - Loss: 0.503\n",
      "Iter 271/1000 - Loss: 0.509\n",
      "Iter 272/1000 - Loss: 0.495\n",
      "Iter 273/1000 - Loss: 0.518\n",
      "Iter 274/1000 - Loss: 0.490\n",
      "Iter 275/1000 - Loss: 0.539\n",
      "Iter 276/1000 - Loss: 0.521\n",
      "Iter 277/1000 - Loss: 0.509\n",
      "Iter 278/1000 - Loss: 0.478\n",
      "Iter 279/1000 - Loss: 0.494\n",
      "Iter 280/1000 - Loss: 0.503\n",
      "Iter 281/1000 - Loss: 0.478\n",
      "Iter 282/1000 - Loss: 0.501\n",
      "Iter 283/1000 - Loss: 0.509\n",
      "Iter 284/1000 - Loss: 0.487\n",
      "Iter 285/1000 - Loss: 0.543\n",
      "Iter 286/1000 - Loss: 0.501\n",
      "Iter 287/1000 - Loss: 0.501\n",
      "Iter 288/1000 - Loss: 0.473\n",
      "Iter 289/1000 - Loss: 0.520\n",
      "Iter 290/1000 - Loss: 0.514\n",
      "Iter 291/1000 - Loss: 0.499\n",
      "Iter 292/1000 - Loss: 0.510\n",
      "Iter 293/1000 - Loss: 0.496\n",
      "Iter 294/1000 - Loss: 0.506\n",
      "Iter 295/1000 - Loss: 0.459\n",
      "Iter 296/1000 - Loss: 0.475\n",
      "Iter 297/1000 - Loss: 0.481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 298/1000 - Loss: 0.475\n",
      "Iter 299/1000 - Loss: 0.545\n",
      "Iter 300/1000 - Loss: 0.473\n",
      "Iter 301/1000 - Loss: 0.468\n",
      "Iter 302/1000 - Loss: 0.476\n",
      "Iter 303/1000 - Loss: 0.522\n",
      "Iter 304/1000 - Loss: 0.485\n",
      "Iter 305/1000 - Loss: 0.499\n",
      "Iter 306/1000 - Loss: 0.491\n",
      "Iter 307/1000 - Loss: 0.472\n",
      "Iter 308/1000 - Loss: 0.436\n",
      "Iter 309/1000 - Loss: 0.550\n",
      "Iter 310/1000 - Loss: 0.472\n",
      "Iter 311/1000 - Loss: 0.491\n",
      "Iter 312/1000 - Loss: 0.431\n",
      "Iter 313/1000 - Loss: 0.465\n",
      "Iter 314/1000 - Loss: 0.474\n",
      "Iter 315/1000 - Loss: 0.458\n",
      "Iter 316/1000 - Loss: 0.450\n",
      "Iter 317/1000 - Loss: 0.460\n",
      "Iter 318/1000 - Loss: 0.473\n",
      "Iter 319/1000 - Loss: 0.459\n",
      "Iter 320/1000 - Loss: 0.461\n",
      "Iter 321/1000 - Loss: 0.457\n",
      "Iter 322/1000 - Loss: 0.495\n",
      "Iter 323/1000 - Loss: 0.440\n",
      "Iter 324/1000 - Loss: 0.434\n",
      "Iter 325/1000 - Loss: 0.479\n",
      "Iter 326/1000 - Loss: 0.443\n",
      "Iter 327/1000 - Loss: 0.431\n",
      "Iter 328/1000 - Loss: 0.454\n",
      "Iter 329/1000 - Loss: 0.461\n",
      "Iter 330/1000 - Loss: 0.441\n",
      "Iter 331/1000 - Loss: 0.455\n",
      "Iter 332/1000 - Loss: 0.446\n",
      "Iter 333/1000 - Loss: 0.473\n",
      "Iter 334/1000 - Loss: 0.486\n",
      "Iter 335/1000 - Loss: 0.450\n",
      "Iter 336/1000 - Loss: 0.455\n",
      "Iter 337/1000 - Loss: 0.424\n",
      "Iter 338/1000 - Loss: 0.437\n",
      "Iter 339/1000 - Loss: 0.445\n",
      "Iter 340/1000 - Loss: 0.451\n",
      "Iter 341/1000 - Loss: 0.433\n",
      "Iter 342/1000 - Loss: 0.455\n",
      "Iter 343/1000 - Loss: 0.444\n",
      "Iter 344/1000 - Loss: 0.447\n",
      "Iter 345/1000 - Loss: 0.431\n",
      "Iter 346/1000 - Loss: 0.459\n",
      "Iter 347/1000 - Loss: 0.440\n",
      "Iter 348/1000 - Loss: 0.423\n",
      "Iter 349/1000 - Loss: 0.441\n",
      "Iter 350/1000 - Loss: 0.429\n",
      "Iter 351/1000 - Loss: 0.453\n",
      "Iter 352/1000 - Loss: 0.421\n",
      "Iter 353/1000 - Loss: 0.416\n",
      "Iter 354/1000 - Loss: 0.428\n",
      "Iter 355/1000 - Loss: 0.426\n",
      "Iter 356/1000 - Loss: 0.424\n",
      "Iter 357/1000 - Loss: 0.426\n",
      "Iter 358/1000 - Loss: 0.424\n",
      "Iter 359/1000 - Loss: 0.419\n",
      "Iter 360/1000 - Loss: 0.429\n",
      "Iter 361/1000 - Loss: 0.444\n",
      "Iter 362/1000 - Loss: 0.425\n",
      "Iter 363/1000 - Loss: 0.436\n",
      "Iter 364/1000 - Loss: 0.427\n",
      "Iter 365/1000 - Loss: 0.429\n",
      "Iter 366/1000 - Loss: 0.431\n",
      "Iter 367/1000 - Loss: 0.423\n",
      "Iter 368/1000 - Loss: 0.423\n",
      "Iter 369/1000 - Loss: 0.413\n",
      "Iter 370/1000 - Loss: 0.433\n",
      "Iter 371/1000 - Loss: 0.416\n",
      "Iter 372/1000 - Loss: 0.416\n",
      "Iter 373/1000 - Loss: 0.397\n",
      "Iter 374/1000 - Loss: 0.416\n",
      "Iter 375/1000 - Loss: 0.437\n",
      "Iter 376/1000 - Loss: 0.407\n",
      "Iter 377/1000 - Loss: 0.414\n",
      "Iter 378/1000 - Loss: 0.421\n",
      "Iter 379/1000 - Loss: 0.416\n",
      "Iter 380/1000 - Loss: 0.422\n",
      "Iter 381/1000 - Loss: 0.425\n",
      "Iter 382/1000 - Loss: 0.420\n",
      "Iter 383/1000 - Loss: 0.395\n",
      "Iter 384/1000 - Loss: 0.409\n",
      "Iter 385/1000 - Loss: 0.431\n",
      "Iter 386/1000 - Loss: 0.434\n",
      "Iter 387/1000 - Loss: 0.424\n",
      "Iter 388/1000 - Loss: 0.412\n",
      "Iter 389/1000 - Loss: 0.415\n",
      "Iter 390/1000 - Loss: 0.400\n",
      "Iter 391/1000 - Loss: 0.401\n",
      "Iter 392/1000 - Loss: 0.395\n",
      "Iter 393/1000 - Loss: 0.406\n",
      "Iter 394/1000 - Loss: 0.413\n",
      "Iter 395/1000 - Loss: 0.397\n",
      "Iter 396/1000 - Loss: 0.404\n",
      "Iter 397/1000 - Loss: 0.417\n",
      "Iter 398/1000 - Loss: 0.395\n",
      "Iter 399/1000 - Loss: 0.396\n",
      "Iter 400/1000 - Loss: 0.408\n",
      "Iter 401/1000 - Loss: 0.410\n",
      "Iter 402/1000 - Loss: 0.411\n",
      "Iter 403/1000 - Loss: 0.404\n",
      "Iter 404/1000 - Loss: 0.394\n",
      "Iter 405/1000 - Loss: 0.407\n",
      "Iter 406/1000 - Loss: 0.428\n",
      "Iter 407/1000 - Loss: 0.394\n",
      "Iter 408/1000 - Loss: 0.408\n",
      "Iter 409/1000 - Loss: 0.418\n",
      "Iter 410/1000 - Loss: 0.389\n",
      "Iter 411/1000 - Loss: 0.424\n",
      "Iter 412/1000 - Loss: 0.398\n",
      "Iter 413/1000 - Loss: 0.407\n",
      "Iter 414/1000 - Loss: 0.382\n",
      "Iter 415/1000 - Loss: 0.398\n",
      "Iter 416/1000 - Loss: 0.401\n",
      "Iter 417/1000 - Loss: 0.413\n",
      "Iter 418/1000 - Loss: 0.397\n",
      "Iter 419/1000 - Loss: 0.407\n",
      "Iter 420/1000 - Loss: 0.412\n",
      "Iter 421/1000 - Loss: 0.407\n",
      "Iter 422/1000 - Loss: 0.415\n",
      "Iter 423/1000 - Loss: 0.394\n",
      "Iter 424/1000 - Loss: 0.407\n",
      "Iter 425/1000 - Loss: 0.395\n",
      "Iter 426/1000 - Loss: 0.406\n",
      "Iter 427/1000 - Loss: 0.382\n",
      "Iter 428/1000 - Loss: 0.398\n",
      "Iter 429/1000 - Loss: 0.389\n",
      "Iter 430/1000 - Loss: 0.395\n",
      "Iter 431/1000 - Loss: 0.387\n",
      "Iter 432/1000 - Loss: 0.404\n",
      "Iter 433/1000 - Loss: 0.400\n",
      "Iter 434/1000 - Loss: 0.389\n",
      "Iter 435/1000 - Loss: 0.392\n",
      "Iter 436/1000 - Loss: 0.396\n",
      "Iter 437/1000 - Loss: 0.393\n",
      "Iter 438/1000 - Loss: 0.396\n",
      "Iter 439/1000 - Loss: 0.399\n",
      "Iter 440/1000 - Loss: 0.384\n",
      "Iter 441/1000 - Loss: 0.388\n",
      "Iter 442/1000 - Loss: 0.383\n",
      "Iter 443/1000 - Loss: 0.381\n",
      "Iter 444/1000 - Loss: 0.397\n",
      "Iter 445/1000 - Loss: 0.384\n",
      "Iter 446/1000 - Loss: 0.386\n",
      "Iter 447/1000 - Loss: 0.384\n",
      "Iter 448/1000 - Loss: 0.386\n",
      "Iter 449/1000 - Loss: 0.374\n",
      "Iter 450/1000 - Loss: 0.394\n",
      "Iter 451/1000 - Loss: 0.381\n",
      "Iter 452/1000 - Loss: 0.393\n",
      "Iter 453/1000 - Loss: 0.387\n",
      "Iter 454/1000 - Loss: 0.379\n",
      "Iter 455/1000 - Loss: 0.383\n",
      "Iter 456/1000 - Loss: 0.391\n",
      "Iter 457/1000 - Loss: 0.371\n",
      "Iter 458/1000 - Loss: 0.383\n",
      "Iter 459/1000 - Loss: 0.392\n",
      "Iter 460/1000 - Loss: 0.381\n",
      "Iter 461/1000 - Loss: 0.384\n",
      "Iter 462/1000 - Loss: 0.369\n",
      "Iter 463/1000 - Loss: 0.387\n",
      "Iter 464/1000 - Loss: 0.381\n",
      "Iter 465/1000 - Loss: 0.375\n",
      "Iter 466/1000 - Loss: 0.373\n",
      "Iter 467/1000 - Loss: 0.386\n",
      "Iter 468/1000 - Loss: 0.367\n",
      "Iter 469/1000 - Loss: 0.374\n",
      "Iter 470/1000 - Loss: 0.378\n",
      "Iter 471/1000 - Loss: 0.372\n",
      "Iter 472/1000 - Loss: 0.385\n",
      "Iter 473/1000 - Loss: 0.385\n",
      "Iter 474/1000 - Loss: 0.401\n",
      "Iter 475/1000 - Loss: 0.377\n",
      "Iter 476/1000 - Loss: 0.370\n",
      "Iter 477/1000 - Loss: 0.376\n",
      "Iter 478/1000 - Loss: 0.386\n",
      "Iter 479/1000 - Loss: 0.379\n",
      "Iter 480/1000 - Loss: 0.390\n",
      "Iter 481/1000 - Loss: 0.384\n",
      "Iter 482/1000 - Loss: 0.392\n",
      "Iter 483/1000 - Loss: 0.374\n",
      "Iter 484/1000 - Loss: 0.379\n",
      "Iter 485/1000 - Loss: 0.383\n",
      "Iter 486/1000 - Loss: 0.373\n",
      "Iter 487/1000 - Loss: 0.388\n",
      "Iter 488/1000 - Loss: 0.386\n",
      "Iter 489/1000 - Loss: 0.379\n",
      "Iter 490/1000 - Loss: 0.377\n",
      "Iter 491/1000 - Loss: 0.364\n",
      "Iter 492/1000 - Loss: 0.378\n",
      "Iter 493/1000 - Loss: 0.385\n",
      "Iter 494/1000 - Loss: 0.378\n",
      "Iter 495/1000 - Loss: 0.371\n",
      "Iter 496/1000 - Loss: 0.367\n",
      "Iter 497/1000 - Loss: 0.381\n",
      "Iter 498/1000 - Loss: 0.376\n",
      "Iter 499/1000 - Loss: 0.375\n",
      "Iter 500/1000 - Loss: 0.377\n",
      "Iter 501/1000 - Loss: 0.385\n",
      "Iter 502/1000 - Loss: 0.369\n",
      "Iter 503/1000 - Loss: 0.376\n",
      "Iter 504/1000 - Loss: 0.368\n",
      "Iter 505/1000 - Loss: 0.373\n",
      "Iter 506/1000 - Loss: 0.390\n",
      "Iter 507/1000 - Loss: 0.365\n",
      "Iter 508/1000 - Loss: 0.376\n",
      "Iter 509/1000 - Loss: 0.369\n",
      "Iter 510/1000 - Loss: 0.368\n",
      "Iter 511/1000 - Loss: 0.374\n",
      "Iter 512/1000 - Loss: 0.364\n",
      "Iter 513/1000 - Loss: 0.355\n",
      "Iter 514/1000 - Loss: 0.359\n",
      "Iter 515/1000 - Loss: 0.373\n",
      "Iter 516/1000 - Loss: 0.376\n",
      "Iter 517/1000 - Loss: 0.371\n",
      "Iter 518/1000 - Loss: 0.359\n",
      "Iter 519/1000 - Loss: 0.377\n",
      "Iter 520/1000 - Loss: 0.376\n",
      "Iter 521/1000 - Loss: 0.371\n",
      "Iter 522/1000 - Loss: 0.376\n",
      "Iter 523/1000 - Loss: 0.366\n",
      "Iter 524/1000 - Loss: 0.375\n",
      "Iter 525/1000 - Loss: 0.375\n",
      "Iter 526/1000 - Loss: 0.365\n",
      "Iter 527/1000 - Loss: 0.372\n",
      "Iter 528/1000 - Loss: 0.371\n",
      "Iter 529/1000 - Loss: 0.366\n",
      "Iter 530/1000 - Loss: 0.351\n",
      "Iter 531/1000 - Loss: 0.363\n",
      "Iter 532/1000 - Loss: 0.372\n",
      "Iter 533/1000 - Loss: 0.370\n",
      "Iter 534/1000 - Loss: 0.366\n",
      "Iter 535/1000 - Loss: 0.385\n",
      "Iter 536/1000 - Loss: 0.367\n",
      "Iter 537/1000 - Loss: 0.354\n",
      "Iter 538/1000 - Loss: 0.365\n",
      "Iter 539/1000 - Loss: 0.376\n",
      "Iter 540/1000 - Loss: 0.373\n",
      "Iter 541/1000 - Loss: 0.369\n",
      "Iter 542/1000 - Loss: 0.365\n",
      "Iter 543/1000 - Loss: 0.379\n",
      "Iter 544/1000 - Loss: 0.361\n",
      "Iter 545/1000 - Loss: 0.361\n",
      "Iter 546/1000 - Loss: 0.372\n",
      "Iter 547/1000 - Loss: 0.359\n",
      "Iter 548/1000 - Loss: 0.357\n",
      "Iter 549/1000 - Loss: 0.371\n",
      "Iter 550/1000 - Loss: 0.366\n",
      "Iter 551/1000 - Loss: 0.362\n",
      "Iter 552/1000 - Loss: 0.364\n",
      "Iter 553/1000 - Loss: 0.375\n",
      "Iter 554/1000 - Loss: 0.369\n",
      "Iter 555/1000 - Loss: 0.376\n",
      "Iter 556/1000 - Loss: 0.365\n",
      "Iter 557/1000 - Loss: 0.368\n",
      "Iter 558/1000 - Loss: 0.361\n",
      "Iter 559/1000 - Loss: 0.365\n",
      "Iter 560/1000 - Loss: 0.354\n",
      "Iter 561/1000 - Loss: 0.355\n",
      "Iter 562/1000 - Loss: 0.366\n",
      "Iter 563/1000 - Loss: 0.369\n",
      "Iter 564/1000 - Loss: 0.368\n",
      "Iter 565/1000 - Loss: 0.374\n",
      "Iter 566/1000 - Loss: 0.364\n",
      "Iter 567/1000 - Loss: 0.369\n",
      "Iter 568/1000 - Loss: 0.364\n",
      "Iter 569/1000 - Loss: 0.366\n",
      "Iter 570/1000 - Loss: 0.359\n",
      "Iter 571/1000 - Loss: 0.370\n",
      "Iter 572/1000 - Loss: 0.365\n",
      "Iter 573/1000 - Loss: 0.365\n",
      "Iter 574/1000 - Loss: 0.372\n",
      "Iter 575/1000 - Loss: 0.353\n",
      "Iter 576/1000 - Loss: 0.374\n",
      "Iter 577/1000 - Loss: 0.353\n",
      "Iter 578/1000 - Loss: 0.365\n",
      "Iter 579/1000 - Loss: 0.355\n",
      "Iter 580/1000 - Loss: 0.359\n",
      "Iter 581/1000 - Loss: 0.369\n",
      "Iter 582/1000 - Loss: 0.354\n",
      "Iter 583/1000 - Loss: 0.352\n",
      "Iter 584/1000 - Loss: 0.358\n",
      "Iter 585/1000 - Loss: 0.361\n",
      "Iter 586/1000 - Loss: 0.350\n",
      "Iter 587/1000 - Loss: 0.362\n",
      "Iter 588/1000 - Loss: 0.355\n",
      "Iter 589/1000 - Loss: 0.369\n",
      "Iter 590/1000 - Loss: 0.364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 591/1000 - Loss: 0.366\n",
      "Iter 592/1000 - Loss: 0.364\n",
      "Iter 593/1000 - Loss: 0.368\n",
      "Iter 594/1000 - Loss: 0.369\n",
      "Iter 595/1000 - Loss: 0.374\n",
      "Iter 596/1000 - Loss: 0.370\n",
      "Iter 597/1000 - Loss: 0.351\n",
      "Iter 598/1000 - Loss: 0.366\n",
      "Iter 599/1000 - Loss: 0.364\n",
      "Iter 600/1000 - Loss: 0.362\n",
      "Iter 601/1000 - Loss: 0.370\n",
      "Iter 602/1000 - Loss: 0.356\n",
      "Iter 603/1000 - Loss: 0.367\n",
      "Iter 604/1000 - Loss: 0.353\n",
      "Iter 605/1000 - Loss: 0.349\n",
      "Iter 606/1000 - Loss: 0.349\n",
      "Iter 607/1000 - Loss: 0.352\n",
      "Iter 608/1000 - Loss: 0.349\n",
      "Iter 609/1000 - Loss: 0.356\n",
      "Iter 610/1000 - Loss: 0.359\n",
      "Iter 611/1000 - Loss: 0.371\n",
      "Iter 612/1000 - Loss: 0.349\n",
      "Iter 613/1000 - Loss: 0.356\n",
      "Iter 614/1000 - Loss: 0.359\n",
      "Iter 615/1000 - Loss: 0.368\n",
      "Iter 616/1000 - Loss: 0.349\n",
      "Iter 617/1000 - Loss: 0.362\n",
      "Iter 618/1000 - Loss: 0.367\n",
      "Iter 619/1000 - Loss: 0.352\n",
      "Iter 620/1000 - Loss: 0.347\n",
      "Iter 621/1000 - Loss: 0.349\n",
      "Iter 622/1000 - Loss: 0.356\n",
      "Iter 623/1000 - Loss: 0.359\n",
      "Iter 624/1000 - Loss: 0.353\n",
      "Iter 625/1000 - Loss: 0.352\n",
      "Iter 626/1000 - Loss: 0.355\n",
      "Iter 627/1000 - Loss: 0.360\n",
      "Iter 628/1000 - Loss: 0.356\n",
      "Iter 629/1000 - Loss: 0.354\n",
      "Iter 630/1000 - Loss: 0.355\n",
      "Iter 631/1000 - Loss: 0.360\n",
      "Iter 632/1000 - Loss: 0.343\n",
      "Iter 633/1000 - Loss: 0.341\n",
      "Iter 634/1000 - Loss: 0.347\n",
      "Iter 635/1000 - Loss: 0.365\n",
      "Iter 636/1000 - Loss: 0.358\n",
      "Iter 637/1000 - Loss: 0.347\n",
      "Iter 638/1000 - Loss: 0.360\n",
      "Iter 639/1000 - Loss: 0.349\n",
      "Iter 640/1000 - Loss: 0.351\n",
      "Iter 641/1000 - Loss: 0.354\n",
      "Iter 642/1000 - Loss: 0.360\n",
      "Iter 643/1000 - Loss: 0.350\n",
      "Iter 644/1000 - Loss: 0.351\n",
      "Iter 645/1000 - Loss: 0.354\n",
      "Iter 646/1000 - Loss: 0.362\n",
      "Iter 647/1000 - Loss: 0.357\n",
      "Iter 648/1000 - Loss: 0.346\n",
      "Iter 649/1000 - Loss: 0.351\n",
      "Iter 650/1000 - Loss: 0.363\n",
      "Iter 651/1000 - Loss: 0.360\n",
      "Iter 652/1000 - Loss: 0.350\n",
      "Iter 653/1000 - Loss: 0.342\n",
      "Iter 654/1000 - Loss: 0.354\n",
      "Iter 655/1000 - Loss: 0.350\n",
      "Iter 656/1000 - Loss: 0.366\n",
      "Iter 657/1000 - Loss: 0.369\n",
      "Iter 658/1000 - Loss: 0.349\n",
      "Iter 659/1000 - Loss: 0.349\n",
      "Iter 660/1000 - Loss: 0.364\n",
      "Iter 661/1000 - Loss: 0.365\n",
      "Iter 662/1000 - Loss: 0.355\n",
      "Iter 663/1000 - Loss: 0.368\n",
      "Iter 664/1000 - Loss: 0.351\n",
      "Iter 665/1000 - Loss: 0.350\n",
      "Iter 666/1000 - Loss: 0.355\n",
      "Iter 667/1000 - Loss: 0.365\n",
      "Iter 668/1000 - Loss: 0.341\n",
      "Iter 669/1000 - Loss: 0.340\n",
      "Iter 670/1000 - Loss: 0.365\n",
      "Iter 671/1000 - Loss: 0.347\n",
      "Iter 672/1000 - Loss: 0.338\n",
      "Iter 673/1000 - Loss: 0.350\n",
      "Iter 674/1000 - Loss: 0.356\n",
      "Iter 675/1000 - Loss: 0.346\n",
      "Iter 676/1000 - Loss: 0.351\n",
      "Iter 677/1000 - Loss: 0.367\n",
      "Iter 678/1000 - Loss: 0.354\n",
      "Iter 679/1000 - Loss: 0.363\n",
      "Iter 680/1000 - Loss: 0.357\n",
      "Iter 681/1000 - Loss: 0.351\n",
      "Iter 682/1000 - Loss: 0.353\n",
      "Iter 683/1000 - Loss: 0.355\n",
      "Iter 684/1000 - Loss: 0.352\n",
      "Iter 685/1000 - Loss: 0.368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-430-b0d5e844633a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_gp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAGP_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreastRP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreastY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp_mll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariationalELBO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAGP_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-161-4795eaa14298>\u001b[0m in \u001b[0;36mfit_gp_model\u001b[0;34m(gp_model, gp_likelihood, xs, ys, lr, gp_mll, training_iter)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Calc loss and backprop gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgp_mll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iter %d/%d - Loss: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/mlls/variational_elbo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, variational_dist_f, target, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mkl_divergence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariational_dist_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/distributions/kl.py\u001b[0m in \u001b[0;36mkl_divergence\u001b[0;34m(p, q)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mkl_mvn_mvn\u001b[0;34m(p_dist, q_dist)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0minv_quad_rhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_p_covar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_diffs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mlog_det_p_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_covar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_det\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mtrace_plus_inv_quad_form\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_det_q_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_covar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_quad_log_det\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_det\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# Compute the KL Divergence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36minv_quad_log_det\u001b[0;34m(self, inv_quad_rhs, log_det, reduce_inv_quad)\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0mpreconditioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preconditioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mlog_det_correction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preconditioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         )(*args)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minv_quad_term\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce_inv_quad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/functions/_inv_quad_log_det.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cg_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mmax_tridiag_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_lanczos_quadrature_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mpreconditioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreconditioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             )\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/utils/linear_cg.py\u001b[0m in \u001b[0;36mlinear_cg\u001b[0;34m(matmul_closure, rhs, n_tridiag, tolerance, eps, max_iter, max_tridiag_iter, initial_guess, preconditioner)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Get next alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# alpha_{k} = (residual_{k-1}^T precon_residual{k-1}) / (p_vec_{k-1}^T mat p_vec_{k-1})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mmvms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmul_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmul_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_t_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_t_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/lazy/constant_mul_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constant_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/lazy/sum_batch_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mrhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_blocks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/lazy/non_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_t_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit_gp_model(AGP_model, l2, breastRP[:400], breastY[:400], lr=0.01, gp_mll=gpytorch.mlls.VariationalELBO(l2, AGP_model, 400), training_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'evaluate_kernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-432-279268baf380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred_agp_rp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAGP_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbreastRP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred_agp_rp_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAGP_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbreastRP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/models/abstract_variational_gp.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariational_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/variational/variational_strategy.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariational_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_variational_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_distribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariational_params_initialized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariationalStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/variational/variational_strategy.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbeta_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagonal_correction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mfake_diagonal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minv_product\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minduc_data_covar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mreal_diagonal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_data_covar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                 \u001b[0mdiag_correction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiagLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_diagonal\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfake_diagonal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mpredictive_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPsdSumLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictive_covar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag_correction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\u001b[0m in \u001b[0;36mdiag\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# Did this Kernel eat the diag option?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/kernels/scale_kernel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, batch_dims, **params)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0moutputscales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputscales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0morig_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0moutputscales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputscales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0morig_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/kernels/kernel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x1, x2, diag, batch_dims, **params)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;31m# Did this Kernel eat the diag option?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/kernels/additive_structure_kernel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, batch_dims, **params)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AdditiveStructureKernel does not accept the batch_dims argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mevaluate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'evaluate_kernel'"
     ]
    }
   ],
   "source": [
    "pred_agp_rp = AGP_model(breastRP[:400])\n",
    "pred_agp_rp_test = AGP_model(breastRP[400:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal - see how the number of random projections affects the performance of a GP in high dimensions\n",
    "\n",
    "Method - on a large number of benchmark datasets, vary the number of components used in a GP, determine the distribution of empirical performance, and study the expected trend in performance. \n",
    " * Should there be setting of hyperparameters? Yes\n",
    " * Should \"scalable GP\" methods be tested in tandem? Perhaps not initially.\n",
    " * Should there be cross-validation? Perhaps yes.\n",
    " * What choice of GP model should be used? ARD with GP is an easy choice, but perhaps not the best?\n",
    " * How should the experiments be performed? It should be reasonably fast and modular and not take too much time to set up.\n",
    " * How should the GP models be optimized? \n",
    "   - Using MLL and, say, ADAM, we'd need see what works for the learning rate and/or number of epochs.\n",
    "   - Maybe we can start with reasonable values (low learning rates and relatively high epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.22222222222222"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10*5*10*10*16/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./early_partial_result.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f1cb7b96f98>"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df['mse'].values, df['test_nll'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9691666666666666"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3489/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>mse</th>\n",
       "      <th>repeat</th>\n",
       "      <th>test_nll</th>\n",
       "      <th>test_nmll</th>\n",
       "      <th>train_nll</th>\n",
       "      <th>train_nmll</th>\n",
       "      <th>train_time</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>203.989563</td>\n",
       "      <td>0</td>\n",
       "      <td>1079.646973</td>\n",
       "      <td>21.169521</td>\n",
       "      <td>5239.901367</td>\n",
       "      <td>11.517622</td>\n",
       "      <td>45.176552</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>203.989029</td>\n",
       "      <td>1</td>\n",
       "      <td>1079.611938</td>\n",
       "      <td>21.169895</td>\n",
       "      <td>5241.134277</td>\n",
       "      <td>11.518874</td>\n",
       "      <td>45.207898</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>203.989609</td>\n",
       "      <td>2</td>\n",
       "      <td>1079.568726</td>\n",
       "      <td>21.168240</td>\n",
       "      <td>5240.715820</td>\n",
       "      <td>11.518463</td>\n",
       "      <td>45.711532</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>203.988739</td>\n",
       "      <td>3</td>\n",
       "      <td>1079.662354</td>\n",
       "      <td>21.168131</td>\n",
       "      <td>5240.639160</td>\n",
       "      <td>11.518350</td>\n",
       "      <td>45.543470</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>203.989792</td>\n",
       "      <td>4</td>\n",
       "      <td>1079.564819</td>\n",
       "      <td>21.166733</td>\n",
       "      <td>5240.546387</td>\n",
       "      <td>11.516733</td>\n",
       "      <td>43.871764</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>203.989029</td>\n",
       "      <td>5</td>\n",
       "      <td>1079.581665</td>\n",
       "      <td>21.167900</td>\n",
       "      <td>5240.642090</td>\n",
       "      <td>11.517544</td>\n",
       "      <td>44.064404</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>203.987473</td>\n",
       "      <td>6</td>\n",
       "      <td>1079.642822</td>\n",
       "      <td>21.169821</td>\n",
       "      <td>5240.490723</td>\n",
       "      <td>11.518460</td>\n",
       "      <td>44.693269</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>203.988144</td>\n",
       "      <td>7</td>\n",
       "      <td>1079.519409</td>\n",
       "      <td>21.169950</td>\n",
       "      <td>5240.214844</td>\n",
       "      <td>11.519488</td>\n",
       "      <td>44.893738</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>203.989197</td>\n",
       "      <td>8</td>\n",
       "      <td>1079.602661</td>\n",
       "      <td>21.168266</td>\n",
       "      <td>5241.418457</td>\n",
       "      <td>11.518588</td>\n",
       "      <td>41.709107</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>203.989166</td>\n",
       "      <td>9</td>\n",
       "      <td>1079.700562</td>\n",
       "      <td>21.170130</td>\n",
       "      <td>5241.502441</td>\n",
       "      <td>11.517249</td>\n",
       "      <td>42.205009</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>125.673332</td>\n",
       "      <td>0</td>\n",
       "      <td>691.022461</td>\n",
       "      <td>13.551296</td>\n",
       "      <td>5327.773926</td>\n",
       "      <td>11.706860</td>\n",
       "      <td>44.427405</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>125.673027</td>\n",
       "      <td>1</td>\n",
       "      <td>690.994141</td>\n",
       "      <td>13.546687</td>\n",
       "      <td>5326.994629</td>\n",
       "      <td>11.707386</td>\n",
       "      <td>44.536489</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>125.673195</td>\n",
       "      <td>2</td>\n",
       "      <td>690.984131</td>\n",
       "      <td>13.553316</td>\n",
       "      <td>5327.832520</td>\n",
       "      <td>11.708908</td>\n",
       "      <td>42.710520</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>125.673416</td>\n",
       "      <td>3</td>\n",
       "      <td>690.854309</td>\n",
       "      <td>13.549697</td>\n",
       "      <td>5327.074707</td>\n",
       "      <td>11.710439</td>\n",
       "      <td>44.692848</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>125.673645</td>\n",
       "      <td>4</td>\n",
       "      <td>691.128906</td>\n",
       "      <td>13.546495</td>\n",
       "      <td>5328.273926</td>\n",
       "      <td>11.709471</td>\n",
       "      <td>42.241838</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>125.673912</td>\n",
       "      <td>5</td>\n",
       "      <td>690.961243</td>\n",
       "      <td>13.547874</td>\n",
       "      <td>5327.824219</td>\n",
       "      <td>11.709208</td>\n",
       "      <td>44.483643</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>125.673042</td>\n",
       "      <td>6</td>\n",
       "      <td>690.896240</td>\n",
       "      <td>13.547659</td>\n",
       "      <td>5326.497559</td>\n",
       "      <td>11.708062</td>\n",
       "      <td>44.320954</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>125.673141</td>\n",
       "      <td>7</td>\n",
       "      <td>691.021301</td>\n",
       "      <td>13.550326</td>\n",
       "      <td>5327.909668</td>\n",
       "      <td>11.708270</td>\n",
       "      <td>43.814817</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>125.672806</td>\n",
       "      <td>8</td>\n",
       "      <td>691.116211</td>\n",
       "      <td>13.551745</td>\n",
       "      <td>5327.137695</td>\n",
       "      <td>11.707745</td>\n",
       "      <td>44.301052</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>125.673515</td>\n",
       "      <td>9</td>\n",
       "      <td>691.047974</td>\n",
       "      <td>13.547903</td>\n",
       "      <td>5327.609375</td>\n",
       "      <td>11.708431</td>\n",
       "      <td>43.969744</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>144.066803</td>\n",
       "      <td>0</td>\n",
       "      <td>782.231750</td>\n",
       "      <td>15.332393</td>\n",
       "      <td>5359.980469</td>\n",
       "      <td>11.778957</td>\n",
       "      <td>44.258393</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>144.067490</td>\n",
       "      <td>1</td>\n",
       "      <td>782.228149</td>\n",
       "      <td>15.331354</td>\n",
       "      <td>5360.310547</td>\n",
       "      <td>11.778550</td>\n",
       "      <td>42.260480</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>144.067078</td>\n",
       "      <td>2</td>\n",
       "      <td>781.970703</td>\n",
       "      <td>15.332851</td>\n",
       "      <td>5361.858887</td>\n",
       "      <td>11.780746</td>\n",
       "      <td>43.503955</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>144.067062</td>\n",
       "      <td>3</td>\n",
       "      <td>782.185730</td>\n",
       "      <td>15.333857</td>\n",
       "      <td>5359.228027</td>\n",
       "      <td>11.778600</td>\n",
       "      <td>44.284820</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>144.067429</td>\n",
       "      <td>4</td>\n",
       "      <td>782.110352</td>\n",
       "      <td>15.333675</td>\n",
       "      <td>5359.296875</td>\n",
       "      <td>11.779386</td>\n",
       "      <td>44.038864</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>144.065720</td>\n",
       "      <td>5</td>\n",
       "      <td>782.022827</td>\n",
       "      <td>15.335702</td>\n",
       "      <td>5359.805176</td>\n",
       "      <td>11.780519</td>\n",
       "      <td>44.385837</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>144.068497</td>\n",
       "      <td>6</td>\n",
       "      <td>782.177979</td>\n",
       "      <td>15.334824</td>\n",
       "      <td>5359.501953</td>\n",
       "      <td>11.781015</td>\n",
       "      <td>42.363004</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>144.068192</td>\n",
       "      <td>7</td>\n",
       "      <td>781.980835</td>\n",
       "      <td>15.335744</td>\n",
       "      <td>5360.102539</td>\n",
       "      <td>11.781274</td>\n",
       "      <td>44.378679</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>144.066986</td>\n",
       "      <td>8</td>\n",
       "      <td>782.047913</td>\n",
       "      <td>15.335321</td>\n",
       "      <td>5360.561035</td>\n",
       "      <td>11.780103</td>\n",
       "      <td>43.694056</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>144.066864</td>\n",
       "      <td>9</td>\n",
       "      <td>782.221863</td>\n",
       "      <td>15.334708</td>\n",
       "      <td>5359.275391</td>\n",
       "      <td>11.781889</td>\n",
       "      <td>44.472252</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>7</td>\n",
       "      <td>166.562866</td>\n",
       "      <td>0</td>\n",
       "      <td>890.115173</td>\n",
       "      <td>17.448832</td>\n",
       "      <td>5284.429688</td>\n",
       "      <td>11.613591</td>\n",
       "      <td>44.358584</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>7</td>\n",
       "      <td>166.562958</td>\n",
       "      <td>1</td>\n",
       "      <td>889.950623</td>\n",
       "      <td>17.448662</td>\n",
       "      <td>5284.339355</td>\n",
       "      <td>11.613272</td>\n",
       "      <td>45.396541</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>7</td>\n",
       "      <td>166.563461</td>\n",
       "      <td>2</td>\n",
       "      <td>889.880005</td>\n",
       "      <td>17.450657</td>\n",
       "      <td>5283.954590</td>\n",
       "      <td>11.615185</td>\n",
       "      <td>41.221562</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7</td>\n",
       "      <td>166.564178</td>\n",
       "      <td>3</td>\n",
       "      <td>890.084534</td>\n",
       "      <td>17.450489</td>\n",
       "      <td>5284.079102</td>\n",
       "      <td>11.614344</td>\n",
       "      <td>43.298423</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>7</td>\n",
       "      <td>166.563690</td>\n",
       "      <td>4</td>\n",
       "      <td>890.025391</td>\n",
       "      <td>17.450304</td>\n",
       "      <td>5284.261719</td>\n",
       "      <td>11.612350</td>\n",
       "      <td>40.546289</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>7</td>\n",
       "      <td>166.563721</td>\n",
       "      <td>5</td>\n",
       "      <td>890.030640</td>\n",
       "      <td>17.449131</td>\n",
       "      <td>5284.938965</td>\n",
       "      <td>11.613790</td>\n",
       "      <td>42.710078</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>7</td>\n",
       "      <td>166.562851</td>\n",
       "      <td>6</td>\n",
       "      <td>889.906372</td>\n",
       "      <td>17.448925</td>\n",
       "      <td>5284.106934</td>\n",
       "      <td>11.613399</td>\n",
       "      <td>40.701096</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>7</td>\n",
       "      <td>166.562897</td>\n",
       "      <td>7</td>\n",
       "      <td>889.985413</td>\n",
       "      <td>17.451281</td>\n",
       "      <td>5284.895996</td>\n",
       "      <td>11.614551</td>\n",
       "      <td>44.917640</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>7</td>\n",
       "      <td>166.561447</td>\n",
       "      <td>8</td>\n",
       "      <td>889.904358</td>\n",
       "      <td>17.450184</td>\n",
       "      <td>5284.448242</td>\n",
       "      <td>11.613913</td>\n",
       "      <td>44.888584</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>7</td>\n",
       "      <td>166.562103</td>\n",
       "      <td>9</td>\n",
       "      <td>889.907898</td>\n",
       "      <td>17.454817</td>\n",
       "      <td>5284.398926</td>\n",
       "      <td>11.613334</td>\n",
       "      <td>44.655655</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>8</td>\n",
       "      <td>185.417343</td>\n",
       "      <td>0</td>\n",
       "      <td>974.118042</td>\n",
       "      <td>19.099785</td>\n",
       "      <td>5281.040039</td>\n",
       "      <td>11.605740</td>\n",
       "      <td>44.911168</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>8</td>\n",
       "      <td>185.418259</td>\n",
       "      <td>1</td>\n",
       "      <td>974.241577</td>\n",
       "      <td>19.097712</td>\n",
       "      <td>5279.621582</td>\n",
       "      <td>11.607057</td>\n",
       "      <td>44.595927</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>8</td>\n",
       "      <td>185.416870</td>\n",
       "      <td>2</td>\n",
       "      <td>974.044434</td>\n",
       "      <td>19.099358</td>\n",
       "      <td>5280.312988</td>\n",
       "      <td>11.605528</td>\n",
       "      <td>45.180433</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>8</td>\n",
       "      <td>185.417740</td>\n",
       "      <td>3</td>\n",
       "      <td>974.108582</td>\n",
       "      <td>19.099936</td>\n",
       "      <td>5280.504883</td>\n",
       "      <td>11.601912</td>\n",
       "      <td>44.576425</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>8</td>\n",
       "      <td>185.419037</td>\n",
       "      <td>4</td>\n",
       "      <td>974.044250</td>\n",
       "      <td>19.100067</td>\n",
       "      <td>5281.006348</td>\n",
       "      <td>11.604141</td>\n",
       "      <td>43.464557</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>8</td>\n",
       "      <td>185.417053</td>\n",
       "      <td>5</td>\n",
       "      <td>974.111023</td>\n",
       "      <td>19.098444</td>\n",
       "      <td>5280.531738</td>\n",
       "      <td>11.604412</td>\n",
       "      <td>40.709961</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>8</td>\n",
       "      <td>185.418564</td>\n",
       "      <td>6</td>\n",
       "      <td>974.080322</td>\n",
       "      <td>19.097467</td>\n",
       "      <td>5279.183105</td>\n",
       "      <td>11.605394</td>\n",
       "      <td>42.727881</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>8</td>\n",
       "      <td>185.416534</td>\n",
       "      <td>7</td>\n",
       "      <td>974.160461</td>\n",
       "      <td>19.098349</td>\n",
       "      <td>5279.800781</td>\n",
       "      <td>11.604929</td>\n",
       "      <td>45.158891</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>8</td>\n",
       "      <td>185.418335</td>\n",
       "      <td>8</td>\n",
       "      <td>974.089172</td>\n",
       "      <td>19.097841</td>\n",
       "      <td>5280.068359</td>\n",
       "      <td>11.605918</td>\n",
       "      <td>43.199178</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>8</td>\n",
       "      <td>185.418549</td>\n",
       "      <td>9</td>\n",
       "      <td>974.104614</td>\n",
       "      <td>19.098536</td>\n",
       "      <td>5280.702148</td>\n",
       "      <td>11.604382</td>\n",
       "      <td>45.694756</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>9</td>\n",
       "      <td>162.787323</td>\n",
       "      <td>0</td>\n",
       "      <td>802.916016</td>\n",
       "      <td>17.084187</td>\n",
       "      <td>5443.924316</td>\n",
       "      <td>11.860009</td>\n",
       "      <td>46.388919</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786499</td>\n",
       "      <td>1</td>\n",
       "      <td>803.012207</td>\n",
       "      <td>17.084263</td>\n",
       "      <td>5444.080078</td>\n",
       "      <td>11.862436</td>\n",
       "      <td>45.528770</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786819</td>\n",
       "      <td>2</td>\n",
       "      <td>802.924438</td>\n",
       "      <td>17.085670</td>\n",
       "      <td>5443.974121</td>\n",
       "      <td>11.859550</td>\n",
       "      <td>45.680831</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>9</td>\n",
       "      <td>162.787491</td>\n",
       "      <td>3</td>\n",
       "      <td>802.925293</td>\n",
       "      <td>17.084959</td>\n",
       "      <td>5443.205078</td>\n",
       "      <td>11.860595</td>\n",
       "      <td>43.391647</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786194</td>\n",
       "      <td>4</td>\n",
       "      <td>803.038513</td>\n",
       "      <td>17.084902</td>\n",
       "      <td>5443.466797</td>\n",
       "      <td>11.860003</td>\n",
       "      <td>45.283158</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786301</td>\n",
       "      <td>5</td>\n",
       "      <td>802.925293</td>\n",
       "      <td>17.085184</td>\n",
       "      <td>5443.829590</td>\n",
       "      <td>11.859897</td>\n",
       "      <td>45.845787</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786499</td>\n",
       "      <td>6</td>\n",
       "      <td>802.867310</td>\n",
       "      <td>17.083405</td>\n",
       "      <td>5444.510742</td>\n",
       "      <td>11.861804</td>\n",
       "      <td>45.713910</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786148</td>\n",
       "      <td>7</td>\n",
       "      <td>802.960449</td>\n",
       "      <td>17.084223</td>\n",
       "      <td>5444.159180</td>\n",
       "      <td>11.857876</td>\n",
       "      <td>43.728417</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786545</td>\n",
       "      <td>8</td>\n",
       "      <td>803.017822</td>\n",
       "      <td>17.084711</td>\n",
       "      <td>5443.786133</td>\n",
       "      <td>11.861763</td>\n",
       "      <td>45.184782</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>9</td>\n",
       "      <td>162.787292</td>\n",
       "      <td>9</td>\n",
       "      <td>802.972717</td>\n",
       "      <td>17.085007</td>\n",
       "      <td>5444.472168</td>\n",
       "      <td>11.861156</td>\n",
       "      <td>45.579501</td>\n",
       "      <td>housing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold         mse  repeat     test_nll  test_nmll    train_nll  train_nmll  \\\n",
       "0      0  203.989563       0  1079.646973  21.169521  5239.901367   11.517622   \n",
       "1      0  203.989029       1  1079.611938  21.169895  5241.134277   11.518874   \n",
       "2      0  203.989609       2  1079.568726  21.168240  5240.715820   11.518463   \n",
       "3      0  203.988739       3  1079.662354  21.168131  5240.639160   11.518350   \n",
       "4      0  203.989792       4  1079.564819  21.166733  5240.546387   11.516733   \n",
       "5      0  203.989029       5  1079.581665  21.167900  5240.642090   11.517544   \n",
       "6      0  203.987473       6  1079.642822  21.169821  5240.490723   11.518460   \n",
       "7      0  203.988144       7  1079.519409  21.169950  5240.214844   11.519488   \n",
       "8      0  203.989197       8  1079.602661  21.168266  5241.418457   11.518588   \n",
       "9      0  203.989166       9  1079.700562  21.170130  5241.502441   11.517249   \n",
       "10     1  125.673332       0   691.022461  13.551296  5327.773926   11.706860   \n",
       "11     1  125.673027       1   690.994141  13.546687  5326.994629   11.707386   \n",
       "12     1  125.673195       2   690.984131  13.553316  5327.832520   11.708908   \n",
       "13     1  125.673416       3   690.854309  13.549697  5327.074707   11.710439   \n",
       "14     1  125.673645       4   691.128906  13.546495  5328.273926   11.709471   \n",
       "15     1  125.673912       5   690.961243  13.547874  5327.824219   11.709208   \n",
       "16     1  125.673042       6   690.896240  13.547659  5326.497559   11.708062   \n",
       "17     1  125.673141       7   691.021301  13.550326  5327.909668   11.708270   \n",
       "18     1  125.672806       8   691.116211  13.551745  5327.137695   11.707745   \n",
       "19     1  125.673515       9   691.047974  13.547903  5327.609375   11.708431   \n",
       "20     2  144.066803       0   782.231750  15.332393  5359.980469   11.778957   \n",
       "21     2  144.067490       1   782.228149  15.331354  5360.310547   11.778550   \n",
       "22     2  144.067078       2   781.970703  15.332851  5361.858887   11.780746   \n",
       "23     2  144.067062       3   782.185730  15.333857  5359.228027   11.778600   \n",
       "24     2  144.067429       4   782.110352  15.333675  5359.296875   11.779386   \n",
       "25     2  144.065720       5   782.022827  15.335702  5359.805176   11.780519   \n",
       "26     2  144.068497       6   782.177979  15.334824  5359.501953   11.781015   \n",
       "27     2  144.068192       7   781.980835  15.335744  5360.102539   11.781274   \n",
       "28     2  144.066986       8   782.047913  15.335321  5360.561035   11.780103   \n",
       "29     2  144.066864       9   782.221863  15.334708  5359.275391   11.781889   \n",
       "..   ...         ...     ...          ...        ...          ...         ...   \n",
       "70     7  166.562866       0   890.115173  17.448832  5284.429688   11.613591   \n",
       "71     7  166.562958       1   889.950623  17.448662  5284.339355   11.613272   \n",
       "72     7  166.563461       2   889.880005  17.450657  5283.954590   11.615185   \n",
       "73     7  166.564178       3   890.084534  17.450489  5284.079102   11.614344   \n",
       "74     7  166.563690       4   890.025391  17.450304  5284.261719   11.612350   \n",
       "75     7  166.563721       5   890.030640  17.449131  5284.938965   11.613790   \n",
       "76     7  166.562851       6   889.906372  17.448925  5284.106934   11.613399   \n",
       "77     7  166.562897       7   889.985413  17.451281  5284.895996   11.614551   \n",
       "78     7  166.561447       8   889.904358  17.450184  5284.448242   11.613913   \n",
       "79     7  166.562103       9   889.907898  17.454817  5284.398926   11.613334   \n",
       "80     8  185.417343       0   974.118042  19.099785  5281.040039   11.605740   \n",
       "81     8  185.418259       1   974.241577  19.097712  5279.621582   11.607057   \n",
       "82     8  185.416870       2   974.044434  19.099358  5280.312988   11.605528   \n",
       "83     8  185.417740       3   974.108582  19.099936  5280.504883   11.601912   \n",
       "84     8  185.419037       4   974.044250  19.100067  5281.006348   11.604141   \n",
       "85     8  185.417053       5   974.111023  19.098444  5280.531738   11.604412   \n",
       "86     8  185.418564       6   974.080322  19.097467  5279.183105   11.605394   \n",
       "87     8  185.416534       7   974.160461  19.098349  5279.800781   11.604929   \n",
       "88     8  185.418335       8   974.089172  19.097841  5280.068359   11.605918   \n",
       "89     8  185.418549       9   974.104614  19.098536  5280.702148   11.604382   \n",
       "90     9  162.787323       0   802.916016  17.084187  5443.924316   11.860009   \n",
       "91     9  162.786499       1   803.012207  17.084263  5444.080078   11.862436   \n",
       "92     9  162.786819       2   802.924438  17.085670  5443.974121   11.859550   \n",
       "93     9  162.787491       3   802.925293  17.084959  5443.205078   11.860595   \n",
       "94     9  162.786194       4   803.038513  17.084902  5443.466797   11.860003   \n",
       "95     9  162.786301       5   802.925293  17.085184  5443.829590   11.859897   \n",
       "96     9  162.786499       6   802.867310  17.083405  5444.510742   11.861804   \n",
       "97     9  162.786148       7   802.960449  17.084223  5444.159180   11.857876   \n",
       "98     9  162.786545       8   803.017822  17.084711  5443.786133   11.861763   \n",
       "99     9  162.787292       9   802.972717  17.085007  5444.472168   11.861156   \n",
       "\n",
       "    train_time  dataset  \n",
       "0    45.176552  housing  \n",
       "1    45.207898  housing  \n",
       "2    45.711532  housing  \n",
       "3    45.543470  housing  \n",
       "4    43.871764  housing  \n",
       "5    44.064404  housing  \n",
       "6    44.693269  housing  \n",
       "7    44.893738  housing  \n",
       "8    41.709107  housing  \n",
       "9    42.205009  housing  \n",
       "10   44.427405  housing  \n",
       "11   44.536489  housing  \n",
       "12   42.710520  housing  \n",
       "13   44.692848  housing  \n",
       "14   42.241838  housing  \n",
       "15   44.483643  housing  \n",
       "16   44.320954  housing  \n",
       "17   43.814817  housing  \n",
       "18   44.301052  housing  \n",
       "19   43.969744  housing  \n",
       "20   44.258393  housing  \n",
       "21   42.260480  housing  \n",
       "22   43.503955  housing  \n",
       "23   44.284820  housing  \n",
       "24   44.038864  housing  \n",
       "25   44.385837  housing  \n",
       "26   42.363004  housing  \n",
       "27   44.378679  housing  \n",
       "28   43.694056  housing  \n",
       "29   44.472252  housing  \n",
       "..         ...      ...  \n",
       "70   44.358584  housing  \n",
       "71   45.396541  housing  \n",
       "72   41.221562  housing  \n",
       "73   43.298423  housing  \n",
       "74   40.546289  housing  \n",
       "75   42.710078  housing  \n",
       "76   40.701096  housing  \n",
       "77   44.917640  housing  \n",
       "78   44.888584  housing  \n",
       "79   44.655655  housing  \n",
       "80   44.911168  housing  \n",
       "81   44.595927  housing  \n",
       "82   45.180433  housing  \n",
       "83   44.576425  housing  \n",
       "84   43.464557  housing  \n",
       "85   40.709961  housing  \n",
       "86   42.727881  housing  \n",
       "87   45.158891  housing  \n",
       "88   43.199178  housing  \n",
       "89   45.694756  housing  \n",
       "90   46.388919  housing  \n",
       "91   45.528770  housing  \n",
       "92   45.680831  housing  \n",
       "93   43.391647  housing  \n",
       "94   45.283158  housing  \n",
       "95   45.845787  housing  \n",
       "96   45.713910  housing  \n",
       "97   43.728417  housing  \n",
       "98   45.184782  housing  \n",
       "99   45.579501  housing  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"2019-02-27 18:11:45.069518 - {'fold': 0, 'repeat': 0, 'mse': 0.07313001155853271, 'train_time': 18.21050319587812, 'train_nmll': -0.2634676694869995, 'test_nmll': 0.1407051980495453, 'train_nll': -44.74034118652344, 'test_nll': 2.7185821533203125}\n",
    "2019-02-27 18:12:00.226547 - {'fold': 0, 'repeat': 1, 'mse': 0.07101680338382721, 'train_time': 15.156932359095663, 'train_nmll': -0.3156649172306061, 'test_nmll': 0.09640350192785263, 'train_nll': -55.485595703125, 'test_nll': 1.827188491821289}\n",
    "2019-02-27 18:12:14.976521 - {'fold': 0, 'repeat': 2, 'mse': 0.0729895606637001, 'train_time': 14.749871701002121, 'train_nmll': -0.33502256870269775, 'test_nmll': 0.12224707752466202, 'train_nll': -60.2880859375, 'test_nll': 1.9846649169921875}\n",
    "2019-02-27 18:12:32.428572 - {'fold': 0, 'repeat': 3, 'mse': 0.07267884910106659, 'train_time': 17.451954272808507, 'train_nmll': -0.3168109357357025, 'test_nmll': 0.10245494544506073, 'train_nll': -57.21910095214844, 'test_nll': 2.033468246459961}\n",
    "2019-02-27 18:12:50.532522 - {'fold': 0, 'repeat': 4, 'mse': 0.07327921688556671, 'train_time': 18.103849983075634, 'train_nmll': -0.30941280722618103, 'test_nmll': 0.10161761939525604, 'train_nll': -56.501953125, 'test_nll': 2.196195602416992}\n",
    "2019-02-27 18:13:09.019194 - {'fold': 0, 'repeat': 5, 'mse': 0.0732535719871521, 'train_time': 18.48656295402907, 'train_nmll': -0.30751970410346985, 'test_nmll': 0.0895085334777832, 'train_nll': -56.59431457519531, 'test_nll': 2.646084785461426}\n",
    "2019-02-27 18:13:26.929771 - {'fold': 0, 'repeat': 6, 'mse': 0.07185467332601547, 'train_time': 17.910462913801894, 'train_nmll': -0.3173889219760895, 'test_nmll': 0.08940563350915909, 'train_nll': -55.40254211425781, 'test_nll': 1.7056026458740234}\n",
    "2019-02-27 18:13:44.154237 - {'fold': 0, 'repeat': 7, 'mse': 0.06981224566698074, 'train_time': 17.224359273910522, 'train_nmll': -0.3206741213798523, 'test_nmll': 0.07380914688110352, 'train_nll': -56.395660400390625, 'test_nll': 1.4421100616455078}\n",
    "2019-02-27 18:14:00.339749 - {'fold': 0, 'repeat': 8, 'mse': 0.07199053466320038, 'train_time': 16.185413589002565, 'train_nmll': -0.29403144121170044, 'test_nmll': 0.11769142001867294, 'train_nll': -55.4827880859375, 'test_nll': 2.8016586303710938}\n",
    "2019-02-27 18:14:15.780302 - {'fold': 0, 'repeat': 9, 'mse': 0.07235368341207504, 'train_time': 15.440453397808596, 'train_nmll': -0.3291492760181427, 'test_nmll': 0.1162177100777626, 'train_nll': -60.36065673828125, 'test_nll': 2.8631792068481445}\n",
    "2019-02-27 18:14:32.861268 - {'fold': 1, 'repeat': 0, 'mse': 0.11360534280538559, 'train_time': 17.077840640908107, 'train_nmll': -0.28347915410995483, 'test_nmll': 0.2866275906562805, 'train_nll': -49.554046630859375, 'test_nll': 5.902166366577148}\n",
    "2019-02-27 18:14:51.460686 - {'fold': 1, 'repeat': 1, 'mse': 0.11146263033151627, 'train_time': 18.59930858388543, 'train_nmll': -0.2905139625072479, 'test_nmll': 0.2832842767238617, 'train_nll': -53.72662353515625, 'test_nll': 5.748804092407227}\n",
    "2019-02-27 18:15:09.042793 - {'fold': 1, 'repeat': 2, 'mse': 0.11420182138681412, 'train_time': 17.582008921075612, 'train_nmll': -0.2900673747062683, 'test_nmll': 0.3057405352592468, 'train_nll': -51.738739013671875, 'test_nll': 6.139369964599609}\n",
    "2019-02-27 18:15:25.784808 - {'fold': 1, 'repeat': 3, 'mse': 0.11382995545864105, 'train_time': 16.741913278819993, 'train_nmll': -0.28129780292510986, 'test_nmll': 0.29796624183654785, 'train_nll': -50.55900573730469, 'test_nll': 5.977133750915527}\n",
    "2019-02-27 18:15:41.206844 - {'fold': 1, 'repeat': 4, 'mse': 0.11060360819101334, 'train_time': 15.42194188782014, 'train_nmll': -0.29422301054000854, 'test_nmll': 0.28671130537986755, 'train_nll': -51.523162841796875, 'test_nll': 5.461488723754883}\n",
    "2019-02-27 18:15:57.275471 - {'fold': 1, 'repeat': 5, 'mse': 0.10469625890254974, 'train_time': 16.06852599978447, 'train_nmll': -0.28496822714805603, 'test_nmll': 0.22790570557117462, 'train_nll': -50.65010070800781, 'test_nll': 4.701238632202148}\n",
    "2019-02-27 18:16:15.330078 - {'fold': 1, 'repeat': 6, 'mse': 0.11308322101831436, 'train_time': 18.054498884128407, 'train_nmll': -0.2845016419887543, 'test_nmll': 0.29927167296409607, 'train_nll': -51.882537841796875, 'test_nll': 6.319060325622559}\n",
    "2019-02-27 18:16:32.099123 - {'fold': 1, 'repeat': 7, 'mse': 0.11144665628671646, 'train_time': 16.76894857501611, 'train_nmll': -0.2794318199157715, 'test_nmll': 0.28132185339927673, 'train_nll': -51.071533203125, 'test_nll': 5.76005744934082}\n",
    "2019-02-27 18:16:49.169997 - {'fold': 1, 'repeat': 8, 'mse': 0.11449108272790909, 'train_time': 17.070773703046143, 'train_nmll': -0.2876618206501007, 'test_nmll': 0.2892300486564636, 'train_nll': -52.158233642578125, 'test_nll': 6.062398910522461}\n",
    "2019-02-27 18:17:07.461416 - {'fold': 1, 'repeat': 9, 'mse': 0.11356166750192642, 'train_time': 18.29131321585737, 'train_nmll': -0.2714404761791229, 'test_nmll': 0.3129608631134033, 'train_nll': -50.911712646484375, 'test_nll': 6.253701210021973}\n",
    "2019-02-27 18:17:23.281131 - {'fold': 2, 'repeat': 0, 'mse': 0.21850375831127167, 'train_time': 15.816631419118494, 'train_nmll': -0.4107917845249176, 'test_nmll': 0.7454472780227661, 'train_nll': -73.01565551757812, 'test_nll': 14.683549880981445}\n",
    "2019-02-27 18:17:41.105429 - {'fold': 2, 'repeat': 1, 'mse': 0.21834659576416016, 'train_time': 17.824198314920068, 'train_nmll': -0.41024237871170044, 'test_nmll': 0.7289853096008301, 'train_nll': -74.57078552246094, 'test_nll': 14.68086051940918}\n",
    "2019-02-27 18:17:59.273297 - {'fold': 2, 'repeat': 2, 'mse': 0.21653692424297333, 'train_time': 18.167761659016833, 'train_nmll': -0.42084741592407227, 'test_nmll': 0.7059102058410645, 'train_nll': -74.41102600097656, 'test_nll': 13.961114883422852}\n",
    "2019-02-27 18:18:17.240324 - {'fold': 2, 'repeat': 3, 'mse': 0.2095552384853363, 'train_time': 17.966930235968903, 'train_nmll': -0.4102265238761902, 'test_nmll': 0.6910058259963989, 'train_nll': -74.63522338867188, 'test_nll': 14.391742706298828}\n",
    "2019-02-27 18:18:31.571884 - {'fold': 2, 'repeat': 4, 'mse': 0.21895098686218262, 'train_time': 14.331469602882862, 'train_nmll': -0.42560508847236633, 'test_nmll': 0.7271007299423218, 'train_nll': -73.88310241699219, 'test_nll': 14.718072891235352}\n",
    "2019-02-27 18:18:46.669751 - {'fold': 2, 'repeat': 5, 'mse': 0.21205875277519226, 'train_time': 15.097777640912682, 'train_nmll': -0.41609853506088257, 'test_nmll': 0.7306585311889648, 'train_nll': -73.60650634765625, 'test_nll': 14.169485092163086}\n",
    "2019-02-27 18:19:01.062892 - {'fold': 2, 'repeat': 6, 'mse': 0.21396282315254211, 'train_time': 14.39304105588235, 'train_nmll': -0.3957200050354004, 'test_nmll': 0.7162789106369019, 'train_nll': -73.50315856933594, 'test_nll': 14.09029769897461}\n",
    "2019-02-27 18:19:17.451044 - {'fold': 2, 'repeat': 7, 'mse': 0.21389946341514587, 'train_time': 16.388057185104117, 'train_nmll': -0.40240225195884705, 'test_nmll': 0.7139172554016113, 'train_nll': -70.59149169921875, 'test_nll': 14.204385757446289}\n",
    "2019-02-27 18:19:31.802826 - {'fold': 2, 'repeat': 8, 'mse': 0.21828272938728333, 'train_time': 14.351690171053633, 'train_nmll': -0.4188632667064667, 'test_nmll': 0.7449111938476562, 'train_nll': -74.04939270019531, 'test_nll': 14.446914672851562}\n",
    "2019-02-27 18:19:46.137856 - {'fold': 2, 'repeat': 9, 'mse': 0.21590809524059296, 'train_time': 14.334940182976425, 'train_nmll': -0.435134619474411, 'test_nmll': 0.7548227310180664, 'train_nll': -75.48574829101562, 'test_nll': 14.754270553588867}\n",
    "2019-02-27 18:20:00.455362 - {'fold': 3, 'repeat': 0, 'mse': 0.853351891040802, 'train_time': 14.315171902999282, 'train_nmll': -0.4241360127925873, 'test_nmll': 1.5160795450210571, 'train_nll': -71.681640625, 'test_nll': 30.431856155395508}\n",
    "2019-02-27 18:20:15.924563 - {'fold': 3, 'repeat': 1, 'mse': 0.8506598472595215, 'train_time': 15.469100632937625, 'train_nmll': -0.41301336884498596, 'test_nmll': 1.5533394813537598, 'train_nll': -81.260009765625, 'test_nll': 30.473098754882812}\n",
    "2019-02-27 18:20:31.171032 - {'fold': 3, 'repeat': 2, 'mse': 0.8377593755722046, 'train_time': 15.246373763075098, 'train_nmll': -0.38563409447669983, 'test_nmll': 1.5473642349243164, 'train_nll': -70.06747436523438, 'test_nll': 30.862133026123047}\n",
    "2019-02-27 18:20:45.432819 - {'fold': 3, 'repeat': 3, 'mse': 0.8601816892623901, 'train_time': 14.261698884889483, 'train_nmll': -0.4595881998538971, 'test_nmll': 1.5485234260559082, 'train_nll': -81.09773254394531, 'test_nll': 30.884611129760742}\n",
    "2019-02-27 18:21:02.312148 - {'fold': 3, 'repeat': 4, 'mse': 0.8530999422073364, 'train_time': 16.879230545135215, 'train_nmll': -0.38162359595298767, 'test_nmll': 1.5379407405853271, 'train_nll': -67.87925720214844, 'test_nll': 30.960702896118164}\n",
    "2019-02-27 18:21:20.492315 - {'fold': 3, 'repeat': 5, 'mse': 0.8423398733139038, 'train_time': 18.180061443010345, 'train_nmll': -0.38558927178382874, 'test_nmll': 1.527492642402649, 'train_nll': -66.33442687988281, 'test_nll': 31.08075523376465}\n",
    "2019-02-27 18:21:37.825674 - {'fold': 3, 'repeat': 6, 'mse': 0.8561797142028809, 'train_time': 17.333263223059475, 'train_nmll': -0.24162986874580383, 'test_nmll': 1.5561165809631348, 'train_nll': -33.7474365234375, 'test_nll': 31.165355682373047}\n",
    "2019-02-27 18:21:53.156303 - {'fold': 3, 'repeat': 7, 'mse': 0.8379753828048706, 'train_time': 15.330527472076938, 'train_nmll': -0.38623929023742676, 'test_nmll': 1.549367070198059, 'train_nll': -68.81118774414062, 'test_nll': 30.703977584838867}\n",
    "2019-02-27 18:22:11.344464 - {'fold': 3, 'repeat': 8, 'mse': 0.8531299829483032, 'train_time': 18.188052335986868, 'train_nmll': -0.3783525228500366, 'test_nmll': 1.5724542140960693, 'train_nll': -67.3887939453125, 'test_nll': 30.90804100036621}\n",
    "2019-02-27 18:22:29.549200 - {'fold': 3, 'repeat': 9, 'mse': 0.8592060804367065, 'train_time': 18.20462896488607, 'train_nmll': 123.69509887695312, 'test_nmll': 1.5460364818572998, 'train_nll': 6584.7353515625, 'test_nll': 30.505956649780273}\n",
    "2019-02-27 18:22:47.781530 - {'fold': 4, 'repeat': 0, 'mse': 0.08149678260087967, 'train_time': 18.229260136839002, 'train_nmll': -0.3289904296398163, 'test_nmll': 0.2194478064775467, 'train_nll': -58.52452087402344, 'test_nll': 4.030065536499023}\n",
    "2019-02-27 18:23:05.986399 - {'fold': 4, 'repeat': 1, 'mse': 0.07448487728834152, 'train_time': 18.204760735156015, 'train_nmll': -0.3267630636692047, 'test_nmll': 0.09723100811243057, 'train_nll': -57.66743469238281, 'test_nll': 1.9170646667480469}\n",
    "2019-02-27 18:23:22.539245 - {'fold': 4, 'repeat': 2, 'mse': 0.08174772560596466, 'train_time': 16.55275223311037, 'train_nmll': -0.33116453886032104, 'test_nmll': 0.1733894795179367, 'train_nll': -56.27386474609375, 'test_nll': 3.099142074584961}\n",
    "2019-02-27 18:23:39.658154 - {'fold': 4, 'repeat': 3, 'mse': 0.07613369077444077, 'train_time': 17.118809775914997, 'train_nmll': -0.32485657930374146, 'test_nmll': 0.10367412865161896, 'train_nll': -55.96417236328125, 'test_nll': 2.265148162841797}\n",
    "2019-02-27 18:23:57.875794 - {'fold': 4, 'repeat': 4, 'mse': 0.07921221852302551, 'train_time': 18.21753255208023, 'train_nmll': -0.31203493475914, 'test_nmll': 0.12974634766578674, 'train_nll': -55.756134033203125, 'test_nll': 2.519791603088379}\n",
    "2019-02-27 18:24:16.102958 - {'fold': 4, 'repeat': 5, 'mse': 0.0886274129152298, 'train_time': 18.227059626951814, 'train_nmll': -0.32428106665611267, 'test_nmll': 0.18425516784191132, 'train_nll': -57.42669677734375, 'test_nll': 3.8311729431152344}\n",
    "2019-02-27 18:24:34.326168 - {'fold': 4, 'repeat': 6, 'mse': 0.07723501324653625, 'train_time': 18.223102311138064, 'train_nmll': -0.31923726201057434, 'test_nmll': 0.11244859546422958, 'train_nll': -59.056640625, 'test_nll': 2.4830026626586914}\n",
    "2019-02-27 18:24:52.364618 - {'fold': 4, 'repeat': 7, 'mse': 0.07900796830654144, 'train_time': 18.038340855855495, 'train_nmll': -0.3318019509315491, 'test_nmll': 0.12183447182178497, 'train_nll': -58.89497375488281, 'test_nll': 2.679959297180176}\n",
    "2019-02-27 18:25:10.510746 - {'fold': 4, 'repeat': 8, 'mse': 0.07450731098651886, 'train_time': 18.14602127391845, 'train_nmll': -0.31643176078796387, 'test_nmll': 0.10555648803710938, 'train_nll': -57.270599365234375, 'test_nll': 1.772836685180664}\n",
    "2019-02-27 18:25:27.772499 - {'fold': 4, 'repeat': 9, 'mse': 0.09144625067710876, 'train_time': 17.261643741978332, 'train_nmll': -0.3159395754337311, 'test_nmll': 0.19917531311511993, 'train_nll': -57.89202880859375, 'test_nll': 3.7566328048706055}\n",
    "2019-02-27 18:25:45.691671 - {'fold': 5, 'repeat': 0, 'mse': 0.14937156438827515, 'train_time': 17.916039165109396, 'train_nmll': -0.2359265685081482, 'test_nmll': 0.34131336212158203, 'train_nll': -42.74237060546875, 'test_nll': 7.060683250427246}\n",
    "2019-02-27 18:26:03.876167 - {'fold': 5, 'repeat': 1, 'mse': 0.14970770478248596, 'train_time': 18.184389875037596, 'train_nmll': -0.23861075937747955, 'test_nmll': 0.3432798981666565, 'train_nll': -42.303466796875, 'test_nll': 6.688614845275879}\n",
    "2019-02-27 18:26:22.098564 - {'fold': 5, 'repeat': 2, 'mse': 0.15101319551467896, 'train_time': 18.22229140298441, 'train_nmll': -0.2335556000471115, 'test_nmll': 0.3489514887332916, 'train_nll': -41.864898681640625, 'test_nll': 7.081062316894531}\n",
    "2019-02-27 18:26:40.353545 - {'fold': 5, 'repeat': 3, 'mse': 0.1504264771938324, 'train_time': 18.254871569108218, 'train_nmll': -0.24253718554973602, 'test_nmll': 0.3594795763492584, 'train_nll': -42.07684326171875, 'test_nll': 7.307163238525391}\n",
    "2019-02-27 18:26:58.628392 - {'fold': 5, 'repeat': 4, 'mse': 0.14764076471328735, 'train_time': 18.2747361198999, 'train_nmll': -0.2222708761692047, 'test_nmll': 0.32825273275375366, 'train_nll': -41.858734130859375, 'test_nll': 6.865884780883789}\n",
    "2019-02-27 18:27:16.918633 - {'fold': 5, 'repeat': 5, 'mse': 0.15069450438022614, 'train_time': 18.2901240051724, 'train_nmll': -0.2447182536125183, 'test_nmll': 0.3386062979698181, 'train_nll': -42.751922607421875, 'test_nll': 6.565106391906738}\n",
    "2019-02-27 18:27:35.105505 - {'fold': 5, 'repeat': 6, 'mse': 0.15027645230293274, 'train_time': 18.18676426797174, 'train_nmll': -0.23199166357517242, 'test_nmll': 0.33426323533058167, 'train_nll': -41.69612121582031, 'test_nll': 6.913460731506348}\n",
    "2019-02-27 18:27:50.657527 - {'fold': 5, 'repeat': 7, 'mse': 0.14804817736148834, 'train_time': 15.551911474904045, 'train_nmll': -0.23907259106636047, 'test_nmll': 0.34354862570762634, 'train_nll': -42.075439453125, 'test_nll': 6.797041893005371}\n",
    "2019-02-27 18:28:04.754666 - {'fold': 5, 'repeat': 8, 'mse': 0.14946624636650085, 'train_time': 14.097027603071183, 'train_nmll': -0.24104046821594238, 'test_nmll': 0.3491881489753723, 'train_nll': -42.01475524902344, 'test_nll': 6.8922576904296875}\n",
    "2019-02-27 18:28:19.457065 - {'fold': 5, 'repeat': 9, 'mse': 0.15092256665229797, 'train_time': 14.702297115931287, 'train_nmll': -0.23815858364105225, 'test_nmll': 0.35303401947021484, 'train_nll': -42.765167236328125, 'test_nll': 6.919231414794922}\n",
    "2019-02-27 18:28:37.371577 - {'fold': 6, 'repeat': 0, 'mse': 0.12068501859903336, 'train_time': 17.911359908990562, 'train_nmll': -0.29917246103286743, 'test_nmll': 0.10735120624303818, 'train_nll': -51.86021423339844, 'test_nll': 1.9154472351074219}\n",
    "2019-02-27 18:28:55.632959 - {'fold': 6, 'repeat': 1, 'mse': 0.13953742384910583, 'train_time': 18.261273419950157, 'train_nmll': -0.36837640404701233, 'test_nmll': 0.19281630218029022, 'train_nll': -59.04396057128906, 'test_nll': 3.939436912536621}\n",
    "2019-02-27 18:29:13.896656 - {'fold': 6, 'repeat': 2, 'mse': 0.12254844605922699, 'train_time': 18.263588688103482, 'train_nmll': -0.3283643126487732, 'test_nmll': 0.09349622577428818, 'train_nll': -62.15473937988281, 'test_nll': 2.289091110229492}\n",
    "2019-02-27 18:29:32.345848 - {'fold': 6, 'repeat': 3, 'mse': 0.13072344660758972, 'train_time': 18.44908790709451, 'train_nmll': -0.3639377951622009, 'test_nmll': 0.17950811982154846, 'train_nll': -58.73896789550781, 'test_nll': 3.2430477142333984}\n",
    "2019-02-27 18:29:49.526263 - {'fold': 6, 'repeat': 4, 'mse': 0.13586106896400452, 'train_time': 17.18031847011298, 'train_nmll': -0.3707706034183502, 'test_nmll': 0.184502512216568, 'train_nll': -72.21722412109375, 'test_nll': 3.6274843215942383}\n",
    "2019-02-27 18:30:06.207829 - {'fold': 6, 'repeat': 5, 'mse': 0.1243552714586258, 'train_time': 16.68147629289888, 'train_nmll': -0.3088621497154236, 'test_nmll': 0.12628111243247986, 'train_nll': -53.31666564941406, 'test_nll': 2.199808120727539}\n",
    "2019-02-27 18:30:21.496770 - {'fold': 6, 'repeat': 6, 'mse': 0.12925125658512115, 'train_time': 15.288851825986058, 'train_nmll': -0.33676019310951233, 'test_nmll': 0.14344587922096252, 'train_nll': -59.41123962402344, 'test_nll': 2.4758548736572266}\n",
    "2019-02-27 18:30:38.705241 - {'fold': 6, 'repeat': 7, 'mse': 0.12151427567005157, 'train_time': 17.20836605411023, 'train_nmll': -0.3051258623600006, 'test_nmll': 0.11023473739624023, 'train_nll': -56.92771911621094, 'test_nll': 1.972787857055664}\n",
    "2019-02-27 18:30:56.924192 - {'fold': 6, 'repeat': 8, 'mse': 0.12382207810878754, 'train_time': 18.218832046957687, 'train_nmll': -0.2953052222728729, 'test_nmll': 0.12415709346532822, 'train_nll': -55.29254150390625, 'test_nll': 2.4248552322387695}\n",
    "2019-02-27 18:31:13.284610 - {'fold': 6, 'repeat': 9, 'mse': 0.1243102103471756, 'train_time': 16.360321642132476, 'train_nmll': -0.3115311563014984, 'test_nmll': 0.10461731255054474, 'train_nll': -54.33343505859375, 'test_nll': 2.040456771850586}\n",
    "2019-02-27 18:31:27.985402 - {'fold': 7, 'repeat': 0, 'mse': 0.4325105547904968, 'train_time': 14.698460306972265, 'train_nmll': -0.32341131567955017, 'test_nmll': 0.45436805486679077, 'train_nll': -55.999267578125, 'test_nll': 8.961737632751465}\n",
    "2019-02-27 18:31:46.273182 - {'fold': 7, 'repeat': 1, 'mse': 0.418281227350235, 'train_time': 18.287673061015084, 'train_nmll': -0.3163553774356842, 'test_nmll': 0.4427938461303711, 'train_nll': -57.802886962890625, 'test_nll': 8.54616641998291}\n",
    "2019-02-27 18:32:03.411046 - {'fold': 7, 'repeat': 2, 'mse': 0.42604732513427734, 'train_time': 17.13775462890044, 'train_nmll': -0.3148457705974579, 'test_nmll': 0.42977532744407654, 'train_nll': -55.556640625, 'test_nll': 8.402076721191406}\n",
    "2019-02-27 18:32:21.671796 - {'fold': 7, 'repeat': 3, 'mse': 0.4223395884037018, 'train_time': 18.260641156928614, 'train_nmll': -0.3231687843799591, 'test_nmll': 0.42918896675109863, 'train_nll': -56.6063232421875, 'test_nll': 8.802035331726074}\n",
    "2019-02-27 18:32:37.282508 - {'fold': 7, 'repeat': 4, 'mse': 0.4170282781124115, 'train_time': 15.610616330057383, 'train_nmll': -0.32495439052581787, 'test_nmll': 0.42543545365333557, 'train_nll': -57.512542724609375, 'test_nll': 8.687837600708008}\n",
    "2019-02-27 18:32:51.553626 - {'fold': 7, 'repeat': 5, 'mse': 0.43593916296958923, 'train_time': 14.271027064882219, 'train_nmll': -0.3383655250072479, 'test_nmll': 0.4166452884674072, 'train_nll': -62.72467041015625, 'test_nll': 8.520566940307617}\n",
    "2019-02-27 18:33:07.695486 - {'fold': 7, 'repeat': 6, 'mse': 0.4296094477176666, 'train_time': 16.14176002610475, 'train_nmll': -0.3348335325717926, 'test_nmll': 0.37096816301345825, 'train_nll': -59.04804992675781, 'test_nll': 7.610174179077148}\n",
    "2019-02-27 18:33:25.937205 - {'fold': 7, 'repeat': 7, 'mse': 0.43011680245399475, 'train_time': 18.241612574085593, 'train_nmll': -0.342119425535202, 'test_nmll': 0.404533326625824, 'train_nll': -58.72698974609375, 'test_nll': 7.887367248535156}\n",
    "2019-02-27 18:33:44.053112 - {'fold': 7, 'repeat': 8, 'mse': 0.42360058426856995, 'train_time': 18.115799690131098, 'train_nmll': -0.32772096991539, 'test_nmll': 0.3781060576438904, 'train_nll': -57.618682861328125, 'test_nll': 7.806077003479004}\n",
    "2019-02-27 18:34:02.253531 - {'fold': 7, 'repeat': 9, 'mse': 0.43700069189071655, 'train_time': 18.200312295928597, 'train_nmll': -0.3429202735424042, 'test_nmll': 0.3720884323120117, 'train_nll': -61.3040771484375, 'test_nll': 7.696181297302246}\n",
    "2019-02-27 18:34:20.273530 - {'fold': 8, 'repeat': 0, 'mse': 0.06592128425836563, 'train_time': 18.016983011038974, 'train_nmll': -0.3614695966243744, 'test_nmll': 0.03901243209838867, 'train_nll': -69.66181945800781, 'test_nll': 0.603912353515625}\n",
    "2019-02-27 18:34:36.160924 - {'fold': 8, 'repeat': 1, 'mse': 0.06254877150058746, 'train_time': 15.887287876103073, 'train_nmll': -0.5266926884651184, 'test_nmll': 0.024799251928925514, 'train_nll': -71.3802490234375, 'test_nll': 0.4683094024658203}\n",
    "2019-02-27 18:34:54.065629 - {'fold': 8, 'repeat': 2, 'mse': 0.05376816913485527, 'train_time': 17.90460748807527, 'train_nmll': -0.35221320390701294, 'test_nmll': -0.06891040503978729, 'train_nll': -64.32748413085938, 'test_nll': -1.2784957885742188}\n",
    "2019-02-27 18:35:09.854046 - {'fold': 8, 'repeat': 3, 'mse': 0.053352098912000656, 'train_time': 15.78831567405723, 'train_nmll': -0.3863074481487274, 'test_nmll': -0.06424150615930557, 'train_nll': -65.45083618164062, 'test_nll': -0.9634552001953125}\n",
    "2019-02-27 18:35:28.241761 - {'fold': 8, 'repeat': 4, 'mse': 0.052479278296232224, 'train_time': 18.387605712050572, 'train_nmll': -0.3430960774421692, 'test_nmll': -0.09386339038610458, 'train_nll': -67.76054382324219, 'test_nll': -1.4993858337402344}\n",
    "2019-02-27 18:35:46.518476 - {'fold': 8, 'repeat': 5, 'mse': 0.06778699159622192, 'train_time': 18.276605509920046, 'train_nmll': -0.39957165718078613, 'test_nmll': 0.05058012157678604, 'train_nll': -73.02555847167969, 'test_nll': 0.9563617706298828}\n",
    "2019-02-27 18:36:04.831461 - {'fold': 8, 'repeat': 6, 'mse': 0.05147678777575493, 'train_time': 18.31287638703361, 'train_nmll': -0.09581103920936584, 'test_nmll': -0.07746925204992294, 'train_nll': -33.42242431640625, 'test_nll': -1.729217529296875}\n",
    "2019-02-27 18:36:21.677077 - {'fold': 8, 'repeat': 7, 'mse': 0.058241426944732666, 'train_time': 16.84550837590359, 'train_nmll': -0.4309307634830475, 'test_nmll': -0.014904499053955078, 'train_nll': -72.79869079589844, 'test_nll': 0.09648704528808594}\n",
    "2019-02-27 18:36:36.652095 - {'fold': 8, 'repeat': 8, 'mse': 0.0545177236199379, 'train_time': 14.974919433007017, 'train_nmll': -0.36213403940200806, 'test_nmll': -0.06717748939990997, 'train_nll': -61.48161315917969, 'test_nll': -1.4332599639892578}\n",
    "2019-02-27 18:36:51.051855 - {'fold': 8, 'repeat': 9, 'mse': 0.06154962629079819, 'train_time': 14.399638823000714, 'train_nmll': -0.35029372572898865, 'test_nmll': 0.023374270647764206, 'train_nll': -70.22381591796875, 'test_nll': 0.10736656188964844}\n",
    "2019-02-27 18:37:10.033467 - {'fold': 9, 'repeat': 0, 'mse': 0.06856930255889893, 'train_time': 18.97771521192044, 'train_nmll': -0.2976055443286896, 'test_nmll': 0.17185993492603302, 'train_nll': -53.04209899902344, 'test_nll': 3.170011520385742}\n",
    "2019-02-27 18:37:24.489696 - {'fold': 9, 'repeat': 1, 'mse': 0.06839479506015778, 'train_time': 14.45613363897428, 'train_nmll': -0.29030948877334595, 'test_nmll': 0.15908809006214142, 'train_nll': -52.35459899902344, 'test_nll': 3.0557374954223633}\n",
    "2019-02-27 18:37:42.271740 - {'fold': 9, 'repeat': 2, 'mse': 0.07007448375225067, 'train_time': 17.781946044880897, 'train_nmll': -0.2761436402797699, 'test_nmll': 0.14913639426231384, 'train_nll': -49.216400146484375, 'test_nll': 3.2513933181762695}\n",
    "2019-02-27 18:38:00.540401 - {'fold': 9, 'repeat': 3, 'mse': 0.06634604185819626, 'train_time': 18.26855525118299, 'train_nmll': -0.27626833319664, 'test_nmll': 0.13522124290466309, 'train_nll': -50.59346008300781, 'test_nll': 2.487208366394043}\n",
    "2019-02-27 18:38:17.912627 - {'fold': 9, 'repeat': 4, 'mse': 0.06903714686632156, 'train_time': 17.37212949199602, 'train_nmll': -0.29232779145240784, 'test_nmll': 0.1609487533569336, 'train_nll': -50.501556396484375, 'test_nll': 3.2997541427612305}\n",
    "2019-02-27 18:38:32.096610 - {'fold': 9, 'repeat': 5, 'mse': 0.06877510249614716, 'train_time': 14.183892650995404, 'train_nmll': -0.282380074262619, 'test_nmll': 0.1466623842716217, 'train_nll': -49.81803894042969, 'test_nll': 3.302095413208008}\n",
    "2019-02-27 18:38:46.690428 - {'fold': 9, 'repeat': 6, 'mse': 0.06895970553159714, 'train_time': 14.593729065032676, 'train_nmll': -0.2775079607963562, 'test_nmll': 0.13829346001148224, 'train_nll': -48.41960144042969, 'test_nll': 2.267496109008789}\n",
    "2019-02-27 18:39:00.899933 - {'fold': 9, 'repeat': 7, 'mse': 0.06874920427799225, 'train_time': 14.209416113095358, 'train_nmll': -0.27769502997398376, 'test_nmll': 0.14425449073314667, 'train_nll': -50.59661865234375, 'test_nll': 2.843547821044922}\n",
    "2019-02-27 18:39:15.222928 - {'fold': 9, 'repeat': 8, 'mse': 0.06575300544500351, 'train_time': 14.322897556005046, 'train_nmll': -0.2766028642654419, 'test_nmll': 0.11983761936426163, 'train_nll': -50.360076904296875, 'test_nll': 2.7878494262695312}\n",
    "2019-02-27 18:39:30.746672 - {'fold': 9, 'repeat': 9, 'mse': 0.06882442533969879, 'train_time': 15.523637135978788, 'train_nmll': -0.2836567461490631, 'test_nmll': 0.13978004455566406, 'train_nll': -50.601043701171875, 'test_nll': 2.8272647857666016}\n",
    "2019-02-27 18:40:48.267454 - {'fold': 0, 'repeat': 0, 'mse': 955.347900390625, 'train_time': 77.5040388489142, 'train_nmll': 57.858680725097656, 'test_nmll': 78.23240661621094, 'train_nll': 53634.703125, 'test_nll': 8058.0556640625}\n",
    "2019-02-27 18:42:04.898932 - {'fold': 0, 'repeat': 1, 'mse': 955.3480224609375, 'train_time': 76.63138629798777, 'train_nmll': 57.858489990234375, 'test_nmll': 78.23225402832031, 'train_nll': 53635.328125, 'test_nll': 8058.02783203125}\n",
    "2019-02-27 18:43:23.303529 - {'fold': 0, 'repeat': 2, 'mse': 955.3475341796875, 'train_time': 78.40450643585064, 'train_nmll': 57.85902404785156, 'test_nmll': 78.23320770263672, 'train_nll': 53634.55859375, 'test_nll': 8058.03466796875}\n",
    "2019-02-27 18:44:41.947175 - {'fold': 0, 'repeat': 3, 'mse': 955.34716796875, 'train_time': 78.64355385210365, 'train_nmll': 57.85844802856445, 'test_nmll': 78.2335433959961, 'train_nll': 53635.72265625, 'test_nll': 8058.03369140625}\n",
    "2019-02-27 18:46:01.539384 - {'fold': 0, 'repeat': 4, 'mse': 955.347900390625, 'train_time': 79.59211578918621, 'train_nmll': 57.85921096801758, 'test_nmll': 78.23342895507812, 'train_nll': 53634.91796875, 'test_nll': 8057.95263671875}\n",
    "2019-02-27 18:47:15.338883 - {'fold': 0, 'repeat': 5, 'mse': 955.347900390625, 'train_time': 73.79940489307046, 'train_nmll': 57.8598518371582, 'test_nmll': 78.23292541503906, 'train_nll': 53635.65625, 'test_nll': 8057.990234375}\n",
    "2019-02-27 18:48:33.788513 - {'fold': 0, 'repeat': 6, 'mse': 955.3478393554688, 'train_time': 78.44953873101622, 'train_nmll': 57.85847854614258, 'test_nmll': 78.23289489746094, 'train_nll': 53635.1015625, 'test_nll': 8057.9716796875}\n",
    "2019-02-27 18:49:49.546781 - {'fold': 0, 'repeat': 7, 'mse': 955.3477172851562, 'train_time': 75.75817862688564, 'train_nmll': 57.8590087890625, 'test_nmll': 78.233642578125, 'train_nll': 53635.44921875, 'test_nll': 8057.98779296875}\n",
    "2019-02-27 18:51:05.910085 - {'fold': 0, 'repeat': 8, 'mse': 955.3478393554688, 'train_time': 76.36321052908897, 'train_nmll': 57.85906219482422, 'test_nmll': 78.23321533203125, 'train_nll': 53635.5234375, 'test_nll': 8058.0146484375}\n",
    "2019-02-27 18:52:24.714196 - {'fold': 0, 'repeat': 9, 'mse': 955.3482055664062, 'train_time': 78.80399611289613, 'train_nmll': 57.85945129394531, 'test_nmll': 78.2335433959961, 'train_nll': 53635.3984375, 'test_nll': 8057.9736328125}\n",
    "2019-02-27 18:53:43.652251 - {'fold': 1, 'repeat': 0, 'mse': 996.5831298828125, 'train_time': 78.93369461479597, 'train_nmll': 57.25476837158203, 'test_nmll': 81.39676666259766, 'train_nll': 53075.68359375, 'test_nll': 8383.8916015625}\n",
    "2019-02-27 18:54:59.361740 - {'fold': 1, 'repeat': 1, 'mse': 996.5830688476562, 'train_time': 75.70939715998247, 'train_nmll': 57.2547492980957, 'test_nmll': 81.39653778076172, 'train_nll': 53075.13671875, 'test_nll': 8383.892578125}\n",
    "2019-02-27 18:56:15.463790 - {'fold': 1, 'repeat': 2, 'mse': 996.5825805664062, 'train_time': 76.10195948393084, 'train_nmll': 57.25442123413086, 'test_nmll': 81.39745330810547, 'train_nll': 53074.5625, 'test_nll': 8383.8642578125}\n",
    "2019-02-27 18:57:29.894421 - {'fold': 1, 'repeat': 3, 'mse': 996.5828247070312, 'train_time': 74.43053819891065, 'train_nmll': 57.25457763671875, 'test_nmll': 81.39700317382812, 'train_nll': 53074.20703125, 'test_nll': 8383.8671875}\n",
    "2019-02-27 18:58:47.950939 - {'fold': 1, 'repeat': 4, 'mse': 996.5830688476562, 'train_time': 78.0564277980011, 'train_nmll': 57.254783630371094, 'test_nmll': 81.39700317382812, 'train_nll': 53074.6328125, 'test_nll': 8383.8427734375}\n",
    "2019-02-27 19:00:04.416139 - {'fold': 1, 'repeat': 5, 'mse': 996.5831298828125, 'train_time': 76.46511019882746, 'train_nmll': 57.2540397644043, 'test_nmll': 81.39651489257812, 'train_nll': 53075.66015625, 'test_nll': 8383.865234375}\n",
    "2019-02-27 19:01:19.999315 - {'fold': 1, 'repeat': 6, 'mse': 996.5830688476562, 'train_time': 75.58308320399374, 'train_nmll': 57.255008697509766, 'test_nmll': 81.39674377441406, 'train_nll': 53075.3046875, 'test_nll': 8383.83984375}\n",
    "2019-02-27 19:02:35.561620 - {'fold': 1, 'repeat': 7, 'mse': 996.5830078125, 'train_time': 75.56221412913874, 'train_nmll': 57.254459381103516, 'test_nmll': 81.39649963378906, 'train_nll': 53075.6171875, 'test_nll': 8383.888671875}\n",
    "2019-02-27 19:03:55.059091 - {'fold': 1, 'repeat': 8, 'mse': 996.5828247070312, 'train_time': 79.4973797108978, 'train_nmll': 57.25463104248047, 'test_nmll': 81.39698791503906, 'train_nll': 53076.02734375, 'test_nll': 8383.7919921875}\n",
    "2019-02-27 19:05:13.153205 - {'fold': 1, 'repeat': 9, 'mse': 996.5825805664062, 'train_time': 78.09402396786027, 'train_nmll': 57.25555419921875, 'test_nmll': 81.39700317382812, 'train_nll': 53074.69921875, 'test_nll': 8383.8916015625}\n",
    "2019-02-27 19:06:24.507883 - {'fold': 2, 'repeat': 0, 'mse': 836.6710205078125, 'train_time': 71.35091961617582, 'train_nmll': 57.640262603759766, 'test_nmll': 68.60418701171875, 'train_nll': 53432.3046875, 'test_nll': 7066.22265625}\n",
    "2019-02-27 19:07:40.214680 - {'fold': 2, 'repeat': 1, 'mse': 836.6709594726562, 'train_time': 75.70670470991172, 'train_nmll': 57.63982009887695, 'test_nmll': 68.60419464111328, 'train_nll': 53432.59765625, 'test_nll': 7066.23193359375}\n",
    "2019-02-27 19:09:01.055387 - {'fold': 2, 'repeat': 2, 'mse': 836.6712646484375, 'train_time': 80.84061586600728, 'train_nmll': 57.639976501464844, 'test_nmll': 68.60424041748047, 'train_nll': 53431.55859375, 'test_nll': 7066.2451171875}\n",
    "2019-02-27 19:10:12.742320 - {'fold': 2, 'repeat': 3, 'mse': 836.6715087890625, 'train_time': 71.68684079009108, 'train_nmll': 57.64065933227539, 'test_nmll': 68.6043472290039, 'train_nll': 53432.5390625, 'test_nll': 7066.2314453125}\n",
    "2019-02-27 19:11:24.860730 - {'fold': 2, 'repeat': 4, 'mse': 836.6708984375, 'train_time': 72.11832011397928, 'train_nmll': 57.6400146484375, 'test_nmll': 68.6042251586914, 'train_nll': 53431.28125, 'test_nll': 7066.2314453125}\n",
    "2019-02-27 19:12:41.800797 - {'fold': 2, 'repeat': 5, 'mse': 836.6712646484375, 'train_time': 76.93997471919283, 'train_nmll': 57.64009094238281, 'test_nmll': 68.60432434082031, 'train_nll': 53432.99609375, 'test_nll': 7066.23486328125}\n",
    "2019-02-27 19:14:00.196701 - {'fold': 2, 'repeat': 6, 'mse': 836.6707153320312, 'train_time': 78.39581456501037, 'train_nmll': 57.640228271484375, 'test_nmll': 68.60418701171875, 'train_nll': 53433.02734375, 'test_nll': 7066.23876953125}\n",
    "2019-02-27 19:15:20.678808 - {'fold': 2, 'repeat': 7, 'mse': 836.6712036132812, 'train_time': 80.48201340716332, 'train_nmll': 57.63945388793945, 'test_nmll': 68.60418701171875, 'train_nll': 53430.96875, 'test_nll': 7066.2353515625}\n",
    "2019-02-27 19:16:38.951943 - {'fold': 2, 'repeat': 8, 'mse': 836.6715698242188, 'train_time': 78.27304436010309, 'train_nmll': 57.639747619628906, 'test_nmll': 68.604248046875, 'train_nll': 53433.0078125, 'test_nll': 7066.24560546875}\n",
    "2019-02-27 19:17:59.075769 - {'fold': 2, 'repeat': 9, 'mse': 836.6712646484375, 'train_time': 80.12373383599333, 'train_nmll': 57.6396484375, 'test_nmll': 68.60432434082031, 'train_nll': 53432.234375, 'test_nll': 7066.2490234375}\n",
    "2019-02-27 19:19:16.097277 - {'fold': 3, 'repeat': 0, 'mse': 801.2163696289062, 'train_time': 77.01782992784865, 'train_nmll': 58.30778121948242, 'test_nmll': 65.46955871582031, 'train_nll': 54052.109375, 'test_nll': 6743.298828125}\n",
    "2019-02-27 19:20:36.212861 - {'fold': 3, 'repeat': 1, 'mse': 801.2161254882812, 'train_time': 80.11549296299927, 'train_nmll': 58.30891036987305, 'test_nmll': 65.47174835205078, 'train_nll': 54051.86328125, 'test_nll': 6743.40673828125}\n",
    "2019-02-27 19:21:56.732195 - {'fold': 3, 'repeat': 2, 'mse': 801.21533203125, 'train_time': 80.51924285595305, 'train_nmll': 58.30879592895508, 'test_nmll': 65.46936798095703, 'train_nll': 54050.734375, 'test_nll': 6743.30810546875}\n",
    "2019-02-27 19:23:13.422224 - {'fold': 3, 'repeat': 3, 'mse': 801.2166137695312, 'train_time': 76.68993655289523, 'train_nmll': 58.30927658081055, 'test_nmll': 65.46927642822266, 'train_nll': 54051.59765625, 'test_nll': 6743.44873046875}\n",
    "2019-02-27 19:24:30.641804 - {'fold': 3, 'repeat': 4, 'mse': 801.216552734375, 'train_time': 77.21948768408038, 'train_nmll': 58.30817794799805, 'test_nmll': 65.47022247314453, 'train_nll': 54051.2109375, 'test_nll': 6743.41455078125}\n",
    "2019-02-27 19:25:47.843149 - {'fold': 3, 'repeat': 5, 'mse': 801.216796875, 'train_time': 77.2012539759744, 'train_nmll': 58.30805969238281, 'test_nmll': 65.46926879882812, 'train_nll': 54051.9765625, 'test_nll': 6743.4375}\n",
    "2019-02-27 19:27:03.357504 - {'fold': 3, 'repeat': 6, 'mse': 801.2162475585938, 'train_time': 75.51426301384345, 'train_nmll': 58.307395935058594, 'test_nmll': 65.46932983398438, 'train_nll': 54052.27734375, 'test_nll': 6743.4912109375}\n",
    "2019-02-27 19:28:20.558031 - {'fold': 3, 'repeat': 7, 'mse': 801.2166748046875, 'train_time': 77.20040875999257, 'train_nmll': 58.30923843383789, 'test_nmll': 65.46942901611328, 'train_nll': 54051.7890625, 'test_nll': 6743.42529296875}\n",
    "2019-02-27 19:29:40.845185 - {'fold': 3, 'repeat': 8, 'mse': 801.2162475585938, 'train_time': 80.28706184611656, 'train_nmll': 58.308258056640625, 'test_nmll': 65.47180938720703, 'train_nll': 54051.5390625, 'test_nll': 6743.4892578125}\n",
    "2019-02-27 19:30:58.844183 - {'fold': 3, 'repeat': 9, 'mse': 801.2158813476562, 'train_time': 77.99890513997525, 'train_nmll': 58.309024810791016, 'test_nmll': 65.47032165527344, 'train_nll': 54051.48046875, 'test_nll': 6743.349609375}\n",
    "2019-02-27 19:32:15.208807 - {'fold': 4, 'repeat': 0, 'mse': 1112.0430908203125, 'train_time': 76.36086719995365, 'train_nmll': 56.342308044433594, 'test_nmll': 90.04755401611328, 'train_nll': 52227.84375, 'test_nll': 9274.8974609375}\n",
    "2019-02-27 19:33:29.398033 - {'fold': 4, 'repeat': 1, 'mse': 1112.043212890625, 'train_time': 74.18911065906286, 'train_nmll': 56.34121322631836, 'test_nmll': 90.04740905761719, 'train_nll': 52228.52734375, 'test_nll': 9274.8935546875}\n",
    "2019-02-27 19:34:43.886712 - {'fold': 4, 'repeat': 2, 'mse': 1112.04345703125, 'train_time': 74.48857585713267, 'train_nmll': 56.341732025146484, 'test_nmll': 90.04828643798828, 'train_nll': 52229.36328125, 'test_nll': 9274.90234375}\n",
    "2019-02-27 19:36:01.264256 - {'fold': 4, 'repeat': 3, 'mse': 1112.0430908203125, 'train_time': 77.37745440285653, 'train_nmll': 56.34172439575195, 'test_nmll': 90.0466537475586, 'train_nll': 52229.25390625, 'test_nll': 9274.8662109375}\n",
    "2019-02-27 19:37:21.476347 - {'fold': 4, 'repeat': 4, 'mse': 1112.042724609375, 'train_time': 80.21200065803714, 'train_nmll': 56.342437744140625, 'test_nmll': 90.04730224609375, 'train_nll': 52228.65234375, 'test_nll': 9274.9052734375}\n",
    "2019-02-27 19:38:33.606601 - {'fold': 4, 'repeat': 5, 'mse': 1112.0430908203125, 'train_time': 72.13015672005713, 'train_nmll': 56.341026306152344, 'test_nmll': 90.0471420288086, 'train_nll': 52229.55078125, 'test_nll': 9274.984375}\n",
    "2019-02-27 19:39:52.740856 - {'fold': 4, 'repeat': 6, 'mse': 1112.04296875, 'train_time': 79.13415570906363, 'train_nmll': 56.341739654541016, 'test_nmll': 90.04597473144531, 'train_nll': 52229.15625, 'test_nll': 9274.888671875}\n",
    "2019-02-27 19:41:10.773246 - {'fold': 4, 'repeat': 7, 'mse': 1112.042724609375, 'train_time': 78.03229585988447, 'train_nmll': 56.34165954589844, 'test_nmll': 90.0476303100586, 'train_nll': 52228.08984375, 'test_nll': 9274.9453125}\n",
    "2019-02-27 19:42:28.784295 - {'fold': 4, 'repeat': 8, 'mse': 1112.043212890625, 'train_time': 78.01095718005672, 'train_nmll': 56.341896057128906, 'test_nmll': 90.04664611816406, 'train_nll': 52229.21875, 'test_nll': 9274.8623046875}\n",
    "2019-02-27 19:43:48.069203 - {'fold': 4, 'repeat': 9, 'mse': 1112.0433349609375, 'train_time': 79.28481751889922, 'train_nmll': 56.34219741821289, 'test_nmll': 90.04706573486328, 'train_nll': 52228.0078125, 'test_nll': 9274.8193359375}\n",
    "2019-02-27 19:45:07.999830 - {'fold': 5, 'repeat': 0, 'mse': 1143.4163818359375, 'train_time': 79.92688682489097, 'train_nmll': 56.31112289428711, 'test_nmll': 92.96473693847656, 'train_nll': 52200.15625, 'test_nll': 9575.314453125}\n",
    "2019-02-27 19:46:27.847447 - {'fold': 5, 'repeat': 1, 'mse': 1143.414794921875, 'train_time': 79.84752620989457, 'train_nmll': 56.310489654541016, 'test_nmll': 92.96276092529297, 'train_nll': 52200.66796875, 'test_nll': 9575.2646484375}\n",
    "2019-02-27 19:47:48.476326 - {'fold': 5, 'repeat': 2, 'mse': 1143.4151611328125, 'train_time': 80.62878756201826, 'train_nmll': 56.310489654541016, 'test_nmll': 92.96379089355469, 'train_nll': 52199.80078125, 'test_nll': 9575.181640625}\n",
    "2019-02-27 19:49:06.385238 - {'fold': 5, 'repeat': 3, 'mse': 1143.4146728515625, 'train_time': 77.90882164100185, 'train_nmll': 56.31103515625, 'test_nmll': 92.96394348144531, 'train_nll': 52198.921875, 'test_nll': 9575.341796875}\n",
    "2019-02-27 19:50:24.363727 - {'fold': 5, 'repeat': 4, 'mse': 1143.41552734375, 'train_time': 77.97839832119644, 'train_nmll': 56.3112907409668, 'test_nmll': 92.96344757080078, 'train_nll': 52200.65625, 'test_nll': 9575.216796875}\n",
    "2019-02-27 19:51:42.768772 - {'fold': 5, 'repeat': 5, 'mse': 1143.4149169921875, 'train_time': 78.40495422505774, 'train_nmll': 56.310569763183594, 'test_nmll': 92.96282196044922, 'train_nll': 52199.8515625, 'test_nll': 9575.13671875}\n",
    "2019-02-27 19:53:00.247607 - {'fold': 5, 'repeat': 6, 'mse': 1143.41552734375, 'train_time': 77.47874333197251, 'train_nmll': 56.310935974121094, 'test_nmll': 92.964599609375, 'train_nll': 52200.3828125, 'test_nll': 9575.3095703125}\n",
    "2019-02-27 19:54:17.295160 - {'fold': 5, 'repeat': 7, 'mse': 1143.414306640625, 'train_time': 77.04745817696676, 'train_nmll': 56.310428619384766, 'test_nmll': 92.963623046875, 'train_nll': 52200.234375, 'test_nll': 9575.0263671875}\n",
    "2019-02-27 19:55:34.138472 - {'fold': 5, 'repeat': 8, 'mse': 1143.4149169921875, 'train_time': 76.84321917500347, 'train_nmll': 56.31067657470703, 'test_nmll': 92.96343994140625, 'train_nll': 52199.56640625, 'test_nll': 9575.251953125}\n",
    "2019-02-27 19:56:48.754310 - {'fold': 5, 'repeat': 9, 'mse': 1143.4151611328125, 'train_time': 74.61574654700235, 'train_nmll': 56.31049346923828, 'test_nmll': 92.96297454833984, 'train_nll': 52199.5234375, 'test_nll': 9575.439453125}\n",
    "2019-02-27 19:58:06.838250 - {'fold': 6, 'repeat': 0, 'mse': 1076.443603515625, 'train_time': 78.08021862991154, 'train_nmll': 56.576393127441406, 'test_nmll': 87.30143737792969, 'train_nll': 52446.28125, 'test_nll': 8992.0322265625}\n",
    "2019-02-27 19:59:26.751210 - {'fold': 6, 'repeat': 1, 'mse': 1076.4437255859375, 'train_time': 79.91287007718347, 'train_nmll': 56.57730484008789, 'test_nmll': 87.30087280273438, 'train_nll': 52447.171875, 'test_nll': 8991.93359375}\n",
    "2019-02-27 20:00:45.696012 - {'fold': 6, 'repeat': 2, 'mse': 1076.4437255859375, 'train_time': 78.94470908283256, 'train_nmll': 56.57636642456055, 'test_nmll': 87.30104064941406, 'train_nll': 52446.24609375, 'test_nll': 8991.9765625}\n",
    "2019-02-27 20:02:05.689914 - {'fold': 6, 'repeat': 3, 'mse': 1076.443603515625, 'train_time': 79.9938131829258, 'train_nmll': 56.577239990234375, 'test_nmll': 87.30241394042969, 'train_nll': 52446.5625, 'test_nll': 8992.0146484375}\n",
    "2019-02-27 20:03:24.864332 - {'fold': 6, 'repeat': 4, 'mse': 1076.4443359375, 'train_time': 79.17431843583472, 'train_nmll': 56.57636260986328, 'test_nmll': 87.30217742919922, 'train_nll': 52446.12890625, 'test_nll': 8991.9560546875}\n",
    "2019-02-27 20:04:38.472116 - {'fold': 6, 'repeat': 5, 'mse': 1076.44384765625, 'train_time': 73.60769183980301, 'train_nmll': 56.57624435424805, 'test_nmll': 87.30144500732422, 'train_nll': 52446.328125, 'test_nll': 8992.080078125}\n",
    "2019-02-27 20:05:51.756956 - {'fold': 6, 'repeat': 6, 'mse': 1076.4439697265625, 'train_time': 73.2847502140794, 'train_nmll': 56.576385498046875, 'test_nmll': 87.3012924194336, 'train_nll': 52446.48046875, 'test_nll': 8991.994140625}\n",
    "2019-02-27 20:07:09.580585 - {'fold': 6, 'repeat': 7, 'mse': 1076.4443359375, 'train_time': 77.82353972294368, 'train_nmll': 56.5754508972168, 'test_nmll': 87.30113983154297, 'train_nll': 52446.61328125, 'test_nll': 8991.8740234375}\n",
    "2019-02-27 20:08:23.339083 - {'fold': 6, 'repeat': 8, 'mse': 1076.4439697265625, 'train_time': 73.75840791501105, 'train_nmll': 56.57632064819336, 'test_nmll': 87.3009262084961, 'train_nll': 52446.41015625, 'test_nll': 8992.140625}\n",
    "2019-02-27 20:09:37.183647 - {'fold': 6, 'repeat': 9, 'mse': 1076.4437255859375, 'train_time': 73.84447392402217, 'train_nmll': 56.576698303222656, 'test_nmll': 87.30206298828125, 'train_nll': 52446.60546875, 'test_nll': 8991.9892578125}\n",
    "2019-02-27 20:10:52.289928 - {'fold': 7, 'repeat': 0, 'mse': 861.9149169921875, 'train_time': 75.10262310411781, 'train_nmll': 57.792694091796875, 'test_nmll': 70.45018005371094, 'train_nll': 53574.8359375, 'test_nll': 7256.25}\n",
    "2019-02-27 20:12:08.534264 - {'fold': 7, 'repeat': 1, 'mse': 861.9158325195312, 'train_time': 76.24423654191196, 'train_nmll': 57.79404067993164, 'test_nmll': 70.4505386352539, 'train_nll': 53574.0625, 'test_nll': 7256.30126953125}\n",
    "2019-02-27 20:13:21.757458 - {'fold': 7, 'repeat': 2, 'mse': 861.9147338867188, 'train_time': 73.22310092300177, 'train_nmll': 57.793739318847656, 'test_nmll': 70.45030212402344, 'train_nll': 53574.3046875, 'test_nll': 7256.4296875}\n",
    "2019-02-27 20:14:37.596439 - {'fold': 7, 'repeat': 3, 'mse': 861.9152221679688, 'train_time': 75.8388737840578, 'train_nmll': 57.79219436645508, 'test_nmll': 70.45036315917969, 'train_nll': 53574.69921875, 'test_nll': 7256.34716796875}\n",
    "2019-02-27 20:15:54.801243 - {'fold': 7, 'repeat': 4, 'mse': 861.91552734375, 'train_time': 77.20469796890393, 'train_nmll': 57.79330062866211, 'test_nmll': 70.45094299316406, 'train_nll': 53574.28125, 'test_nll': 7256.34326171875}\n",
    "2019-02-27 20:17:13.040665 - {'fold': 7, 'repeat': 5, 'mse': 861.9146728515625, 'train_time': 78.23933063284494, 'train_nmll': 57.79370880126953, 'test_nmll': 70.45001983642578, 'train_nll': 53575.08984375, 'test_nll': 7256.42041015625}\n",
    "2019-02-27 20:18:31.169150 - {'fold': 7, 'repeat': 6, 'mse': 861.9152221679688, 'train_time': 78.1283942181617, 'train_nmll': 57.79331970214844, 'test_nmll': 70.44951629638672, 'train_nll': 53574.46875, 'test_nll': 7256.4296875}\n",
    "2019-02-27 20:19:49.773038 - {'fold': 7, 'repeat': 7, 'mse': 861.9152221679688, 'train_time': 78.60379610513337, 'train_nmll': 57.795143127441406, 'test_nmll': 70.45001983642578, 'train_nll': 53574.19140625, 'test_nll': 7256.31884765625}\n",
    "2019-02-27 20:21:00.724620 - {'fold': 7, 'repeat': 8, 'mse': 861.9149169921875, 'train_time': 70.95149029511958, 'train_nmll': 57.793052673339844, 'test_nmll': 70.45026397705078, 'train_nll': 53574.2421875, 'test_nll': 7256.45068359375}\n",
    "2019-02-27 20:22:15.678474 - {'fold': 7, 'repeat': 9, 'mse': 861.9149780273438, 'train_time': 74.95376289286651, 'train_nmll': 57.79324722290039, 'test_nmll': 70.4501953125, 'train_nll': 53574.125, 'test_nll': 7256.33984375}\n",
    "2019-02-27 20:23:34.988843 - {'fold': 8, 'repeat': 0, 'mse': 1014.2457275390625, 'train_time': 79.30668984004296, 'train_nmll': 57.032752990722656, 'test_nmll': 82.43026733398438, 'train_nll': 52869.84765625, 'test_nll': 8490.375}\n",
    "2019-02-27 20:24:53.175245 - {'fold': 8, 'repeat': 1, 'mse': 1014.2457275390625, 'train_time': 78.18631092994474, 'train_nmll': 57.032466888427734, 'test_nmll': 82.42975616455078, 'train_nll': 52869.8359375, 'test_nll': 8490.46484375}\n",
    "2019-02-27 20:26:03.818351 - {'fold': 8, 'repeat': 2, 'mse': 1014.2465209960938, 'train_time': 70.64300468191504, 'train_nmll': 57.033756256103516, 'test_nmll': 82.42973327636719, 'train_nll': 52869.68359375, 'test_nll': 8490.3447265625}\n",
    "2019-02-27 20:27:23.045036 - {'fold': 8, 'repeat': 3, 'mse': 1014.2454223632812, 'train_time': 79.22658875002526, 'train_nmll': 57.033409118652344, 'test_nmll': 82.429931640625, 'train_nll': 52870.0390625, 'test_nll': 8490.3056640625}\n",
    "2019-02-27 20:28:39.057930 - {'fold': 8, 'repeat': 4, 'mse': 1014.2457275390625, 'train_time': 76.01280298898928, 'train_nmll': 57.03447341918945, 'test_nmll': 82.43133544921875, 'train_nll': 52870.4921875, 'test_nll': 8490.4208984375}\n",
    "2019-02-27 20:29:56.732718 - {'fold': 8, 'repeat': 5, 'mse': 1014.2457275390625, 'train_time': 77.67469316814095, 'train_nmll': 57.033782958984375, 'test_nmll': 82.43062591552734, 'train_nll': 52870.921875, 'test_nll': 8490.3974609375}\n",
    "2019-02-27 20:31:15.070578 - {'fold': 8, 'repeat': 6, 'mse': 1014.24560546875, 'train_time': 78.33776825107634, 'train_nmll': 57.03297805786133, 'test_nmll': 82.43031311035156, 'train_nll': 52870.21484375, 'test_nll': 8490.416015625}\n",
    "2019-02-27 20:32:32.271631 - {'fold': 8, 'repeat': 7, 'mse': 1014.2462158203125, 'train_time': 77.20095959398896, 'train_nmll': 57.03321075439453, 'test_nmll': 82.42992401123047, 'train_nll': 52869.375, 'test_nll': 8490.39453125}\n",
    "2019-02-27 20:33:51.595619 - {'fold': 8, 'repeat': 8, 'mse': 1014.2452392578125, 'train_time': 79.32389029301703, 'train_nmll': 57.0327033996582, 'test_nmll': 82.42992401123047, 'train_nll': 52869.51171875, 'test_nll': 8490.33203125}\n",
    "2019-02-27 20:35:11.334120 - {'fold': 8, 'repeat': 9, 'mse': 1014.2452392578125, 'train_time': 79.73841100605205, 'train_nmll': 57.034000396728516, 'test_nmll': 82.43084716796875, 'train_nll': 52869.58984375, 'test_nll': 8490.392578125}\n",
    "2019-02-27 20:36:29.560004 - {'fold': 9, 'repeat': 0, 'mse': 946.1050415039062, 'train_time': 78.22230255510658, 'train_nmll': 57.60898208618164, 'test_nmll': 77.38915252685547, 'train_nll': 53404.48046875, 'test_nll': 7971.00732421875}\n",
    "2019-02-27 20:37:46.359105 - {'fold': 9, 'repeat': 1, 'mse': 946.1052856445312, 'train_time': 76.79900997318327, 'train_nmll': 57.60960006713867, 'test_nmll': 77.3890151977539, 'train_nll': 53404.1015625, 'test_nll': 7971.06005859375}\n",
    "2019-02-27 20:39:02.725329 - {'fold': 9, 'repeat': 2, 'mse': 946.10498046875, 'train_time': 76.36613265913911, 'train_nmll': 57.6096076965332, 'test_nmll': 77.38870239257812, 'train_nll': 53403.72265625, 'test_nll': 7971.04541015625}\n",
    "2019-02-27 20:40:20.258917 - {'fold': 9, 'repeat': 3, 'mse': 946.1046752929688, 'train_time': 77.53349650721066, 'train_nmll': 57.60908508300781, 'test_nmll': 77.38983154296875, 'train_nll': 53403.44140625, 'test_nll': 7970.99169921875}\n",
    "2019-02-27 20:41:38.525679 - {'fold': 9, 'repeat': 4, 'mse': 946.1052856445312, 'train_time': 78.26667246990837, 'train_nmll': 57.609954833984375, 'test_nmll': 77.38922882080078, 'train_nll': 53404.09765625, 'test_nll': 7971.03125}\n",
    "2019-02-27 20:42:55.563855 - {'fold': 9, 'repeat': 5, 'mse': 946.1046142578125, 'train_time': 77.03808204620145, 'train_nmll': 57.60902404785156, 'test_nmll': 77.38835906982422, 'train_nll': 53403.36328125, 'test_nll': 7971.1279296875}\n",
    "2019-02-27 20:44:10.796077 - {'fold': 9, 'repeat': 6, 'mse': 946.1052856445312, 'train_time': 75.23212889302522, 'train_nmll': 57.608848571777344, 'test_nmll': 77.38834381103516, 'train_nll': 53403.625, 'test_nll': 7971.16748046875}\n",
    "2019-02-27 20:45:26.032453 - {'fold': 9, 'repeat': 7, 'mse': 946.105224609375, 'train_time': 75.23628263897263, 'train_nmll': 57.609737396240234, 'test_nmll': 77.38851165771484, 'train_nll': 53404.1640625, 'test_nll': 7971.01513671875}\n",
    "2019-02-27 20:46:43.283794 - {'fold': 9, 'repeat': 8, 'mse': 946.1047973632812, 'train_time': 77.25124897621572, 'train_nmll': 57.60977554321289, 'test_nmll': 77.38829040527344, 'train_nll': 53404.07421875, 'test_nll': 7971.17431640625}\n",
    "2019-02-27 20:48:01.965223 - {'fold': 9, 'repeat': 9, 'mse': 946.1044921875, 'train_time': 78.68133692303672, 'train_nmll': 57.609230041503906, 'test_nmll': 77.38883209228516, 'train_nll': 53403.66015625, 'test_nll': 7971.1123046875}\n",
    "2019-02-27 20:48:47.154138 - {'fold': 0, 'repeat': 0, 'mse': 203.98956298828125, 'train_time': 45.176552242133766, 'train_nmll': 11.517621994018555, 'test_nmll': 21.16952133178711, 'train_nll': 5239.9013671875, 'test_nll': 1079.64697265625}\n",
    "2019-02-27 20:49:32.362124 - {'fold': 0, 'repeat': 1, 'mse': 203.98902893066406, 'train_time': 45.2078984209802, 'train_nmll': 11.518874168395996, 'test_nmll': 21.16989517211914, 'train_nll': 5241.13427734375, 'test_nll': 1079.6119384765625}\n",
    "2019-02-27 20:50:18.073746 - {'fold': 0, 'repeat': 2, 'mse': 203.98960876464844, 'train_time': 45.71153175015934, 'train_nmll': 11.518463134765625, 'test_nmll': 21.16823959350586, 'train_nll': 5240.7158203125, 'test_nll': 1079.5687255859375}\n",
    "2019-02-27 20:51:03.617306 - {'fold': 0, 'repeat': 3, 'mse': 203.98873901367188, 'train_time': 45.543470170814544, 'train_nmll': 11.518349647521973, 'test_nmll': 21.16813087463379, 'train_nll': 5240.63916015625, 'test_nll': 1079.662353515625}\n",
    "2019-02-27 20:51:47.489161 - {'fold': 0, 'repeat': 4, 'mse': 203.9897918701172, 'train_time': 43.87176379095763, 'train_nmll': 11.516733169555664, 'test_nmll': 21.166732788085938, 'train_nll': 5240.54638671875, 'test_nll': 1079.5648193359375}\n",
    "2019-02-27 20:52:31.553654 - {'fold': 0, 'repeat': 5, 'mse': 203.98902893066406, 'train_time': 44.06440387107432, 'train_nmll': 11.51754379272461, 'test_nmll': 21.16790008544922, 'train_nll': 5240.64208984375, 'test_nll': 1079.5816650390625}\n",
    "2019-02-27 20:53:16.247014 - {'fold': 0, 'repeat': 6, 'mse': 203.9874725341797, 'train_time': 44.693268834846094, 'train_nmll': 11.518460273742676, 'test_nmll': 21.16982078552246, 'train_nll': 5240.49072265625, 'test_nll': 1079.642822265625}\n",
    "2019-02-27 20:54:01.140843 - {'fold': 0, 'repeat': 7, 'mse': 203.98814392089844, 'train_time': 44.89373831707053, 'train_nmll': 11.519488334655762, 'test_nmll': 21.169950485229492, 'train_nll': 5240.21484375, 'test_nll': 1079.5194091796875}\n",
    "2019-02-27 20:54:42.850041 - {'fold': 0, 'repeat': 8, 'mse': 203.98919677734375, 'train_time': 41.709107192000374, 'train_nmll': 11.518588066101074, 'test_nmll': 21.16826629638672, 'train_nll': 5241.41845703125, 'test_nll': 1079.6026611328125}\n",
    "2019-02-27 20:55:25.055140 - {'fold': 0, 'repeat': 9, 'mse': 203.98916625976562, 'train_time': 42.205008822958916, 'train_nmll': 11.51724910736084, 'test_nmll': 21.170129776000977, 'train_nll': 5241.50244140625, 'test_nll': 1079.7005615234375}\n",
    "2019-02-27 20:56:09.486858 - {'fold': 1, 'repeat': 0, 'mse': 125.67333221435547, 'train_time': 44.42740524117835, 'train_nmll': 11.706859588623047, 'test_nmll': 13.55129623413086, 'train_nll': 5327.77392578125, 'test_nll': 691.0224609375}\n",
    "2019-02-27 20:56:54.023435 - {'fold': 1, 'repeat': 1, 'mse': 125.67302703857422, 'train_time': 44.53648876491934, 'train_nmll': 11.707386016845703, 'test_nmll': 13.546687126159668, 'train_nll': 5326.99462890625, 'test_nll': 690.994140625}\n",
    "2019-02-27 20:57:36.734045 - {'fold': 1, 'repeat': 2, 'mse': 125.6731948852539, 'train_time': 42.71052032895386, 'train_nmll': 11.708908081054688, 'test_nmll': 13.553316116333008, 'train_nll': 5327.83251953125, 'test_nll': 690.984130859375}\n",
    "2019-02-27 20:58:21.426981 - {'fold': 1, 'repeat': 3, 'mse': 125.67341613769531, 'train_time': 44.69284757389687, 'train_nmll': 11.71043872833252, 'test_nmll': 13.549696922302246, 'train_nll': 5327.07470703125, 'test_nll': 690.8543090820312}\n",
    "2019-02-27 20:59:03.668909 - {'fold': 1, 'repeat': 4, 'mse': 125.67364501953125, 'train_time': 42.24183768895455, 'train_nmll': 11.709470748901367, 'test_nmll': 13.54649543762207, 'train_nll': 5328.27392578125, 'test_nll': 691.12890625}\n",
    "2019-02-27 20:59:48.152639 - {'fold': 1, 'repeat': 5, 'mse': 125.67391204833984, 'train_time': 44.48364329012111, 'train_nmll': 11.709207534790039, 'test_nmll': 13.547874450683594, 'train_nll': 5327.82421875, 'test_nll': 690.9612426757812}\n",
    "2019-02-27 21:00:32.473682 - {'fold': 1, 'repeat': 6, 'mse': 125.67304229736328, 'train_time': 44.32095403014682, 'train_nmll': 11.708062171936035, 'test_nmll': 13.547658920288086, 'train_nll': 5326.49755859375, 'test_nll': 690.896240234375}\n",
    "2019-02-27 21:01:16.288586 - {'fold': 1, 'repeat': 7, 'mse': 125.67314147949219, 'train_time': 43.81481731985696, 'train_nmll': 11.708270072937012, 'test_nmll': 13.550326347351074, 'train_nll': 5327.90966796875, 'test_nll': 691.0213012695312}\n",
    "2019-02-27 21:02:00.589735 - {'fold': 1, 'repeat': 8, 'mse': 125.67280578613281, 'train_time': 44.301052483031526, 'train_nmll': 11.707744598388672, 'test_nmll': 13.551745414733887, 'train_nll': 5327.1376953125, 'test_nll': 691.1162109375}\n",
    "2019-02-27 21:02:44.559581 - {'fold': 1, 'repeat': 9, 'mse': 125.67351531982422, 'train_time': 43.969743984984234, 'train_nmll': 11.708431243896484, 'test_nmll': 13.547903060913086, 'train_nll': 5327.609375, 'test_nll': 691.0479736328125}\n",
    "2019-02-27 21:03:28.823004 - {'fold': 2, 'repeat': 0, 'mse': 144.06680297851562, 'train_time': 44.25839294702746, 'train_nmll': 11.77895736694336, 'test_nmll': 15.332392692565918, 'train_nll': 5359.98046875, 'test_nll': 782.2317504882812}\n",
    "2019-02-27 21:04:11.083574 - {'fold': 2, 'repeat': 1, 'mse': 144.06748962402344, 'train_time': 42.260479676071554, 'train_nmll': 11.778550148010254, 'test_nmll': 15.331354141235352, 'train_nll': 5360.310546875, 'test_nll': 782.2281494140625}\n",
    "2019-02-27 21:04:54.587641 - {'fold': 2, 'repeat': 2, 'mse': 144.06707763671875, 'train_time': 43.50395454117097, 'train_nmll': 11.780745506286621, 'test_nmll': 15.33285140991211, 'train_nll': 5361.85888671875, 'test_nll': 781.970703125}\n",
    "2019-02-27 21:05:38.872564 - {'fold': 2, 'repeat': 3, 'mse': 144.0670623779297, 'train_time': 44.28482042206451, 'train_nmll': 11.778599739074707, 'test_nmll': 15.333856582641602, 'train_nll': 5359.22802734375, 'test_nll': 782.1857299804688}\n",
    "2019-02-27 21:06:22.911516 - {'fold': 2, 'repeat': 4, 'mse': 144.0674285888672, 'train_time': 44.03886406100355, 'train_nmll': 11.779385566711426, 'test_nmll': 15.333675384521484, 'train_nll': 5359.296875, 'test_nll': 782.1103515625}\n",
    "2019-02-27 21:07:07.297442 - {'fold': 2, 'repeat': 5, 'mse': 144.0657196044922, 'train_time': 44.38583724596538, 'train_nmll': 11.780518531799316, 'test_nmll': 15.335701942443848, 'train_nll': 5359.80517578125, 'test_nll': 782.0228271484375}\n",
    "2019-02-27 21:07:49.660534 - {'fold': 2, 'repeat': 6, 'mse': 144.06849670410156, 'train_time': 42.363003995036706, 'train_nmll': 11.781015396118164, 'test_nmll': 15.334823608398438, 'train_nll': 5359.501953125, 'test_nll': 782.177978515625}\n",
    "2019-02-27 21:08:34.039300 - {'fold': 2, 'repeat': 7, 'mse': 144.0681915283203, 'train_time': 44.37867948017083, 'train_nmll': 11.78127384185791, 'test_nmll': 15.33574390411377, 'train_nll': 5360.1025390625, 'test_nll': 781.9808349609375}\n",
    "2019-02-27 21:09:17.733446 - {'fold': 2, 'repeat': 8, 'mse': 144.06698608398438, 'train_time': 43.69405583804473, 'train_nmll': 11.780102729797363, 'test_nmll': 15.335321426391602, 'train_nll': 5360.56103515625, 'test_nll': 782.0479125976562}\n",
    "2019-02-27 21:10:02.205788 - {'fold': 2, 'repeat': 9, 'mse': 144.06686401367188, 'train_time': 44.47225248417817, 'train_nmll': 11.781888961791992, 'test_nmll': 15.334708213806152, 'train_nll': 5359.275390625, 'test_nll': 782.2218627929688}\n",
    "2019-02-27 21:10:46.901854 - {'fold': 3, 'repeat': 0, 'mse': 130.17147827148438, 'train_time': 44.69174798205495, 'train_nmll': 11.829907417297363, 'test_nmll': 14.01328182220459, 'train_nll': 5381.896484375, 'test_nll': 714.5844116210938}\n",
    "2019-02-27 21:11:32.228439 - {'fold': 3, 'repeat': 1, 'mse': 130.17079162597656, 'train_time': 45.326495066983625, 'train_nmll': 11.82749080657959, 'test_nmll': 14.012728691101074, 'train_nll': 5381.876953125, 'test_nll': 714.588134765625}\n",
    "2019-02-27 21:12:16.903883 - {'fold': 3, 'repeat': 2, 'mse': 130.1717071533203, 'train_time': 44.67535568913445, 'train_nmll': 11.827045440673828, 'test_nmll': 14.01160717010498, 'train_nll': 5382.47314453125, 'test_nll': 714.5404052734375}\n",
    "2019-02-27 21:12:57.724086 - {'fold': 3, 'repeat': 3, 'mse': 130.17112731933594, 'train_time': 40.82011364400387, 'train_nmll': 11.828099250793457, 'test_nmll': 14.015081405639648, 'train_nll': 5382.46240234375, 'test_nll': 714.4310913085938}\n",
    "2019-02-27 21:13:40.984837 - {'fold': 3, 'repeat': 4, 'mse': 130.1714324951172, 'train_time': 43.260657152859494, 'train_nmll': 11.82976245880127, 'test_nmll': 14.01248836517334, 'train_nll': 5382.46435546875, 'test_nll': 714.4622192382812}\n",
    "2019-02-27 21:14:24.340967 - {'fold': 3, 'repeat': 5, 'mse': 130.1719512939453, 'train_time': 43.35604029498063, 'train_nmll': 11.82888126373291, 'test_nmll': 14.013806343078613, 'train_nll': 5382.18994140625, 'test_nll': 714.5477905273438}\n",
    "2019-02-27 21:15:08.419664 - {'fold': 3, 'repeat': 6, 'mse': 130.17095947265625, 'train_time': 44.078610317083076, 'train_nmll': 11.829961776733398, 'test_nmll': 14.010798454284668, 'train_nll': 5382.05078125, 'test_nll': 714.5359497070312}\n",
    "2019-02-27 21:15:53.444831 - {'fold': 3, 'repeat': 7, 'mse': 130.1715850830078, 'train_time': 45.02507785591297, 'train_nmll': 11.828397750854492, 'test_nmll': 14.0115966796875, 'train_nll': 5381.65966796875, 'test_nll': 714.700439453125}\n",
    "2019-02-27 21:16:38.514393 - {'fold': 3, 'repeat': 8, 'mse': 130.1721649169922, 'train_time': 45.069473213050514, 'train_nmll': 11.829930305480957, 'test_nmll': 14.01187801361084, 'train_nll': 5381.2138671875, 'test_nll': 714.498046875}\n",
    "2019-02-27 21:17:23.646464 - {'fold': 3, 'repeat': 9, 'mse': 130.1717987060547, 'train_time': 45.13198093697429, 'train_nmll': 11.829703330993652, 'test_nmll': 14.01152229309082, 'train_nll': 5382.9140625, 'test_nll': 714.6651000976562}\n",
    "2019-02-27 21:18:08.728599 - {'fold': 4, 'repeat': 0, 'mse': 169.4866485595703, 'train_time': 45.07784951408394, 'train_nmll': 11.594202041625977, 'test_nmll': 17.65975570678711, 'train_nll': 5275.310546875, 'test_nll': 900.6644287109375}\n",
    "2019-02-27 21:18:53.095413 - {'fold': 4, 'repeat': 1, 'mse': 169.4879150390625, 'train_time': 44.36672522383742, 'train_nmll': 11.593290328979492, 'test_nmll': 17.6588077545166, 'train_nll': 5274.98583984375, 'test_nll': 900.7817993164062}\n",
    "2019-02-27 21:19:38.713481 - {'fold': 4, 'repeat': 2, 'mse': 169.4873046875, 'train_time': 45.6179783588741, 'train_nmll': 11.593398094177246, 'test_nmll': 17.662818908691406, 'train_nll': 5275.171875, 'test_nll': 900.7658081054688}\n",
    "2019-02-27 21:20:23.333481 - {'fold': 4, 'repeat': 3, 'mse': 169.48782348632812, 'train_time': 44.61991075007245, 'train_nmll': 11.594029426574707, 'test_nmll': 17.661727905273438, 'train_nll': 5275.96630859375, 'test_nll': 900.6493530273438}\n",
    "2019-02-27 21:21:04.972853 - {'fold': 4, 'repeat': 4, 'mse': 169.4872283935547, 'train_time': 41.639281696872786, 'train_nmll': 11.593113899230957, 'test_nmll': 17.661134719848633, 'train_nll': 5275.2919921875, 'test_nll': 900.7422485351562}\n",
    "2019-02-27 21:21:46.439392 - {'fold': 4, 'repeat': 5, 'mse': 169.48785400390625, 'train_time': 41.466448459075764, 'train_nmll': 11.593343734741211, 'test_nmll': 17.66217041015625, 'train_nll': 5275.03857421875, 'test_nll': 900.6366577148438}\n",
    "2019-02-27 21:22:30.585830 - {'fold': 4, 'repeat': 6, 'mse': 169.48867797851562, 'train_time': 44.14634961099364, 'train_nmll': 11.593812942504883, 'test_nmll': 17.664134979248047, 'train_nll': 5275.24169921875, 'test_nll': 900.7863159179688}\n",
    "2019-02-27 21:23:13.474946 - {'fold': 4, 'repeat': 7, 'mse': 169.4886474609375, 'train_time': 42.88902855385095, 'train_nmll': 11.593173027038574, 'test_nmll': 17.664077758789062, 'train_nll': 5274.9912109375, 'test_nll': 900.707275390625}\n",
    "2019-02-27 21:23:58.642617 - {'fold': 4, 'repeat': 8, 'mse': 169.4871063232422, 'train_time': 45.167576445965096, 'train_nmll': 11.59365463256836, 'test_nmll': 17.660831451416016, 'train_nll': 5275.0029296875, 'test_nll': 900.6983032226562}\n",
    "2019-02-27 21:24:40.622960 - {'fold': 4, 'repeat': 9, 'mse': 169.48765563964844, 'train_time': 41.98024427797645, 'train_nmll': 11.593624114990234, 'test_nmll': 17.66229820251465, 'train_nll': 5274.80322265625, 'test_nll': 900.777587890625}\n",
    "2019-02-27 21:25:23.576570 - {'fold': 5, 'repeat': 0, 'mse': 166.92042541503906, 'train_time': 42.94928499008529, 'train_nmll': 11.924307823181152, 'test_nmll': 17.41547966003418, 'train_nll': 5425.15478515625, 'test_nll': 888.142822265625}\n",
    "2019-02-27 21:26:08.393701 - {'fold': 5, 'repeat': 1, 'mse': 166.9210968017578, 'train_time': 44.81704261503182, 'train_nmll': 11.922451972961426, 'test_nmll': 17.414199829101562, 'train_nll': 5425.08935546875, 'test_nll': 888.1935424804688}\n",
    "2019-02-27 21:26:49.351768 - {'fold': 5, 'repeat': 2, 'mse': 166.921875, 'train_time': 40.95797888818197, 'train_nmll': 11.921998023986816, 'test_nmll': 17.41396713256836, 'train_nll': 5424.5244140625, 'test_nll': 888.0679931640625}\n",
    "2019-02-27 21:27:33.908630 - {'fold': 5, 'repeat': 3, 'mse': 166.92193603515625, 'train_time': 44.556772920070216, 'train_nmll': 11.921218872070312, 'test_nmll': 17.413219451904297, 'train_nll': 5424.798828125, 'test_nll': 888.0651245117188}\n",
    "2019-02-27 21:28:17.487866 - {'fold': 5, 'repeat': 4, 'mse': 166.92144775390625, 'train_time': 43.57914703898132, 'train_nmll': 11.923295974731445, 'test_nmll': 17.41553497314453, 'train_nll': 5424.13037109375, 'test_nll': 888.1580810546875}\n",
    "2019-02-27 21:29:00.948032 - {'fold': 5, 'repeat': 5, 'mse': 166.92091369628906, 'train_time': 43.46007570903748, 'train_nmll': 11.923990249633789, 'test_nmll': 17.413333892822266, 'train_nll': 5424.85888671875, 'test_nll': 888.0123901367188}\n",
    "2019-02-27 21:29:44.909144 - {'fold': 5, 'repeat': 6, 'mse': 166.92141723632812, 'train_time': 43.961019648006186, 'train_nmll': 11.923174858093262, 'test_nmll': 17.41512680053711, 'train_nll': 5424.5966796875, 'test_nll': 888.0890502929688}\n",
    "2019-02-27 21:30:29.650775 - {'fold': 5, 'repeat': 7, 'mse': 166.92173767089844, 'train_time': 44.741538411937654, 'train_nmll': 11.923877716064453, 'test_nmll': 17.413095474243164, 'train_nll': 5423.93505859375, 'test_nll': 888.0787963867188}\n",
    "2019-02-27 21:31:13.270121 - {'fold': 5, 'repeat': 8, 'mse': 166.92112731933594, 'train_time': 43.61925805895589, 'train_nmll': 11.921801567077637, 'test_nmll': 17.413719177246094, 'train_nll': 5424.28173828125, 'test_nll': 888.1847534179688}\n",
    "2019-02-27 21:31:55.991479 - {'fold': 5, 'repeat': 9, 'mse': 166.92007446289062, 'train_time': 42.721268775872886, 'train_nmll': 11.922646522521973, 'test_nmll': 17.413869857788086, 'train_nll': 5425.9482421875, 'test_nll': 888.0928344726562}\n",
    "2019-02-27 21:32:40.849227 - {'fold': 6, 'repeat': 0, 'mse': 172.1309051513672, 'train_time': 44.853480816818774, 'train_nmll': 11.82139778137207, 'test_nmll': 18.0426082611084, 'train_nll': 5378.17333984375, 'test_nll': 920.2071533203125}\n",
    "2019-02-27 21:33:23.455955 - {'fold': 6, 'repeat': 1, 'mse': 172.13218688964844, 'train_time': 42.60663761291653, 'train_nmll': 11.821118354797363, 'test_nmll': 18.041786193847656, 'train_nll': 5378.2939453125, 'test_nll': 920.2713012695312}\n",
    "2019-02-27 21:34:08.816030 - {'fold': 6, 'repeat': 2, 'mse': 172.13206481933594, 'train_time': 45.35998655995354, 'train_nmll': 11.81978702545166, 'test_nmll': 18.043563842773438, 'train_nll': 5377.9521484375, 'test_nll': 920.1988525390625}\n",
    "2019-02-27 21:34:53.622058 - {'fold': 6, 'repeat': 3, 'mse': 172.1311798095703, 'train_time': 44.805940886959434, 'train_nmll': 11.820399284362793, 'test_nmll': 18.043617248535156, 'train_nll': 5378.1591796875, 'test_nll': 920.1443481445312}\n",
    "2019-02-27 21:35:38.315204 - {'fold': 6, 'repeat': 4, 'mse': 172.1311798095703, 'train_time': 44.69305598596111, 'train_nmll': 11.819252967834473, 'test_nmll': 18.04193687438965, 'train_nll': 5377.8271484375, 'test_nll': 920.2195434570312}\n",
    "2019-02-27 21:36:19.149578 - {'fold': 6, 'repeat': 5, 'mse': 172.13148498535156, 'train_time': 40.83428345597349, 'train_nmll': 11.822181701660156, 'test_nmll': 18.042098999023438, 'train_nll': 5378.69189453125, 'test_nll': 920.1862182617188}\n",
    "2019-02-27 21:37:03.881105 - {'fold': 6, 'repeat': 6, 'mse': 172.13140869140625, 'train_time': 44.73143944796175, 'train_nmll': 11.821227073669434, 'test_nmll': 18.04167366027832, 'train_nll': 5378.5810546875, 'test_nll': 920.1561889648438}\n",
    "2019-02-27 21:37:48.916648 - {'fold': 6, 'repeat': 7, 'mse': 172.13046264648438, 'train_time': 45.035446361172944, 'train_nmll': 11.820636749267578, 'test_nmll': 18.041906356811523, 'train_nll': 5377.9228515625, 'test_nll': 920.191650390625}\n",
    "2019-02-27 21:38:33.301989 - {'fold': 6, 'repeat': 8, 'mse': 172.12953186035156, 'train_time': 44.38525057793595, 'train_nmll': 11.820039749145508, 'test_nmll': 18.043794631958008, 'train_nll': 5379.65966796875, 'test_nll': 920.2020874023438}\n",
    "2019-02-27 21:39:17.741024 - {'fold': 6, 'repeat': 9, 'mse': 172.13095092773438, 'train_time': 44.43894519400783, 'train_nmll': 11.821671485900879, 'test_nmll': 18.045656204223633, 'train_nll': 5378.9697265625, 'test_nll': 920.1806640625}\n",
    "2019-02-27 21:40:02.103900 - {'fold': 7, 'repeat': 0, 'mse': 166.5628662109375, 'train_time': 44.3585836510174, 'train_nmll': 11.613591194152832, 'test_nmll': 17.44883155822754, 'train_nll': 5284.4296875, 'test_nll': 890.1151733398438}\n",
    "2019-02-27 21:40:47.500531 - {'fold': 7, 'repeat': 1, 'mse': 166.56295776367188, 'train_time': 45.396541136782616, 'train_nmll': 11.613271713256836, 'test_nmll': 17.44866180419922, 'train_nll': 5284.33935546875, 'test_nll': 889.9506225585938}\n",
    "2019-02-27 21:41:28.722187 - {'fold': 7, 'repeat': 2, 'mse': 166.56346130371094, 'train_time': 41.22156219393946, 'train_nmll': 11.615184783935547, 'test_nmll': 17.45065689086914, 'train_nll': 5283.95458984375, 'test_nll': 889.8800048828125}\n",
    "2019-02-27 21:42:12.020700 - {'fold': 7, 'repeat': 3, 'mse': 166.56417846679688, 'train_time': 43.29842253704555, 'train_nmll': 11.614343643188477, 'test_nmll': 17.450489044189453, 'train_nll': 5284.0791015625, 'test_nll': 890.0845336914062}\n",
    "2019-02-27 21:42:52.567096 - {'fold': 7, 'repeat': 4, 'mse': 166.56369018554688, 'train_time': 40.546288608107716, 'train_nmll': 11.612349510192871, 'test_nmll': 17.45030403137207, 'train_nll': 5284.26171875, 'test_nll': 890.025390625}\n",
    "2019-02-27 21:43:35.277278 - {'fold': 7, 'repeat': 5, 'mse': 166.563720703125, 'train_time': 42.71007763501257, 'train_nmll': 11.613789558410645, 'test_nmll': 17.44913101196289, 'train_nll': 5284.93896484375, 'test_nll': 890.0306396484375}\n",
    "2019-02-27 21:44:15.978473 - {'fold': 7, 'repeat': 6, 'mse': 166.56285095214844, 'train_time': 40.701096228091046, 'train_nmll': 11.613398551940918, 'test_nmll': 17.448925018310547, 'train_nll': 5284.10693359375, 'test_nll': 889.9063720703125}\n",
    "2019-02-27 21:45:00.896201 - {'fold': 7, 'repeat': 7, 'mse': 166.56289672851562, 'train_time': 44.91763994679786, 'train_nmll': 11.614550590515137, 'test_nmll': 17.45128059387207, 'train_nll': 5284.89599609375, 'test_nll': 889.9854125976562}\n",
    "2019-02-27 21:45:45.784874 - {'fold': 7, 'repeat': 8, 'mse': 166.5614471435547, 'train_time': 44.88858415000141, 'train_nmll': 11.613912582397461, 'test_nmll': 17.450183868408203, 'train_nll': 5284.4482421875, 'test_nll': 889.9043579101562}\n",
    "2019-02-27 21:46:30.440617 - {'fold': 7, 'repeat': 9, 'mse': 166.56210327148438, 'train_time': 44.655655357055366, 'train_nmll': 11.613333702087402, 'test_nmll': 17.454816818237305, 'train_nll': 5284.39892578125, 'test_nll': 889.9078979492188}\n",
    "2019-02-27 21:47:15.356070 - {'fold': 8, 'repeat': 0, 'mse': 185.41734313964844, 'train_time': 44.91116838995367, 'train_nmll': 11.60573959350586, 'test_nmll': 19.09978485107422, 'train_nll': 5281.0400390625, 'test_nll': 974.1180419921875}\n",
    "2019-02-27 21:47:59.952088 - {'fold': 8, 'repeat': 1, 'mse': 185.4182586669922, 'train_time': 44.59592650108971, 'train_nmll': 11.607056617736816, 'test_nmll': 19.09771156311035, 'train_nll': 5279.62158203125, 'test_nll': 974.2415771484375}\n",
    "2019-02-27 21:48:45.132612 - {'fold': 8, 'repeat': 2, 'mse': 185.4168701171875, 'train_time': 45.180432584835216, 'train_nmll': 11.605527877807617, 'test_nmll': 19.09935760498047, 'train_nll': 5280.31298828125, 'test_nll': 974.04443359375}\n",
    "2019-02-27 21:49:29.709134 - {'fold': 8, 'repeat': 3, 'mse': 185.41773986816406, 'train_time': 44.57642548088916, 'train_nmll': 11.601911544799805, 'test_nmll': 19.09993553161621, 'train_nll': 5280.5048828125, 'test_nll': 974.1085815429688}\n",
    "2019-02-27 21:50:13.173791 - {'fold': 8, 'repeat': 4, 'mse': 185.41903686523438, 'train_time': 43.46455706190318, 'train_nmll': 11.604141235351562, 'test_nmll': 19.100067138671875, 'train_nll': 5281.00634765625, 'test_nll': 974.0442504882812}\n",
    "2019-02-27 21:50:53.883843 - {'fold': 8, 'repeat': 5, 'mse': 185.41705322265625, 'train_time': 40.7099607270211, 'train_nmll': 11.604412078857422, 'test_nmll': 19.09844398498535, 'train_nll': 5280.53173828125, 'test_nll': 974.1110229492188}\n",
    "2019-02-27 21:51:36.611815 - {'fold': 8, 'repeat': 6, 'mse': 185.41856384277344, 'train_time': 42.727881252067164, 'train_nmll': 11.60539436340332, 'test_nmll': 19.09746742248535, 'train_nll': 5279.18310546875, 'test_nll': 974.080322265625}\n",
    "2019-02-27 21:52:21.770793 - {'fold': 8, 'repeat': 7, 'mse': 185.41653442382812, 'train_time': 45.158891196129844, 'train_nmll': 11.604928970336914, 'test_nmll': 19.09834861755371, 'train_nll': 5279.80078125, 'test_nll': 974.1604614257812}\n",
    "2019-02-27 21:53:04.970060 - {'fold': 8, 'repeat': 8, 'mse': 185.4183349609375, 'train_time': 43.19917820789851, 'train_nmll': 11.605917930603027, 'test_nmll': 19.097841262817383, 'train_nll': 5280.068359375, 'test_nll': 974.0891723632812}\n",
    "2019-02-27 21:53:50.664905 - {'fold': 8, 'repeat': 9, 'mse': 185.41854858398438, 'train_time': 45.69475648016669, 'train_nmll': 11.604381561279297, 'test_nmll': 19.098535537719727, 'train_nll': 5280.7021484375, 'test_nll': 974.1046142578125}\n",
    "2019-02-27 21:54:37.057965 - {'fold': 9, 'repeat': 0, 'mse': 162.78732299804688, 'train_time': 46.38891898305155, 'train_nmll': 11.86000919342041, 'test_nmll': 17.084186553955078, 'train_nll': 5443.92431640625, 'test_nll': 802.916015625}\n",
    "2019-02-27 21:55:22.586825 - {'fold': 9, 'repeat': 1, 'mse': 162.7864990234375, 'train_time': 45.52877040579915, 'train_nmll': 11.862436294555664, 'test_nmll': 17.08426284790039, 'train_nll': 5444.080078125, 'test_nll': 803.01220703125}\n",
    "2019-02-27 21:56:08.267746 - {'fold': 9, 'repeat': 2, 'mse': 162.7868194580078, 'train_time': 45.68083148309961, 'train_nmll': 11.859550476074219, 'test_nmll': 17.085670471191406, 'train_nll': 5443.97412109375, 'test_nll': 802.9244384765625}\n",
    "2019-02-27 21:56:51.659483 - {'fold': 9, 'repeat': 3, 'mse': 162.78749084472656, 'train_time': 43.391647409880534, 'train_nmll': 11.860594749450684, 'test_nmll': 17.084959030151367, 'train_nll': 5443.205078125, 'test_nll': 802.92529296875}\n",
    "2019-02-27 21:57:36.942731 - {'fold': 9, 'repeat': 4, 'mse': 162.78619384765625, 'train_time': 45.283157794969156, 'train_nmll': 11.860003471374512, 'test_nmll': 17.084901809692383, 'train_nll': 5443.466796875, 'test_nll': 803.0385131835938}\n",
    "2019-02-27 21:58:22.788609 - {'fold': 9, 'repeat': 5, 'mse': 162.7863006591797, 'train_time': 45.84578718082048, 'train_nmll': 11.859896659851074, 'test_nmll': 17.08518409729004, 'train_nll': 5443.82958984375, 'test_nll': 802.92529296875}\n",
    "2019-02-27 21:59:08.502609 - {'fold': 9, 'repeat': 6, 'mse': 162.7864990234375, 'train_time': 45.71391024789773, 'train_nmll': 11.861804008483887, 'test_nmll': 17.083404541015625, 'train_nll': 5444.5107421875, 'test_nll': 802.8673095703125}\n",
    "2019-02-27 21:59:52.231118 - {'fold': 9, 'repeat': 7, 'mse': 162.78614807128906, 'train_time': 43.72841720492579, 'train_nmll': 11.85787582397461, 'test_nmll': 17.0842227935791, 'train_nll': 5444.1591796875, 'test_nll': 802.96044921875}\n",
    "2019-02-27 22:00:37.415988 - {'fold': 9, 'repeat': 8, 'mse': 162.7865447998047, 'train_time': 45.18478226708248, 'train_nmll': 11.861763000488281, 'test_nmll': 17.0847110748291, 'train_nll': 5443.7861328125, 'test_nll': 803.017822265625}\n",
    "2019-02-27 22:01:22.995582 - {'fold': 9, 'repeat': 9, 'mse': 162.78729248046875, 'train_time': 45.57950115902349, 'train_nmll': 11.861156463623047, 'test_nmll': 17.085006713867188, 'train_nll': 5444.47216796875, 'test_nll': 802.9727172851562}\n",
    "2019-02-27 22:59:53.917136 - {'fold': 0, 'repeat': 0, 'mse': 10.673661231994629, 'train_time': 3510.8984171960037, 'train_nmll': 2.7019095420837402, 'test_nmll': 2.82454776763916, 'train_nll': 19922.82421875, 'test_nll': 2315.75}\n",
    "2019-02-27 23:58:20.719881 - {'fold': 0, 'repeat': 1, 'mse': 10.667232513427734, 'train_time': 3506.802523803897, 'train_nmll': 2.702115058898926, 'test_nmll': 2.82379150390625, 'train_nll': 19920.09765625, 'test_nll': 2315.4169921875}\n",
    "2019-02-28 00:56:48.325270 - {'fold': 0, 'repeat': 2, 'mse': 10.667656898498535, 'train_time': 3507.6022373801097, 'train_nmll': 2.701997995376587, 'test_nmll': 2.8236467838287354, 'train_nll': 19922.1484375, 'test_nll': 2315.8349609375}\n",
    "2019-02-28 01:55:21.021984 - {'fold': 0, 'repeat': 3, 'mse': 10.668597221374512, 'train_time': 3512.6947868170682, 'train_nmll': 2.7026169300079346, 'test_nmll': 2.8238203525543213, 'train_nll': 19921.01171875, 'test_nll': 2315.66162109375}\n",
    "2019-02-28 02:53:52.651623 - {'fold': 0, 'repeat': 4, 'mse': 10.666977882385254, 'train_time': 3511.62943929201, 'train_nmll': 2.701923131942749, 'test_nmll': 2.823831081390381, 'train_nll': 19919.37890625, 'test_nll': 2315.166748046875}\n",
    "2019-02-28 03:51:59.218014 - {'fold': 0, 'repeat': 5, 'mse': 10.667276382446289, 'train_time': 3486.5661798149813, 'train_nmll': 2.702113389968872, 'test_nmll': 2.823700428009033, 'train_nll': 19919.68359375, 'test_nll': 2315.2919921875}\n",
    "2019-02-28 04:49:58.520137 - {'fold': 0, 'repeat': 6, 'mse': 10.66855239868164, 'train_time': 3479.301914294949, 'train_nmll': 2.7023980617523193, 'test_nmll': 2.824004888534546, 'train_nll': 19920.048828125, 'test_nll': 2316.0341796875}\n",
    "2019-02-28 05:48:04.400571 - {'fold': 0, 'repeat': 7, 'mse': 10.66887378692627, 'train_time': 3485.8802250940353, 'train_nmll': 2.701791763305664, 'test_nmll': 2.8240859508514404, 'train_nll': 19922.23046875, 'test_nll': 2315.55908203125}\n",
    "2019-02-28 06:46:06.322627 - {'fold': 0, 'repeat': 8, 'mse': 10.667701721191406, 'train_time': 3481.9218348129652, 'train_nmll': 2.702275037765503, 'test_nmll': 2.823824644088745, 'train_nll': 19918.1875, 'test_nll': 2315.2626953125}\n",
    "2019-02-28 07:44:09.669025 - {'fold': 0, 'repeat': 9, 'mse': 10.668407440185547, 'train_time': 3483.346176692052, 'train_nmll': 2.701828956604004, 'test_nmll': 2.8239972591400146, 'train_nll': 19918.8125, 'test_nll': 2315.68212890625}\n",
    "2019-02-28 08:42:17.916841 - {'fold': 1, 'repeat': 0, 'mse': 9.390775680541992, 'train_time': 3488.209831733955, 'train_nmll': 2.7133774757385254, 'test_nmll': 2.688349485397339, 'train_nll': 20002.11328125, 'test_nll': 2204.4599609375}\n",
    "2019-02-28 09:40:27.011580 - {'fold': 1, 'repeat': 1, 'mse': 9.391161918640137, 'train_time': 3489.0927787120454, 'train_nmll': 2.713571071624756, 'test_nmll': 2.6885085105895996, 'train_nll': 20004.7578125, 'test_nll': 2204.7236328125}\n",
    "2019-02-28 10:38:36.982950 - {'fold': 1, 'repeat': 2, 'mse': 9.390363693237305, 'train_time': 3489.9711588928476, 'train_nmll': 2.7136409282684326, 'test_nmll': 2.688720941543579, 'train_nll': 20005.640625, 'test_nll': 2204.676025390625}\n",
    "2019-02-28 11:37:06.984353 - {'fold': 1, 'repeat': 3, 'mse': 9.39102554321289, 'train_time': 3510.0011979960836, 'train_nmll': 2.7136118412017822, 'test_nmll': 2.6892027854919434, 'train_nll': 20001.171875, 'test_nll': 2204.597900390625}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = [json.loads(l[29:].replace('\\'', '\\\"')) for l in text.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>mse</th>\n",
       "      <th>repeat</th>\n",
       "      <th>test_nll</th>\n",
       "      <th>test_nmll</th>\n",
       "      <th>train_nll</th>\n",
       "      <th>train_nmll</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.073130</td>\n",
       "      <td>0</td>\n",
       "      <td>2.718582</td>\n",
       "      <td>0.140705</td>\n",
       "      <td>-44.740341</td>\n",
       "      <td>-0.263468</td>\n",
       "      <td>18.210503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.071017</td>\n",
       "      <td>1</td>\n",
       "      <td>1.827188</td>\n",
       "      <td>0.096404</td>\n",
       "      <td>-55.485596</td>\n",
       "      <td>-0.315665</td>\n",
       "      <td>15.156932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.072990</td>\n",
       "      <td>2</td>\n",
       "      <td>1.984665</td>\n",
       "      <td>0.122247</td>\n",
       "      <td>-60.288086</td>\n",
       "      <td>-0.335023</td>\n",
       "      <td>14.749872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.072679</td>\n",
       "      <td>3</td>\n",
       "      <td>2.033468</td>\n",
       "      <td>0.102455</td>\n",
       "      <td>-57.219101</td>\n",
       "      <td>-0.316811</td>\n",
       "      <td>17.451954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.073279</td>\n",
       "      <td>4</td>\n",
       "      <td>2.196196</td>\n",
       "      <td>0.101618</td>\n",
       "      <td>-56.501953</td>\n",
       "      <td>-0.309413</td>\n",
       "      <td>18.103850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.073254</td>\n",
       "      <td>5</td>\n",
       "      <td>2.646085</td>\n",
       "      <td>0.089509</td>\n",
       "      <td>-56.594315</td>\n",
       "      <td>-0.307520</td>\n",
       "      <td>18.486563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.071855</td>\n",
       "      <td>6</td>\n",
       "      <td>1.705603</td>\n",
       "      <td>0.089406</td>\n",
       "      <td>-55.402542</td>\n",
       "      <td>-0.317389</td>\n",
       "      <td>17.910463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.069812</td>\n",
       "      <td>7</td>\n",
       "      <td>1.442110</td>\n",
       "      <td>0.073809</td>\n",
       "      <td>-56.395660</td>\n",
       "      <td>-0.320674</td>\n",
       "      <td>17.224359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.071991</td>\n",
       "      <td>8</td>\n",
       "      <td>2.801659</td>\n",
       "      <td>0.117691</td>\n",
       "      <td>-55.482788</td>\n",
       "      <td>-0.294031</td>\n",
       "      <td>16.185414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.072354</td>\n",
       "      <td>9</td>\n",
       "      <td>2.863179</td>\n",
       "      <td>0.116218</td>\n",
       "      <td>-60.360657</td>\n",
       "      <td>-0.329149</td>\n",
       "      <td>15.440453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.113605</td>\n",
       "      <td>0</td>\n",
       "      <td>5.902166</td>\n",
       "      <td>0.286628</td>\n",
       "      <td>-49.554047</td>\n",
       "      <td>-0.283479</td>\n",
       "      <td>17.077841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.111463</td>\n",
       "      <td>1</td>\n",
       "      <td>5.748804</td>\n",
       "      <td>0.283284</td>\n",
       "      <td>-53.726624</td>\n",
       "      <td>-0.290514</td>\n",
       "      <td>18.599309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.114202</td>\n",
       "      <td>2</td>\n",
       "      <td>6.139370</td>\n",
       "      <td>0.305741</td>\n",
       "      <td>-51.738739</td>\n",
       "      <td>-0.290067</td>\n",
       "      <td>17.582009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.113830</td>\n",
       "      <td>3</td>\n",
       "      <td>5.977134</td>\n",
       "      <td>0.297966</td>\n",
       "      <td>-50.559006</td>\n",
       "      <td>-0.281298</td>\n",
       "      <td>16.741913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.110604</td>\n",
       "      <td>4</td>\n",
       "      <td>5.461489</td>\n",
       "      <td>0.286711</td>\n",
       "      <td>-51.523163</td>\n",
       "      <td>-0.294223</td>\n",
       "      <td>15.421942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.104696</td>\n",
       "      <td>5</td>\n",
       "      <td>4.701239</td>\n",
       "      <td>0.227906</td>\n",
       "      <td>-50.650101</td>\n",
       "      <td>-0.284968</td>\n",
       "      <td>16.068526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0.113083</td>\n",
       "      <td>6</td>\n",
       "      <td>6.319060</td>\n",
       "      <td>0.299272</td>\n",
       "      <td>-51.882538</td>\n",
       "      <td>-0.284502</td>\n",
       "      <td>18.054499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.111447</td>\n",
       "      <td>7</td>\n",
       "      <td>5.760057</td>\n",
       "      <td>0.281322</td>\n",
       "      <td>-51.071533</td>\n",
       "      <td>-0.279432</td>\n",
       "      <td>16.768949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.114491</td>\n",
       "      <td>8</td>\n",
       "      <td>6.062399</td>\n",
       "      <td>0.289230</td>\n",
       "      <td>-52.158234</td>\n",
       "      <td>-0.287662</td>\n",
       "      <td>17.070774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.113562</td>\n",
       "      <td>9</td>\n",
       "      <td>6.253701</td>\n",
       "      <td>0.312961</td>\n",
       "      <td>-50.911713</td>\n",
       "      <td>-0.271440</td>\n",
       "      <td>18.291313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>0.218504</td>\n",
       "      <td>0</td>\n",
       "      <td>14.683550</td>\n",
       "      <td>0.745447</td>\n",
       "      <td>-73.015656</td>\n",
       "      <td>-0.410792</td>\n",
       "      <td>15.816631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>0.218347</td>\n",
       "      <td>1</td>\n",
       "      <td>14.680861</td>\n",
       "      <td>0.728985</td>\n",
       "      <td>-74.570786</td>\n",
       "      <td>-0.410242</td>\n",
       "      <td>17.824198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>0.216537</td>\n",
       "      <td>2</td>\n",
       "      <td>13.961115</td>\n",
       "      <td>0.705910</td>\n",
       "      <td>-74.411026</td>\n",
       "      <td>-0.420847</td>\n",
       "      <td>18.167762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>0.209555</td>\n",
       "      <td>3</td>\n",
       "      <td>14.391743</td>\n",
       "      <td>0.691006</td>\n",
       "      <td>-74.635223</td>\n",
       "      <td>-0.410227</td>\n",
       "      <td>17.966930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>0.218951</td>\n",
       "      <td>4</td>\n",
       "      <td>14.718073</td>\n",
       "      <td>0.727101</td>\n",
       "      <td>-73.883102</td>\n",
       "      <td>-0.425605</td>\n",
       "      <td>14.331470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>0.212059</td>\n",
       "      <td>5</td>\n",
       "      <td>14.169485</td>\n",
       "      <td>0.730659</td>\n",
       "      <td>-73.606506</td>\n",
       "      <td>-0.416099</td>\n",
       "      <td>15.097778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>0.213963</td>\n",
       "      <td>6</td>\n",
       "      <td>14.090298</td>\n",
       "      <td>0.716279</td>\n",
       "      <td>-73.503159</td>\n",
       "      <td>-0.395720</td>\n",
       "      <td>14.393041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>0.213899</td>\n",
       "      <td>7</td>\n",
       "      <td>14.204386</td>\n",
       "      <td>0.713917</td>\n",
       "      <td>-70.591492</td>\n",
       "      <td>-0.402402</td>\n",
       "      <td>16.388057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>0.218283</td>\n",
       "      <td>8</td>\n",
       "      <td>14.446915</td>\n",
       "      <td>0.744911</td>\n",
       "      <td>-74.049393</td>\n",
       "      <td>-0.418863</td>\n",
       "      <td>14.351690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>0.215908</td>\n",
       "      <td>9</td>\n",
       "      <td>14.754271</td>\n",
       "      <td>0.754823</td>\n",
       "      <td>-75.485748</td>\n",
       "      <td>-0.435135</td>\n",
       "      <td>14.334940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>8</td>\n",
       "      <td>185.419037</td>\n",
       "      <td>4</td>\n",
       "      <td>974.044250</td>\n",
       "      <td>19.100067</td>\n",
       "      <td>5281.006348</td>\n",
       "      <td>11.604141</td>\n",
       "      <td>43.464557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>8</td>\n",
       "      <td>185.417053</td>\n",
       "      <td>5</td>\n",
       "      <td>974.111023</td>\n",
       "      <td>19.098444</td>\n",
       "      <td>5280.531738</td>\n",
       "      <td>11.604412</td>\n",
       "      <td>40.709961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>8</td>\n",
       "      <td>185.418564</td>\n",
       "      <td>6</td>\n",
       "      <td>974.080322</td>\n",
       "      <td>19.097467</td>\n",
       "      <td>5279.183105</td>\n",
       "      <td>11.605394</td>\n",
       "      <td>42.727881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>8</td>\n",
       "      <td>185.416534</td>\n",
       "      <td>7</td>\n",
       "      <td>974.160461</td>\n",
       "      <td>19.098349</td>\n",
       "      <td>5279.800781</td>\n",
       "      <td>11.604929</td>\n",
       "      <td>45.158891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>8</td>\n",
       "      <td>185.418335</td>\n",
       "      <td>8</td>\n",
       "      <td>974.089172</td>\n",
       "      <td>19.097841</td>\n",
       "      <td>5280.068359</td>\n",
       "      <td>11.605918</td>\n",
       "      <td>43.199178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>8</td>\n",
       "      <td>185.418549</td>\n",
       "      <td>9</td>\n",
       "      <td>974.104614</td>\n",
       "      <td>19.098536</td>\n",
       "      <td>5280.702148</td>\n",
       "      <td>11.604382</td>\n",
       "      <td>45.694756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>9</td>\n",
       "      <td>162.787323</td>\n",
       "      <td>0</td>\n",
       "      <td>802.916016</td>\n",
       "      <td>17.084187</td>\n",
       "      <td>5443.924316</td>\n",
       "      <td>11.860009</td>\n",
       "      <td>46.388919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786499</td>\n",
       "      <td>1</td>\n",
       "      <td>803.012207</td>\n",
       "      <td>17.084263</td>\n",
       "      <td>5444.080078</td>\n",
       "      <td>11.862436</td>\n",
       "      <td>45.528770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786819</td>\n",
       "      <td>2</td>\n",
       "      <td>802.924438</td>\n",
       "      <td>17.085670</td>\n",
       "      <td>5443.974121</td>\n",
       "      <td>11.859550</td>\n",
       "      <td>45.680831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>9</td>\n",
       "      <td>162.787491</td>\n",
       "      <td>3</td>\n",
       "      <td>802.925293</td>\n",
       "      <td>17.084959</td>\n",
       "      <td>5443.205078</td>\n",
       "      <td>11.860595</td>\n",
       "      <td>43.391647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786194</td>\n",
       "      <td>4</td>\n",
       "      <td>803.038513</td>\n",
       "      <td>17.084902</td>\n",
       "      <td>5443.466797</td>\n",
       "      <td>11.860003</td>\n",
       "      <td>45.283158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786301</td>\n",
       "      <td>5</td>\n",
       "      <td>802.925293</td>\n",
       "      <td>17.085184</td>\n",
       "      <td>5443.829590</td>\n",
       "      <td>11.859897</td>\n",
       "      <td>45.845787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786499</td>\n",
       "      <td>6</td>\n",
       "      <td>802.867310</td>\n",
       "      <td>17.083405</td>\n",
       "      <td>5444.510742</td>\n",
       "      <td>11.861804</td>\n",
       "      <td>45.713910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786148</td>\n",
       "      <td>7</td>\n",
       "      <td>802.960449</td>\n",
       "      <td>17.084223</td>\n",
       "      <td>5444.159180</td>\n",
       "      <td>11.857876</td>\n",
       "      <td>43.728417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>9</td>\n",
       "      <td>162.786545</td>\n",
       "      <td>8</td>\n",
       "      <td>803.017822</td>\n",
       "      <td>17.084711</td>\n",
       "      <td>5443.786133</td>\n",
       "      <td>11.861763</td>\n",
       "      <td>45.184782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>9</td>\n",
       "      <td>162.787292</td>\n",
       "      <td>9</td>\n",
       "      <td>802.972717</td>\n",
       "      <td>17.085007</td>\n",
       "      <td>5444.472168</td>\n",
       "      <td>11.861156</td>\n",
       "      <td>45.579501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0</td>\n",
       "      <td>10.673661</td>\n",
       "      <td>0</td>\n",
       "      <td>2315.750000</td>\n",
       "      <td>2.824548</td>\n",
       "      <td>19922.824219</td>\n",
       "      <td>2.701910</td>\n",
       "      <td>3510.898417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0</td>\n",
       "      <td>10.667233</td>\n",
       "      <td>1</td>\n",
       "      <td>2315.416992</td>\n",
       "      <td>2.823792</td>\n",
       "      <td>19920.097656</td>\n",
       "      <td>2.702115</td>\n",
       "      <td>3506.802524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0</td>\n",
       "      <td>10.667657</td>\n",
       "      <td>2</td>\n",
       "      <td>2315.834961</td>\n",
       "      <td>2.823647</td>\n",
       "      <td>19922.148438</td>\n",
       "      <td>2.701998</td>\n",
       "      <td>3507.602237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0</td>\n",
       "      <td>10.668597</td>\n",
       "      <td>3</td>\n",
       "      <td>2315.661621</td>\n",
       "      <td>2.823820</td>\n",
       "      <td>19921.011719</td>\n",
       "      <td>2.702617</td>\n",
       "      <td>3512.694787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0</td>\n",
       "      <td>10.666978</td>\n",
       "      <td>4</td>\n",
       "      <td>2315.166748</td>\n",
       "      <td>2.823831</td>\n",
       "      <td>19919.378906</td>\n",
       "      <td>2.701923</td>\n",
       "      <td>3511.629439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0</td>\n",
       "      <td>10.667276</td>\n",
       "      <td>5</td>\n",
       "      <td>2315.291992</td>\n",
       "      <td>2.823700</td>\n",
       "      <td>19919.683594</td>\n",
       "      <td>2.702113</td>\n",
       "      <td>3486.566180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0</td>\n",
       "      <td>10.668552</td>\n",
       "      <td>6</td>\n",
       "      <td>2316.034180</td>\n",
       "      <td>2.824005</td>\n",
       "      <td>19920.048828</td>\n",
       "      <td>2.702398</td>\n",
       "      <td>3479.301914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0</td>\n",
       "      <td>10.668874</td>\n",
       "      <td>7</td>\n",
       "      <td>2315.559082</td>\n",
       "      <td>2.824086</td>\n",
       "      <td>19922.230469</td>\n",
       "      <td>2.701792</td>\n",
       "      <td>3485.880225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0</td>\n",
       "      <td>10.667702</td>\n",
       "      <td>8</td>\n",
       "      <td>2315.262695</td>\n",
       "      <td>2.823825</td>\n",
       "      <td>19918.187500</td>\n",
       "      <td>2.702275</td>\n",
       "      <td>3481.921835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0</td>\n",
       "      <td>10.668407</td>\n",
       "      <td>9</td>\n",
       "      <td>2315.682129</td>\n",
       "      <td>2.823997</td>\n",
       "      <td>19918.812500</td>\n",
       "      <td>2.701829</td>\n",
       "      <td>3483.346177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>1</td>\n",
       "      <td>9.390776</td>\n",
       "      <td>0</td>\n",
       "      <td>2204.459961</td>\n",
       "      <td>2.688349</td>\n",
       "      <td>20002.113281</td>\n",
       "      <td>2.713377</td>\n",
       "      <td>3488.209832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1</td>\n",
       "      <td>9.391162</td>\n",
       "      <td>1</td>\n",
       "      <td>2204.723633</td>\n",
       "      <td>2.688509</td>\n",
       "      <td>20004.757812</td>\n",
       "      <td>2.713571</td>\n",
       "      <td>3489.092779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>1</td>\n",
       "      <td>9.390364</td>\n",
       "      <td>2</td>\n",
       "      <td>2204.676025</td>\n",
       "      <td>2.688721</td>\n",
       "      <td>20005.640625</td>\n",
       "      <td>2.713641</td>\n",
       "      <td>3489.971159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1</td>\n",
       "      <td>9.391026</td>\n",
       "      <td>3</td>\n",
       "      <td>2204.597900</td>\n",
       "      <td>2.689203</td>\n",
       "      <td>20001.171875</td>\n",
       "      <td>2.713612</td>\n",
       "      <td>3510.001198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fold         mse  repeat     test_nll  test_nmll     train_nll  \\\n",
       "0       0    0.073130       0     2.718582   0.140705    -44.740341   \n",
       "1       0    0.071017       1     1.827188   0.096404    -55.485596   \n",
       "2       0    0.072990       2     1.984665   0.122247    -60.288086   \n",
       "3       0    0.072679       3     2.033468   0.102455    -57.219101   \n",
       "4       0    0.073279       4     2.196196   0.101618    -56.501953   \n",
       "5       0    0.073254       5     2.646085   0.089509    -56.594315   \n",
       "6       0    0.071855       6     1.705603   0.089406    -55.402542   \n",
       "7       0    0.069812       7     1.442110   0.073809    -56.395660   \n",
       "8       0    0.071991       8     2.801659   0.117691    -55.482788   \n",
       "9       0    0.072354       9     2.863179   0.116218    -60.360657   \n",
       "10      1    0.113605       0     5.902166   0.286628    -49.554047   \n",
       "11      1    0.111463       1     5.748804   0.283284    -53.726624   \n",
       "12      1    0.114202       2     6.139370   0.305741    -51.738739   \n",
       "13      1    0.113830       3     5.977134   0.297966    -50.559006   \n",
       "14      1    0.110604       4     5.461489   0.286711    -51.523163   \n",
       "15      1    0.104696       5     4.701239   0.227906    -50.650101   \n",
       "16      1    0.113083       6     6.319060   0.299272    -51.882538   \n",
       "17      1    0.111447       7     5.760057   0.281322    -51.071533   \n",
       "18      1    0.114491       8     6.062399   0.289230    -52.158234   \n",
       "19      1    0.113562       9     6.253701   0.312961    -50.911713   \n",
       "20      2    0.218504       0    14.683550   0.745447    -73.015656   \n",
       "21      2    0.218347       1    14.680861   0.728985    -74.570786   \n",
       "22      2    0.216537       2    13.961115   0.705910    -74.411026   \n",
       "23      2    0.209555       3    14.391743   0.691006    -74.635223   \n",
       "24      2    0.218951       4    14.718073   0.727101    -73.883102   \n",
       "25      2    0.212059       5    14.169485   0.730659    -73.606506   \n",
       "26      2    0.213963       6    14.090298   0.716279    -73.503159   \n",
       "27      2    0.213899       7    14.204386   0.713917    -70.591492   \n",
       "28      2    0.218283       8    14.446915   0.744911    -74.049393   \n",
       "29      2    0.215908       9    14.754271   0.754823    -75.485748   \n",
       "..    ...         ...     ...          ...        ...           ...   \n",
       "284     8  185.419037       4   974.044250  19.100067   5281.006348   \n",
       "285     8  185.417053       5   974.111023  19.098444   5280.531738   \n",
       "286     8  185.418564       6   974.080322  19.097467   5279.183105   \n",
       "287     8  185.416534       7   974.160461  19.098349   5279.800781   \n",
       "288     8  185.418335       8   974.089172  19.097841   5280.068359   \n",
       "289     8  185.418549       9   974.104614  19.098536   5280.702148   \n",
       "290     9  162.787323       0   802.916016  17.084187   5443.924316   \n",
       "291     9  162.786499       1   803.012207  17.084263   5444.080078   \n",
       "292     9  162.786819       2   802.924438  17.085670   5443.974121   \n",
       "293     9  162.787491       3   802.925293  17.084959   5443.205078   \n",
       "294     9  162.786194       4   803.038513  17.084902   5443.466797   \n",
       "295     9  162.786301       5   802.925293  17.085184   5443.829590   \n",
       "296     9  162.786499       6   802.867310  17.083405   5444.510742   \n",
       "297     9  162.786148       7   802.960449  17.084223   5444.159180   \n",
       "298     9  162.786545       8   803.017822  17.084711   5443.786133   \n",
       "299     9  162.787292       9   802.972717  17.085007   5444.472168   \n",
       "300     0   10.673661       0  2315.750000   2.824548  19922.824219   \n",
       "301     0   10.667233       1  2315.416992   2.823792  19920.097656   \n",
       "302     0   10.667657       2  2315.834961   2.823647  19922.148438   \n",
       "303     0   10.668597       3  2315.661621   2.823820  19921.011719   \n",
       "304     0   10.666978       4  2315.166748   2.823831  19919.378906   \n",
       "305     0   10.667276       5  2315.291992   2.823700  19919.683594   \n",
       "306     0   10.668552       6  2316.034180   2.824005  19920.048828   \n",
       "307     0   10.668874       7  2315.559082   2.824086  19922.230469   \n",
       "308     0   10.667702       8  2315.262695   2.823825  19918.187500   \n",
       "309     0   10.668407       9  2315.682129   2.823997  19918.812500   \n",
       "310     1    9.390776       0  2204.459961   2.688349  20002.113281   \n",
       "311     1    9.391162       1  2204.723633   2.688509  20004.757812   \n",
       "312     1    9.390364       2  2204.676025   2.688721  20005.640625   \n",
       "313     1    9.391026       3  2204.597900   2.689203  20001.171875   \n",
       "\n",
       "     train_nmll   train_time  \n",
       "0     -0.263468    18.210503  \n",
       "1     -0.315665    15.156932  \n",
       "2     -0.335023    14.749872  \n",
       "3     -0.316811    17.451954  \n",
       "4     -0.309413    18.103850  \n",
       "5     -0.307520    18.486563  \n",
       "6     -0.317389    17.910463  \n",
       "7     -0.320674    17.224359  \n",
       "8     -0.294031    16.185414  \n",
       "9     -0.329149    15.440453  \n",
       "10    -0.283479    17.077841  \n",
       "11    -0.290514    18.599309  \n",
       "12    -0.290067    17.582009  \n",
       "13    -0.281298    16.741913  \n",
       "14    -0.294223    15.421942  \n",
       "15    -0.284968    16.068526  \n",
       "16    -0.284502    18.054499  \n",
       "17    -0.279432    16.768949  \n",
       "18    -0.287662    17.070774  \n",
       "19    -0.271440    18.291313  \n",
       "20    -0.410792    15.816631  \n",
       "21    -0.410242    17.824198  \n",
       "22    -0.420847    18.167762  \n",
       "23    -0.410227    17.966930  \n",
       "24    -0.425605    14.331470  \n",
       "25    -0.416099    15.097778  \n",
       "26    -0.395720    14.393041  \n",
       "27    -0.402402    16.388057  \n",
       "28    -0.418863    14.351690  \n",
       "29    -0.435135    14.334940  \n",
       "..          ...          ...  \n",
       "284   11.604141    43.464557  \n",
       "285   11.604412    40.709961  \n",
       "286   11.605394    42.727881  \n",
       "287   11.604929    45.158891  \n",
       "288   11.605918    43.199178  \n",
       "289   11.604382    45.694756  \n",
       "290   11.860009    46.388919  \n",
       "291   11.862436    45.528770  \n",
       "292   11.859550    45.680831  \n",
       "293   11.860595    43.391647  \n",
       "294   11.860003    45.283158  \n",
       "295   11.859897    45.845787  \n",
       "296   11.861804    45.713910  \n",
       "297   11.857876    43.728417  \n",
       "298   11.861763    45.184782  \n",
       "299   11.861156    45.579501  \n",
       "300    2.701910  3510.898417  \n",
       "301    2.702115  3506.802524  \n",
       "302    2.701998  3507.602237  \n",
       "303    2.702617  3512.694787  \n",
       "304    2.701923  3511.629439  \n",
       "305    2.702113  3486.566180  \n",
       "306    2.702398  3479.301914  \n",
       "307    2.701792  3485.880225  \n",
       "308    2.702275  3481.921835  \n",
       "309    2.701829  3483.346177  \n",
       "310    2.713377  3488.209832  \n",
       "311    2.713571  3489.092779  \n",
       "312    2.713641  3489.971159  \n",
       "313    2.713612  3510.001198  \n",
       "\n",
       "[314 rows x 8 columns]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:100, 'dataset'] = 'bach'\n",
    "df.loc[100:200, 'dataset'] = 'concrete'\n",
    "df.loc[200:300, 'dataset'] = 'housing'\n",
    "df.loc[300:, 'dataset'] = 'pumadyn-8nh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=4, figsize=(12,5))\n",
    "for i, (dataset, gp) in enumerate(df.groupby('dataset')):\n",
    "    gp['test_nll'].plot(kind='kde', ax=axes[i])\n",
    "    axes[i].set_title(dataset)\n",
    "    axes[i].set_xlabel('test nll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./_partial_ablation_results.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>error</th>\n",
       "      <th>fold</th>\n",
       "      <th>mse</th>\n",
       "      <th>n_projections</th>\n",
       "      <th>repeat</th>\n",
       "      <th>test_nll</th>\n",
       "      <th>test_nmll</th>\n",
       "      <th>train_nll</th>\n",
       "      <th>train_nmll</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>0</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>0</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                              error  fold  mse  \\\n",
       "440      0  Traceback (most recent call last):\\n  File \"/h...   NaN  NaN   \n",
       "441      0  Traceback (most recent call last):\\n  File \"/h...   NaN  NaN   \n",
       "442      0  Traceback (most recent call last):\\n  File \"/h...   NaN  NaN   \n",
       "443      0  Traceback (most recent call last):\\n  File \"/h...   NaN  NaN   \n",
       "684      0  Traceback (most recent call last):\\n  File \"/h...   NaN  NaN   \n",
       "\n",
       "     n_projections  repeat  test_nll  test_nmll  train_nll  train_nmll  \\\n",
       "440            512     NaN       NaN        NaN        NaN         NaN   \n",
       "441           1024     NaN       NaN        NaN        NaN         NaN   \n",
       "442           2048     NaN       NaN        NaN        NaN         NaN   \n",
       "443           4096     NaN       NaN        NaN        NaN         NaN   \n",
       "684           4096     NaN       NaN        NaN        NaN         NaN   \n",
       "\n",
       "     train_time  \n",
       "440         NaN  \n",
       "441         NaN  \n",
       "442         NaN  \n",
       "443         NaN  \n",
       "684         NaN  "
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[~pd.isnull(df['error'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 256, in rp_ablation\n",
      "    dataset, split, cv, repeats=inner_repeats)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 109, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 226, in train_rp_gp\n",
      "    patience=patience)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 52, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/adam.py\", line 58, in step\n",
      "    loss = closure()\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 50, in closure\n",
      "    loss.backward()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/tensor.py\", line 102, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 90, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: $ Torch: not enough memory: you tried to allocate 3GB. Buy new RAM! at /opt/conda/conda-bld/pytorch-nightly-cpu_1542614797320/work/aten/src/TH/THGeneral.cpp:204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df[~pd.isnull(df['error'])].iloc[-1]['error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>error</th>\n",
       "      <th>fold</th>\n",
       "      <th>mse</th>\n",
       "      <th>n_projections</th>\n",
       "      <th>repeat</th>\n",
       "      <th>test_nll</th>\n",
       "      <th>test_nmll</th>\n",
       "      <th>train_nll</th>\n",
       "      <th>train_nmll</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486855</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.069698</td>\n",
       "      <td>1.203484</td>\n",
       "      <td>261.778412</td>\n",
       "      <td>1.454324</td>\n",
       "      <td>0.662972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.527890</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.196873</td>\n",
       "      <td>1.160030</td>\n",
       "      <td>248.519241</td>\n",
       "      <td>1.380614</td>\n",
       "      <td>0.576021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.411384</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.703733</td>\n",
       "      <td>1.135183</td>\n",
       "      <td>263.102417</td>\n",
       "      <td>1.461680</td>\n",
       "      <td>0.705728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.410259</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.812260</td>\n",
       "      <td>1.140615</td>\n",
       "      <td>262.532196</td>\n",
       "      <td>1.458512</td>\n",
       "      <td>0.492423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.890565</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.352665</td>\n",
       "      <td>1.367627</td>\n",
       "      <td>254.533264</td>\n",
       "      <td>1.414092</td>\n",
       "      <td>0.648426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.674492</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.650726</td>\n",
       "      <td>1.282536</td>\n",
       "      <td>258.448120</td>\n",
       "      <td>1.435823</td>\n",
       "      <td>0.515353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.598660</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.651253</td>\n",
       "      <td>3.482563</td>\n",
       "      <td>224.592819</td>\n",
       "      <td>1.247738</td>\n",
       "      <td>0.406367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.667796</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.452293</td>\n",
       "      <td>4.122632</td>\n",
       "      <td>227.409195</td>\n",
       "      <td>1.263384</td>\n",
       "      <td>0.384980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.504163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.056507</td>\n",
       "      <td>1.202825</td>\n",
       "      <td>258.057495</td>\n",
       "      <td>1.433652</td>\n",
       "      <td>0.667018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.503880</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.386456</td>\n",
       "      <td>1.169323</td>\n",
       "      <td>262.110901</td>\n",
       "      <td>1.456172</td>\n",
       "      <td>0.585067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.982161</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.481251</td>\n",
       "      <td>1.424685</td>\n",
       "      <td>227.633575</td>\n",
       "      <td>1.260521</td>\n",
       "      <td>0.562112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.903072</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.316723</td>\n",
       "      <td>1.365835</td>\n",
       "      <td>257.008362</td>\n",
       "      <td>1.427824</td>\n",
       "      <td>0.578139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.536056</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.229675</td>\n",
       "      <td>1.211482</td>\n",
       "      <td>261.918762</td>\n",
       "      <td>1.455104</td>\n",
       "      <td>1.177308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.539076</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.229721</td>\n",
       "      <td>1.211493</td>\n",
       "      <td>261.946228</td>\n",
       "      <td>1.455257</td>\n",
       "      <td>1.532951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.332868</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.762001</td>\n",
       "      <td>1.588100</td>\n",
       "      <td>253.847214</td>\n",
       "      <td>1.410262</td>\n",
       "      <td>0.782785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.303171</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.687965</td>\n",
       "      <td>1.584393</td>\n",
       "      <td>254.573425</td>\n",
       "      <td>1.414297</td>\n",
       "      <td>0.688496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.375996</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.699350</td>\n",
       "      <td>1.084967</td>\n",
       "      <td>260.493500</td>\n",
       "      <td>1.447186</td>\n",
       "      <td>0.445363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.375350</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.133970</td>\n",
       "      <td>1.106700</td>\n",
       "      <td>259.982971</td>\n",
       "      <td>1.444349</td>\n",
       "      <td>0.857361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.084314</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.086664</td>\n",
       "      <td>1.500380</td>\n",
       "      <td>235.445496</td>\n",
       "      <td>1.311137</td>\n",
       "      <td>0.506733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.175835</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.125061</td>\n",
       "      <td>1.506253</td>\n",
       "      <td>255.850616</td>\n",
       "      <td>1.421392</td>\n",
       "      <td>0.743413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397469</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.428295</td>\n",
       "      <td>1.122100</td>\n",
       "      <td>246.106522</td>\n",
       "      <td>1.367243</td>\n",
       "      <td>0.645539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522464</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.043568</td>\n",
       "      <td>1.203390</td>\n",
       "      <td>242.386414</td>\n",
       "      <td>1.342957</td>\n",
       "      <td>0.455785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.380914</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.358711</td>\n",
       "      <td>1.117914</td>\n",
       "      <td>259.212219</td>\n",
       "      <td>1.440328</td>\n",
       "      <td>0.417651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.192896</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.769171</td>\n",
       "      <td>0.637314</td>\n",
       "      <td>134.630280</td>\n",
       "      <td>0.748400</td>\n",
       "      <td>0.687942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.483250</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.938698</td>\n",
       "      <td>1.092167</td>\n",
       "      <td>224.752991</td>\n",
       "      <td>1.250167</td>\n",
       "      <td>0.466935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.707746</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.805183</td>\n",
       "      <td>1.290240</td>\n",
       "      <td>257.223145</td>\n",
       "      <td>1.429043</td>\n",
       "      <td>0.420621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.597770</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.236298</td>\n",
       "      <td>2.661744</td>\n",
       "      <td>225.769455</td>\n",
       "      <td>1.254258</td>\n",
       "      <td>0.442809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.677948</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.521965</td>\n",
       "      <td>2.626264</td>\n",
       "      <td>231.618561</td>\n",
       "      <td>1.286913</td>\n",
       "      <td>0.420273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.296127</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.826561</td>\n",
       "      <td>0.844797</td>\n",
       "      <td>158.544785</td>\n",
       "      <td>0.881556</td>\n",
       "      <td>0.602624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.445369</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.363976</td>\n",
       "      <td>1.020929</td>\n",
       "      <td>209.985657</td>\n",
       "      <td>1.162896</td>\n",
       "      <td>0.438699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>158.276733</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>450.337494</td>\n",
       "      <td>4.371682</td>\n",
       "      <td>3767.122070</td>\n",
       "      <td>4.062759</td>\n",
       "      <td>170.602079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>193.355209</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>468.833710</td>\n",
       "      <td>4.551615</td>\n",
       "      <td>4028.610840</td>\n",
       "      <td>4.345501</td>\n",
       "      <td>222.014159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>207.560715</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>479.859283</td>\n",
       "      <td>4.659150</td>\n",
       "      <td>4190.471191</td>\n",
       "      <td>4.520651</td>\n",
       "      <td>192.023334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>156.881119</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>444.162445</td>\n",
       "      <td>4.312387</td>\n",
       "      <td>3813.792969</td>\n",
       "      <td>4.113348</td>\n",
       "      <td>199.097547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>116.836761</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>408.589722</td>\n",
       "      <td>3.967266</td>\n",
       "      <td>3839.215332</td>\n",
       "      <td>4.141599</td>\n",
       "      <td>178.378920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>148.961090</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>431.820984</td>\n",
       "      <td>4.192482</td>\n",
       "      <td>4077.969482</td>\n",
       "      <td>4.398829</td>\n",
       "      <td>220.518709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>259.783386</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>519.695679</td>\n",
       "      <td>5.046206</td>\n",
       "      <td>4216.179688</td>\n",
       "      <td>4.548173</td>\n",
       "      <td>253.231534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>243.098297</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>510.921600</td>\n",
       "      <td>4.960836</td>\n",
       "      <td>4090.397949</td>\n",
       "      <td>4.412670</td>\n",
       "      <td>232.140371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>216.545059</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>484.178131</td>\n",
       "      <td>4.700006</td>\n",
       "      <td>4111.795898</td>\n",
       "      <td>4.435584</td>\n",
       "      <td>264.895025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>138.809753</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>432.305206</td>\n",
       "      <td>4.198452</td>\n",
       "      <td>3742.429688</td>\n",
       "      <td>4.037301</td>\n",
       "      <td>175.880916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.524582</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>489.913177</td>\n",
       "      <td>4.756218</td>\n",
       "      <td>4262.595703</td>\n",
       "      <td>4.598202</td>\n",
       "      <td>371.461235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.306244</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>506.962067</td>\n",
       "      <td>4.922304</td>\n",
       "      <td>4431.780762</td>\n",
       "      <td>4.780852</td>\n",
       "      <td>399.689448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>271.743652</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>524.578857</td>\n",
       "      <td>5.093104</td>\n",
       "      <td>4353.426270</td>\n",
       "      <td>4.696011</td>\n",
       "      <td>386.762144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.633545</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>526.131836</td>\n",
       "      <td>5.108457</td>\n",
       "      <td>4372.373047</td>\n",
       "      <td>4.716630</td>\n",
       "      <td>399.177316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>213.014664</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>478.693451</td>\n",
       "      <td>4.647692</td>\n",
       "      <td>4271.931641</td>\n",
       "      <td>4.608116</td>\n",
       "      <td>366.820050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>207.267670</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>474.505493</td>\n",
       "      <td>4.607518</td>\n",
       "      <td>4244.931152</td>\n",
       "      <td>4.579573</td>\n",
       "      <td>361.852239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>223.105392</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>485.602142</td>\n",
       "      <td>4.715030</td>\n",
       "      <td>4427.113770</td>\n",
       "      <td>4.775211</td>\n",
       "      <td>410.380397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>225.352570</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>486.173004</td>\n",
       "      <td>4.719998</td>\n",
       "      <td>4437.042969</td>\n",
       "      <td>4.786407</td>\n",
       "      <td>404.095735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>207.868134</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>474.345612</td>\n",
       "      <td>4.605543</td>\n",
       "      <td>4292.550781</td>\n",
       "      <td>4.630400</td>\n",
       "      <td>354.615576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>227.707718</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>487.156097</td>\n",
       "      <td>4.729504</td>\n",
       "      <td>4423.794434</td>\n",
       "      <td>4.772450</td>\n",
       "      <td>411.994870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>188.778351</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>461.782257</td>\n",
       "      <td>4.483953</td>\n",
       "      <td>3965.735840</td>\n",
       "      <td>4.277911</td>\n",
       "      <td>317.825090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>227.562576</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>489.624054</td>\n",
       "      <td>4.753129</td>\n",
       "      <td>4245.042480</td>\n",
       "      <td>4.579505</td>\n",
       "      <td>360.225515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>227.511932</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>489.879608</td>\n",
       "      <td>4.756271</td>\n",
       "      <td>4228.006348</td>\n",
       "      <td>4.561178</td>\n",
       "      <td>351.563224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>245.028229</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>502.046844</td>\n",
       "      <td>4.874053</td>\n",
       "      <td>4346.425781</td>\n",
       "      <td>4.688416</td>\n",
       "      <td>385.966979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>185.016464</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>454.570648</td>\n",
       "      <td>4.414115</td>\n",
       "      <td>4340.222656</td>\n",
       "      <td>4.682002</td>\n",
       "      <td>380.040651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>185.628891</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>456.299347</td>\n",
       "      <td>4.430540</td>\n",
       "      <td>4363.044922</td>\n",
       "      <td>4.706779</td>\n",
       "      <td>391.085977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>275.961609</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>530.502441</td>\n",
       "      <td>5.150322</td>\n",
       "      <td>4276.539551</td>\n",
       "      <td>4.613160</td>\n",
       "      <td>355.725018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>276.461182</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>529.794006</td>\n",
       "      <td>5.143322</td>\n",
       "      <td>4259.486328</td>\n",
       "      <td>4.595038</td>\n",
       "      <td>358.294146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>204.243546</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>475.338715</td>\n",
       "      <td>4.614749</td>\n",
       "      <td>3972.505859</td>\n",
       "      <td>4.284935</td>\n",
       "      <td>312.353994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>256.168335</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>512.689575</td>\n",
       "      <td>4.978135</td>\n",
       "      <td>4301.663086</td>\n",
       "      <td>4.641007</td>\n",
       "      <td>372.434925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1305 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index error  fold         mse  n_projections  repeat    test_nll  \\\n",
       "0         0   NaN   0.0    0.486855              1     0.0   24.069698   \n",
       "1         1   NaN   0.0    0.527890              1     1.0   23.196873   \n",
       "2         2   NaN   1.0    0.411384              1     0.0   22.703733   \n",
       "3         3   NaN   1.0    0.410259              1     1.0   22.812260   \n",
       "4         4   NaN   2.0    0.890565              1     0.0   27.352665   \n",
       "5         5   NaN   2.0    0.674492              1     1.0   25.650726   \n",
       "6         6   NaN   3.0    3.598660              1     0.0   69.651253   \n",
       "7         7   NaN   3.0    3.667796              1     1.0   82.452293   \n",
       "8         8   NaN   4.0    0.504163              1     0.0   24.056507   \n",
       "9         9   NaN   4.0    0.503880              1     1.0   23.386456   \n",
       "10       10   NaN   5.0    0.982161              1     0.0   28.481251   \n",
       "11       11   NaN   5.0    0.903072              1     1.0   27.316723   \n",
       "12       12   NaN   6.0    0.536056              1     0.0   24.229675   \n",
       "13       13   NaN   6.0    0.539076              1     1.0   24.229721   \n",
       "14       14   NaN   7.0    1.332868              1     0.0   31.762001   \n",
       "15       15   NaN   7.0    1.303171              1     1.0   31.687965   \n",
       "16       16   NaN   8.0    0.375996              1     0.0   21.699350   \n",
       "17       17   NaN   8.0    0.375350              1     1.0   22.133970   \n",
       "18       18   NaN   9.0    1.084314              1     0.0   30.086664   \n",
       "19       19   NaN   9.0    1.175835              1     1.0   30.125061   \n",
       "20        0   NaN   0.0    0.397469              2     0.0   22.428295   \n",
       "21        1   NaN   0.0    0.522464              2     1.0   24.043568   \n",
       "22        2   NaN   1.0    0.380914              2     0.0   22.358711   \n",
       "23        3   NaN   1.0    0.192896              2     1.0   12.769171   \n",
       "24        4   NaN   2.0    0.483250              2     0.0   21.938698   \n",
       "25        5   NaN   2.0    0.707746              2     1.0   25.805183   \n",
       "26        6   NaN   3.0    3.597770              2     0.0   53.236298   \n",
       "27        7   NaN   3.0    3.677948              2     1.0   52.521965   \n",
       "28        8   NaN   4.0    0.296127              2     0.0   16.826561   \n",
       "29        9   NaN   4.0    0.445369              2     1.0   20.363976   \n",
       "...     ...   ...   ...         ...            ...     ...         ...   \n",
       "1275     10   NaN   5.0  158.276733              8     0.0  450.337494   \n",
       "1276     11   NaN   5.0  193.355209              8     1.0  468.833710   \n",
       "1277     12   NaN   6.0  207.560715              8     0.0  479.859283   \n",
       "1278     13   NaN   6.0  156.881119              8     1.0  444.162445   \n",
       "1279     14   NaN   7.0  116.836761              8     0.0  408.589722   \n",
       "1280     15   NaN   7.0  148.961090              8     1.0  431.820984   \n",
       "1281     16   NaN   8.0  259.783386              8     0.0  519.695679   \n",
       "1282     17   NaN   8.0  243.098297              8     1.0  510.921600   \n",
       "1283     18   NaN   9.0  216.545059              8     0.0  484.178131   \n",
       "1284     19   NaN   9.0  138.809753              8     1.0  432.305206   \n",
       "1285      0   NaN   0.0  228.524582             16     0.0  489.913177   \n",
       "1286      1   NaN   0.0  253.306244             16     1.0  506.962067   \n",
       "1287      2   NaN   1.0  271.743652             16     0.0  524.578857   \n",
       "1288      3   NaN   1.0  273.633545             16     1.0  526.131836   \n",
       "1289      4   NaN   2.0  213.014664             16     0.0  478.693451   \n",
       "1290      5   NaN   2.0  207.267670             16     1.0  474.505493   \n",
       "1291      6   NaN   3.0  223.105392             16     0.0  485.602142   \n",
       "1292      7   NaN   3.0  225.352570             16     1.0  486.173004   \n",
       "1293      8   NaN   4.0  207.868134             16     0.0  474.345612   \n",
       "1294      9   NaN   4.0  227.707718             16     1.0  487.156097   \n",
       "1295     10   NaN   5.0  188.778351             16     0.0  461.782257   \n",
       "1296     11   NaN   5.0  227.562576             16     1.0  489.624054   \n",
       "1297     12   NaN   6.0  227.511932             16     0.0  489.879608   \n",
       "1298     13   NaN   6.0  245.028229             16     1.0  502.046844   \n",
       "1299     14   NaN   7.0  185.016464             16     0.0  454.570648   \n",
       "1300     15   NaN   7.0  185.628891             16     1.0  456.299347   \n",
       "1301     16   NaN   8.0  275.961609             16     0.0  530.502441   \n",
       "1302     17   NaN   8.0  276.461182             16     1.0  529.794006   \n",
       "1303     18   NaN   9.0  204.243546             16     0.0  475.338715   \n",
       "1304     19   NaN   9.0  256.168335             16     1.0  512.689575   \n",
       "\n",
       "      test_nmll    train_nll  train_nmll  train_time  \n",
       "0      1.203484   261.778412    1.454324    0.662972  \n",
       "1      1.160030   248.519241    1.380614    0.576021  \n",
       "2      1.135183   263.102417    1.461680    0.705728  \n",
       "3      1.140615   262.532196    1.458512    0.492423  \n",
       "4      1.367627   254.533264    1.414092    0.648426  \n",
       "5      1.282536   258.448120    1.435823    0.515353  \n",
       "6      3.482563   224.592819    1.247738    0.406367  \n",
       "7      4.122632   227.409195    1.263384    0.384980  \n",
       "8      1.202825   258.057495    1.433652    0.667018  \n",
       "9      1.169323   262.110901    1.456172    0.585067  \n",
       "10     1.424685   227.633575    1.260521    0.562112  \n",
       "11     1.365835   257.008362    1.427824    0.578139  \n",
       "12     1.211482   261.918762    1.455104    1.177308  \n",
       "13     1.211493   261.946228    1.455257    1.532951  \n",
       "14     1.588100   253.847214    1.410262    0.782785  \n",
       "15     1.584393   254.573425    1.414297    0.688496  \n",
       "16     1.084967   260.493500    1.447186    0.445363  \n",
       "17     1.106700   259.982971    1.444349    0.857361  \n",
       "18     1.500380   235.445496    1.311137    0.506733  \n",
       "19     1.506253   255.850616    1.421392    0.743413  \n",
       "20     1.122100   246.106522    1.367243    0.645539  \n",
       "21     1.203390   242.386414    1.342957    0.455785  \n",
       "22     1.117914   259.212219    1.440328    0.417651  \n",
       "23     0.637314   134.630280    0.748400    0.687942  \n",
       "24     1.092167   224.752991    1.250167    0.466935  \n",
       "25     1.290240   257.223145    1.429043    0.420621  \n",
       "26     2.661744   225.769455    1.254258    0.442809  \n",
       "27     2.626264   231.618561    1.286913    0.420273  \n",
       "28     0.844797   158.544785    0.881556    0.602624  \n",
       "29     1.020929   209.985657    1.162896    0.438699  \n",
       "...         ...          ...         ...         ...  \n",
       "1275   4.371682  3767.122070    4.062759  170.602079  \n",
       "1276   4.551615  4028.610840    4.345501  222.014159  \n",
       "1277   4.659150  4190.471191    4.520651  192.023334  \n",
       "1278   4.312387  3813.792969    4.113348  199.097547  \n",
       "1279   3.967266  3839.215332    4.141599  178.378920  \n",
       "1280   4.192482  4077.969482    4.398829  220.518709  \n",
       "1281   5.046206  4216.179688    4.548173  253.231534  \n",
       "1282   4.960836  4090.397949    4.412670  232.140371  \n",
       "1283   4.700006  4111.795898    4.435584  264.895025  \n",
       "1284   4.198452  3742.429688    4.037301  175.880916  \n",
       "1285   4.756218  4262.595703    4.598202  371.461235  \n",
       "1286   4.922304  4431.780762    4.780852  399.689448  \n",
       "1287   5.093104  4353.426270    4.696011  386.762144  \n",
       "1288   5.108457  4372.373047    4.716630  399.177316  \n",
       "1289   4.647692  4271.931641    4.608116  366.820050  \n",
       "1290   4.607518  4244.931152    4.579573  361.852239  \n",
       "1291   4.715030  4427.113770    4.775211  410.380397  \n",
       "1292   4.719998  4437.042969    4.786407  404.095735  \n",
       "1293   4.605543  4292.550781    4.630400  354.615576  \n",
       "1294   4.729504  4423.794434    4.772450  411.994870  \n",
       "1295   4.483953  3965.735840    4.277911  317.825090  \n",
       "1296   4.753129  4245.042480    4.579505  360.225515  \n",
       "1297   4.756271  4228.006348    4.561178  351.563224  \n",
       "1298   4.874053  4346.425781    4.688416  385.966979  \n",
       "1299   4.414115  4340.222656    4.682002  380.040651  \n",
       "1300   4.430540  4363.044922    4.706779  391.085977  \n",
       "1301   5.150322  4276.539551    4.613160  355.725018  \n",
       "1302   5.143322  4259.486328    4.595038  358.294146  \n",
       "1303   4.614749  3972.505859    4.284935  312.353994  \n",
       "1304   4.978135  4301.663086    4.641007  372.434925  \n",
       "\n",
       "[1305 rows x 11 columns]"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>error</th>\n",
       "      <th>fold</th>\n",
       "      <th>mse</th>\n",
       "      <th>n_projections</th>\n",
       "      <th>repeat</th>\n",
       "      <th>test_nll</th>\n",
       "      <th>test_nmll</th>\n",
       "      <th>train_nll</th>\n",
       "      <th>train_nmll</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486855</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.069698</td>\n",
       "      <td>1.203484</td>\n",
       "      <td>261.778412</td>\n",
       "      <td>1.454324</td>\n",
       "      <td>0.662972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397469</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.428295</td>\n",
       "      <td>1.122100</td>\n",
       "      <td>246.106522</td>\n",
       "      <td>1.367243</td>\n",
       "      <td>0.645539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.158680</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.724103</td>\n",
       "      <td>0.530556</td>\n",
       "      <td>94.036537</td>\n",
       "      <td>0.520256</td>\n",
       "      <td>0.693838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204252</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.980025</td>\n",
       "      <td>0.600485</td>\n",
       "      <td>80.732178</td>\n",
       "      <td>0.454219</td>\n",
       "      <td>0.948707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088827</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.687931</td>\n",
       "      <td>0.227642</td>\n",
       "      <td>-39.321976</td>\n",
       "      <td>-0.221840</td>\n",
       "      <td>1.058221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076476</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.979223</td>\n",
       "      <td>0.316861</td>\n",
       "      <td>-52.879028</td>\n",
       "      <td>-0.284290</td>\n",
       "      <td>1.132384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055167</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.596035</td>\n",
       "      <td>0.055423</td>\n",
       "      <td>-62.916428</td>\n",
       "      <td>-0.358086</td>\n",
       "      <td>1.372175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049707</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957937</td>\n",
       "      <td>0.048633</td>\n",
       "      <td>-56.815567</td>\n",
       "      <td>-0.326975</td>\n",
       "      <td>2.204242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735402</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.793686</td>\n",
       "      <td>1.739684</td>\n",
       "      <td>-103.460632</td>\n",
       "      <td>-0.574781</td>\n",
       "      <td>1.124098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735402</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.793686</td>\n",
       "      <td>1.739684</td>\n",
       "      <td>-103.460632</td>\n",
       "      <td>-0.574781</td>\n",
       "      <td>1.961373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735402</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.793686</td>\n",
       "      <td>1.739684</td>\n",
       "      <td>-103.460632</td>\n",
       "      <td>-0.574781</td>\n",
       "      <td>3.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735402</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.793686</td>\n",
       "      <td>1.739684</td>\n",
       "      <td>-103.460632</td>\n",
       "      <td>-0.574781</td>\n",
       "      <td>6.243299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735402</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.793686</td>\n",
       "      <td>1.739684</td>\n",
       "      <td>-103.460632</td>\n",
       "      <td>-0.574781</td>\n",
       "      <td>11.200247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>244.826416</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>491.872772</td>\n",
       "      <td>4.775186</td>\n",
       "      <td>4340.371094</td>\n",
       "      <td>4.682216</td>\n",
       "      <td>117.133879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275.582550</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>519.907837</td>\n",
       "      <td>5.047361</td>\n",
       "      <td>4493.709961</td>\n",
       "      <td>4.847638</td>\n",
       "      <td>152.663322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.186081</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>467.022003</td>\n",
       "      <td>4.534254</td>\n",
       "      <td>4088.247559</td>\n",
       "      <td>4.409452</td>\n",
       "      <td>120.452161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.709747</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>451.651794</td>\n",
       "      <td>4.384912</td>\n",
       "      <td>3924.648682</td>\n",
       "      <td>4.233642</td>\n",
       "      <td>190.421624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.086899</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.559998</td>\n",
       "      <td>4.879279</td>\n",
       "      <td>4395.738281</td>\n",
       "      <td>4.742021</td>\n",
       "      <td>406.528468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274.143250</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>523.785522</td>\n",
       "      <td>5.085313</td>\n",
       "      <td>4571.914062</td>\n",
       "      <td>4.931849</td>\n",
       "      <td>521.232278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279.290466</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>528.493652</td>\n",
       "      <td>5.131070</td>\n",
       "      <td>4609.757324</td>\n",
       "      <td>4.972842</td>\n",
       "      <td>644.919273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>284.249786</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>532.811768</td>\n",
       "      <td>5.172683</td>\n",
       "      <td>4640.427246</td>\n",
       "      <td>5.005727</td>\n",
       "      <td>926.966477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.096283</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>533.826538</td>\n",
       "      <td>5.182837</td>\n",
       "      <td>4648.835938</td>\n",
       "      <td>5.014842</td>\n",
       "      <td>1565.749189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>91.099182</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.810211</td>\n",
       "      <td>3.819118</td>\n",
       "      <td>1613.772949</td>\n",
       "      <td>3.547114</td>\n",
       "      <td>27.994812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>92.094994</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>202.241745</td>\n",
       "      <td>3.965478</td>\n",
       "      <td>1630.928101</td>\n",
       "      <td>3.585021</td>\n",
       "      <td>34.665867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>87.313705</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.661240</td>\n",
       "      <td>3.935060</td>\n",
       "      <td>1608.988525</td>\n",
       "      <td>3.536076</td>\n",
       "      <td>43.696445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>84.456726</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>197.823242</td>\n",
       "      <td>3.880985</td>\n",
       "      <td>1604.204102</td>\n",
       "      <td>3.525907</td>\n",
       "      <td>40.699664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>83.481071</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>196.700027</td>\n",
       "      <td>3.856776</td>\n",
       "      <td>1593.127441</td>\n",
       "      <td>3.500379</td>\n",
       "      <td>59.779340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>80.523476</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>195.309372</td>\n",
       "      <td>3.829285</td>\n",
       "      <td>1579.296143</td>\n",
       "      <td>3.470822</td>\n",
       "      <td>82.295341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>76.623596</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.378494</td>\n",
       "      <td>3.793178</td>\n",
       "      <td>1578.163696</td>\n",
       "      <td>3.468875</td>\n",
       "      <td>111.266468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>77.069740</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.320511</td>\n",
       "      <td>3.790446</td>\n",
       "      <td>1574.570068</td>\n",
       "      <td>3.463456</td>\n",
       "      <td>174.566016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>75.704147</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.153610</td>\n",
       "      <td>3.766580</td>\n",
       "      <td>1570.963989</td>\n",
       "      <td>3.452714</td>\n",
       "      <td>285.689137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>67.904251</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188.985519</td>\n",
       "      <td>3.704692</td>\n",
       "      <td>1537.380493</td>\n",
       "      <td>3.380184</td>\n",
       "      <td>405.936971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>68.688332</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188.003006</td>\n",
       "      <td>3.686166</td>\n",
       "      <td>1523.368774</td>\n",
       "      <td>3.348815</td>\n",
       "      <td>919.137026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                              error  fold  \\\n",
       "0        0                                                NaN   0.0   \n",
       "20       0                                                NaN   0.0   \n",
       "40       0                                                NaN   0.0   \n",
       "60       0                                                NaN   0.0   \n",
       "80       0                                                NaN   0.0   \n",
       "100      0                                                NaN   0.0   \n",
       "120      0                                                NaN   0.0   \n",
       "140      0                                                NaN   0.0   \n",
       "160      0                                                NaN   0.0   \n",
       "180      0                                                NaN   0.0   \n",
       "200      0                                                NaN   0.0   \n",
       "220      0                                                NaN   0.0   \n",
       "240      0                                                NaN   0.0   \n",
       "260      0                                                NaN   0.0   \n",
       "280      0                                                NaN   0.0   \n",
       "300      0                                                NaN   0.0   \n",
       "320      0                                                NaN   0.0   \n",
       "340      0                                                NaN   0.0   \n",
       "360      0                                                NaN   0.0   \n",
       "380      0                                                NaN   0.0   \n",
       "400      0                                                NaN   0.0   \n",
       "420      0                                                NaN   0.0   \n",
       "440      0  Traceback (most recent call last):\\n  File \"/h...   NaN   \n",
       "460     16                                                NaN   8.0   \n",
       "480     16                                                NaN   8.0   \n",
       "500     16                                                NaN   8.0   \n",
       "520     16                                                NaN   8.0   \n",
       "540     16                                                NaN   8.0   \n",
       "560     16                                                NaN   8.0   \n",
       "580     16                                                NaN   8.0   \n",
       "600     16                                                NaN   8.0   \n",
       "620     16                                                NaN   8.0   \n",
       "640     16                                                NaN   8.0   \n",
       "660     16                                                NaN   8.0   \n",
       "\n",
       "            mse  n_projections  repeat    test_nll  test_nmll    train_nll  \\\n",
       "0      0.486855              1     0.0   24.069698   1.203484   261.778412   \n",
       "20     0.397469              2     0.0   22.428295   1.122100   246.106522   \n",
       "40     0.158680              4     0.0   10.724103   0.530556    94.036537   \n",
       "60     0.204252              8     0.0   11.980025   0.600485    80.732178   \n",
       "80     0.088827             16     0.0    4.687931   0.227642   -39.321976   \n",
       "100    0.076476             32     0.0    5.979223   0.316861   -52.879028   \n",
       "120    0.055167             64     0.0    1.596035   0.055423   -62.916428   \n",
       "140    0.049707            128     0.0    0.957937   0.048633   -56.815567   \n",
       "160    0.735402            256     0.0   34.793686   1.739684  -103.460632   \n",
       "180    0.735402            512     0.0   34.793686   1.739684  -103.460632   \n",
       "200    0.735402           1024     0.0   34.793686   1.739684  -103.460632   \n",
       "220    0.735402           2048     0.0   34.793686   1.739684  -103.460632   \n",
       "240    0.735402           4096     0.0   34.793686   1.739684  -103.460632   \n",
       "260  244.826416              1     0.0  491.872772   4.775186  4340.371094   \n",
       "280  275.582550              2     0.0  519.907837   5.047361  4493.709961   \n",
       "300  183.186081              4     0.0  467.022003   4.534254  4088.247559   \n",
       "320  166.709747              8     0.0  451.651794   4.384912  3924.648682   \n",
       "340  245.086899             16     0.0  502.559998   4.879279  4395.738281   \n",
       "360  274.143250             32     0.0  523.785522   5.085313  4571.914062   \n",
       "380  279.290466             64     0.0  528.493652   5.131070  4609.757324   \n",
       "400  284.249786            128     0.0  532.811768   5.172683  4640.427246   \n",
       "420  285.096283            256     0.0  533.826538   5.182837  4648.835938   \n",
       "440         NaN            512     NaN         NaN        NaN          NaN   \n",
       "460   91.099182              1     0.0  194.810211   3.819118  1613.772949   \n",
       "480   92.094994              2     0.0  202.241745   3.965478  1630.928101   \n",
       "500   87.313705              4     0.0  200.661240   3.935060  1608.988525   \n",
       "520   84.456726              8     0.0  197.823242   3.880985  1604.204102   \n",
       "540   83.481071             16     0.0  196.700027   3.856776  1593.127441   \n",
       "560   80.523476             32     0.0  195.309372   3.829285  1579.296143   \n",
       "580   76.623596             64     0.0  193.378494   3.793178  1578.163696   \n",
       "600   77.069740            128     0.0  193.320511   3.790446  1574.570068   \n",
       "620   75.704147            256     0.0  192.153610   3.766580  1570.963989   \n",
       "640   67.904251            512     0.0  188.985519   3.704692  1537.380493   \n",
       "660   68.688332           1024     0.0  188.003006   3.686166  1523.368774   \n",
       "\n",
       "     train_nmll   train_time  \n",
       "0      1.454324     0.662972  \n",
       "20     1.367243     0.645539  \n",
       "40     0.520256     0.693838  \n",
       "60     0.454219     0.948707  \n",
       "80    -0.221840     1.058221  \n",
       "100   -0.284290     1.132384  \n",
       "120   -0.358086     1.372175  \n",
       "140   -0.326975     2.204242  \n",
       "160   -0.574781     1.124098  \n",
       "180   -0.574781     1.961373  \n",
       "200   -0.574781     3.000275  \n",
       "220   -0.574781     6.243299  \n",
       "240   -0.574781    11.200247  \n",
       "260    4.682216   117.133879  \n",
       "280    4.847638   152.663322  \n",
       "300    4.409452   120.452161  \n",
       "320    4.233642   190.421624  \n",
       "340    4.742021   406.528468  \n",
       "360    4.931849   521.232278  \n",
       "380    4.972842   644.919273  \n",
       "400    5.005727   926.966477  \n",
       "420    5.014842  1565.749189  \n",
       "440         NaN          NaN  \n",
       "460    3.547114    27.994812  \n",
       "480    3.585021    34.665867  \n",
       "500    3.536076    43.696445  \n",
       "520    3.525907    40.699664  \n",
       "540    3.500379    59.779340  \n",
       "560    3.470822    82.295341  \n",
       "580    3.468875   111.266468  \n",
       "600    3.463456   174.566016  \n",
       "620    3.452714   285.689137  \n",
       "640    3.380184   405.936971  \n",
       "660    3.348815   919.137026  "
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.arange(0, 664, step=20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1c991550b8>"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['n_projections'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0:259, 'dataset'] = 'bach'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[260:443, 'dataset'] = 'concrete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[444:684, 'dataset'] = 'housing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[685:944, 'dataset'] = 'servo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[945:1204, 'dataset'] = 'bach'\n",
    "df.loc[1205:, 'dataset'] = 'concrete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "bach_ard = pd.read_csv('./ard_bach_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.626373147964477"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bach_ard['test_nll'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = df.groupby(['dataset', 'n_projections'])['test_nll'].mean()\n",
    "fig, ax = plt.subplots(4, sharex=True, figsize=(10,6))\n",
    "tmp.loc['bach'].plot(ax=ax[0], logx=True, marker='.')\n",
    "ax[0].axhline(bach_ard['test_nll'].mean(), label='ARD', color='r', )\n",
    "ax[0].legend()\n",
    "ax[0].set_title('bach')\n",
    "tmp.loc['concrete'].plot(ax=ax[1], marker='.')\n",
    "ax[1].set_title('concrete')\n",
    "tmp.loc['housing'].plot(ax=ax[2], marker='.')\n",
    "# ax[2].axhline(159, color='r', label='ARD')\n",
    "ax[2].legend()\n",
    "ax[2].set_title('housing')\n",
    "tmp.loc['servo'].plot(ax=ax[3], marker='.')\n",
    "ax[3].set_title('servo')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import AdditiveKernel, RBFKernel, AdditiveStructureKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdditiveStructureKernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_module = RBFKernel(active_dims=torch.tensor([0])) + RBFKernel(active_dims=torch.tensor([1]))\n",
    "x1 = torch.randn(50, 2)\n",
    "additive_kernel_matrix = covar_module(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.2974e-01, 6.1584e-02, 2.9683e-01, 7.3305e-01, 9.0773e-01,\n",
       "        6.5709e-01, 4.5333e-01, 5.0640e-02, 3.1225e-06, 7.6006e-01, 5.5906e-01,\n",
       "        9.8568e-01, 1.1503e-01, 9.9708e-01, 1.0082e-03, 1.3331e-02, 2.2860e-03,\n",
       "        6.3964e-01, 1.0194e-01, 7.1816e-02, 9.7346e-01, 2.5662e-02, 2.3526e-01,\n",
       "        2.9755e-04, 7.4691e-02, 3.8728e-03, 2.8905e-01, 9.8994e-01, 5.6178e-01,\n",
       "        1.2967e-03, 8.8941e-01, 2.1058e-04, 4.0759e-02, 9.9616e-01, 9.1698e-01,\n",
       "        5.6877e-01, 9.2835e-01, 7.1356e-01, 3.1349e-01, 7.0825e-01, 7.1739e-01,\n",
       "        5.3447e-01, 1.3908e-01, 2.8387e-02, 7.3677e-01, 1.8212e-01, 4.9728e-02,\n",
       "        8.4898e-01, 6.0986e-02], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBFKernel()(x1.index_select(-1, torch.tensor([0]))).evaluate()[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9512, -2.1194],\n",
       "        [-0.4497,  0.0408],\n",
       "        [ 2.5877,  1.4482],\n",
       "        [-0.1292,  0.8974],\n",
       "        [ 0.4049, -1.1008],\n",
       "        [ 0.6462,  0.2432],\n",
       "        [ 0.3159,  1.0221],\n",
       "        [ 0.0793, -1.0760],\n",
       "        [-0.7419,  0.3326],\n",
       "        [-2.5390,  1.3161],\n",
       "        [ 0.4377, -0.6893],\n",
       "        [ 0.2037, -0.0882],\n",
       "        [ 1.0689,  1.8429],\n",
       "        [-0.4904,  2.0375],\n",
       "        [ 1.0042,  0.0210],\n",
       "        [-1.6237,  0.5130],\n",
       "        [-1.0857, -1.8123],\n",
       "        [-1.4661,  0.0312],\n",
       "        [ 0.2959,  0.7716],\n",
       "        [-0.5301, -1.2391],\n",
       "        [-0.6396, -0.4983],\n",
       "        [ 1.1119, -0.2757],\n",
       "        [-0.9249, -1.4485],\n",
       "        [-0.2280, -0.3480],\n",
       "        [-1.8421,  0.2441],\n",
       "        [-0.6277, -0.5349],\n",
       "        [-1.3590,  0.2654],\n",
       "        [-0.1409,  0.4415],\n",
       "        [ 1.0497,  1.3418],\n",
       "        [ 0.2068,  0.1580],\n",
       "        [-1.5763, -1.1919],\n",
       "        [ 1.2868, -0.6950],\n",
       "        [-1.9010, -1.6662],\n",
       "        [-0.8024,  1.3457],\n",
       "        [ 0.8904,  0.2418],\n",
       "        [ 1.2398,  2.4870],\n",
       "        [ 0.2148,  0.5519],\n",
       "        [ 0.6839, -3.1600],\n",
       "        [ 0.3817, -0.5108],\n",
       "        [-0.1046,  1.3431],\n",
       "        [ 0.3754, -0.4858],\n",
       "        [ 0.3862,  0.0126],\n",
       "        [ 0.1753, -0.5711],\n",
       "        [-0.4256, -0.4311],\n",
       "        [ 2.8012, -1.0488],\n",
       "        [ 0.4094, -1.3089],\n",
       "        [-0.3281,  0.1262],\n",
       "        [ 2.6494,  0.1155],\n",
       "        [ 0.5545, -2.6089],\n",
       "        [-0.6883,  0.3849]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1194],\n",
       "        [ 0.0408],\n",
       "        [ 1.4482],\n",
       "        [ 0.8974],\n",
       "        [-1.1008],\n",
       "        [ 0.2432],\n",
       "        [ 1.0221],\n",
       "        [-1.0760],\n",
       "        [ 0.3326],\n",
       "        [ 1.3161],\n",
       "        [-0.6893],\n",
       "        [-0.0882],\n",
       "        [ 1.8429],\n",
       "        [ 2.0375],\n",
       "        [ 0.0210],\n",
       "        [ 0.5130],\n",
       "        [-1.8123],\n",
       "        [ 0.0312],\n",
       "        [ 0.7716],\n",
       "        [-1.2391],\n",
       "        [-0.4983],\n",
       "        [-0.2757],\n",
       "        [-1.4485],\n",
       "        [-0.3480],\n",
       "        [ 0.2441],\n",
       "        [-0.5349],\n",
       "        [ 0.2654],\n",
       "        [ 0.4415],\n",
       "        [ 1.3418],\n",
       "        [ 0.1580],\n",
       "        [-1.1919],\n",
       "        [-0.6950],\n",
       "        [-1.6662],\n",
       "        [ 1.3457],\n",
       "        [ 0.2418],\n",
       "        [ 2.4870],\n",
       "        [ 0.5519],\n",
       "        [-3.1600],\n",
       "        [-0.5108],\n",
       "        [ 1.3431],\n",
       "        [-0.4858],\n",
       "        [ 0.0126],\n",
       "        [-0.5711],\n",
       "        [-0.4311],\n",
       "        [-1.0488],\n",
       "        [-1.3089],\n",
       "        [ 0.1262],\n",
       "        [ 0.1155],\n",
       "        [-2.6089],\n",
       "        [ 0.3849]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.index_select(-1, torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: out of range at /opt/conda/conda-bld/pytorch-nightly-cpu_1542614797320/work/aten/src/TH/generic/THTensor.cpp:348",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-322839dced74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRBFKernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/utils/memoize.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcache_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcache_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memoize_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memoize_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memoize_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\u001b[0m in \u001b[0;36mevaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazily_evaluate_kernels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 self._cached_kernel_eval = self.kernel(\n\u001b[0;32m--> 199\u001b[0;31m                     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 )\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_row\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/kernels/kernel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x1, x2, diag, batch_dims, **params)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# Select the active dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mx1_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx2_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mx2_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx2_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: out of range at /opt/conda/conda-bld/pytorch-nightly-cpu_1542614797320/work/aten/src/TH/generic/THTensor.cpp:348"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/kernels/kernel.py\u001b[0m(313)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    311 \u001b[0;31m        \u001b[0;31m# Select the active dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    312 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 313 \u001b[0;31m            \u001b[0mx1_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    314 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mx2_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    315 \u001b[0;31m                \u001b[0mx2_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx2_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "RBFKernel(active_dims=[1])(x1).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: out of range at /opt/conda/conda-bld/pytorch-nightly-cpu_1542614797320/work/aten/src/TH/generic/THTensor.cpp:348",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-1c5d95f4da3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pdb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRBFKernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/utils/memoize.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcache_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcache_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memoize_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memoize_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memoize_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\u001b[0m in \u001b[0;36mevaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazily_evaluate_kernels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 self._cached_kernel_eval = self.kernel(\n\u001b[0;32m--> 199\u001b[0;31m                     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 )\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_row\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/kernels/kernel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x1, x2, diag, batch_dims, **params)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# Select the active dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mx1_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx2_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mx2_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx2_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: out of range at /opt/conda/conda-bld/pytorch-nightly-cpu_1542614797320/work/aten/src/TH/generic/THTensor.cpp:348"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/gpytorch/kernels/kernel.py\u001b[0m(313)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    311 \u001b[0;31m        \u001b[0;31m# Select the active dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    312 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 313 \u001b[0;31m            \u001b[0mx1_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    314 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mx2_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    315 \u001b[0;31m                \u001b[0mx2_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx2_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%pdb 1\n",
    "RBFKernel(active_dims=[1])(x1).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    K = additive_kernel_matrix.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9511709, -2.119357 ], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.numpy()[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9512, -2.1194],\n",
       "        [-0.4497,  0.0408],\n",
       "        [ 2.5877,  1.4482],\n",
       "        [-0.1292,  0.8974],\n",
       "        [ 0.4049, -1.1008],\n",
       "        [ 0.6462,  0.2432],\n",
       "        [ 0.3159,  1.0221],\n",
       "        [ 0.0793, -1.0760],\n",
       "        [-0.7419,  0.3326],\n",
       "        [-2.5390,  1.3161],\n",
       "        [ 0.4377, -0.6893],\n",
       "        [ 0.2037, -0.0882],\n",
       "        [ 1.0689,  1.8429],\n",
       "        [-0.4904,  2.0375],\n",
       "        [ 1.0042,  0.0210],\n",
       "        [-1.6237,  0.5130],\n",
       "        [-1.0857, -1.8123],\n",
       "        [-1.4661,  0.0312],\n",
       "        [ 0.2959,  0.7716],\n",
       "        [-0.5301, -1.2391],\n",
       "        [-0.6396, -0.4983],\n",
       "        [ 1.1119, -0.2757],\n",
       "        [-0.9249, -1.4485],\n",
       "        [-0.2280, -0.3480],\n",
       "        [-1.8421,  0.2441],\n",
       "        [-0.6277, -0.5349],\n",
       "        [-1.3590,  0.2654],\n",
       "        [-0.1409,  0.4415],\n",
       "        [ 1.0497,  1.3418],\n",
       "        [ 0.2068,  0.1580],\n",
       "        [-1.5763, -1.1919],\n",
       "        [ 1.2868, -0.6950],\n",
       "        [-1.9010, -1.6662],\n",
       "        [-0.8024,  1.3457],\n",
       "        [ 0.8904,  0.2418],\n",
       "        [ 1.2398,  2.4870],\n",
       "        [ 0.2148,  0.5519],\n",
       "        [ 0.6839, -3.1600],\n",
       "        [ 0.3817, -0.5108],\n",
       "        [-0.1046,  1.3431],\n",
       "        [ 0.3754, -0.4858],\n",
       "        [ 0.3862,  0.0126],\n",
       "        [ 0.1753, -0.5711],\n",
       "        [-0.4256, -0.4311],\n",
       "        [ 2.8012, -1.0488],\n",
       "        [ 0.4094, -1.3089],\n",
       "        [-0.3281,  0.1262],\n",
       "        [ 2.6494,  0.1155],\n",
       "        [ 0.5545, -2.6089],\n",
       "        [-0.6883,  0.3849]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k(z1, z2):\n",
    "    return np.exp(-1/2*(z1[0] - z2[0])**2) + np.exp(-1/2*(z1[1] - z2[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt_k(z1, z2):\n",
    "    return np.exp(-1/2*((z1 - z2)**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k(x1.numpy()[0, :], x1.numpy()[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4718611608136692"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k(x1.numpy()[0, :], x1.numpy()[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2637801985719343"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k(x1.numpy()[0, :], x1.numpy()[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036358830302657526"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_k(x1.numpy()[0, :], x1.numpy()[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.dtype"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_datasets():\n",
    "    folders = os.listdir('./uci')\n",
    "    folders = [f for f in folders if os.path.isdir(os.path.join('./uci', f))]\n",
    "    return folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rp_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['challenger', 'fertility', 'concreteslump', 'autos', 'servo',\n",
       "       'breastcancer', 'machine', 'yacht', 'autompg', 'housing', 'forest',\n",
       "       'stock', 'pendulum', 'energy', 'concrete', 'solar', 'airfoil',\n",
       "       'wine', 'gas', 'skillcraft', 'sml', 'parkinsons', 'pumadyn32nm',\n",
       "       'pol', 'elevators', 'bike', 'kin40k', 'protein', 'tamielectric',\n",
       "       'keggdirected', 'slice', 'keggundirected', '3droad', 'song',\n",
       "       'buzz', 'houseelectric'], dtype='<U14')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = get_datasets()\n",
    "lengths = [] \n",
    "for d in datasets:\n",
    "    df = rp_experiments.load_dataset(d)\n",
    "    lengths.append(len(df))\n",
    "idx = np.argsort(lengths)\n",
    "sorted_datasets = np.array(datasets)[idx]\n",
    "sorted_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = rp_experiments.load_dataset('pumadyn32nm')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = rp_experiments.load_dataset('concrete')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./unfitted_rp_compare_ablation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./fitted_rp_compare_ablation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "challenger       619\n",
       "yacht            310\n",
       "stock            294\n",
       "servo            278\n",
       "autos            266\n",
       "pendulum         253\n",
       "energy           251\n",
       "machine          243\n",
       "fertility        239\n",
       "forest           231\n",
       "housing          226\n",
       "autompg          225\n",
       "concreteslump    220\n",
       "breastcancer     220\n",
       "concrete          98\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>J</th>\n",
       "      <th>RP</th>\n",
       "      <th>d</th>\n",
       "      <th>dataset</th>\n",
       "      <th>error</th>\n",
       "      <th>fold</th>\n",
       "      <th>k</th>\n",
       "      <th>mse</th>\n",
       "      <th>n</th>\n",
       "      <th>repeat</th>\n",
       "      <th>test_nll</th>\n",
       "      <th>test_nmll</th>\n",
       "      <th>train_nll</th>\n",
       "      <th>train_nmll</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.047998</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.639138</td>\n",
       "      <td>0.879713</td>\n",
       "      <td>15.938777</td>\n",
       "      <td>0.802482</td>\n",
       "      <td>3.499574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.631643</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.532650</td>\n",
       "      <td>1.177550</td>\n",
       "      <td>-44.532104</td>\n",
       "      <td>-2.226649</td>\n",
       "      <td>3.245336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.289048</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.907558</td>\n",
       "      <td>0.969186</td>\n",
       "      <td>12.207608</td>\n",
       "      <td>0.612222</td>\n",
       "      <td>2.725384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.705085</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.091580</td>\n",
       "      <td>1.030527</td>\n",
       "      <td>14.126294</td>\n",
       "      <td>0.702780</td>\n",
       "      <td>2.813419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.413526</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.358122</td>\n",
       "      <td>7.452707</td>\n",
       "      <td>-0.860893</td>\n",
       "      <td>-0.070002</td>\n",
       "      <td>1.152074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.101079</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.880436</td>\n",
       "      <td>0.626812</td>\n",
       "      <td>15.446164</td>\n",
       "      <td>0.790465</td>\n",
       "      <td>3.671856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.440934</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.940188</td>\n",
       "      <td>0.980063</td>\n",
       "      <td>14.799351</td>\n",
       "      <td>0.738958</td>\n",
       "      <td>2.370483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.170012</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.681394</td>\n",
       "      <td>0.840697</td>\n",
       "      <td>16.300705</td>\n",
       "      <td>0.765093</td>\n",
       "      <td>1.808852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>challenger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.389495</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.093950</td>\n",
       "      <td>1.031317</td>\n",
       "      <td>21.473656</td>\n",
       "      <td>1.053268</td>\n",
       "      <td>1.240371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>426.059631</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>465.857208</td>\n",
       "      <td>4.522400</td>\n",
       "      <td>3513.045654</td>\n",
       "      <td>3.789535</td>\n",
       "      <td>44.945050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74.299774</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.365906</td>\n",
       "      <td>3.548427</td>\n",
       "      <td>3366.599121</td>\n",
       "      <td>3.632607</td>\n",
       "      <td>48.242240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>Traceback (most recent call last):\\n  File \"/h...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154.237488</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>405.652557</td>\n",
       "      <td>3.939266</td>\n",
       "      <td>3388.132324</td>\n",
       "      <td>3.654821</td>\n",
       "      <td>56.477083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105.812134</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381.378357</td>\n",
       "      <td>3.702277</td>\n",
       "      <td>3291.422852</td>\n",
       "      <td>3.550130</td>\n",
       "      <td>43.322761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>7</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>102.778564</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>384.271027</td>\n",
       "      <td>3.731615</td>\n",
       "      <td>3559.031494</td>\n",
       "      <td>3.839285</td>\n",
       "      <td>32.081798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123.365341</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>388.676392</td>\n",
       "      <td>3.770835</td>\n",
       "      <td>3472.784668</td>\n",
       "      <td>3.745676</td>\n",
       "      <td>46.156782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3950</th>\n",
       "      <td>9</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>111.734367</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>388.143951</td>\n",
       "      <td>3.768409</td>\n",
       "      <td>3561.120850</td>\n",
       "      <td>3.842275</td>\n",
       "      <td>45.182536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951</th>\n",
       "      <td>10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.818626</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>371.964294</td>\n",
       "      <td>3.610154</td>\n",
       "      <td>3461.269043</td>\n",
       "      <td>3.734284</td>\n",
       "      <td>60.263929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3952</th>\n",
       "      <td>11</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.535164</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>389.108734</td>\n",
       "      <td>3.778241</td>\n",
       "      <td>3495.256592</td>\n",
       "      <td>3.771160</td>\n",
       "      <td>41.174428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3953</th>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.353210</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>406.869385</td>\n",
       "      <td>3.948837</td>\n",
       "      <td>3358.445312</td>\n",
       "      <td>3.622278</td>\n",
       "      <td>56.621021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3954</th>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>478.078400</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>473.450043</td>\n",
       "      <td>4.597026</td>\n",
       "      <td>3399.484375</td>\n",
       "      <td>3.666851</td>\n",
       "      <td>59.051496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3955</th>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.685123</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>339.671661</td>\n",
       "      <td>3.296205</td>\n",
       "      <td>3265.668701</td>\n",
       "      <td>3.523572</td>\n",
       "      <td>97.146129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3956</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144.755096</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>403.968018</td>\n",
       "      <td>3.921975</td>\n",
       "      <td>3259.202881</td>\n",
       "      <td>3.516346</td>\n",
       "      <td>56.285182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3957</th>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>143.817413</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>397.025482</td>\n",
       "      <td>3.855250</td>\n",
       "      <td>3331.773926</td>\n",
       "      <td>3.594292</td>\n",
       "      <td>60.982040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3958</th>\n",
       "      <td>5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.735825</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>370.925812</td>\n",
       "      <td>3.601603</td>\n",
       "      <td>3353.390137</td>\n",
       "      <td>3.617283</td>\n",
       "      <td>65.181871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3959</th>\n",
       "      <td>6</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>115.281616</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>378.781860</td>\n",
       "      <td>3.677911</td>\n",
       "      <td>3314.750488</td>\n",
       "      <td>3.576153</td>\n",
       "      <td>68.006822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3960</th>\n",
       "      <td>7</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74.225380</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.894226</td>\n",
       "      <td>3.553377</td>\n",
       "      <td>3396.558838</td>\n",
       "      <td>3.665021</td>\n",
       "      <td>65.601569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3961</th>\n",
       "      <td>8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>106.753510</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381.984772</td>\n",
       "      <td>3.710602</td>\n",
       "      <td>3410.785889</td>\n",
       "      <td>3.679975</td>\n",
       "      <td>48.922977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962</th>\n",
       "      <td>9</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>91.254204</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>372.733154</td>\n",
       "      <td>3.614942</td>\n",
       "      <td>3341.489990</td>\n",
       "      <td>3.604311</td>\n",
       "      <td>62.664575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>153.152664</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>412.100525</td>\n",
       "      <td>4.000880</td>\n",
       "      <td>3895.951904</td>\n",
       "      <td>4.203095</td>\n",
       "      <td>10.586636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>854.075928</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>569.246216</td>\n",
       "      <td>5.526286</td>\n",
       "      <td>3786.396973</td>\n",
       "      <td>4.084448</td>\n",
       "      <td>10.626095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3965</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>161.736008</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>412.957214</td>\n",
       "      <td>4.008985</td>\n",
       "      <td>3898.524658</td>\n",
       "      <td>4.205553</td>\n",
       "      <td>10.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3966</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>323.539825</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>444.588654</td>\n",
       "      <td>4.316245</td>\n",
       "      <td>3880.109375</td>\n",
       "      <td>4.185746</td>\n",
       "      <td>9.706180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>221.581787</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>425.028076</td>\n",
       "      <td>4.126282</td>\n",
       "      <td>3906.993652</td>\n",
       "      <td>4.214819</td>\n",
       "      <td>11.610697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3968</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>237.306793</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>425.798126</td>\n",
       "      <td>4.134073</td>\n",
       "      <td>3838.180420</td>\n",
       "      <td>4.140192</td>\n",
       "      <td>12.898263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3969</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>339.559235</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>444.385101</td>\n",
       "      <td>4.314382</td>\n",
       "      <td>3850.325928</td>\n",
       "      <td>4.153371</td>\n",
       "      <td>7.925459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>239.583435</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>427.128632</td>\n",
       "      <td>4.146728</td>\n",
       "      <td>3893.973389</td>\n",
       "      <td>4.200436</td>\n",
       "      <td>7.930025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>153.354828</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>409.265198</td>\n",
       "      <td>3.973175</td>\n",
       "      <td>3814.206299</td>\n",
       "      <td>4.114583</td>\n",
       "      <td>7.936750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3972</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>concrete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>158.172043</td>\n",
       "      <td>1030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>413.782043</td>\n",
       "      <td>4.017474</td>\n",
       "      <td>3919.675537</td>\n",
       "      <td>4.228492</td>\n",
       "      <td>11.110586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3973 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     J     RP  d     dataset  \\\n",
       "0              0   NaN  False  2  challenger   \n",
       "1              1   NaN  False  2  challenger   \n",
       "2              2   NaN  False  2  challenger   \n",
       "3              3   NaN  False  2  challenger   \n",
       "4              4   NaN  False  2  challenger   \n",
       "5              5   NaN  False  2  challenger   \n",
       "6              6   NaN  False  2  challenger   \n",
       "7              7   NaN  False  2  challenger   \n",
       "8              8   NaN  False  2  challenger   \n",
       "9              9   NaN  False  2  challenger   \n",
       "10            10   NaN  False  2  challenger   \n",
       "11            11   NaN  False  2  challenger   \n",
       "12            12   NaN  False  2  challenger   \n",
       "13            13   NaN  False  2  challenger   \n",
       "14            14   NaN  False  2  challenger   \n",
       "15            15   NaN  False  2  challenger   \n",
       "16            16   NaN  False  2  challenger   \n",
       "17            17   NaN  False  2  challenger   \n",
       "18            18   NaN  False  2  challenger   \n",
       "19            19   NaN  False  2  challenger   \n",
       "20            20   NaN  False  2  challenger   \n",
       "21            21   NaN  False  2  challenger   \n",
       "22            22   NaN  False  2  challenger   \n",
       "23            23   NaN  False  2  challenger   \n",
       "24            24   NaN  False  2  challenger   \n",
       "25            25   NaN  False  2  challenger   \n",
       "26            26   NaN  False  2  challenger   \n",
       "27            27   NaN  False  2  challenger   \n",
       "28            28   NaN  False  2  challenger   \n",
       "29             0   1.0   True  2  challenger   \n",
       "...          ...   ...    ... ..         ...   \n",
       "3943           2  13.0   True  6    concrete   \n",
       "3944           3  13.0   True  6    concrete   \n",
       "3945           4  13.0   True  6    concrete   \n",
       "3946           5  13.0   True  6    concrete   \n",
       "3947           6  13.0   True  6    concrete   \n",
       "3948           7  13.0   True  6    concrete   \n",
       "3949           8  13.0   True  6    concrete   \n",
       "3950           9  13.0   True  6    concrete   \n",
       "3951          10  13.0   True  6    concrete   \n",
       "3952          11  13.0   True  6    concrete   \n",
       "3953           0  20.0   True  6    concrete   \n",
       "3954           1  20.0   True  6    concrete   \n",
       "3955           2  20.0   True  6    concrete   \n",
       "3956           3  20.0   True  6    concrete   \n",
       "3957           4  20.0   True  6    concrete   \n",
       "3958           5  20.0   True  6    concrete   \n",
       "3959           6  20.0   True  6    concrete   \n",
       "3960           7  20.0   True  6    concrete   \n",
       "3961           8  20.0   True  6    concrete   \n",
       "3962           9  20.0   True  6    concrete   \n",
       "3963           0   1.0   True  6    concrete   \n",
       "3964           1   1.0   True  6    concrete   \n",
       "3965           2   1.0   True  6    concrete   \n",
       "3966           3   1.0   True  6    concrete   \n",
       "3967           4   1.0   True  6    concrete   \n",
       "3968           5   1.0   True  6    concrete   \n",
       "3969           6   1.0   True  6    concrete   \n",
       "3970           7   1.0   True  6    concrete   \n",
       "3971           8   1.0   True  6    concrete   \n",
       "3972           9   1.0   True  6    concrete   \n",
       "\n",
       "                                                  error  fold    k  \\\n",
       "0                                                   NaN     0  NaN   \n",
       "1                                                   NaN     1  NaN   \n",
       "2     Traceback (most recent call last):\\n  File \"/h...     2  NaN   \n",
       "3                                                   NaN     2  NaN   \n",
       "4                                                   NaN     3  NaN   \n",
       "5                                                   NaN     4  NaN   \n",
       "6                                                   NaN     5  NaN   \n",
       "7                                                   NaN     6  NaN   \n",
       "8                                                   NaN     7  NaN   \n",
       "9     Traceback (most recent call last):\\n  File \"/h...     8  NaN   \n",
       "10    Traceback (most recent call last):\\n  File \"/h...     8  NaN   \n",
       "11    Traceback (most recent call last):\\n  File \"/h...     8  NaN   \n",
       "12    Traceback (most recent call last):\\n  File \"/h...     8  NaN   \n",
       "13    Traceback (most recent call last):\\n  File \"/h...     8  NaN   \n",
       "14    Traceback (most recent call last):\\n  File \"/h...     8  NaN   \n",
       "15    Traceback (most recent call last):\\n  File \"/h...     8  NaN   \n",
       "16    Traceback (most recent call last):\\n  File \"/h...     8  NaN   \n",
       "17    Traceback (most recent call last):\\n  File \"/h...     8  NaN   \n",
       "18    Traceback (most recent call last):\\n  File \"/h...     8  NaN   \n",
       "19    Traceback (most recent call last):\\n  File \"/h...     9  NaN   \n",
       "20    Traceback (most recent call last):\\n  File \"/h...     9  NaN   \n",
       "21    Traceback (most recent call last):\\n  File \"/h...     9  NaN   \n",
       "22    Traceback (most recent call last):\\n  File \"/h...     9  NaN   \n",
       "23    Traceback (most recent call last):\\n  File \"/h...     9  NaN   \n",
       "24    Traceback (most recent call last):\\n  File \"/h...     9  NaN   \n",
       "25    Traceback (most recent call last):\\n  File \"/h...     9  NaN   \n",
       "26    Traceback (most recent call last):\\n  File \"/h...     9  NaN   \n",
       "27    Traceback (most recent call last):\\n  File \"/h...     9  NaN   \n",
       "28    Traceback (most recent call last):\\n  File \"/h...     9  NaN   \n",
       "29                                                  NaN     0  1.0   \n",
       "...                                                 ...   ...  ...   \n",
       "3943                                                NaN     1  1.0   \n",
       "3944                                                NaN     2  1.0   \n",
       "3945  Traceback (most recent call last):\\n  File \"/h...     3  1.0   \n",
       "3946                                                NaN     3  1.0   \n",
       "3947                                                NaN     4  1.0   \n",
       "3948                                                NaN     5  1.0   \n",
       "3949                                                NaN     6  1.0   \n",
       "3950                                                NaN     7  1.0   \n",
       "3951                                                NaN     8  1.0   \n",
       "3952                                                NaN     9  1.0   \n",
       "3953                                                NaN     0  1.0   \n",
       "3954                                                NaN     1  1.0   \n",
       "3955                                                NaN     2  1.0   \n",
       "3956                                                NaN     3  1.0   \n",
       "3957                                                NaN     4  1.0   \n",
       "3958                                                NaN     5  1.0   \n",
       "3959                                                NaN     6  1.0   \n",
       "3960                                                NaN     7  1.0   \n",
       "3961                                                NaN     8  1.0   \n",
       "3962                                                NaN     9  1.0   \n",
       "3963                                                NaN     0  4.0   \n",
       "3964                                                NaN     1  4.0   \n",
       "3965                                                NaN     2  4.0   \n",
       "3966                                                NaN     3  4.0   \n",
       "3967                                                NaN     4  4.0   \n",
       "3968                                                NaN     5  4.0   \n",
       "3969                                                NaN     6  4.0   \n",
       "3970                                                NaN     7  4.0   \n",
       "3971                                                NaN     8  4.0   \n",
       "3972                                                NaN     9  4.0   \n",
       "\n",
       "             mse     n  repeat    test_nll  test_nmll    train_nll  \\\n",
       "0       0.047998    23     0.0    2.639138   0.879713    15.938777   \n",
       "1       0.631643    23     0.0    3.532650   1.177550   -44.532104   \n",
       "2            NaN    23     NaN         NaN        NaN          NaN   \n",
       "3       0.289048    23     0.0    2.907558   0.969186    12.207608   \n",
       "4       0.705085    23     0.0    3.091580   1.030527    14.126294   \n",
       "5       1.413526    23     0.0   22.358122   7.452707    -0.860893   \n",
       "6       0.101079    23     0.0    1.880436   0.626812    15.446164   \n",
       "7       0.440934    23     0.0    2.940188   0.980063    14.799351   \n",
       "8       0.170012    23     0.0    1.681394   0.840697    16.300705   \n",
       "9            NaN    23     NaN         NaN        NaN          NaN   \n",
       "10           NaN    23     NaN         NaN        NaN          NaN   \n",
       "11           NaN    23     NaN         NaN        NaN          NaN   \n",
       "12           NaN    23     NaN         NaN        NaN          NaN   \n",
       "13           NaN    23     NaN         NaN        NaN          NaN   \n",
       "14           NaN    23     NaN         NaN        NaN          NaN   \n",
       "15           NaN    23     NaN         NaN        NaN          NaN   \n",
       "16           NaN    23     NaN         NaN        NaN          NaN   \n",
       "17           NaN    23     NaN         NaN        NaN          NaN   \n",
       "18           NaN    23     NaN         NaN        NaN          NaN   \n",
       "19           NaN    23     NaN         NaN        NaN          NaN   \n",
       "20           NaN    23     NaN         NaN        NaN          NaN   \n",
       "21           NaN    23     NaN         NaN        NaN          NaN   \n",
       "22           NaN    23     NaN         NaN        NaN          NaN   \n",
       "23           NaN    23     NaN         NaN        NaN          NaN   \n",
       "24           NaN    23     NaN         NaN        NaN          NaN   \n",
       "25           NaN    23     NaN         NaN        NaN          NaN   \n",
       "26           NaN    23     NaN         NaN        NaN          NaN   \n",
       "27           NaN    23     NaN         NaN        NaN          NaN   \n",
       "28           NaN    23     NaN         NaN        NaN          NaN   \n",
       "29      0.389495    23     0.0    3.093950   1.031317    21.473656   \n",
       "...          ...   ...     ...         ...        ...          ...   \n",
       "3943  426.059631  1030     0.0  465.857208   4.522400  3513.045654   \n",
       "3944   74.299774  1030     0.0  365.365906   3.548427  3366.599121   \n",
       "3945         NaN  1030     NaN         NaN        NaN          NaN   \n",
       "3946  154.237488  1030     0.0  405.652557   3.939266  3388.132324   \n",
       "3947  105.812134  1030     0.0  381.378357   3.702277  3291.422852   \n",
       "3948  102.778564  1030     0.0  384.271027   3.731615  3559.031494   \n",
       "3949  123.365341  1030     0.0  388.676392   3.770835  3472.784668   \n",
       "3950  111.734367  1030     0.0  388.143951   3.768409  3561.120850   \n",
       "3951   76.818626  1030     0.0  371.964294   3.610154  3461.269043   \n",
       "3952  125.535164  1030     0.0  389.108734   3.778241  3495.256592   \n",
       "3953  177.353210  1030     0.0  406.869385   3.948837  3358.445312   \n",
       "3954  478.078400  1030     0.0  473.450043   4.597026  3399.484375   \n",
       "3955   36.685123  1030     0.0  339.671661   3.296205  3265.668701   \n",
       "3956  144.755096  1030     0.0  403.968018   3.921975  3259.202881   \n",
       "3957  143.817413  1030     0.0  397.025482   3.855250  3331.773926   \n",
       "3958   87.735825  1030     0.0  370.925812   3.601603  3353.390137   \n",
       "3959  115.281616  1030     0.0  378.781860   3.677911  3314.750488   \n",
       "3960   74.225380  1030     0.0  365.894226   3.553377  3396.558838   \n",
       "3961  106.753510  1030     0.0  381.984772   3.710602  3410.785889   \n",
       "3962   91.254204  1030     0.0  372.733154   3.614942  3341.489990   \n",
       "3963  153.152664  1030     0.0  412.100525   4.000880  3895.951904   \n",
       "3964  854.075928  1030     0.0  569.246216   5.526286  3786.396973   \n",
       "3965  161.736008  1030     0.0  412.957214   4.008985  3898.524658   \n",
       "3966  323.539825  1030     0.0  444.588654   4.316245  3880.109375   \n",
       "3967  221.581787  1030     0.0  425.028076   4.126282  3906.993652   \n",
       "3968  237.306793  1030     0.0  425.798126   4.134073  3838.180420   \n",
       "3969  339.559235  1030     0.0  444.385101   4.314382  3850.325928   \n",
       "3970  239.583435  1030     0.0  427.128632   4.146728  3893.973389   \n",
       "3971  153.354828  1030     0.0  409.265198   3.973175  3814.206299   \n",
       "3972  158.172043  1030     0.0  413.782043   4.017474  3919.675537   \n",
       "\n",
       "      train_nmll  train_time  \n",
       "0       0.802482    3.499574  \n",
       "1      -2.226649    3.245336  \n",
       "2            NaN         NaN  \n",
       "3       0.612222    2.725384  \n",
       "4       0.702780    2.813419  \n",
       "5      -0.070002    1.152074  \n",
       "6       0.790465    3.671856  \n",
       "7       0.738958    2.370483  \n",
       "8       0.765093    1.808852  \n",
       "9            NaN         NaN  \n",
       "10           NaN         NaN  \n",
       "11           NaN         NaN  \n",
       "12           NaN         NaN  \n",
       "13           NaN         NaN  \n",
       "14           NaN         NaN  \n",
       "15           NaN         NaN  \n",
       "16           NaN         NaN  \n",
       "17           NaN         NaN  \n",
       "18           NaN         NaN  \n",
       "19           NaN         NaN  \n",
       "20           NaN         NaN  \n",
       "21           NaN         NaN  \n",
       "22           NaN         NaN  \n",
       "23           NaN         NaN  \n",
       "24           NaN         NaN  \n",
       "25           NaN         NaN  \n",
       "26           NaN         NaN  \n",
       "27           NaN         NaN  \n",
       "28           NaN         NaN  \n",
       "29      1.053268    1.240371  \n",
       "...          ...         ...  \n",
       "3943    3.789535   44.945050  \n",
       "3944    3.632607   48.242240  \n",
       "3945         NaN         NaN  \n",
       "3946    3.654821   56.477083  \n",
       "3947    3.550130   43.322761  \n",
       "3948    3.839285   32.081798  \n",
       "3949    3.745676   46.156782  \n",
       "3950    3.842275   45.182536  \n",
       "3951    3.734284   60.263929  \n",
       "3952    3.771160   41.174428  \n",
       "3953    3.622278   56.621021  \n",
       "3954    3.666851   59.051496  \n",
       "3955    3.523572   97.146129  \n",
       "3956    3.516346   56.285182  \n",
       "3957    3.594292   60.982040  \n",
       "3958    3.617283   65.181871  \n",
       "3959    3.576153   68.006822  \n",
       "3960    3.665021   65.601569  \n",
       "3961    3.679975   48.922977  \n",
       "3962    3.604311   62.664575  \n",
       "3963    4.203095   10.586636  \n",
       "3964    4.084448   10.626095  \n",
       "3965    4.205553   10.507500  \n",
       "3966    4.185746    9.706180  \n",
       "3967    4.214819   11.610697  \n",
       "3968    4.140192   12.898263  \n",
       "3969    4.153371    7.925459  \n",
       "3970    4.200436    7.930025  \n",
       "3971    4.114583    7.936750  \n",
       "3972    4.228492   11.110586  \n",
       "\n",
       "[3973 rows x 16 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[df['error'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'J', 'RP', 'd', 'dataset', 'error', 'fold', 'k', 'mse',\n",
       "       'n', 'repeat', 'test_nll', 'test_nmll', 'train_nll', 'train_nmll',\n",
       "       'train_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Interpreting tuple 'by' as a list of keys, rather than a single key. Use 'by=[...]' instead of 'by=(...)'. In the future, a tuple will always mean a single key.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset  RP    k     J   \n",
       "autompg  True  1.0   1.0     10\n",
       "                     2.0     10\n",
       "                     3.0     10\n",
       "                     5.0     10\n",
       "                     8.0     10\n",
       "                     13.0    10\n",
       "                     20.0    10\n",
       "               4.0   1.0     10\n",
       "                     2.0     10\n",
       "                     3.0     10\n",
       "                     5.0     10\n",
       "                     8.0     10\n",
       "                     13.0    10\n",
       "                     20.0    10\n",
       "               10.0  1.0     10\n",
       "                     2.0     10\n",
       "                     3.0     10\n",
       "                     5.0     10\n",
       "                     8.0     10\n",
       "                     13.0    10\n",
       "                     20.0    10\n",
       "autos    True  1.0   1.0     10\n",
       "                     2.0     10\n",
       "                     3.0     10\n",
       "                     5.0     10\n",
       "                     8.0     10\n",
       "                     13.0    10\n",
       "                     20.0     9\n",
       "               4.0   1.0     10\n",
       "                     2.0     10\n",
       "                             ..\n",
       "stock    True  4.0   13.0    10\n",
       "                     20.0    10\n",
       "               10.0  1.0     10\n",
       "                     2.0     10\n",
       "                     3.0     10\n",
       "                     5.0     10\n",
       "                     8.0     10\n",
       "                     13.0    10\n",
       "                     20.0    10\n",
       "yacht    True  1.0   1.0     10\n",
       "                     2.0     10\n",
       "                     3.0      9\n",
       "                     5.0     10\n",
       "                     8.0      7\n",
       "                     13.0     5\n",
       "                     20.0     7\n",
       "               4.0   1.0     10\n",
       "                     2.0      6\n",
       "                     3.0      6\n",
       "                     5.0      9\n",
       "                     8.0      9\n",
       "                     13.0     8\n",
       "                     20.0     7\n",
       "               10.0  1.0      7\n",
       "                     2.0      8\n",
       "                     3.0     10\n",
       "                     5.0      8\n",
       "                     8.0      8\n",
       "                     13.0     6\n",
       "                     20.0     9\n",
       "Name: test_nll, Length: 302, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.groupby(('dataset', 'RP', 'k', 'J'))['test_nll'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>0.862735</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>44.172039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>1.055651</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>26.072039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>3.551340</td>\n",
       "      <td>4.452039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>5.055221</td>\n",
       "      <td>5.232039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.790075</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>4.976069</td>\n",
       "      <td>8.482039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.145138</td>\n",
       "      <td>0.464818</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-1.291914</td>\n",
       "      <td>0.701883</td>\n",
       "      <td>11.212039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.945704</td>\n",
       "      <td>0.244603</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>5.055221</td>\n",
       "      <td>7.882039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.945704</td>\n",
       "      <td>0.244603</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>0.632039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.145138</td>\n",
       "      <td>0.464818</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-1.291914</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>10.032039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.854740</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>3.472039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.790075</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>0.701883</td>\n",
       "      <td>2.252039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>-0.790075</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-7.797961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1.400222</td>\n",
       "      <td>-0.305934</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>3.551340</td>\n",
       "      <td>7.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>-0.872367</td>\n",
       "      <td>1.345678</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-1.291914</td>\n",
       "      <td>0.701883</td>\n",
       "      <td>6.512039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.218476</td>\n",
       "      <td>0.024388</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-1.291914</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>11.992039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.945704</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-1.291914</td>\n",
       "      <td>0.701883</td>\n",
       "      <td>17.092039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>-1.354634</td>\n",
       "      <td>1.570529</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.952763</td>\n",
       "      <td>0.415580</td>\n",
       "      <td>0.701883</td>\n",
       "      <td>3.542039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>-0.416042</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-1.291914</td>\n",
       "      <td>5.055221</td>\n",
       "      <td>20.322039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.945704</td>\n",
       "      <td>0.244603</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>0.701883</td>\n",
       "      <td>4.742039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1.854740</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>2.126612</td>\n",
       "      <td>6.802039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1.400222</td>\n",
       "      <td>-0.305934</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>2.126612</td>\n",
       "      <td>6.022039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>-1.354634</td>\n",
       "      <td>1.570529</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.952763</td>\n",
       "      <td>0.415580</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-7.577961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>-1.354634</td>\n",
       "      <td>1.570529</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.952763</td>\n",
       "      <td>0.415580</td>\n",
       "      <td>-0.675355</td>\n",
       "      <td>-27.757961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>-1.354634</td>\n",
       "      <td>1.570529</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.952763</td>\n",
       "      <td>0.415580</td>\n",
       "      <td>2.126612</td>\n",
       "      <td>8.392039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.945704</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-1.291914</td>\n",
       "      <td>5.055221</td>\n",
       "      <td>16.702039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.945704</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-1.291914</td>\n",
       "      <td>3.551340</td>\n",
       "      <td>17.482039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.945704</td>\n",
       "      <td>0.244603</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>3.551340</td>\n",
       "      <td>5.332039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>-0.416042</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-1.291914</td>\n",
       "      <td>2.126612</td>\n",
       "      <td>16.302039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1.400222</td>\n",
       "      <td>-0.305934</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>1.612039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1.854740</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>-0.612034</td>\n",
       "      <td>2.782039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>-1.332626</td>\n",
       "      <td>1.074465</td>\n",
       "      <td>1.179925</td>\n",
       "      <td>-0.377784</td>\n",
       "      <td>0.785983</td>\n",
       "      <td>-1.161599</td>\n",
       "      <td>0.146172</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>8.792039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1001</td>\n",
       "      <td>0.159150</td>\n",
       "      <td>0.733712</td>\n",
       "      <td>0.823658</td>\n",
       "      <td>0.924067</td>\n",
       "      <td>-0.034259</td>\n",
       "      <td>-1.215616</td>\n",
       "      <td>-1.475261</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>17.702039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1002</td>\n",
       "      <td>0.384016</td>\n",
       "      <td>1.046649</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.418312</td>\n",
       "      <td>-0.268614</td>\n",
       "      <td>-1.323649</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>21.402039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1003</td>\n",
       "      <td>0.811741</td>\n",
       "      <td>1.310907</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.455775</td>\n",
       "      <td>0.066178</td>\n",
       "      <td>-1.911400</td>\n",
       "      <td>-0.208048</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>30.092039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1004</td>\n",
       "      <td>-0.013089</td>\n",
       "      <td>0.637513</td>\n",
       "      <td>0.722091</td>\n",
       "      <td>-0.429296</td>\n",
       "      <td>0.551628</td>\n",
       "      <td>-1.901111</td>\n",
       "      <td>0.390635</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>17.012039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1005</td>\n",
       "      <td>-0.278144</td>\n",
       "      <td>0.268942</td>\n",
       "      <td>0.334573</td>\n",
       "      <td>0.572848</td>\n",
       "      <td>0.350753</td>\n",
       "      <td>-1.767356</td>\n",
       "      <td>0.596432</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-2.417961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>1006</td>\n",
       "      <td>-1.115414</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>1.503377</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.919901</td>\n",
       "      <td>0.647955</td>\n",
       "      <td>-0.557280</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-17.787961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>1007</td>\n",
       "      <td>-1.201533</td>\n",
       "      <td>1.965756</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.059346</td>\n",
       "      <td>0.752504</td>\n",
       "      <td>0.631236</td>\n",
       "      <td>-0.946424</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>1.542039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>1008</td>\n",
       "      <td>-1.157517</td>\n",
       "      <td>1.322497</td>\n",
       "      <td>1.440874</td>\n",
       "      <td>1.013042</td>\n",
       "      <td>0.852942</td>\n",
       "      <td>-1.854812</td>\n",
       "      <td>-0.796753</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-0.507961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>1009</td>\n",
       "      <td>0.162020</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>0.825221</td>\n",
       "      <td>0.226312</td>\n",
       "      <td>-0.017520</td>\n",
       "      <td>-1.207899</td>\n",
       "      <td>0.519102</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>6.822039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>1010</td>\n",
       "      <td>0.351482</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>1.129922</td>\n",
       "      <td>1.317431</td>\n",
       "      <td>-0.084478</td>\n",
       "      <td>-1.445829</td>\n",
       "      <td>-0.461241</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>4.242039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1011</td>\n",
       "      <td>0.058677</td>\n",
       "      <td>0.540155</td>\n",
       "      <td>0.620524</td>\n",
       "      <td>0.282507</td>\n",
       "      <td>0.501409</td>\n",
       "      <td>-0.881228</td>\n",
       "      <td>-0.968875</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>7.982039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>1012</td>\n",
       "      <td>0.425162</td>\n",
       "      <td>1.072147</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.354369</td>\n",
       "      <td>0.451190</td>\n",
       "      <td>-1.174460</td>\n",
       "      <td>0.204793</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>25.422039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1013</td>\n",
       "      <td>0.715097</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>1.365871</td>\n",
       "      <td>0.549433</td>\n",
       "      <td>0.802723</td>\n",
       "      <td>-2.205919</td>\n",
       "      <td>0.060112</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>5.052039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>1014</td>\n",
       "      <td>-1.427357</td>\n",
       "      <td>1.536917</td>\n",
       "      <td>1.667448</td>\n",
       "      <td>-0.124907</td>\n",
       "      <td>-0.117958</td>\n",
       "      <td>-1.363518</td>\n",
       "      <td>-0.473714</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-2.507961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>1015</td>\n",
       "      <td>0.395499</td>\n",
       "      <td>0.865841</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.198215</td>\n",
       "      <td>0.384232</td>\n",
       "      <td>-0.281901</td>\n",
       "      <td>-0.799248</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>16.612039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>1016</td>\n",
       "      <td>-1.119242</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>2.279976</td>\n",
       "      <td>-0.017199</td>\n",
       "      <td>1.070557</td>\n",
       "      <td>-1.589873</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-20.727961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1017</td>\n",
       "      <td>0.312250</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>0.912725</td>\n",
       "      <td>-0.546369</td>\n",
       "      <td>0.652066</td>\n",
       "      <td>-0.612431</td>\n",
       "      <td>0.116238</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>2.642039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>1018</td>\n",
       "      <td>0.384973</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>1.151798</td>\n",
       "      <td>0.043678</td>\n",
       "      <td>0.886421</td>\n",
       "      <td>-1.322363</td>\n",
       "      <td>0.076326</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>1.452039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>1019</td>\n",
       "      <td>-1.353677</td>\n",
       "      <td>1.043172</td>\n",
       "      <td>1.148673</td>\n",
       "      <td>2.581819</td>\n",
       "      <td>-0.067739</td>\n",
       "      <td>-1.341654</td>\n",
       "      <td>-1.471519</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-0.587961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>1020</td>\n",
       "      <td>0.069203</td>\n",
       "      <td>0.545950</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.195150</td>\n",
       "      <td>0.133137</td>\n",
       "      <td>-0.836214</td>\n",
       "      <td>0.697460</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>6.322039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>1021</td>\n",
       "      <td>0.162977</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>0.825221</td>\n",
       "      <td>1.317431</td>\n",
       "      <td>0.819463</td>\n",
       "      <td>-1.200182</td>\n",
       "      <td>-0.366450</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-3.937961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>1022</td>\n",
       "      <td>-0.159491</td>\n",
       "      <td>0.430047</td>\n",
       "      <td>0.504893</td>\n",
       "      <td>0.652457</td>\n",
       "      <td>-0.050999</td>\n",
       "      <td>-1.804653</td>\n",
       "      <td>0.209782</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>5.722039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>1023</td>\n",
       "      <td>-1.161344</td>\n",
       "      <td>2.041093</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.616613</td>\n",
       "      <td>1.003599</td>\n",
       "      <td>0.982343</td>\n",
       "      <td>-1.064914</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>3.642039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>1024</td>\n",
       "      <td>-1.102018</td>\n",
       "      <td>2.153519</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.076459</td>\n",
       "      <td>1.087297</td>\n",
       "      <td>-1.467693</td>\n",
       "      <td>0.663784</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>2.102039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>1025</td>\n",
       "      <td>-0.045623</td>\n",
       "      <td>0.487998</td>\n",
       "      <td>0.564271</td>\n",
       "      <td>-0.092126</td>\n",
       "      <td>0.451190</td>\n",
       "      <td>-1.322363</td>\n",
       "      <td>-0.065861</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>8.462039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>1026</td>\n",
       "      <td>0.392628</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>0.959602</td>\n",
       "      <td>0.675872</td>\n",
       "      <td>0.702285</td>\n",
       "      <td>-1.993711</td>\n",
       "      <td>0.496651</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-4.637961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>1027</td>\n",
       "      <td>-1.269472</td>\n",
       "      <td>0.759210</td>\n",
       "      <td>0.850222</td>\n",
       "      <td>0.521336</td>\n",
       "      <td>-0.017520</td>\n",
       "      <td>-1.035561</td>\n",
       "      <td>0.080068</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-12.117961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>1028</td>\n",
       "      <td>-1.168042</td>\n",
       "      <td>1.307430</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.279443</td>\n",
       "      <td>0.852942</td>\n",
       "      <td>0.214537</td>\n",
       "      <td>0.191074</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-3.047961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>1029</td>\n",
       "      <td>-0.193939</td>\n",
       "      <td>0.308349</td>\n",
       "      <td>0.376762</td>\n",
       "      <td>0.891286</td>\n",
       "      <td>0.400971</td>\n",
       "      <td>-1.394385</td>\n",
       "      <td>-0.150675</td>\n",
       "      <td>-0.279597</td>\n",
       "      <td>-3.417961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index         0         1         2         3         4         5  \\\n",
       "0         0  2.476712 -0.856472 -0.846733 -0.916319 -0.620147  0.862735   \n",
       "1         1  2.476712 -0.856472 -0.846733 -0.916319 -0.620147  1.055651   \n",
       "2         2  0.491187  0.795140 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "3         3  0.491187  0.795140 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "4         4 -0.790075  0.678079 -0.846733  0.488555 -1.038638  0.070492   \n",
       "5         5 -0.145138  0.464818 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "6         6  0.945704  0.244603 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "7         7  0.945704  0.244603 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "8         8 -0.145138  0.464818 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "9         9  1.854740 -0.856472 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "10       10 -0.790075  0.678079 -0.846733  0.488555 -1.038638  0.070492   \n",
       "11       11 -0.790075  0.678079 -0.846733  0.488555 -1.038638  0.070492   \n",
       "12       12  1.400222 -0.305934 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "13       13 -0.872367  1.345678 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "14       14  0.218476  0.024388 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "15       15  0.945704 -0.856472 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "16       16 -1.354634  1.570529 -0.846733  0.488555 -1.038638  0.952763   \n",
       "17       17  0.582090 -0.416042 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "18       18  0.945704  0.244603 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "19       19  1.854740 -0.856472 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "20       20  1.400222 -0.305934 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "21       21 -1.354634  1.570529 -0.846733  0.488555 -1.038638  0.952763   \n",
       "22       22 -1.354634  1.570529 -0.846733  0.488555 -1.038638  0.952763   \n",
       "23       23 -1.354634  1.570529 -0.846733  0.488555 -1.038638  0.952763   \n",
       "24       24  0.945704 -0.856472 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "25       25  0.945704 -0.856472 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "26       26  0.945704  0.244603 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "27       27  0.582090 -0.416042 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "28       28  1.400222 -0.305934 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "29       29  1.854740 -0.856472 -0.846733  2.174405 -1.038638 -0.526262   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "1000   1000 -1.332626  1.074465  1.179925 -0.377784  0.785983 -1.161599   \n",
       "1001   1001  0.159150  0.733712  0.823658  0.924067 -0.034259 -1.215616   \n",
       "1002   1002  0.384016  1.046649 -0.846733  0.418312 -0.268614 -1.323649   \n",
       "1003   1003  0.811741  1.310907 -0.846733  0.455775  0.066178 -1.911400   \n",
       "1004   1004 -0.013089  0.637513  0.722091 -0.429296  0.551628 -1.901111   \n",
       "1005   1005 -0.278144  0.268942  0.334573  0.572848  0.350753 -1.767356   \n",
       "1006   1006 -1.115414 -0.856472  1.503377  0.001532  0.919901  0.647955   \n",
       "1007   1007 -1.201533  1.965756 -0.846733 -0.059346  0.752504  0.631236   \n",
       "1008   1008 -1.157517  1.322497  1.440874  1.013042  0.852942 -1.854812   \n",
       "1009   1009  0.162020 -0.856472  0.825221  0.226312 -0.017520 -1.207899   \n",
       "1010   1010  0.351482 -0.856472  1.129922  1.317431 -0.084478 -1.445829   \n",
       "1011   1011  0.058677  0.540155  0.620524  0.282507  0.501409 -0.881228   \n",
       "1012   1012  0.425162  1.072147 -0.846733 -0.354369  0.451190 -1.174460   \n",
       "1013   1013  0.715097 -0.856472  1.365871  0.549433  0.802723 -2.205919   \n",
       "1014   1014 -1.427357  1.536917  1.667448 -0.124907 -0.117958 -1.363518   \n",
       "1015   1015  0.395499  0.865841 -0.846733  0.198215  0.384232 -0.281901   \n",
       "1016   1016 -1.119242 -0.856472  2.279976 -0.017199  1.070557 -1.589873   \n",
       "1017   1017  0.312250 -0.856472  0.912725 -0.546369  0.652066 -0.612431   \n",
       "1018   1018  0.384973 -0.856472  1.151798  0.043678  0.886421 -1.322363   \n",
       "1019   1019 -1.353677  1.043172  1.148673  2.581819 -0.067739 -1.341654   \n",
       "1020   1020  0.069203  0.545950 -0.846733 -0.195150  0.133137 -0.836214   \n",
       "1021   1021  0.162977 -0.856472  0.825221  1.317431  0.819463 -1.200182   \n",
       "1022   1022 -0.159491  0.430047  0.504893  0.652457 -0.050999 -1.804653   \n",
       "1023   1023 -1.161344  2.041093 -0.846733 -0.616613  1.003599  0.982343   \n",
       "1024   1024 -1.102018  2.153519 -0.846733  0.076459  1.087297 -1.467693   \n",
       "1025   1025 -0.045623  0.487998  0.564271 -0.092126  0.451190 -1.322363   \n",
       "1026   1026  0.392628 -0.856472  0.959602  0.675872  0.702285 -1.993711   \n",
       "1027   1027 -1.269472  0.759210  0.850222  0.521336 -0.017520 -1.035561   \n",
       "1028   1028 -1.168042  1.307430 -0.846733 -0.279443  0.852942  0.214537   \n",
       "1029   1029 -0.193939  0.308349  0.376762  0.891286  0.400971 -1.394385   \n",
       "\n",
       "             6         7     target  \n",
       "0    -1.217079 -0.279597  44.172039  \n",
       "1    -1.217079 -0.279597  26.072039  \n",
       "2    -2.239829  3.551340   4.452039  \n",
       "3    -2.239829  5.055221   5.232039  \n",
       "4     0.647569  4.976069   8.482039  \n",
       "5    -1.291914  0.701883  11.212039  \n",
       "6    -2.239829  5.055221   7.882039  \n",
       "7    -2.239829 -0.279597   0.632039  \n",
       "8    -1.291914 -0.279597  10.032039  \n",
       "9    -2.239829 -0.279597   3.472039  \n",
       "10    0.647569  0.701883   2.252039  \n",
       "11    0.647569 -0.279597  -7.797961  \n",
       "12   -2.239829  3.551340   7.192039  \n",
       "13   -1.291914  0.701883   6.512039  \n",
       "14   -1.291914 -0.279597  11.992039  \n",
       "15   -1.291914  0.701883  17.092039  \n",
       "16    0.415580  0.701883   3.542039  \n",
       "17   -1.291914  5.055221  20.322039  \n",
       "18   -2.239829  0.701883   4.742039  \n",
       "19   -2.239829  2.126612   6.802039  \n",
       "20   -2.239829  2.126612   6.022039  \n",
       "21    0.415580 -0.279597  -7.577961  \n",
       "22    0.415580 -0.675355 -27.757961  \n",
       "23    0.415580  2.126612   8.392039  \n",
       "24   -1.291914  5.055221  16.702039  \n",
       "25   -1.291914  3.551340  17.482039  \n",
       "26   -2.239829  3.551340   5.332039  \n",
       "27   -1.291914  2.126612  16.302039  \n",
       "28   -2.239829 -0.279597   1.612039  \n",
       "29   -2.239829 -0.612034   2.782039  \n",
       "...        ...       ...        ...  \n",
       "1000  0.146172 -0.279597   8.792039  \n",
       "1001 -1.475261 -0.279597  17.702039  \n",
       "1002  0.005232 -0.279597  21.402039  \n",
       "1003 -0.208048 -0.279597  30.092039  \n",
       "1004  0.390635 -0.279597  17.012039  \n",
       "1005  0.596432 -0.279597  -2.417961  \n",
       "1006 -0.557280 -0.279597 -17.787961  \n",
       "1007 -0.946424 -0.279597   1.542039  \n",
       "1008 -0.796753 -0.279597  -0.507961  \n",
       "1009  0.519102 -0.279597   6.822039  \n",
       "1010 -0.461241 -0.279597   4.242039  \n",
       "1011 -0.968875 -0.279597   7.982039  \n",
       "1012  0.204793 -0.279597  25.422039  \n",
       "1013  0.060112 -0.279597   5.052039  \n",
       "1014 -0.473714 -0.279597  -2.507961  \n",
       "1015 -0.799248 -0.279597  16.612039  \n",
       "1016  0.903257 -0.279597 -20.727961  \n",
       "1017  0.116238 -0.279597   2.642039  \n",
       "1018  0.076326 -0.279597   1.452039  \n",
       "1019 -1.471519 -0.279597  -0.587961  \n",
       "1020  0.697460 -0.279597   6.322039  \n",
       "1021 -0.366450 -0.279597  -3.937961  \n",
       "1022  0.209782 -0.279597   5.722039  \n",
       "1023 -1.064914 -0.279597   3.642039  \n",
       "1024  0.663784 -0.279597   2.102039  \n",
       "1025 -0.065861 -0.279597   8.462039  \n",
       "1026  0.496651 -0.279597  -4.637961  \n",
       "1027  0.080068 -0.279597 -12.117961  \n",
       "1028  0.191074 -0.279597  -3.047961  \n",
       "1029 -0.150675 -0.279597  -3.417961  \n",
       "\n",
       "[1030 rows x 10 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Interpreting tuple 'by' as a list of keys, rather than a single key. Use 'by=[...]' instead of 'by=(...)'. In the future, a tuple will always mean a single key.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset        RP   \n",
       "autompg        False    0.400000\n",
       "               True     0.109524\n",
       "autos          False    1.000000\n",
       "               True     0.428571\n",
       "breastcancer   False    0.100000\n",
       "               True     0.000000\n",
       "challenger     False    0.900000\n",
       "               True     0.395238\n",
       "concreteslump  False    0.000000\n",
       "               True     0.000000\n",
       "fertility      False    1.000000\n",
       "               True     0.380952\n",
       "forest         False    1.000000\n",
       "               True     0.057143\n",
       "housing        False    0.600000\n",
       "               True     0.090476\n",
       "machine        False    0.900000\n",
       "               True     0.433333\n",
       "pendulum       False    0.300000\n",
       "               True     0.066667\n",
       "servo          False    1.000000\n",
       "               True     0.252381\n",
       "stock          False    1.000000\n",
       "               True     0.376190\n",
       "yacht          False    1.000000\n",
       "               True     0.428571\n",
       "Name: errored_out, dtype: float64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(('dataset', 'RP'))['errored_out'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gp_helpers' from '/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py'>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gp_helpers\n",
    "import gpytorch\n",
    "import imp\n",
    "imp.reload(gp_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = RBFKernel().initialize(lengthscale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2 = RBFKernel().initialize(lengthscale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "k3 = gpytorch.kernels.MaternKernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  0  1  target\n",
       "0      0  1  2       3\n",
       "1      1  4  5       6"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[1,2,3],[4,5,6]], columns=list(range(2))+['target']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(3, 1)\n",
    "w2 = torch.randn(3, 1)\n",
    "b = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2965],\n",
       "        [0.3718],\n",
       "        [1.2945]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4312, 0.7876, 0.2615],\n",
       "        [0.1323, 0.3638, 0.8795],\n",
       "        [0.5182, 0.9314, 0.4416],\n",
       "        [0.5821, 0.3041, 0.2143],\n",
       "        [0.7950, 0.8816, 0.2877],\n",
       "        [0.0133, 0.7323, 0.1628],\n",
       "        [0.6067, 0.2846, 0.8322],\n",
       "        [0.1019, 0.5423, 0.3672],\n",
       "        [0.0066, 0.3660, 0.7272],\n",
       "        [0.7739, 0.4451, 0.2889]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(10, 3)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_kernel = gp_helpers.RPKernel(2, 1, 3, [k1, k2], [w1, w2], [b, b], 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/nn/functional.py:1174: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 1.9999, 1.9990, 1.9973, 1.9994, 1.9989, 1.9997, 1.9991, 1.9996,\n",
       "         1.9993],\n",
       "        [1.9999, 2.0000, 1.9987, 1.9978, 1.9992, 1.9989, 1.9999, 1.9993, 1.9997,\n",
       "         1.9996],\n",
       "        [1.9990, 1.9987, 2.0000, 1.9933, 1.9999, 1.9961, 1.9988, 1.9964, 1.9973,\n",
       "         1.9971],\n",
       "        [1.9973, 1.9978, 1.9933, 2.0000, 1.9945, 1.9989, 1.9975, 1.9993, 1.9990,\n",
       "         1.9992],\n",
       "        [1.9994, 1.9992, 1.9999, 1.9945, 2.0000, 1.9968, 1.9993, 1.9972, 1.9980,\n",
       "         1.9979],\n",
       "        [1.9989, 1.9989, 1.9961, 1.9989, 1.9968, 2.0000, 1.9983, 1.9999, 1.9997,\n",
       "         1.9992],\n",
       "        [1.9997, 1.9999, 1.9988, 1.9975, 1.9993, 1.9983, 2.0000, 1.9988, 1.9994,\n",
       "         1.9995],\n",
       "        [1.9991, 1.9993, 1.9964, 1.9993, 1.9972, 1.9999, 1.9988, 2.0000, 1.9999,\n",
       "         1.9996],\n",
       "        [1.9996, 1.9997, 1.9973, 1.9990, 1.9980, 1.9997, 1.9994, 1.9999, 2.0000,\n",
       "         1.9999],\n",
       "        [1.9993, 1.9996, 1.9971, 1.9992, 1.9979, 1.9992, 1.9995, 1.9996, 1.9999,\n",
       "         2.0000]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp_kernel(X).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.999038034853454"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "def myk1(x, y):\n",
    "    return np.exp(-1/2*(x - y)**2/25)\n",
    "def myk2(x, y):\n",
    "    return np.exp(-1/2*(x - y)**2)\n",
    "def numpy_rp_kernel(x, y):\n",
    "    x1 = sigmoid(w1.numpy().flatten().dot(x))\n",
    "    x2 = sigmoid(w2.numpy().flatten().dot(x))\n",
    "    y1 = sigmoid(w1.numpy().flatten().dot(y))\n",
    "    y2 = sigmoid(w2.numpy().flatten().dot(y))\n",
    "    \n",
    "    kval1 = myk1(x1, y1)\n",
    "    kval2 = myk2(x2, y2)\n",
    "    return kval1 + kval2\n",
    "xnp = X.numpy()\n",
    "numpy_rp_kernel(xnp[0], xnp[2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a614d55028ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrp_experiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgp_helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/rp_experiments.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgp_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExactGPModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_gp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRPKernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mELMModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_linear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpytorch'"
     ]
    }
   ],
   "source": [
    "import rp_experiments\n",
    "import rp\n",
    "import gp_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gp_helpers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3ceeac08f4f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgp_helpers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gp_helpers' is not defined"
     ]
    }
   ],
   "source": [
    "imp.reload(gp_helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas = rp_experiments.load_dataset('gas')\n",
    "X = torch.Tensor(gas.values[:, 1:-1])\n",
    "Y = torch.Tensor(gas.values[:, -1])\n",
    "trainX = X[:9*len(X)//10, :]\n",
    "trainY = Y[:9*len(X)//10]\n",
    "testX = X[9*len(X)//10:, :]\n",
    "testY = Y[9*len(X)//10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 0.981\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-321-f79c46916297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrp_experiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_additive_rp_gp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/rp_experiments.py\u001b[0m in \u001b[0;36mtrain_additive_rp_gp\u001b[0;34m(trainX, trainY, testX, testY, k, J, ard, activation, optimizer, n_epochs, lr, verbose, patience)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# regular marginal log likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0mmll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExactMarginalLogLikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0;31m# fit GP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     fit_gp_model(model, likelihood, trainXprime, trainY, optimizer=optimizer_,\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mfit_gp_model\u001b[0;34m(gp_model, gp_likelihood, xs, ys, optimizer, lr, gp_mll, n_epochs, verbose, patience, conv_tol, check_conv)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_conv\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m                 print(\n\u001b[1;32m     94\u001b[0m                     'Iter %d/%d - Loss: %.3f' % (i + 1, n_epochs, loss.item()))\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation"
     ]
    }
   ],
   "source": [
    "rp_experiments.train_additive_rp_gp(trainX, trainY, testX, testY, ard=False, activation=None, patience=5, verbose=True, k=1, J=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = RBFKernel(ard_num_dims=X.shape[1])\n",
    "kernel = gpytorch.kernels.ScaleKernel(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 1.077\n",
      "Iter 2/100 - Loss: 1.028\n",
      "Iter 3/100 - Loss: 0.975\n",
      "Iter 4/100 - Loss: 0.929\n",
      "Iter 5/100 - Loss: 0.889\n",
      "Iter 6/100 - Loss: 0.843\n",
      "Iter 7/100 - Loss: 0.797\n",
      "Iter 8/100 - Loss: 0.755\n",
      "Iter 9/100 - Loss: 0.706\n",
      "Iter 10/100 - Loss: 0.659\n",
      "Iter 11/100 - Loss: 0.612\n",
      "Iter 12/100 - Loss: 0.570\n",
      "Iter 13/100 - Loss: 0.517\n",
      "Iter 14/100 - Loss: 0.469\n",
      "Iter 15/100 - Loss: 0.428\n",
      "Iter 16/100 - Loss: 0.381\n",
      "Iter 17/100 - Loss: 0.335\n",
      "Iter 18/100 - Loss: 0.289\n",
      "Iter 19/100 - Loss: 0.252\n",
      "Iter 20/100 - Loss: 0.209\n",
      "Iter 21/100 - Loss: 0.162\n",
      "Iter 22/100 - Loss: 0.126\n",
      "Iter 23/100 - Loss: 0.075\n",
      "Iter 24/100 - Loss: 0.031\n",
      "Iter 25/100 - Loss: -0.006\n",
      "Iter 26/100 - Loss: -0.050\n",
      "Iter 27/100 - Loss: -0.091\n",
      "Iter 28/100 - Loss: -0.123\n",
      "Iter 29/100 - Loss: -0.169\n",
      "Iter 30/100 - Loss: -0.199\n",
      "Iter 31/100 - Loss: -0.241\n",
      "Iter 32/100 - Loss: -0.278\n",
      "Iter 33/100 - Loss: -0.303\n",
      "Iter 34/100 - Loss: -0.358\n",
      "Iter 35/100 - Loss: -0.380\n",
      "Iter 36/100 - Loss: -0.413\n",
      "Iter 37/100 - Loss: -0.446\n",
      "Iter 38/100 - Loss: -0.486\n",
      "Iter 39/100 - Loss: -0.520\n",
      "Iter 40/100 - Loss: -0.541\n",
      "Iter 41/100 - Loss: -0.586\n",
      "Iter 42/100 - Loss: -0.598\n",
      "Iter 43/100 - Loss: -0.631\n",
      "Iter 44/100 - Loss: -0.651\n",
      "Iter 45/100 - Loss: -0.687\n",
      "Iter 46/100 - Loss: -0.708\n",
      "Iter 47/100 - Loss: -0.739\n",
      "Iter 48/100 - Loss: -0.768\n",
      "Iter 49/100 - Loss: -0.789\n",
      "Iter 50/100 - Loss: -0.825\n",
      "Iter 51/100 - Loss: -0.827\n",
      "Iter 52/100 - Loss: -0.867\n",
      "Iter 53/100 - Loss: -0.874\n",
      "Iter 54/100 - Loss: -0.893\n",
      "Iter 55/100 - Loss: -0.912\n",
      "Iter 56/100 - Loss: -0.945\n",
      "Iter 57/100 - Loss: -0.953\n",
      "Iter 58/100 - Loss: -0.985\n",
      "Iter 59/100 - Loss: -1.003\n",
      "Iter 60/100 - Loss: -1.018\n",
      "Iter 61/100 - Loss: -1.038\n",
      "Iter 62/100 - Loss: -1.031\n",
      "Iter 63/100 - Loss: -1.055\n",
      "Iter 64/100 - Loss: -1.077\n",
      "Iter 65/100 - Loss: -1.102\n",
      "Iter 66/100 - Loss: -1.121\n",
      "Iter 67/100 - Loss: -1.111\n",
      "Iter 68/100 - Loss: -1.168\n",
      "Iter 69/100 - Loss: -1.143\n",
      "Iter 70/100 - Loss: -1.177\n",
      "Iter 71/100 - Loss: -1.173\n",
      "Iter 72/100 - Loss: -1.163\n",
      "Iter 73/100 - Loss: -1.154\n",
      "Iter 74/100 - Loss: -1.163\n",
      "Iter 75/100 - Loss: -1.146\n",
      "Iter 76/100 - Loss: -1.146\n",
      "Iter 77/100 - Loss: -1.166\n",
      "Iter 78/100 - Loss: -1.175\n",
      "Iter 79/100 - Loss: -1.177\n",
      "Iter 80/100 - Loss: -1.194\n",
      "Iter 81/100 - Loss: -1.210\n",
      "Iter 82/100 - Loss: -1.206\n",
      "Iter 83/100 - Loss: -1.277\n",
      "Iter 84/100 - Loss: -1.276\n",
      "Iter 85/100 - Loss: -1.277\n",
      "Iter 86/100 - Loss: -1.317\n",
      "Iter 87/100 - Loss: -1.282\n",
      "Iter 88/100 - Loss: -1.305\n",
      "Iter 89/100 - Loss: -1.357\n",
      "Iter 90/100 - Loss: -1.336\n",
      "Iter 91/100 - Loss: -1.346\n",
      "Iter 92/100 - Loss: -1.303\n",
      "Iter 93/100 - Loss: -1.327\n",
      "Iter 94/100 - Loss: -1.299\n",
      "Iter 95/100 - Loss: -1.309\n",
      "Iter 96/100 - Loss: -1.325\n",
      "Iter 97/100 - Loss: -1.369\n",
      "Iter 98/100 - Loss: -1.310\n",
      "Iter 99/100 - Loss: -1.360\n",
      "Iter 100/100 - Loss: -1.381\n"
     ]
    }
   ],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = gp_helpers.ExactGPModel(trainX, trainY, likelihood, kernel)\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "gp_helpers.fit_gp_model(\n",
    "    model,\n",
    "    likelihood,\n",
    "    trainX, trainY,\n",
    "    torch.optim.Adam,\n",
    "    lr=0.1, \n",
    "    n_epochs=100,\n",
    "    gp_mll=mll,\n",
    "    verbose=True,\n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0003]], grad_fn=<SoftplusBackward>)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([-1.3616], requires_grad=True), Parameter containing:\n",
       " tensor([[[ 8.2300,  9.9539,  7.5712,  8.4116,  8.7345,  8.3944,  8.6121,\n",
       "            9.1584,  2.6534, 10.1278,  6.9371,  7.8115,  8.3512,  6.9355,\n",
       "            7.2168,  9.1016,  5.8206,  9.5956,  7.2222,  7.4172,  8.2212,\n",
       "            5.5663,  7.6434,  9.2380,  5.7259,  9.5648,  7.0014,  7.1603,\n",
       "            8.0579,  5.3922,  7.9859,  9.7566,  5.0349,  8.1895,  6.3009,\n",
       "            6.0172,  8.0087,  2.2746,  6.9714,  7.6899,  3.1094,  3.8098,\n",
       "            6.4657,  7.2874,  8.1667,  2.1612,  6.4230,  8.9385,  6.0749,\n",
       "            8.3636,  7.1474,  6.7626,  7.4691,  6.1605,  7.9524,  9.4307,\n",
       "            6.4509,  7.4320,  7.3175,  7.1579,  7.6668,  6.8280,  8.2536,\n",
       "            8.9846,  0.1036,  9.7622,  3.5719,  7.2114,  8.1148,  5.5528,\n",
       "            7.6338,  8.5782,  0.7152,  9.8478,  7.1845,  8.3517,  8.8291,\n",
       "            5.1034,  7.1914,  8.8369,  6.5047,  9.7333,  7.2966,  7.5994,\n",
       "            8.1453,  6.7957,  7.9140,  9.8342,  5.6631,  9.4673,  7.3346,\n",
       "            7.1953,  7.9904,  6.1696,  8.0626,  9.5254,  4.4552,  7.0122,\n",
       "            4.4005,  6.6167,  8.0973,  5.7348,  7.3871,  7.9855,  4.2130,\n",
       "            7.3186,  0.7350,  6.3425,  7.8551,  6.0824,  7.7591,  8.5608,\n",
       "            6.7093,  8.9784,  6.8491,  6.5479,  7.4098,  7.8590,  8.4268,\n",
       "            9.5713,  6.6316,  8.6638,  6.6762,  6.0479,  7.4687,  7.4307,\n",
       "            8.0755,  9.6904]]], requires_grad=True)]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.covar_module.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.2303,  9.9539,  7.5717,  8.4118,  8.7346,  8.3946,  8.6123,\n",
       "           9.1585,  2.7214, 10.1278,  6.9380,  7.8119,  8.3514,  6.9364,\n",
       "           7.2175,  9.1017,  5.8235,  9.5957,  7.2230,  7.4178,  8.2214,\n",
       "           5.5702,  7.6439,  9.2381,  5.7291,  9.5649,  7.0024,  7.1611,\n",
       "           8.0582,  5.3967,  7.9863,  9.7567,  5.0414,  8.1898,  6.3027,\n",
       "           6.0196,  8.0091,  2.3725,  6.9723,  7.6904,  3.1530,  3.8317,\n",
       "           6.4672,  7.2881,  8.1669,  2.2702,  6.4246,  8.9386,  6.0772,\n",
       "           8.3639,  7.1482,  6.7637,  7.4697,  6.1626,  7.9528,  9.4308,\n",
       "           6.4525,  7.4325,  7.3181,  7.1586,  7.6672,  6.8291,  8.2539,\n",
       "           8.9847,  0.7463,  9.7623,  3.5996,  7.2122,  8.1151,  5.5567,\n",
       "           7.6343,  8.5784,  1.1134,  9.8479,  7.1852,  8.3519,  8.8292,\n",
       "           5.1094,  7.1921,  8.8371,  6.5062,  9.7333,  7.2973,  7.5999,\n",
       "           8.1456,  6.7969,  7.9144,  9.8342,  5.6666,  9.4673,  7.3353,\n",
       "           7.1960,  7.9907,  6.1717,  8.0629,  9.5255,  4.4667,  7.0131,\n",
       "           4.4127,  6.6180,  8.0976,  5.7380,  7.3877,  7.9859,  4.2277,\n",
       "           7.3193,  1.1267,  6.3443,  7.8555,  6.0847,  7.7595,  8.5610,\n",
       "           6.7105,  8.9785,  6.8501,  6.5493,  7.4104,  7.8594,  8.4270,\n",
       "           9.5713,  6.6329,  8.6640,  6.6775,  6.0503,  7.4693,  7.4313,\n",
       "           8.0758,  9.6904]]], grad_fn=<ClampBackward>)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.covar_module.base_kernel.lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2281], grad_fn=<SoftplusBackward>)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.covar_module.outputscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8879, 0.8879, 0.8879,  ..., 0.8879, 0.8879, 0.8879],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(trainX).mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0.,    0.,    0.,    0.,    0., 2308.,    0.,    0.,    0.,\n",
       "           0.]),\n",
       " array([0.3878907, 0.4878907, 0.5878907, 0.6878907, 0.7878907, 0.8878907,\n",
       "        0.9878907, 1.0878907, 1.1878906, 1.2878907, 1.3878907],\n",
       "       dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model(trainX).mean.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 38.,   1., 257.,  85.,  74., 516., 431., 552., 340.,  14.]),\n",
       " array([-3.1037    , -2.5891168 , -2.0745335 , -1.5599504 , -1.0453671 ,\n",
       "        -0.5307839 , -0.01620069,  0.4983825 ,  1.0129657 ,  1.5275489 ,\n",
       "         2.0421321 ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(trainY.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autos starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py:101: RuntimeWarning: Mean of empty slice.\n",
      "  ma[i] = losses[i-patience+1:i+1].mean()\n",
      "/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py:63: UserWarning: NaNs encountered in preconditioner computation. Attempting to continue without preconditioning.\n",
      "  \"NaNs encountered in preconditioner computation. Attempting to continue without preconditioning.\"\n",
      "/home/ian/gpytorch/gpytorch/models/exact_gp.py:190: UserWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  \"The input matches the stored training data. Did you forget to call model.train()?\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 13:48:59.279113 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 59.168853759765625, 'train_time': 5.175243242003489, 'train_nmll': 0.45276331901550293, 'test_nmll': 0.46097275614738464, 'train_nll': 64.74515533447266, 'test_nll': 7.375564098358154}\n",
      "2019-03-08 13:49:20.619561 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 97.55072784423828, 'train_time': 21.337001956999302, 'train_nmll': 6.297364711761475, 'test_nmll': 6.2573561668396, 'train_nll': 900.5231323242188, 'test_nll': 100.1176986694336}\n",
      "2019-03-08 13:49:29.569992 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 337442594881536.0, 'train_time': 8.946739618986612, 'train_nmll': 0.45276331901550293, 'test_nmll': 0.46097275614738464, 'train_nll': 64.74515533447266, 'test_nll': 7.375564098358154}\n",
      "2019-03-08 13:49:50.873953 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 152.59066772460938, 'train_time': 21.300463017978473, 'train_nmll': 7.5755085945129395, 'test_nmll': 7.860793113708496, 'train_nll': 1083.105224609375, 'test_nll': 125.77268981933594}\n",
      "2019-03-08 13:50:11.347588 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 427986592.0, 'train_time': 20.469904350989964, 'train_nmll': 0.45276331901550293, 'test_nmll': 0.46097275614738464, 'train_nll': 64.74515533447266, 'test_nll': 7.375564098358154}\n",
      "2019-03-08 13:50:53.392728 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 252319432704.0, 'train_time': 42.039151242992375, 'train_nmll': 0.45276331901550293, 'test_nmll': 0.46097275614738464, 'train_nll': 64.74515533447266, 'test_nll': 7.375564098358154}\n",
      "2019-03-08 13:51:46.525897 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.6569920182228088, 'train_time': 53.12954098399496, 'train_nmll': 1.4354400634765625, 'test_nmll': 1.2532676458358765, 'train_nll': 205.26792907714844, 'test_nll': 20.052282333374023}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 7, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 13:52:13.092653 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 65183.1171875, 'train_time': 7.6967540370242205, 'train_nmll': 0.45276331901550293, 'test_nmll': 0.46097275614738464, 'train_nll': 64.74515533447266, 'test_nll': 7.375564098358154}\n",
      "2019-03-08 13:52:21.565458 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 10160948903936.0, 'train_time': 8.468426944979, 'train_nmll': 0.45276331901550293, 'test_nmll': 0.46097275614738464, 'train_nll': 64.74515533447266, 'test_nll': 7.375564098358154}\n",
      "2019-03-08 13:53:19.457751 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 96782312.0, 'train_time': 57.88807578399428, 'train_nmll': 0.4527634382247925, 'test_nmll': 0.4636884331703186, 'train_nll': 65.19793701171875, 'test_nll': 6.955326557159424}\n",
      "2019-03-08 13:53:32.227513 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.17757460474967957, 'train_time': 12.742602529004216, 'train_nmll': 0.9197594523429871, 'test_nmll': 0.6742397546768188, 'train_nll': 131.52569580078125, 'test_nll': 10.787824630737305}\n",
      "2019-03-08 13:53:42.104548 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.9039916396141052, 'train_time': 9.873206900985679, 'train_nmll': 1.1208409070968628, 'test_nmll': 1.3924744129180908, 'train_nll': 160.28024291992188, 'test_nll': 22.279592514038086}\n",
      "2019-03-08 13:53:46.666918 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.9373231530189514, 'train_time': 4.558557036012644, 'train_nmll': 1.2577054500579834, 'test_nmll': 1.3999282121658325, 'train_nll': 179.8519744873047, 'test_nll': 22.39885711669922}\n",
      "2019-03-08 13:53:57.793858 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.8466455936431885, 'train_time': 11.122731157985982, 'train_nmll': 1.3075071573257446, 'test_nmll': 1.3443529605865479, 'train_nll': 186.97357177734375, 'test_nll': 21.5096435546875}\n",
      "2019-03-08 13:54:04.444366 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4656928479671478, 'train_time': 6.644831210986013, 'train_nmll': 1.3070517778396606, 'test_nmll': 1.1047918796539307, 'train_nll': 186.908447265625, 'test_nll': 17.67667007446289}\n",
      "2019-03-08 13:54:24.536783 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.8996952176094055, 'train_time': 20.086754342977656, 'train_nmll': 1.2588272094726562, 'test_nmll': 1.399228811264038, 'train_nll': 180.0122833251953, 'test_nll': 22.387664794921875}\n",
      "2019-03-08 13:54:36.923883 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.5409424901008606, 'train_time': 12.383249029982835, 'train_nmll': 1.385554552078247, 'test_nmll': 1.1781879663467407, 'train_nll': 198.1343231201172, 'test_nll': 18.851016998291016}\n",
      "2019-03-08 13:54:40.984845 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 1.4709402322769165, 'train_time': 4.057361485000001, 'train_nmll': 1.3378725051879883, 'test_nmll': 1.6843349933624268, 'train_nll': 191.31573486328125, 'test_nll': 26.949359893798828}\n",
      "2019-03-08 13:54:48.701975 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.6191688776016235, 'train_time': 7.7133695959928446, 'train_nmll': 1.0321358442306519, 'test_nmll': 1.1481863260269165, 'train_nll': 147.59564208984375, 'test_nll': 18.370973587036133}\n",
      "2019-03-08 13:54:54.029757 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.48663678765296936, 'train_time': 5.321091746009188, 'train_nmll': 1.0551276206970215, 'test_nmll': 1.0767358541488647, 'train_nll': 151.93841552734375, 'test_nll': 16.151029586791992}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py:530: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  df = pd.concat([df, result])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 13:55:01.650493 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.2893752455711365, 'train_time': 7.599420998012647, 'train_nmll': 1.0444300174713135, 'test_nmll': 0.869263768196106, 'train_nll': 149.3863067626953, 'test_nll': 13.908226013183594}\n",
      "2019-03-08 13:55:10.702180 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.44297242164611816, 'train_time': 9.047850544011453, 'train_nmll': 0.8842072486877441, 'test_nmll': 1.0046026706695557, 'train_nll': 126.34515380859375, 'test_nll': 16.07362937927246}\n",
      "2019-03-08 13:55:16.490737 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.2851460576057434, 'train_time': 5.784824731003027, 'train_nmll': 0.9374583959579468, 'test_nmll': 0.8332677483558655, 'train_nll': 134.6587677001953, 'test_nll': 13.332265853881836}\n",
      "2019-03-08 13:55:25.393499 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.24735482037067413, 'train_time': 8.89884575799806, 'train_nmll': 0.8447364568710327, 'test_nmll': 0.7425084710121155, 'train_nll': 120.80814361572266, 'test_nll': 11.880144119262695}\n",
      "2019-03-08 13:55:34.294711 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.46343469619750977, 'train_time': 8.895249910012353, 'train_nmll': 1.145930528640747, 'test_nmll': 1.0543080568313599, 'train_nll': 163.8936767578125, 'test_nll': 16.868947982788086}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 5, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 13:55:44.149857 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.418037086725235, 'train_time': 9.149201816995628, 'train_nmll': 1.038954734802246, 'test_nmll': 0.9931157827377319, 'train_nll': 148.56044006347656, 'test_nll': 15.889838218688965}\n",
      "2019-03-08 13:55:48.977123 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 1044923416576.0, 'train_time': 4.823512117989594, 'train_nmll': 0.45276331901550293, 'test_nmll': 0.46097275614738464, 'train_nll': 64.74515533447266, 'test_nll': 7.375564098358154}\n",
      "2019-03-08 13:55:53.017091 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 1825173504.0, 'train_time': 4.0360774580040015, 'train_nmll': 0.45276331901550293, 'test_nmll': 0.46097275614738464, 'train_nll': 64.74515533447266, 'test_nll': 7.375564098358154}\n",
      "2019-03-08 13:56:07.727273 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.6311161518096924, 'train_time': 14.706509137002286, 'train_nmll': 0.9744921922683716, 'test_nmll': 1.2566074132919312, 'train_nll': 139.4287109375, 'test_nll': 20.105728149414062}\n",
      "2019-03-08 13:56:18.494408 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.2797766923904419, 'train_time': 10.763307537010405, 'train_nmll': 0.5452326536178589, 'test_nmll': 0.8244836926460266, 'train_nll': 78.8696060180664, 'test_nll': 12.367252349853516}\n",
      "2019-03-08 13:56:28.205896 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.21198733150959015, 'train_time': 9.689401445008116, 'train_nmll': 1.070237398147583, 'test_nmll': 0.8398779630661011, 'train_nll': 152.6026611328125, 'test_nll': 13.438051223754883}\n",
      "2019-03-08 13:56:36.523933 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.9511528611183167, 'train_time': 8.31423900500522, 'train_nmll': 1.1030007600784302, 'test_nmll': 1.441528081893921, 'train_nll': 157.76109313964844, 'test_nll': 23.064594268798828}\n",
      "2019-03-08 13:56:50.603096 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.27035224437713623, 'train_time': 14.075515923992498, 'train_nmll': 0.7174004912376404, 'test_nmll': 0.7919802665710449, 'train_nll': 102.77752685546875, 'test_nll': 12.671582221984863}\n",
      "2019-03-08 13:56:59.550475 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.47480350732803345, 'train_time': 8.937294814008055, 'train_nmll': 0.7900547981262207, 'test_nmll': 1.1141200065612793, 'train_nll': 112.87738800048828, 'test_nll': 17.825925827026367}\n",
      "2019-03-08 13:57:09.600083 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.44354885816574097, 'train_time': 10.045154374995036, 'train_nmll': 1.0660374164581299, 'test_nmll': 1.0247347354888916, 'train_nll': 152.31710815429688, 'test_nll': 16.395753860473633}\n",
      "2019-03-08 13:57:18.503851 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 1.0269060134887695, 'train_time': 8.900001658010297, 'train_nmll': 1.112486720085144, 'test_nmll': 1.5225608348846436, 'train_nll': 159.42431640625, 'test_nll': 24.360980987548828}\n",
      "2019-03-08 13:57:26.048149 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.41400158405303955, 'train_time': 7.540628808987094, 'train_nmll': 1.0807651281356812, 'test_nmll': 0.9972846508026123, 'train_nll': 154.41275024414062, 'test_nll': 15.956550598144531}\n",
      "2019-03-08 13:57:35.027206 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.411484032869339, 'train_time': 8.97528944798978, 'train_nmll': 0.8924234509468079, 'test_nmll': 0.9629095792770386, 'train_nll': 128.02584838867188, 'test_nll': 15.406869888305664}\n",
      "2019-03-08 13:57:42.020564 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.8258318305015564, 'train_time': 6.989178709016414, 'train_nmll': 1.2022744417190552, 'test_nmll': 1.3414947986602783, 'train_nll': 171.62835693359375, 'test_nll': 21.463905334472656}\n",
      "2019-03-08 13:57:47.726530 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.5467485785484314, 'train_time': 5.701662598992698, 'train_nmll': 1.0365681648254395, 'test_nmll': 1.086419701576233, 'train_nll': 149.48641967773438, 'test_nll': 16.296287536621094}\n",
      "2019-03-08 13:58:01.864704 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.17446869611740112, 'train_time': 14.105881524999859, 'train_nmll': 0.4988347887992859, 'test_nmll': 0.553084135055542, 'train_nll': 71.21559143066406, 'test_nll': 8.849161148071289}\n",
      "2019-03-08 13:58:15.443645 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.45917630195617676, 'train_time': 13.575486620975425, 'train_nmll': 0.8532547354698181, 'test_nmll': 0.934852123260498, 'train_nll': 122.4405288696289, 'test_nll': 14.95762825012207}\n",
      "2019-03-08 13:58:36.781795 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1435128003358841, 'train_time': 21.334504560014466, 'train_nmll': 0.4038677513599396, 'test_nmll': 0.4604514241218567, 'train_nll': 57.33228302001953, 'test_nll': 7.364192962646484}\n",
      "2019-03-08 13:59:11.279511 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.37352681159973145, 'train_time': 34.49431377800647, 'train_nmll': 0.8350718021392822, 'test_nmll': 0.8828495740890503, 'train_nll': 120.7935791015625, 'test_nll': 14.125582695007324}\n",
      "2019-03-08 13:59:25.846654 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3340145945549011, 'train_time': 14.56223893997958, 'train_nmll': 0.23559661209583282, 'test_nmll': 1.0359554290771484, 'train_nll': 33.00006866455078, 'test_nll': 16.57494354248047}\n",
      "2019-03-08 13:59:35.092169 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.24696797132492065, 'train_time': 9.24028769999859, 'train_nmll': 0.5569525361061096, 'test_nmll': 0.6947507262229919, 'train_nll': 80.38235473632812, 'test_nll': 11.117502212524414}\n",
      "2019-03-08 13:59:43.121222 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.14086072146892548, 'train_time': 8.025520138995489, 'train_nmll': 0.5913621783256531, 'test_nmll': 0.49153637886047363, 'train_nll': 84.13955688476562, 'test_nll': 7.864387512207031}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 7, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 13:59:56.647312 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.7059181332588196, 'train_time': 10.085808331001317, 'train_nmll': 0.8036907911300659, 'test_nmll': 1.2166162729263306, 'train_nll': 114.5573959350586, 'test_nll': 19.466045379638672}\n",
      "2019-03-08 14:00:06.724674 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.6068747639656067, 'train_time': 10.073570945998654, 'train_nmll': 0.575423002243042, 'test_nmll': 1.3992482423782349, 'train_nll': 81.89657592773438, 'test_nll': 22.387840270996094}\n",
      "2019-03-08 14:00:16.621874 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.5867995023727417, 'train_time': 9.89345107099507, 'train_nmll': 0.994683563709259, 'test_nmll': 1.1514966487884521, 'train_nll': 143.43167114257812, 'test_nll': 17.272449493408203}\n",
      "2019-03-08 14:00:27.780506 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.12127098441123962, 'train_time': 11.137226378021296, 'train_nmll': 0.17314137518405914, 'test_nmll': 0.3609575629234314, 'train_nll': 25.480743408203125, 'test_nll': 5.771479606628418}\n",
      "2019-03-08 14:00:38.106648 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.32283613085746765, 'train_time': 10.32245311801671, 'train_nmll': 0.4660907983779907, 'test_nmll': 0.694536566734314, 'train_nll': 66.68425750732422, 'test_nll': 11.113693237304688}\n",
      "2019-03-08 14:00:47.574550 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.38941770792007446, 'train_time': 9.46268763000262, 'train_nmll': 0.6557617783546448, 'test_nmll': 0.9481738209724426, 'train_nll': 93.60152435302734, 'test_nll': 15.170574188232422}\n",
      "2019-03-08 14:01:02.032011 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.10168349742889404, 'train_time': 14.453796030022204, 'train_nmll': 0.23213782906532288, 'test_nmll': 0.3161250352859497, 'train_nll': 33.055999755859375, 'test_nll': 5.061131477355957}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 4, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:01:13.166751 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.324707567691803, 'train_time': 10.51249680199544, 'train_nmll': 0.763055145740509, 'test_nmll': 0.8472974300384521, 'train_nll': 107.98573303222656, 'test_nll': 13.560449600219727}\n",
      "2019-03-08 14:01:22.593050 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.12283086031675339, 'train_time': 9.420951772015542, 'train_nmll': 0.4151480197906494, 'test_nmll': 0.4495130181312561, 'train_nll': 59.67192840576172, 'test_nll': 7.186639785766602}\n",
      "2019-03-08 14:01:34.346209 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.15204930305480957, 'train_time': 11.749468885012902, 'train_nmll': 0.49094924330711365, 'test_nmll': 0.496702641248703, 'train_nll': 69.74943542480469, 'test_nll': 7.947028636932373}\n",
      "2019-03-08 14:01:48.013636 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3596118986606598, 'train_time': 13.663904841989279, 'train_nmll': 0.49136364459991455, 'test_nmll': 0.9203458428382874, 'train_nll': 70.84635925292969, 'test_nll': 14.727005004882812}\n",
      "2019-03-08 14:02:01.261778 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3044479191303253, 'train_time': 13.244109978986671, 'train_nmll': 0.2683303952217102, 'test_nmll': 0.4940035045146942, 'train_nll': 38.77244567871094, 'test_nll': 7.903174877166748}\n",
      "2019-03-08 14:02:19.103797 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.15471652150154114, 'train_time': 17.834490084991558, 'train_nmll': 0.3531453311443329, 'test_nmll': 0.5103499293327332, 'train_nll': 50.99755096435547, 'test_nll': 7.6555256843566895}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\\n    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\\n  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\\n    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\\n  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\\n    chol = torch.cholesky(inner_mat)\\nRuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\\n', 'fold': 0, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\n",
      "    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\n",
      "    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\n",
      "    chol = torch.cholesky(inner_mat)\n",
      "RuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:02:35.143306 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.12151297181844711, 'train_time': 14.084500833996572, 'train_nmll': 0.13302356004714966, 'test_nmll': 0.3805065155029297, 'train_nll': 18.742813110351562, 'test_nll': 6.084962844848633}\n",
      "2019-03-08 14:02:52.158143 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.18085336685180664, 'train_time': 17.010681220010156, 'train_nmll': 0.152189239859581, 'test_nmll': 0.4192869961261749, 'train_nll': 22.589279174804688, 'test_nll': 6.71261739730835}\n",
      "2019-03-08 14:03:14.458144 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.13066893815994263, 'train_time': 22.29644237298635, 'train_nmll': 0.2108057737350464, 'test_nmll': 0.3828120827674866, 'train_nll': 28.72270965576172, 'test_nll': 6.124215126037598}\n",
      "2019-03-08 14:03:35.549740 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.255144327878952, 'train_time': 21.08795573300449, 'train_nmll': 0.03098137117922306, 'test_nmll': 0.870764970779419, 'train_nll': 5.159049987792969, 'test_nll': 13.936487197875977}\n",
      "2019-03-08 14:03:55.339408 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1948680877685547, 'train_time': 19.78398282499984, 'train_nmll': 0.18937544524669647, 'test_nmll': 0.695645809173584, 'train_nll': 27.0274658203125, 'test_nll': 11.127823829650879}\n",
      "2019-03-08 14:04:23.497791 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.2533802092075348, 'train_time': 28.15415083800326, 'train_nmll': 0.5366421937942505, 'test_nmll': 0.6390067338943481, 'train_nll': 76.60072326660156, 'test_nll': 10.22358512878418}\n",
      "2019-03-08 14:04:41.113852 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.10751562565565109, 'train_time': 17.612385077984072, 'train_nmll': 0.11911208182573318, 'test_nmll': 0.2199397087097168, 'train_nll': 16.33184051513672, 'test_nll': 3.5204010009765625}\n",
      "2019-03-08 14:04:53.400696 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.14649061858654022, 'train_time': 12.275395849021152, 'train_nmll': 0.2575797140598297, 'test_nmll': 0.48281508684158325, 'train_nll': 36.42577362060547, 'test_nll': 7.725161552429199}\n",
      "2019-03-08 14:05:17.951977 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.22317379713058472, 'train_time': 24.54584109899588, 'train_nmll': 0.08979584276676178, 'test_nmll': 0.44335442781448364, 'train_nll': 12.593032836914062, 'test_nll': 7.094260215759277}\n",
      "2019-03-08 14:05:29.289863 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.11840421706438065, 'train_time': 11.33414695999818, 'train_nmll': 0.19397370517253876, 'test_nmll': 0.3534901440143585, 'train_nll': 27.53583526611328, 'test_nll': 5.302416801452637}\n",
      "2019-03-08 14:06:09.222300 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 3450.2177734375, 'train_time': 39.91114537999965, 'train_nmll': 5.545372486114502, 'test_nmll': 5.521833419799805, 'train_nll': 792.8369140625, 'test_nll': 88.34933471679688}\n",
      "2019-03-08 14:06:31.277771 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.10601673275232315, 'train_time': 22.05171301899827, 'train_nmll': 0.034358471632003784, 'test_nmll': 0.21928030252456665, 'train_nll': 6.583518981933594, 'test_nll': 3.522467613220215}\n",
      "2019-03-08 14:06:50.022453 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.15644609928131104, 'train_time': 18.734002209006576, 'train_nmll': -0.050163961946964264, 'test_nmll': 0.3654990792274475, 'train_nll': -8.0081787109375, 'test_nll': 5.831185340881348}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\\n    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\\n  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\\n    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\\n  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\\n    chol = torch.cholesky(inner_mat)\\nRuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\\n', 'fold': 3, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\n",
      "    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\n",
      "    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\n",
      "    chol = torch.cholesky(inner_mat)\n",
      "RuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:07:17.660449 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.10076569020748138, 'train_time': 27.18018418000429, 'train_nmll': 0.011822894215583801, 'test_nmll': 0.2528536915779114, 'train_nll': 2.0349578857421875, 'test_nll': 4.0392045974731445}\n",
      "2019-03-08 14:07:38.086243 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.16731511056423187, 'train_time': 20.421453940012725, 'train_nmll': 0.04594848304986954, 'test_nmll': 0.6277163028717041, 'train_nll': 6.311553955078125, 'test_nll': 10.033773422241211}\n",
      "2019-03-08 14:07:56.910840 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.07429185509681702, 'train_time': 18.818723719014088, 'train_nmll': 0.046986933797597885, 'test_nmll': 0.21038538217544556, 'train_nll': 6.397125244140625, 'test_nll': 3.3709535598754883}\n",
      "2019-03-08 14:08:10.084171 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.06088205426931381, 'train_time': 13.169415623007808, 'train_nmll': 0.040060725063085556, 'test_nmll': 0.06677299737930298, 'train_nll': 6.0059356689453125, 'test_nll': 1.062321662902832}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 7, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:08:30.611442 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.2911008596420288, 'train_time': 20.083921800018288, 'train_nmll': -0.05328582599759102, 'test_nmll': 0.9173120260238647, 'train_nll': -8.040878295898438, 'test_nll': 14.686849594116211}\n",
      "2019-03-08 14:08:54.426800 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.11789576709270477, 'train_time': 23.811484087986173, 'train_nmll': -0.03106614761054516, 'test_nmll': 0.3030065894126892, 'train_nll': -1.94451904296875, 'test_nll': 4.834296226501465}\n",
      "2019-03-08 14:09:15.778989 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.15582042932510376, 'train_time': 21.348608658998273, 'train_nmll': 0.0837021917104721, 'test_nmll': 0.3654387891292572, 'train_nll': 12.514190673828125, 'test_nll': 5.476931571960449}\n",
      "2019-03-08 14:09:22.820978 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.39071914553642273, 'train_time': 7.016576995025389, 'train_nmll': 0.2829347550868988, 'test_nmll': 1.0442476272583008, 'train_nll': 40.17792510986328, 'test_nll': 16.765188217163086}\n",
      "2019-03-08 14:09:32.697819 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.655467689037323, 'train_time': 9.873114224988967, 'train_nmll': 0.03075094148516655, 'test_nmll': 0.8479654788970947, 'train_nll': 5.082244873046875, 'test_nll': 13.63964557647705}\n",
      "2019-03-08 14:09:42.519958 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4774678945541382, 'train_time': 9.81861296700663, 'train_nmll': -0.017470140010118484, 'test_nmll': 1.2642205953598022, 'train_nll': -1.886199951171875, 'test_nll': 20.169872283935547}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\\n    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\\n  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\\n    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\\n  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\\n    chol = torch.cholesky(inner_mat)\\nRuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\\n', 'fold': 3, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\n",
      "    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\n",
      "    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\n",
      "    chol = torch.cholesky(inner_mat)\n",
      "RuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:09:52.966072 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.7266501784324646, 'train_time': 7.416531199996825, 'train_nmll': 0.8722905516624451, 'test_nmll': 1.3158257007598877, 'train_nll': 125.94912719726562, 'test_nll': 21.053104400634766}\n",
      "2019-03-08 14:09:58.765175 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.35304903984069824, 'train_time': 5.795374790002825, 'train_nmll': 0.8423023223876953, 'test_nmll': 0.9002928733825684, 'train_nll': 121.78070068359375, 'test_nll': 14.399946212768555}\n",
      "2019-03-08 14:10:08.345621 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.18667158484458923, 'train_time': 9.573786577006103, 'train_nmll': 0.22534136474132538, 'test_nmll': 0.5282726287841797, 'train_nll': 33.09990692138672, 'test_nll': 8.404766082763672}\n",
      "2019-03-08 14:10:19.938069 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.2593492865562439, 'train_time': 11.58899836699129, 'train_nmll': 0.19392459094524384, 'test_nmll': 0.6492409110069275, 'train_nll': 27.445579528808594, 'test_nll': 10.416126251220703}\n",
      "2019-03-08 14:10:33.868732 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.6305086612701416, 'train_time': 13.926837359991623, 'train_nmll': 0.4006546139717102, 'test_nmll': 1.2234787940979004, 'train_nll': 57.45985412597656, 'test_nll': 19.581188201904297}\n",
      "2019-03-08 14:10:40.731294 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.8387894034385681, 'train_time': 6.857527467014734, 'train_nmll': 0.5168185234069824, 'test_nmll': 1.1193747520446777, 'train_nll': 73.42184448242188, 'test_nll': 17.908931732177734}\n",
      "2019-03-08 14:10:58.115393 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.23883309960365295, 'train_time': 17.38058231800096, 'train_nmll': 0.6235234141349792, 'test_nmll': 0.853502631187439, 'train_nll': 88.71823120117188, 'test_nll': 12.801271438598633}\n",
      "2019-03-08 14:11:19.622421 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.17656974494457245, 'train_time': 21.483805258001667, 'train_nmll': 0.11690212041139603, 'test_nmll': 0.5689595937728882, 'train_nll': 17.25737762451172, 'test_nll': 9.16086483001709}\n",
      "2019-03-08 14:11:33.769910 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.5776062607765198, 'train_time': 14.14400816601119, 'train_nmll': 0.04719047248363495, 'test_nmll': 0.9743664264678955, 'train_nll': 6.621696472167969, 'test_nll': 15.606555938720703}\n",
      "2019-03-08 14:11:46.612740 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.24008680880069733, 'train_time': 12.839529824996134, 'train_nmll': 0.04564816132187843, 'test_nmll': 0.6397778987884521, 'train_nll': 6.454063415527344, 'test_nll': 10.260117530822754}\n",
      "2019-03-08 14:12:10.803643 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.15698227286338806, 'train_time': 24.186110718990676, 'train_nmll': 0.0234375, 'test_nmll': 0.552398681640625, 'train_nll': 3.4620208740234375, 'test_nll': 8.83218765258789}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\\n    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\\n  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\\n    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\\n  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\\n    chol = torch.cholesky(inner_mat)\\nRuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\\n', 'fold': 4, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\n",
      "    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\n",
      "    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\n",
      "    chol = torch.cholesky(inner_mat)\n",
      "RuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:12:20.608435 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1945161372423172, 'train_time': 9.34160316598718, 'train_nmll': -0.0903041809797287, 'test_nmll': 0.641638994216919, 'train_nll': -12.279647827148438, 'test_nll': 10.262689590454102}\n",
      "2019-03-08 14:12:32.434836 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.12187107652425766, 'train_time': 11.82284920901293, 'train_nmll': -0.06065507233142853, 'test_nmll': 0.41546106338500977, 'train_nll': -11.22613525390625, 'test_nll': 6.666135787963867}\n",
      "2019-03-08 14:13:32.368798 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.312171995639801, 'train_time': 59.930365412990795, 'train_nmll': -0.20224767923355103, 'test_nmll': 0.5178393721580505, 'train_nll': -29.865631103515625, 'test_nll': 8.370487213134766}\n",
      "2019-03-08 14:13:42.718091 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.34569960832595825, 'train_time': 10.345654984004796, 'train_nmll': -0.1514802873134613, 'test_nmll': 0.8119896054267883, 'train_nll': -21.878433227539062, 'test_nll': 13.041666984558105}\n",
      "2019-03-08 14:13:48.182868 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.7233875393867493, 'train_time': 5.461087892996147, 'train_nmll': -0.09555181860923767, 'test_nmll': 1.2575700283050537, 'train_nll': -12.345932006835938, 'test_nll': 20.125762939453125}\n",
      "2019-03-08 14:14:00.362221 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1736532300710678, 'train_time': 12.175666283001192, 'train_nmll': -0.022878222167491913, 'test_nmll': 0.5252187252044678, 'train_nll': -2.844482421875, 'test_nll': 7.888259410858154}\n",
      "2019-03-08 14:14:06.841242 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.16592654585838318, 'train_time': 6.44900215297821, 'train_nmll': -0.11772464960813522, 'test_nmll': 0.46332308650016785, 'train_nll': -16.311431884765625, 'test_nll': 7.412599086761475}\n",
      "2019-03-08 14:14:17.121923 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4346371591091156, 'train_time': 10.276802275009686, 'train_nmll': -0.14090648293495178, 'test_nmll': 0.7836636304855347, 'train_nll': -18.89154052734375, 'test_nll': 12.460189819335938}\n",
      "2019-03-08 14:14:30.315571 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.18518581986427307, 'train_time': 13.190024102979805, 'train_nmll': -0.1034187376499176, 'test_nmll': 0.5150939226150513, 'train_nll': -15.100433349609375, 'test_nll': 8.227919578552246}\n",
      "2019-03-08 14:14:40.016749 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.11795708537101746, 'train_time': 9.697432409011526, 'train_nmll': -0.1428419053554535, 'test_nmll': 0.44561684131622314, 'train_nll': -20.46820068359375, 'test_nll': 7.078121185302734}\n",
      "2019-03-08 14:15:07.150139 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.19090856611728668, 'train_time': 27.129687526001362, 'train_nmll': -0.16228565573692322, 'test_nmll': 0.5823249220848083, 'train_nll': -23.799118041992188, 'test_nll': 9.348342895507812}\n",
      "2019-03-08 14:15:16.731454 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.20232638716697693, 'train_time': 9.57750888899318, 'train_nmll': -0.050389643758535385, 'test_nmll': 0.5421998500823975, 'train_nll': -7.09759521484375, 'test_nll': 8.646913528442383}\n",
      "2019-03-08 14:15:33.682116 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.11353116482496262, 'train_time': 16.946772777009755, 'train_nmll': -0.003508667927235365, 'test_nmll': 0.3900919556617737, 'train_nll': -1.26177978515625, 'test_nll': 6.208030700683594}\n",
      "2019-03-08 14:15:48.617468 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.35482627153396606, 'train_time': 14.931471200980013, 'train_nmll': -0.12777891755104065, 'test_nmll': 0.867424726486206, 'train_nll': -18.377487182617188, 'test_nll': 13.868416786193848}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 8, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 103, in step\\n    orig_loss = closure()\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 8, 'n': 159, 'd': 22}\n",
      "errors:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 103, in step\n",
      "    orig_loss = closure()\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:15:53.923426 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 41505.9296875, 'train_time': 1.9586251740111038, 'train_nmll': 0.45276331901550293, 'test_nmll': 0.46097275614738464, 'train_nll': 64.74515533447266, 'test_nll': 7.375564098358154}\n",
      "2019-03-08 14:16:30.215533 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.13076968491077423, 'train_time': 36.28841728399857, 'train_nmll': -0.12709447741508484, 'test_nmll': 0.4864048957824707, 'train_nll': -19.690414428710938, 'test_nll': 7.2825846672058105}\n",
      "2019-03-08 14:16:38.152286 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.09044604003429413, 'train_time': 7.906923036993248, 'train_nmll': -0.0825367122888565, 'test_nmll': 0.3932546377182007, 'train_nll': -11.666275024414062, 'test_nll': 6.315622329711914}\n",
      "2019-03-08 14:16:47.654684 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.40585705637931824, 'train_time': 9.49640967999585, 'train_nmll': -0.055061180144548416, 'test_nmll': 0.7465199828147888, 'train_nll': -8.788528442382812, 'test_nll': 11.916960716247559}\n",
      "2019-03-08 14:16:53.419528 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.2457074075937271, 'train_time': 5.761020931007806, 'train_nmll': -0.2003328502178192, 'test_nmll': 0.6051837801933289, 'train_nll': -28.334625244140625, 'test_nll': 9.641568183898926}\n",
      "2019-03-08 14:17:05.023050 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1795099824666977, 'train_time': 11.599560708011268, 'train_nmll': -0.1496696174144745, 'test_nmll': 0.5656091570854187, 'train_nll': -20.732894897460938, 'test_nll': 9.036949157714844}\n",
      "2019-03-08 14:17:25.989377 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.20437803864479065, 'train_time': 20.9626572499983, 'train_nmll': -0.122774139046669, 'test_nmll': 0.6563225388526917, 'train_nll': -18.315872192382812, 'test_nll': 10.503209114074707}\n",
      "2019-03-08 14:17:36.498631 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.23018339276313782, 'train_time': 10.505344723002054, 'train_nmll': -0.06848561018705368, 'test_nmll': 0.607882559299469, 'train_nll': -9.0186767578125, 'test_nll': 9.721675872802734}\n",
      "2019-03-08 14:17:45.964313 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1917218267917633, 'train_time': 9.462143396987813, 'train_nmll': -0.09417874366044998, 'test_nmll': 0.46893584728240967, 'train_nll': -14.027862548828125, 'test_nll': 7.514325141906738}\n",
      "2019-03-08 14:18:00.545908 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.406509131193161, 'train_time': 14.575970892998157, 'train_nmll': -0.1295274794101715, 'test_nmll': 0.8720583319664001, 'train_nll': -18.948516845703125, 'test_nll': 13.960840225219727}\n",
      "2019-03-08 14:18:07.717098 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.6525846719741821, 'train_time': 7.167509077989962, 'train_nmll': -0.12355777621269226, 'test_nmll': 0.9574419260025024, 'train_nll': -18.521377563476562, 'test_nll': 15.304884910583496}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 9, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 9, 'n': 159, 'd': 22}\n",
      "errors:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:18:18.444484 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.13661883771419525, 'train_time': 7.149603889003629, 'train_nmll': -0.13326729834079742, 'test_nmll': 0.5164383053779602, 'train_nll': -19.358551025390625, 'test_nll': 7.72872257232666}\n",
      "2019-03-08 14:18:30.331524 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.08308592438697815, 'train_time': 11.862930837000022, 'train_nmll': -0.07928808033466339, 'test_nmll': 0.36901962757110596, 'train_nll': -12.385223388671875, 'test_nll': 5.940049171447754}\n",
      "2019-03-08 14:18:44.555082 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3801354169845581, 'train_time': 14.219874179980252, 'train_nmll': -0.060783013701438904, 'test_nmll': 0.7157980799674988, 'train_nll': -9.171951293945312, 'test_nll': 11.460224151611328}\n",
      "2019-03-08 14:18:53.680670 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.14629855751991272, 'train_time': 9.122115468984703, 'train_nmll': -0.1367320865392685, 'test_nmll': 0.44139328598976135, 'train_nll': -20.68218994140625, 'test_nll': 7.058413982391357}\n",
      "2019-03-08 14:19:07.239495 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1417609304189682, 'train_time': 13.555335388984531, 'train_nmll': -0.18039581179618835, 'test_nmll': 0.4635550081729889, 'train_nll': -25.848495483398438, 'test_nll': 7.404731273651123}\n",
      "2019-03-08 14:19:23.927465 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.24277056753635406, 'train_time': 16.683429021009943, 'train_nmll': -0.26608115434646606, 'test_nmll': 0.8075517416000366, 'train_nll': -39.127044677734375, 'test_nll': 12.919578552246094}\n",
      "2019-03-08 14:19:32.283061 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.14385196566581726, 'train_time': 8.352149842015933, 'train_nmll': -0.09783380478620529, 'test_nmll': 0.47959625720977783, 'train_nll': -12.529449462890625, 'test_nll': 7.664990425109863}\n",
      "2019-03-08 14:19:46.345658 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.12413246929645538, 'train_time': 14.056488722999347, 'train_nmll': -0.1234273836016655, 'test_nmll': 0.41152167320251465, 'train_nll': -18.142730712890625, 'test_nll': 6.5477142333984375}\n",
      "2019-03-08 14:20:23.920742 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.2992643713951111, 'train_time': 37.57157892000396, 'train_nmll': -0.1631982922554016, 'test_nmll': 0.769626259803772, 'train_nll': -23.584823608398438, 'test_nll': 12.323799133300781}\n",
      "2019-03-08 14:20:34.442611 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.7068706750869751, 'train_time': 10.517614837997826, 'train_nmll': -0.08706825226545334, 'test_nmll': 1.081244945526123, 'train_nll': -11.960983276367188, 'test_nll': 17.304161071777344}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 9, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:20:51.510048 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.10636666417121887, 'train_time': 14.194881809991784, 'train_nmll': -0.09115155786275864, 'test_nmll': 0.4556005597114563, 'train_nll': -13.22381591796875, 'test_nll': 6.825105667114258}\n",
      "2019-03-08 14:21:09.903431 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.10736089944839478, 'train_time': 18.368212895002216, 'train_nmll': -0.1417558640241623, 'test_nmll': 0.42060020565986633, 'train_nll': -20.408050537109375, 'test_nll': 6.7927775382995605}\n",
      "2019-03-08 14:21:42.940771 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4208256006240845, 'train_time': 33.03380965697579, 'train_nmll': -0.08480194956064224, 'test_nmll': 0.7953730821609497, 'train_nll': -11.72943115234375, 'test_nll': 12.698384284973145}\n",
      "2019-03-08 14:21:53.567960 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.17346805334091187, 'train_time': 10.623219763016095, 'train_nmll': -0.10853256285190582, 'test_nmll': 0.49012935161590576, 'train_nll': -16.725112915039062, 'test_nll': 7.863009452819824}\n",
      "2019-03-08 14:22:08.013920 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.17526158690452576, 'train_time': 14.442307585995877, 'train_nmll': -0.10819345712661743, 'test_nmll': 0.5324387550354004, 'train_nll': -14.474441528320312, 'test_nll': 8.49127197265625}\n",
      "2019-03-08 14:22:21.895557 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.20030879974365234, 'train_time': 13.875234734005062, 'train_nmll': -0.18966428935527802, 'test_nmll': 0.7549245357513428, 'train_nll': -27.547348022460938, 'test_nll': 12.081963539123535}\n",
      "2019-03-08 14:22:35.152567 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.12302247434854507, 'train_time': 13.252782211988233, 'train_nmll': -0.07356464862823486, 'test_nmll': 0.4411713778972626, 'train_nll': -10.56048583984375, 'test_nll': 7.0663580894470215}\n",
      "2019-03-08 14:22:52.449999 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1382351666688919, 'train_time': 17.289537999022286, 'train_nmll': -0.18691109120845795, 'test_nmll': 0.3815033435821533, 'train_nll': -26.175506591796875, 'test_nll': 6.088277816772461}\n",
      "2019-03-08 14:23:05.496278 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.37260621786117554, 'train_time': 13.04209540897864, 'train_nmll': -0.1766902655363083, 'test_nmll': 0.8731400370597839, 'train_nll': -26.897216796875, 'test_nll': 13.981550216674805}\n",
      "2019-03-08 14:23:19.987654 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.6102450489997864, 'train_time': 14.4877474810055, 'train_nmll': -0.16831959784030914, 'test_nmll': 0.9354223012924194, 'train_nll': -24.495132446289062, 'test_nll': 14.944448471069336}\n",
      "2019-03-08 14:23:34.487713 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.13460007309913635, 'train_time': 14.49573063000571, 'train_nmll': -0.10260507464408875, 'test_nmll': 0.4830334484577179, 'train_nll': -15.427108764648438, 'test_nll': 7.248943328857422}\n",
      "2019-03-08 14:23:51.246635 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.14368963241577148, 'train_time': 16.733490524988156, 'train_nmll': -0.10828159749507904, 'test_nmll': 0.41230714321136475, 'train_nll': -14.027236938476562, 'test_nll': 6.610198974609375}\n",
      "2019-03-08 14:24:05.482982 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.41261833906173706, 'train_time': 14.232513413997367, 'train_nmll': -0.10697297006845474, 'test_nmll': 0.794696033000946, 'train_nll': -14.627685546875, 'test_nll': 12.680578231811523}\n",
      "2019-03-08 14:24:40.029198 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.13370200991630554, 'train_time': 34.54249319800874, 'train_nmll': -0.16555434465408325, 'test_nmll': 0.41887545585632324, 'train_nll': -24.143722534179688, 'test_nll': 6.686639785766602}\n",
      "2019-03-08 14:25:00.172480 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.1475655734539032, 'train_time': 20.139952246972825, 'train_nmll': -0.12532928586006165, 'test_nmll': 0.5069997310638428, 'train_nll': -16.917510986328125, 'test_nll': 8.107280731201172}\n",
      "2019-03-08 14:25:24.004968 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.15610875189304352, 'train_time': 23.828500989009626, 'train_nmll': -0.3230317533016205, 'test_nmll': 0.6891703605651855, 'train_nll': -46.2662353515625, 'test_nll': 11.063097953796387}\n",
      "2019-03-08 14:25:38.523957 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.12124950438737869, 'train_time': 14.515390774991829, 'train_nmll': -0.20460841059684753, 'test_nmll': 0.4594149887561798, 'train_nll': -29.158935546875, 'test_nll': 7.367782115936279}\n",
      "2019-03-08 14:25:50.558121 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.11365914344787598, 'train_time': 12.030534223013092, 'train_nmll': -0.15540724992752075, 'test_nmll': 0.37665921449661255, 'train_nll': -22.65460205078125, 'test_nll': 6.0179595947265625}\n",
      "2019-03-08 14:26:03.947747 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.35883116722106934, 'train_time': 13.385787862993311, 'train_nmll': -0.16636539995670319, 'test_nmll': 0.870684802532196, 'train_nll': -23.556121826171875, 'test_nll': 13.950422286987305}\n",
      "2019-03-08 14:26:22.823938 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.610133945941925, 'train_time': 18.87291009601904, 'train_nmll': -0.19221603870391846, 'test_nmll': 0.9812753200531006, 'train_nll': -27.064483642578125, 'test_nll': 15.693445205688477}\n",
      "2019-03-08 14:26:33.911231 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.10151815414428711, 'train_time': 11.08334447699599, 'train_nmll': -0.19293434917926788, 'test_nmll': 0.43915435671806335, 'train_nll': -28.47412109375, 'test_nll': 6.589225769042969}\n",
      "2019-03-08 14:26:42.933425 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.27893856167793274, 'train_time': 8.98368556299829, 'train_nmll': -0.19153429567813873, 'test_nmll': 0.7609740495681763, 'train_nll': -27.654022216796875, 'test_nll': 11.92703914642334}\n",
      "2019-03-08 14:26:49.674222 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.7693889737129211, 'train_time': 6.732058819994563, 'train_nmll': -0.007058416958898306, 'test_nmll': 1.2149207592010498, 'train_nll': -1.5875091552734375, 'test_nll': 19.437458038330078}\n",
      "2019-03-08 14:27:04.125845 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4158634543418884, 'train_time': 14.447896522004157, 'train_nmll': -0.07496365904808044, 'test_nmll': 0.8533337712287903, 'train_nll': -10.857772827148438, 'test_nll': 13.651052474975586}\n",
      "2019-03-08 14:27:11.146286 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3297727406024933, 'train_time': 7.013420428003883, 'train_nmll': -0.0666440948843956, 'test_nmll': 0.7935744524002075, 'train_nll': -10.55780029296875, 'test_nll': 12.702720642089844}\n",
      "2019-03-08 14:27:22.536718 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.32291164994239807, 'train_time': 11.38680487300735, 'train_nmll': -0.10963375866413116, 'test_nmll': 0.7767175436019897, 'train_nll': -15.372344970703125, 'test_nll': 12.40067195892334}\n",
      "2019-03-08 14:27:35.803425 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4984557032585144, 'train_time': 13.262863757991, 'train_nmll': -0.18438933789730072, 'test_nmll': 0.9102010130882263, 'train_nll': -25.557952880859375, 'test_nll': 14.56247329711914}\n",
      "2019-03-08 14:27:54.541440 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.272691935300827, 'train_time': 18.734195450000698, 'train_nmll': 0.051910560578107834, 'test_nmll': 0.7181528806686401, 'train_nll': 7.370475769042969, 'test_nll': 11.495240211486816}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 7, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:28:22.363067 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.8056730031967163, 'train_time': 27.10585295999772, 'train_nmll': -0.1479204148054123, 'test_nmll': 1.33848237991333, 'train_nll': -22.092391967773438, 'test_nll': 21.387901306152344}\n",
      "2019-03-08 14:28:33.838718 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 1.0440566539764404, 'train_time': 11.4717164329777, 'train_nmll': -0.006478689610958099, 'test_nmll': 1.2990843057632446, 'train_nll': -0.598968505859375, 'test_nll': 20.785564422607422}\n",
      "2019-03-08 14:28:47.854568 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3516008257865906, 'train_time': 14.011833450989798, 'train_nmll': -0.2589479982852936, 'test_nmll': 0.9008261561393738, 'train_nll': -36.918426513671875, 'test_nll': 13.51250171661377}\n",
      "2019-03-08 14:29:01.530772 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.2406129240989685, 'train_time': 13.6373815429979, 'train_nmll': 0.03352062404155731, 'test_nmll': 0.714123010635376, 'train_nll': 5.539543151855469, 'test_nll': 11.362279891967773}\n",
      "2019-03-08 14:29:09.983559 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.7554051280021667, 'train_time': 8.449481132993242, 'train_nmll': -0.059838246554136276, 'test_nmll': 1.1330854892730713, 'train_nll': -8.226028442382812, 'test_nll': 18.172494888305664}\n",
      "2019-03-08 14:29:27.988119 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.38220176100730896, 'train_time': 18.00098590302514, 'train_nmll': 0.11841097474098206, 'test_nmll': 0.8118153810501099, 'train_nll': 16.640045166015625, 'test_nll': 12.980876922607422}\n",
      "2019-03-08 14:29:36.048624 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.2908666431903839, 'train_time': 8.056786433997331, 'train_nmll': -0.11746632307767868, 'test_nmll': 0.7831901907920837, 'train_nll': -16.86138916015625, 'test_nll': 12.525798797607422}\n",
      "2019-03-08 14:29:43.877644 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3312620222568512, 'train_time': 7.825455483980477, 'train_nmll': -0.0780467838048935, 'test_nmll': 0.8535538911819458, 'train_nll': -12.1771240234375, 'test_nll': 13.668631553649902}\n",
      "2019-03-08 14:29:57.865651 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.41294997930526733, 'train_time': 13.98146617700695, 'train_nmll': -0.2591683864593506, 'test_nmll': 0.884095311164856, 'train_nll': -37.423553466796875, 'test_nll': 14.140970230102539}\n",
      "2019-03-08 14:30:06.676180 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.34467944502830505, 'train_time': 8.806888165010605, 'train_nmll': -0.03051256388425827, 'test_nmll': 0.7608528137207031, 'train_nll': -4.7044525146484375, 'test_nll': 12.169740676879883}\n",
      "2019-03-08 14:30:21.862029 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.7322721481323242, 'train_time': 15.18215128700831, 'train_nmll': -0.09517856687307358, 'test_nmll': 1.2325713634490967, 'train_nll': -14.348464965820312, 'test_nll': 19.724821090698242}\n",
      "2019-03-08 14:30:33.128480 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 1.0129518508911133, 'train_time': 11.262719243008178, 'train_nmll': -0.09136514365673065, 'test_nmll': 1.2819898128509521, 'train_nll': -11.58721923828125, 'test_nll': 20.512311935424805}\n",
      "2019-03-08 14:30:43.260572 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3524872660636902, 'train_time': 10.128293036977993, 'train_nmll': -0.026272879913449287, 'test_nmll': 0.8655138611793518, 'train_nll': -3.8814849853515625, 'test_nll': 12.983380317687988}\n",
      "2019-03-08 14:30:51.030139 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.22010375559329987, 'train_time': 7.74158415698912, 'train_nmll': 0.08659037202596664, 'test_nmll': 0.689298152923584, 'train_nll': 11.506858825683594, 'test_nll': 11.069173812866211}\n",
      "2019-03-08 14:31:03.434681 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.838775634765625, 'train_time': 12.400796964997426, 'train_nmll': -0.09363187849521637, 'test_nmll': 1.259068250656128, 'train_nll': -14.1807861328125, 'test_nll': 20.136669158935547}\n",
      "2019-03-08 14:31:11.967204 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3593193292617798, 'train_time': 8.52737039900967, 'train_nmll': 0.004876303486526012, 'test_nmll': 0.796618640422821, 'train_nll': 0.7188568115234375, 'test_nll': 12.744314193725586}\n",
      "2019-03-08 14:31:21.153165 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4063740670681, 'train_time': 9.1821994180209, 'train_nmll': -0.09126175194978714, 'test_nmll': 0.8059689998626709, 'train_nll': -12.700515747070312, 'test_nll': 12.921320915222168}\n",
      "2019-03-08 14:31:34.531819 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3157857060432434, 'train_time': 13.374807870015502, 'train_nmll': -0.0023560423869639635, 'test_nmll': 0.8213368654251099, 'train_nll': -1.07958984375, 'test_nll': 13.142111778259277}\n",
      "2019-03-08 14:31:41.919819 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.47997280955314636, 'train_time': 7.384446265001316, 'train_nmll': -0.12940093874931335, 'test_nmll': 0.9175721406936646, 'train_nll': -18.566741943359375, 'test_nll': 14.674173355102539}\n",
      "2019-03-08 14:31:52.492053 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.25332263112068176, 'train_time': 10.568562124011805, 'train_nmll': 0.05658828467130661, 'test_nmll': 0.7423734664916992, 'train_nll': 7.889244079589844, 'test_nll': 11.87951946258545}\n",
      "2019-03-08 14:32:07.077100 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.7974663972854614, 'train_time': 14.581604328996036, 'train_nmll': -0.1181773990392685, 'test_nmll': 1.296560525894165, 'train_nll': -16.12841796875, 'test_nll': 20.742311477661133}\n",
      "2019-03-08 14:32:14.080052 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.9784110188484192, 'train_time': 6.999397248990135, 'train_nmll': -0.03742927685379982, 'test_nmll': 1.280076503753662, 'train_nll': -5.197967529296875, 'test_nll': 20.51386833190918}\n",
      "2019-03-08 14:32:21.655868 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3105441927909851, 'train_time': 7.571680584980641, 'train_nmll': -0.04826121777296066, 'test_nmll': 0.8466572165489197, 'train_nll': -7.7037353515625, 'test_nll': 12.701116561889648}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 0, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:32:33.612221 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.23729956150054932, 'train_time': 10.311507329024607, 'train_nmll': 0.011728459969162941, 'test_nmll': 0.706745982170105, 'train_nll': 1.6125640869140625, 'test_nll': 11.256771087646484}\n",
      "2019-03-08 14:32:48.527680 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.8799811005592346, 'train_time': 14.91170613700524, 'train_nmll': 0.0038738118018954992, 'test_nmll': 1.2625765800476074, 'train_nll': 0.5995025634765625, 'test_nll': 20.207355499267578}\n",
      "2019-03-08 14:32:56.504221 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.31762516498565674, 'train_time': 7.9730669840064365, 'train_nmll': -0.04397764429450035, 'test_nmll': 0.7613165378570557, 'train_nll': -6.6075592041015625, 'test_nll': 12.181962966918945}\n",
      "2019-03-08 14:33:05.150898 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.27200260758399963, 'train_time': 8.640040491998661, 'train_nmll': -0.030257325619459152, 'test_nmll': 0.7373051047325134, 'train_nll': -4.1565704345703125, 'test_nll': 11.79772663116455}\n",
      "2019-03-08 14:33:20.241830 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3544825315475464, 'train_time': 15.08729505399242, 'train_nmll': -0.03859363868832588, 'test_nmll': 0.8545262813568115, 'train_nll': -4.9763031005859375, 'test_nll': 13.677152633666992}\n",
      "2019-03-08 14:33:28.559820 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4036470651626587, 'train_time': 8.314237173995934, 'train_nmll': 0.017214475199580193, 'test_nmll': 0.8454638719558716, 'train_nll': 2.19293212890625, 'test_nll': 13.52223014831543}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 6, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:33:41.797835 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3035311698913574, 'train_time': 12.344826486019883, 'train_nmll': -0.06404593586921692, 'test_nmll': 0.7755491733551025, 'train_nll': -8.471343994140625, 'test_nll': 12.401782035827637}\n",
      "2019-03-08 14:33:49.948563 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.6898965835571289, 'train_time': 8.146650928014424, 'train_nmll': -0.08684747666120529, 'test_nmll': 1.2064937353134155, 'train_nll': -12.13031005859375, 'test_nll': 19.30109977722168}\n",
      "2019-03-08 14:34:00.490611 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.9898037910461426, 'train_time': 10.538494542008266, 'train_nmll': -0.03390993922948837, 'test_nmll': 1.2881741523742676, 'train_nll': -5.2034454345703125, 'test_nll': 20.58578109741211}\n",
      "2019-03-08 14:34:21.606501 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.31545260548591614, 'train_time': 21.11253996400046, 'train_nmll': -0.2058866322040558, 'test_nmll': 0.8447378277778625, 'train_nll': -29.97784423828125, 'test_nll': 12.66854476928711}\n",
      "2019-03-08 14:34:32.010526 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.22920666635036469, 'train_time': 10.364301560010063, 'train_nmll': -0.01250484399497509, 'test_nmll': 0.7052047848701477, 'train_nll': -2.344970703125, 'test_nll': 11.283954620361328}\n",
      "2019-03-08 14:34:45.454889 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.8694082498550415, 'train_time': 13.440844920987729, 'train_nmll': -0.0526222288608551, 'test_nmll': 1.2627878189086914, 'train_nll': -7.5894012451171875, 'test_nll': 20.18362045288086}\n",
      "2019-03-08 14:35:01.652814 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3624790608882904, 'train_time': 16.19441158999689, 'train_nmll': 0.0019327444024384022, 'test_nmll': 0.8065248727798462, 'train_nll': 1.0848236083984375, 'test_nll': 12.902665138244629}\n",
      "2019-03-08 14:35:22.632573 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.32474061846733093, 'train_time': 20.976111102994764, 'train_nmll': -0.06391287595033646, 'test_nmll': 0.7732614278793335, 'train_nll': -8.9788818359375, 'test_nll': 12.373355865478516}\n",
      "2019-03-08 14:35:43.703677 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3718062937259674, 'train_time': 21.06721509998897, 'train_nmll': -0.16855797171592712, 'test_nmll': 0.9017978310585022, 'train_nll': -23.214859008789062, 'test_nll': 14.42662239074707}\n",
      "2019-03-08 14:35:54.536790 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.43001386523246765, 'train_time': 10.829119586996967, 'train_nmll': -0.07271890342235565, 'test_nmll': 0.8527277708053589, 'train_nll': -9.732772827148438, 'test_nll': 13.633977890014648}\n",
      "2019-03-08 14:36:05.686449 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.27199786901474, 'train_time': 11.145848730986472, 'train_nmll': -0.058740150183439255, 'test_nmll': 0.7362580299377441, 'train_nll': -9.781585693359375, 'test_nll': 11.784875869750977}\n",
      "2019-03-08 14:36:21.437704 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.7241330146789551, 'train_time': 15.747308984980918, 'train_nmll': -0.10151427239179611, 'test_nmll': 1.2260624170303345, 'train_nll': -14.41302490234375, 'test_nll': 19.604122161865234}\n",
      "2019-03-08 14:36:34.098956 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.9708901643753052, 'train_time': 12.655010887014214, 'train_nmll': -0.03267984464764595, 'test_nmll': 1.2688508033752441, 'train_nll': -4.1753692626953125, 'test_nll': 20.402137756347656}\n",
      "2019-03-08 14:36:43.456612 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.31646251678466797, 'train_time': 9.349065685004462, 'train_nmll': -0.04721863940358162, 'test_nmll': 0.8452065587043762, 'train_nll': -6.1408233642578125, 'test_nll': 12.679878234863281}\n",
      "2019-03-08 14:37:16.456089 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.23431454598903656, 'train_time': 32.97162235301221, 'train_nmll': -0.11564870923757553, 'test_nmll': 0.700040876865387, 'train_nll': -15.844467163085938, 'test_nll': 11.198128700256348}\n",
      "2019-03-08 14:37:33.147745 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.8071094751358032, 'train_time': 16.68815042701317, 'train_nmll': -0.04349203035235405, 'test_nmll': 1.221433401107788, 'train_nll': -6.942779541015625, 'test_nll': 19.547161102294922}\n",
      "2019-03-08 14:37:49.111862 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.35476893186569214, 'train_time': 15.960514513979433, 'train_nmll': -0.016508622094988823, 'test_nmll': 0.792110800743103, 'train_nll': -1.9571685791015625, 'test_nll': 12.664642333984375}\n",
      "2019-03-08 14:38:01.990321 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.33317723870277405, 'train_time': 12.874662927992176, 'train_nmll': -0.08082260191440582, 'test_nmll': 0.7797892093658447, 'train_nll': -10.871185302734375, 'test_nll': 12.478660583496094}\n",
      "2019-03-08 14:38:13.869949 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.32520803809165955, 'train_time': 11.87604459602153, 'train_nmll': -0.15022629499435425, 'test_nmll': 0.8509217500686646, 'train_nll': -22.28363037109375, 'test_nll': 13.606977462768555}\n",
      "2019-03-08 14:38:25.945079 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4196067452430725, 'train_time': 12.067652270983672, 'train_nmll': -0.05873236060142517, 'test_nmll': 0.8709287643432617, 'train_nll': -9.253768920898438, 'test_nll': 13.92979621887207}\n",
      "2019-03-08 14:38:41.793406 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3011998236179352, 'train_time': 15.844757891987683, 'train_nmll': -0.09624795615673065, 'test_nmll': 0.7879551649093628, 'train_nll': -14.042160034179688, 'test_nll': 12.607784271240234}\n",
      "2019-03-08 14:38:56.694704 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.7239530682563782, 'train_time': 14.897581786994124, 'train_nmll': -0.059722900390625, 'test_nmll': 1.229365587234497, 'train_nll': -8.091110229492188, 'test_nll': 19.666767120361328}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\\n    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\\n  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\\n    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\\n  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\\n    chol = torch.cholesky(inner_mat)\\nRuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\\n', 'fold': 8, 'n': 159, 'd': 22}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\n",
      "    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\n",
      "    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\n",
      "    chol = torch.cholesky(inner_mat)\n",
      "RuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:39:11.961994 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 1.007610559463501, 'train_time': 12.54413704900071, 'train_nmll': -0.133518785238266, 'test_nmll': 1.2724322080612183, 'train_nll': -18.7601318359375, 'test_nll': 20.329105377197266}\n",
      "2019-03-08 14:39:26.962389 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.33666521310806274, 'train_time': 14.994443010975374, 'train_nmll': 0.006609810516238213, 'test_nmll': 0.8599516749382019, 'train_nll': 0.406463623046875, 'test_nll': 12.898994445800781}\n",
      "2019-03-08 14:39:50.803680 - {'fold': 0, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.23760825395584106, 'train_time': 23.813724523992278, 'train_nmll': 0.09305460751056671, 'test_nmll': 0.7170102000236511, 'train_nll': 13.613479614257812, 'test_nll': 11.556758880615234}\n",
      "2019-03-08 14:40:20.623755 - {'fold': 1, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.775691032409668, 'train_time': 29.81642229098361, 'train_nmll': -0.1887618899345398, 'test_nmll': 1.2140438556671143, 'train_nll': -26.82379150390625, 'test_nll': 19.400127410888672}\n",
      "2019-03-08 14:40:49.986640 - {'fold': 2, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3469666540622711, 'train_time': 29.357963568007108, 'train_nmll': -0.10050356388092041, 'test_nmll': 0.7843126058578491, 'train_nll': -14.540283203125, 'test_nll': 12.54167366027832}\n",
      "2019-03-08 14:41:08.668042 - {'fold': 3, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.32757073640823364, 'train_time': 18.67470501700882, 'train_nmll': -6.711733294650912e-05, 'test_nmll': 0.7878948450088501, 'train_nll': -0.0810546875, 'test_nll': 12.608223915100098}\n",
      "2019-03-08 14:41:20.597550 - {'fold': 4, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.32526126503944397, 'train_time': 11.925869786995463, 'train_nmll': -0.21609753370285034, 'test_nmll': 0.8425049781799316, 'train_nll': -30.455047607421875, 'test_nll': 13.490076065063477}\n",
      "2019-03-08 14:41:44.000690 - {'fold': 5, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.4246385097503662, 'train_time': 23.39952074800385, 'train_nmll': 0.02015920728445053, 'test_nmll': 0.8641582131385803, 'train_nll': 2.9452667236328125, 'test_nll': 13.832160949707031}\n",
      "2019-03-08 14:42:27.112506 - {'fold': 6, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.31694644689559937, 'train_time': 43.108126983977854, 'train_nmll': -0.10663962364196777, 'test_nmll': 0.7812652587890625, 'train_nll': -14.671768188476562, 'test_nll': 12.498332023620605}\n",
      "2019-03-08 14:42:41.227842 - {'fold': 7, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.7127813100814819, 'train_time': 14.111729183990974, 'train_nmll': -0.08647550642490387, 'test_nmll': 1.2292206287384033, 'train_nll': -12.147293090820312, 'test_nll': 19.67540740966797}\n",
      "2019-03-08 14:43:07.155334 - {'fold': 8, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.9849609732627869, 'train_time': 25.923914638988208, 'train_nmll': -0.09415771812200546, 'test_nmll': 1.2762420177459717, 'train_nll': -13.02142333984375, 'test_nll': 20.4769287109375}\n",
      "2019-03-08 14:43:33.988975 - {'fold': 9, 'repeat': 0, 'n': 159, 'd': 22, 'mse': 0.3281995952129364, 'train_time': 26.830079145001946, 'train_nmll': -0.2388269603252411, 'test_nmll': 0.8826568722724915, 'train_nll': -34.57002258300781, 'test_nll': 13.2365083694458}\n",
      "fertility starting\n",
      "2019-03-08 14:43:48.290202 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.264928936958313, 'train_time': 14.26223996700719, 'train_nmll': 0.8890060186386108, 'test_nmll': 1.5740500688552856, 'train_nll': 79.83500671386719, 'test_nll': 15.740500450134277}\n",
      "2019-03-08 14:44:08.194456 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0403099060058594, 'train_time': 19.90051661900361, 'train_nmll': 0.8284170627593994, 'test_nmll': 1.3888746500015259, 'train_nll': 74.84123229980469, 'test_nll': 13.88874626159668}\n",
      "2019-03-08 14:44:35.832109 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 445364.4375, 'train_time': 27.634035997005412, 'train_nmll': 7.99030876159668, 'test_nmll': 8.245348930358887, 'train_nll': 719.39208984375, 'test_nll': 82.4534912109375}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\\n    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\\n  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\\n    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\\n  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\\n    chol = torch.cholesky(inner_mat)\\nRuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\\n', 'fold': 3, 'n': 100, 'd': 7}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\n",
      "    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\n",
      "    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\n",
      "    chol = torch.cholesky(inner_mat)\n",
      "RuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:45:22.680621 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 16491718115328.0, 'train_time': 16.734787861001678, 'train_nmll': 31590.5390625, 'test_nmll': 84850.984375, 'train_nll': 2843148.5, 'test_nll': 848509.8125}\n",
      "2019-03-08 14:45:38.024605 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.6219282150268555, 'train_time': 15.340287744009402, 'train_nmll': 0.8970413208007812, 'test_nmll': 1.2441246509552002, 'train_nll': 79.65084838867188, 'test_nll': 12.441247940063477}\n",
      "2019-03-08 14:45:50.179813 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.079095721244812, 'train_time': 12.151423428003909, 'train_nmll': 0.9457610249519348, 'test_nmll': 1.4531781673431396, 'train_nll': 84.83412170410156, 'test_nll': 14.531781196594238}\n",
      "2019-03-08 14:45:58.023500 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 9.927581787109375, 'train_time': 7.839041178987827, 'train_nmll': 7.104040622711182, 'test_nmll': 7.159770965576172, 'train_nll': 640.25830078125, 'test_nll': 71.59770965576172}\n",
      "2019-03-08 14:46:17.903701 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4925163686275482, 'train_time': 19.87607878498966, 'train_nmll': 0.9967119097709656, 'test_nmll': 1.1540700197219849, 'train_nll': 89.84651947021484, 'test_nll': 11.54069995880127}\n",
      "2019-03-08 14:46:37.689830 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.5033934116363525, 'train_time': 19.782618413999444, 'train_nmll': 0.8642967343330383, 'test_nmll': 1.6584255695343018, 'train_nll': 77.01590728759766, 'test_nll': 16.58425521850586}\n",
      "2019-03-08 14:46:56.832864 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4115186631679535, 'train_time': 19.13921662600478, 'train_nmll': 1.2134060859680176, 'test_nmll': 1.1109511852264404, 'train_nll': 109.23860168457031, 'test_nll': 11.109511375427246}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py:502: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  df = pd.concat([df, result])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:46:59.538281 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 20800.30078125, 'train_time': 2.6771150800050236, 'train_nmll': 0.4527634382247925, 'test_nmll': 0.4850500524044037, 'train_nll': 40.74871063232422, 'test_nll': 4.850500583648682}\n",
      "2019-03-08 14:47:08.205372 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.234758734703064, 'train_time': 8.663382529979572, 'train_nmll': 1.4009380340576172, 'test_nmll': 1.5498870611190796, 'train_nll': 126.08441162109375, 'test_nll': 15.49885368347168}\n",
      "2019-03-08 14:47:15.678395 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0930030345916748, 'train_time': 7.469159871019656, 'train_nmll': 1.378194808959961, 'test_nmll': 1.4871351718902588, 'train_nll': 124.03754425048828, 'test_nll': 14.871349334716797}\n",
      "2019-03-08 14:47:21.252678 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1339285373687744, 'train_time': 5.5705698219826445, 'train_nmll': 1.397795557975769, 'test_nmll': 1.495357871055603, 'train_nll': 125.80157470703125, 'test_nll': 14.953575134277344}\n",
      "2019-03-08 14:47:31.609424 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.38133206963539124, 'train_time': 10.352817212988157, 'train_nmll': 1.4524025917053223, 'test_nmll': 1.1439001560211182, 'train_nll': 130.71624755859375, 'test_nll': 11.439004898071289}\n",
      "2019-03-08 14:47:38.542549 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.367119312286377, 'train_time': 6.929346557997633, 'train_nmll': 1.3595523834228516, 'test_nmll': 1.5960747003555298, 'train_nll': 122.35971069335938, 'test_nll': 15.96074390411377}\n",
      "2019-03-08 14:47:46.606457 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2938010692596436, 'train_time': 8.060186762013473, 'train_nmll': 1.4000358581542969, 'test_nmll': 1.5540493726730347, 'train_nll': 126.00321960449219, 'test_nll': 15.540491104125977}\n",
      "2019-03-08 14:47:55.796423 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.49088019132614136, 'train_time': 9.18208800200955, 'train_nmll': 1.4413706064224243, 'test_nmll': 1.181410551071167, 'train_nll': 129.72337341308594, 'test_nll': 11.814108848571777}\n",
      "2019-03-08 14:48:04.399607 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2807154655456543, 'train_time': 8.598892067995621, 'train_nmll': 1.3879081010818481, 'test_nmll': 1.524907112121582, 'train_nll': 124.91172790527344, 'test_nll': 15.249074935913086}\n",
      "2019-03-08 14:48:11.094026 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.3507159352302551, 'train_time': 6.689289230009308, 'train_nmll': 1.4062577486038208, 'test_nmll': 1.0972418785095215, 'train_nll': 126.56314849853516, 'test_nll': 10.97241497039795}\n",
      "2019-03-08 14:48:19.388602 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9978078603744507, 'train_time': 8.266530490014702, 'train_nmll': 1.371442437171936, 'test_nmll': 1.4093754291534424, 'train_nll': 123.43016052246094, 'test_nll': 14.093754768371582}\n",
      "2019-03-08 14:48:29.435942 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.290586233139038, 'train_time': 10.043637816997943, 'train_nmll': 1.3729687929153442, 'test_nmll': 1.5492156744003296, 'train_nll': 123.5524673461914, 'test_nll': 15.492158889770508}\n",
      "2019-03-08 14:48:38.899380 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0113716125488281, 'train_time': 9.458334433991695, 'train_nmll': 1.3907173871994019, 'test_nmll': 1.4465974569320679, 'train_nll': 125.16458129882812, 'test_nll': 14.46597671508789}\n",
      "2019-03-08 14:48:47.326374 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0808460712432861, 'train_time': 8.423465732019395, 'train_nmll': 1.369693636894226, 'test_nmll': 1.4757028818130493, 'train_nll': 123.2763671875, 'test_nll': 14.757028579711914}\n",
      "2019-03-08 14:48:55.263780 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.3770430088043213, 'train_time': 7.933471613010624, 'train_nmll': 1.340590000152588, 'test_nmll': 1.089167833328247, 'train_nll': 120.6571044921875, 'test_nll': 10.891682624816895}\n",
      "2019-03-08 14:49:05.418659 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.8066099286079407, 'train_time': 10.151000081008533, 'train_nmll': 1.367210865020752, 'test_nmll': 1.309124231338501, 'train_nll': 123.04835510253906, 'test_nll': 13.091242790222168}\n",
      "2019-03-08 14:49:13.134770 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.4899766445159912, 'train_time': 7.702977615001146, 'train_nmll': 1.2994270324707031, 'test_nmll': 1.6623445749282837, 'train_nll': 116.91667175292969, 'test_nll': 16.623437881469727}\n",
      "2019-03-08 14:49:26.159791 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.5158423185348511, 'train_time': 13.020656364999013, 'train_nmll': 1.3824408054351807, 'test_nmll': 1.1815801858901978, 'train_nll': 124.35205078125, 'test_nll': 11.815803527832031}\n",
      "2019-03-08 14:49:34.053691 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0972968339920044, 'train_time': 7.890071403991897, 'train_nmll': 1.28008234500885, 'test_nmll': 1.439121961593628, 'train_nll': 115.20658874511719, 'test_nll': 14.391216278076172}\n",
      "2019-03-08 14:49:43.129641 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.37361419200897217, 'train_time': 9.072123570018448, 'train_nmll': 1.4369378089904785, 'test_nmll': 1.137393832206726, 'train_nll': 129.32858276367188, 'test_nll': 11.37393856048584}\n",
      "2019-03-08 14:50:00.437119 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.348083257675171, 'train_time': 17.276129739999305, 'train_nmll': 1.388431429862976, 'test_nmll': 1.5969921350479126, 'train_nll': 124.97183227539062, 'test_nll': 15.969921112060547}\n",
      "2019-03-08 14:50:08.055183 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0536515712738037, 'train_time': 7.613405741023598, 'train_nmll': 1.3143037557601929, 'test_nmll': 1.39742112159729, 'train_nll': 118.30406188964844, 'test_nll': 13.974210739135742}\n",
      "2019-03-08 14:50:17.874823 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.097521185874939, 'train_time': 9.809060986997792, 'train_nmll': 1.2762054204940796, 'test_nmll': 1.4989229440689087, 'train_nll': 115.02099609375, 'test_nll': 14.989229202270508}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\\n    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\\n  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\\n    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\\n  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\\n    chol = torch.cholesky(inner_mat)\\nRuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\\n', 'fold': 3, 'n': 100, 'd': 7}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\n",
      "    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\n",
      "    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\n",
      "    chol = torch.cholesky(inner_mat)\n",
      "RuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:50:33.004523 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9635717272758484, 'train_time': 6.851173711998854, 'train_nmll': 1.2947497367858887, 'test_nmll': 1.4040790796279907, 'train_nll': 116.95860290527344, 'test_nll': 14.040788650512695}\n",
      "2019-03-08 14:50:56.693688 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.21281597018241882, 'train_time': 23.68324826200842, 'train_nmll': 1.3299086093902588, 'test_nmll': 1.0038920640945435, 'train_nll': 119.80040740966797, 'test_nll': 10.038920402526855}\n",
      "2019-03-08 14:51:05.745319 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1366841793060303, 'train_time': 9.047942071978468, 'train_nmll': 1.2888596057891846, 'test_nmll': 1.5162943601608276, 'train_nll': 115.82136535644531, 'test_nll': 15.162944793701172}\n",
      "2019-03-08 14:51:26.197319 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.4030392169952393, 'train_time': 20.448035156005062, 'train_nmll': 1.3310960531234741, 'test_nmll': 1.6092472076416016, 'train_nll': 119.90960693359375, 'test_nll': 16.092472076416016}\n",
      "2019-03-08 14:51:35.983058 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.5772430300712585, 'train_time': 9.781870222999714, 'train_nmll': 1.3101871013641357, 'test_nmll': 1.2045061588287354, 'train_nll': 118.15950012207031, 'test_nll': 12.045063972473145}\n",
      "2019-03-08 14:51:46.439574 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.7384812831878662, 'train_time': 10.452520505990833, 'train_nmll': 1.2458977699279785, 'test_nmll': 1.798555612564087, 'train_nll': 111.79933166503906, 'test_nll': 17.985553741455078}\n",
      "2019-03-08 14:51:52.915942 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.512022078037262, 'train_time': 6.47234159099753, 'train_nmll': 1.33366858959198, 'test_nmll': 1.1768057346343994, 'train_nll': 119.89918518066406, 'test_nll': 11.768060684204102}\n",
      "2019-03-08 14:52:03.442439 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.465968370437622, 'train_time': 10.495862713985844, 'train_nmll': 1.343091607093811, 'test_nmll': 1.6841850280761719, 'train_nll': 120.50605773925781, 'test_nll': 16.84185028076172}\n",
      "2019-03-08 14:52:15.820136 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.083330750465393, 'train_time': 12.373225924005965, 'train_nmll': 1.346921682357788, 'test_nmll': 1.4786382913589478, 'train_nll': 121.47450256347656, 'test_nll': 14.786381721496582}\n",
      "2019-03-08 14:52:23.715537 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2901040315628052, 'train_time': 7.8915701159858145, 'train_nmll': 1.3191845417022705, 'test_nmll': 1.560213327407837, 'train_nll': 118.94075012207031, 'test_nll': 15.602132797241211}\n",
      "2019-03-08 14:52:32.179791 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.167921543121338, 'train_time': 8.460427284997422, 'train_nmll': 1.3125132322311401, 'test_nmll': 1.5376144647598267, 'train_nll': 118.31163024902344, 'test_nll': 15.37614631652832}\n",
      "2019-03-08 14:52:41.938441 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.523246169090271, 'train_time': 9.754861131019425, 'train_nmll': 1.3090330362319946, 'test_nmll': 1.161994218826294, 'train_nll': 117.60701751708984, 'test_nll': 11.619937896728516}\n",
      "2019-03-08 14:52:53.055045 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0856573581695557, 'train_time': 11.112654685013695, 'train_nmll': 1.3108407258987427, 'test_nmll': 1.4746737480163574, 'train_nll': 118.3980941772461, 'test_nll': 14.746740341186523}\n",
      "2019-03-08 14:53:01.092989 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3683655261993408, 'train_time': 8.033270535990596, 'train_nmll': 1.235012412071228, 'test_nmll': 1.5853075981140137, 'train_nll': 111.068603515625, 'test_nll': 15.853076934814453}\n",
      "2019-03-08 14:53:12.763788 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.47868281602859497, 'train_time': 11.663270559016382, 'train_nmll': 1.333884596824646, 'test_nmll': 1.1421239376068115, 'train_nll': 120.20669555664062, 'test_nll': 11.421236991882324}\n",
      "2019-03-08 14:53:46.145873 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3342736959457397, 'train_time': 33.377325855981326, 'train_nmll': 1.3080222606658936, 'test_nmll': 1.561565637588501, 'train_nll': 117.961669921875, 'test_nll': 15.615657806396484}\n",
      "2019-03-08 14:53:57.022468 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.46177569031715393, 'train_time': 10.872524988022633, 'train_nmll': 1.2668187618255615, 'test_nmll': 1.1186408996582031, 'train_nll': 113.91703796386719, 'test_nll': 11.186408042907715}\n",
      "2019-03-08 14:54:04.597286 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0855021476745605, 'train_time': 7.544429305999074, 'train_nmll': 1.2125136852264404, 'test_nmll': 1.4780938625335693, 'train_nll': 109.24201965332031, 'test_nll': 14.780939102172852}\n",
      "2019-03-08 14:54:11.346846 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2314432859420776, 'train_time': 6.745417553989682, 'train_nmll': 1.197031855583191, 'test_nmll': 1.547053575515747, 'train_nll': 107.59007263183594, 'test_nll': 15.470536231994629}\n",
      "2019-03-08 14:54:43.663243 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.18961501121521, 'train_time': 32.31027526498656, 'train_nmll': 1.19833505153656, 'test_nmll': 1.5567747354507446, 'train_nll': 108.1694564819336, 'test_nll': 15.567748069763184}\n",
      "2019-03-08 14:54:58.141983 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.7465506792068481, 'train_time': 14.474868626974057, 'train_nmll': 1.1450536251068115, 'test_nmll': 1.313237190246582, 'train_nll': 102.74207305908203, 'test_nll': 13.132369041442871}\n",
      "2019-03-08 14:55:07.475756 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.3816607594490051, 'train_time': 9.329757803003304, 'train_nmll': 1.2913556098937988, 'test_nmll': 1.090105652809143, 'train_nll': 116.72836303710938, 'test_nll': 10.901056289672852}\n",
      "2019-03-08 14:55:17.957538 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.235282063484192, 'train_time': 10.478034446015954, 'train_nmll': 1.2400625944137573, 'test_nmll': 1.5509364604949951, 'train_nll': 111.70661926269531, 'test_nll': 15.50936508178711}\n",
      "2019-03-08 14:55:25.251131 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3096873760223389, 'train_time': 7.283674995996989, 'train_nmll': 1.1927355527877808, 'test_nmll': 1.6067663431167603, 'train_nll': 106.53038787841797, 'test_nll': 16.06766128540039}\n",
      "2019-03-08 14:55:40.198947 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.3417482376098633, 'train_time': 14.943926995998481, 'train_nmll': 1.2581638097763062, 'test_nmll': 1.0674651861190796, 'train_nll': 113.55221557617188, 'test_nll': 10.674651145935059}\n",
      "2019-03-08 14:55:50.482894 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.6185362339019775, 'train_time': 10.279692754993448, 'train_nmll': 1.230891227722168, 'test_nmll': 1.741906762123108, 'train_nll': 110.27982330322266, 'test_nll': 17.4190673828125}\n",
      "2019-03-08 14:55:57.547800 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.42128437757492065, 'train_time': 7.061023991991533, 'train_nmll': 1.2364330291748047, 'test_nmll': 1.0997543334960938, 'train_nll': 111.41716003417969, 'test_nll': 10.997542381286621}\n",
      "2019-03-08 14:56:11.291584 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1062482595443726, 'train_time': 13.7113330209977, 'train_nmll': 1.1695117950439453, 'test_nmll': 1.4511085748672485, 'train_nll': 104.68415069580078, 'test_nll': 14.511085510253906}\n",
      "2019-03-08 14:56:35.644253 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2632542848587036, 'train_time': 24.347896049002884, 'train_nmll': 1.1950750350952148, 'test_nmll': 1.5916941165924072, 'train_nll': 107.8067626953125, 'test_nll': 15.916940689086914}\n",
      "2019-03-08 14:56:54.846421 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3809082508087158, 'train_time': 19.198351251980057, 'train_nmll': 1.1951638460159302, 'test_nmll': 1.6771637201309204, 'train_nll': 107.25739288330078, 'test_nll': 16.771636962890625}\n",
      "2019-03-08 14:57:00.633859 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9232759475708008, 'train_time': 5.782487853983184, 'train_nmll': 1.2192387580871582, 'test_nmll': 1.372717022895813, 'train_nll': 109.21623992919922, 'test_nll': 13.72716999053955}\n",
      "2019-03-08 14:57:11.907188 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.2955033779144287, 'train_time': 11.269806172000244, 'train_nmll': 1.1457074880599976, 'test_nmll': 1.015824556350708, 'train_nll': 102.62140655517578, 'test_nll': 10.158246040344238}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 14:57:20.345832 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0258893966674805, 'train_time': 8.43485918402439, 'train_nmll': 1.173207402229309, 'test_nmll': 1.4168391227722168, 'train_nll': 105.15347290039062, 'test_nll': 14.168391227722168}\n",
      "2019-03-08 14:57:41.444357 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1336584091186523, 'train_time': 21.09439333001501, 'train_nmll': 1.2190454006195068, 'test_nmll': 1.4866167306900024, 'train_nll': 109.19100952148438, 'test_nll': 14.866167068481445}\n",
      "2019-03-08 14:57:54.340584 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.46431487798690796, 'train_time': 12.892431886983104, 'train_nmll': 1.2407082319259644, 'test_nmll': 1.1199538707733154, 'train_nll': 111.68094635009766, 'test_nll': 11.199536323547363}\n",
      "2019-03-08 14:58:07.410346 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.624249815940857, 'train_time': 13.065088233997812, 'train_nmll': 1.2037755250930786, 'test_nmll': 1.6814771890640259, 'train_nll': 107.89654541015625, 'test_nll': 16.81477165222168}\n",
      "2019-03-08 14:58:18.984355 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4604283273220062, 'train_time': 11.570246663002763, 'train_nmll': 1.2849417924880981, 'test_nmll': 1.1403944492340088, 'train_nll': 115.22919464111328, 'test_nll': 11.403943061828613}\n",
      "2019-03-08 14:58:34.589649 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0312050580978394, 'train_time': 15.573387831012951, 'train_nmll': 1.2148303985595703, 'test_nmll': 1.4277747869491577, 'train_nll': 110.22175598144531, 'test_nll': 14.277748107910156}\n",
      "2019-03-08 14:59:11.960966 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9280256032943726, 'train_time': 37.36534213798586, 'train_nmll': 1.1546000242233276, 'test_nmll': 1.3966064453125, 'train_nll': 103.72352600097656, 'test_nll': 13.966062545776367}\n",
      "2019-03-08 14:59:24.807263 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9897260665893555, 'train_time': 12.842317414993886, 'train_nmll': 1.1716201305389404, 'test_nmll': 1.4393501281738281, 'train_nll': 105.66075134277344, 'test_nll': 14.393501281738281}\n",
      "2019-03-08 14:59:35.954262 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1552881002426147, 'train_time': 11.13842262502294, 'train_nmll': 1.1269822120666504, 'test_nmll': 1.530423641204834, 'train_nll': 100.76229095458984, 'test_nll': 15.304235458374023}\n",
      "2019-03-08 14:59:48.812312 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.3164794147014618, 'train_time': 12.854293951997533, 'train_nmll': 1.2081732749938965, 'test_nmll': 1.0586249828338623, 'train_nll': 107.63639831542969, 'test_nll': 10.586250305175781}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 5, 'n': 100, 'd': 7}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 15:00:02.670652 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.8541119694709778, 'train_time': 12.767304843990132, 'train_nmll': 1.17422616481781, 'test_nmll': 1.3443543910980225, 'train_nll': 105.53443908691406, 'test_nll': 13.443544387817383}\n",
      "2019-03-08 15:00:22.059337 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3727688789367676, 'train_time': 19.3850865449931, 'train_nmll': 1.1805741786956787, 'test_nmll': 1.6167621612548828, 'train_nll': 106.46190643310547, 'test_nll': 16.167621612548828}\n",
      "2019-03-08 15:00:31.451022 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.7738416194915771, 'train_time': 9.387877487984952, 'train_nmll': 1.1394760608673096, 'test_nmll': 1.2686007022857666, 'train_nll': 103.52351379394531, 'test_nll': 12.686001777648926}\n",
      "2019-03-08 15:00:46.762134 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.5967137813568115, 'train_time': 15.307494794018567, 'train_nmll': 1.1988996267318726, 'test_nmll': 1.6801064014434814, 'train_nll': 107.51063537597656, 'test_nll': 16.80106544494629}\n",
      "2019-03-08 15:00:59.696203 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4104992747306824, 'train_time': 12.930411362991435, 'train_nmll': 1.1913896799087524, 'test_nmll': 1.10269033908844, 'train_nll': 107.34703063964844, 'test_nll': 11.02690315246582}\n",
      "2019-03-08 15:01:07.006404 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3478552103042603, 'train_time': 7.2781875020009466, 'train_nmll': 1.1009074449539185, 'test_nmll': 1.691545844078064, 'train_nll': 99.21331787109375, 'test_nll': 16.91545867919922}\n",
      "2019-03-08 15:01:12.813486 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2550212144851685, 'train_time': 5.803324298001826, 'train_nmll': 1.099133849143982, 'test_nmll': 1.5816339254379272, 'train_nll': 99.39139556884766, 'test_nll': 15.816338539123535}\n",
      "2019-03-08 15:01:17.910002 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0934950113296509, 'train_time': 5.090936580993002, 'train_nmll': 0.985359251499176, 'test_nmll': 1.4250261783599854, 'train_nll': 88.03529357910156, 'test_nll': 14.250261306762695}\n",
      "2019-03-08 15:01:24.504011 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9023311734199524, 'train_time': 6.58969404202071, 'train_nmll': 1.0747359991073608, 'test_nmll': 1.3558777570724487, 'train_nll': 96.45796966552734, 'test_nll': 13.558777809143066}\n",
      "2019-03-08 15:01:37.466099 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.5771342515945435, 'train_time': 12.958623801998328, 'train_nmll': 1.206760287284851, 'test_nmll': 1.2048391103744507, 'train_nll': 108.41188049316406, 'test_nll': 12.04839038848877}\n",
      "2019-03-08 15:01:44.246633 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.5467171669006348, 'train_time': 6.776958984992234, 'train_nmll': 1.1796656847000122, 'test_nmll': 1.7056901454925537, 'train_nll': 106.73736572265625, 'test_nll': 17.056903839111328}\n",
      "2019-03-08 15:01:48.760967 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.4146496057510376, 'train_time': 4.511003898020135, 'train_nmll': 1.0456148386001587, 'test_nmll': 1.6584428548812866, 'train_nll': 93.63333892822266, 'test_nll': 16.584430694580078}\n",
      "2019-03-08 15:01:56.704601 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.532776951789856, 'train_time': 7.9396454859816, 'train_nmll': 1.144528865814209, 'test_nmll': 1.157588243484497, 'train_nll': 102.75674438476562, 'test_nll': 11.575881004333496}\n",
      "2019-03-08 15:02:06.948047 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.8527952432632446, 'train_time': 10.239842157025123, 'train_nmll': 1.1089967489242554, 'test_nmll': 1.7987693548202515, 'train_nll': 99.88775634765625, 'test_nll': 17.987693786621094}\n",
      "2019-03-08 15:02:12.927222 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.31115996837615967, 'train_time': 5.966894003999187, 'train_nmll': 1.1062376499176025, 'test_nmll': 1.0644361972808838, 'train_nll': 99.4054946899414, 'test_nll': 10.644362449645996}\n",
      "2019-03-08 15:02:20.289187 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9999603033065796, 'train_time': 7.30920873300056, 'train_nmll': 1.047676920890808, 'test_nmll': 1.4076385498046875, 'train_nll': 94.82003021240234, 'test_nll': 14.076385498046875}\n",
      "2019-03-08 15:02:25.082711 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.8758590817451477, 'train_time': 4.788651095004752, 'train_nmll': 1.0291295051574707, 'test_nmll': 1.321523904800415, 'train_nll': 92.26087951660156, 'test_nll': 13.215240478515625}\n",
      "2019-03-08 15:02:29.609877 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2295414209365845, 'train_time': 4.5240798110025935, 'train_nmll': 0.9725534319877625, 'test_nmll': 1.567310094833374, 'train_nll': 88.11155700683594, 'test_nll': 15.673101425170898}\n",
      "2019-03-08 15:02:36.506606 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.302924394607544, 'train_time': 6.892944232997252, 'train_nmll': 0.9695263504981995, 'test_nmll': 1.5927238464355469, 'train_nll': 86.98936462402344, 'test_nll': 15.927238464355469}\n",
      "2019-03-08 15:02:43.290505 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4910641610622406, 'train_time': 6.78030483899056, 'train_nmll': 1.0964584350585938, 'test_nmll': 1.1584277153015137, 'train_nll': 99.47227478027344, 'test_nll': 11.584277153015137}\n",
      "2019-03-08 15:02:53.349007 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.8285675048828125, 'train_time': 10.054516881005839, 'train_nmll': 0.9997422099113464, 'test_nmll': 1.3458077907562256, 'train_nll': 89.65396118164062, 'test_nll': 13.45807933807373}\n",
      "2019-03-08 15:02:58.549595 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.465014934539795, 'train_time': 5.196632343984675, 'train_nmll': 0.9737815260887146, 'test_nmll': 1.6271286010742188, 'train_nll': 87.4033203125, 'test_nll': 16.271286010742188}\n",
      "2019-03-08 15:03:05.375822 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4500315189361572, 'train_time': 6.819784838007763, 'train_nmll': 1.1052393913269043, 'test_nmll': 1.1128581762313843, 'train_nll': 98.70103454589844, 'test_nll': 11.128582000732422}\n",
      "2019-03-08 15:03:11.861468 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.6911201477050781, 'train_time': 6.482009100989671, 'train_nmll': 1.0417544841766357, 'test_nmll': 1.746354103088379, 'train_nll': 93.10771179199219, 'test_nll': 17.46354103088379}\n",
      "2019-03-08 15:03:17.621015 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.5607908964157104, 'train_time': 5.753123456001049, 'train_nmll': 1.0185151100158691, 'test_nmll': 1.1864725351333618, 'train_nll': 91.28303527832031, 'test_nll': 11.864725112915039}\n",
      "2019-03-08 15:03:25.085926 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1806273460388184, 'train_time': 7.431332542997552, 'train_nmll': 1.0939960479736328, 'test_nmll': 1.507258415222168, 'train_nll': 98.35940551757812, 'test_nll': 15.07258415222168}\n",
      "2019-03-08 15:03:33.785493 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.26223623752594, 'train_time': 8.695660500990925, 'train_nmll': 1.0073423385620117, 'test_nmll': 1.541311502456665, 'train_nll': 90.34876251220703, 'test_nll': 15.413114547729492}\n",
      "2019-03-08 15:03:40.667820 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3819031715393066, 'train_time': 6.874656683998182, 'train_nmll': 0.9342329502105713, 'test_nmll': 1.586667776107788, 'train_nll': 84.36578369140625, 'test_nll': 15.866677284240723}\n",
      "2019-03-08 15:03:53.068769 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0484344959259033, 'train_time': 12.397294541005976, 'train_nmll': 1.070253849029541, 'test_nmll': 1.4776818752288818, 'train_nll': 95.77751159667969, 'test_nll': 14.776819229125977}\n",
      "2019-03-08 15:03:58.936543 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4427977204322815, 'train_time': 5.860041893000016, 'train_nmll': 1.0113993883132935, 'test_nmll': 1.1328938007354736, 'train_nll': 90.91957092285156, 'test_nll': 11.328937530517578}\n",
      "2019-03-08 15:04:27.485764 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3183605670928955, 'train_time': 28.545229281997308, 'train_nmll': 0.7632219195365906, 'test_nmll': 1.5826520919799805, 'train_nll': 68.69924926757812, 'test_nll': 15.826522827148438}\n",
      "2019-03-08 15:04:32.494923 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0658029317855835, 'train_time': 5.005108417011797, 'train_nmll': 0.946830153465271, 'test_nmll': 1.4184763431549072, 'train_nll': 85.49979400634766, 'test_nll': 14.184762954711914}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 15:04:39.911526 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.3537965416908264, 'train_time': 7.410355692991288, 'train_nmll': 1.085411787033081, 'test_nmll': 1.0792264938354492, 'train_nll': 97.59095764160156, 'test_nll': 10.792264938354492}\n",
      "2019-03-08 15:04:53.003107 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.5312732458114624, 'train_time': 13.087610237009358, 'train_nmll': 0.9653192162513733, 'test_nmll': 1.6879860162734985, 'train_nll': 87.34309387207031, 'test_nll': 16.879859924316406}\n",
      "2019-03-08 15:05:01.197171 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.33579543232917786, 'train_time': 8.190440000995295, 'train_nmll': 1.0764073133468628, 'test_nmll': 1.0771557092666626, 'train_nll': 96.80448913574219, 'test_nll': 10.771556854248047}\n",
      "2019-03-08 15:05:17.994707 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9805547595024109, 'train_time': 16.764017768990016, 'train_nmll': 1.0278809070587158, 'test_nmll': 1.3930208683013916, 'train_nll': 92.49296569824219, 'test_nll': 13.930209159851074}\n",
      "2019-03-08 15:05:28.595675 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3232606649398804, 'train_time': 10.597241581999697, 'train_nmll': 0.9736023545265198, 'test_nmll': 1.5510574579238892, 'train_nll': 87.20518493652344, 'test_nll': 15.510574340820312}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 2, 'n': 100, 'd': 7}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 388, in train_additive_rp_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 15:05:40.208030 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3042612075805664, 'train_time': 9.584314748994075, 'train_nmll': 0.9661604166030884, 'test_nmll': 1.595992088317871, 'train_nll': 87.3468017578125, 'test_nll': 15.959920883178711}\n",
      "2019-03-08 15:05:46.079889 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.7035318613052368, 'train_time': 5.866207523999037, 'train_nmll': 0.9929393529891968, 'test_nmll': 1.2688044309616089, 'train_nll': 89.11622619628906, 'test_nll': 12.688044548034668}\n",
      "2019-03-08 15:05:56.369433 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.3965611755847931, 'train_time': 10.285818228992866, 'train_nmll': 1.002253532409668, 'test_nmll': 1.1060993671417236, 'train_nll': 90.1305160522461, 'test_nll': 11.060994148254395}\n",
      "2019-03-08 15:06:03.621325 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9954131841659546, 'train_time': 7.248003487999085, 'train_nmll': 0.9587247967720032, 'test_nmll': 1.4175803661346436, 'train_nll': 86.18453216552734, 'test_nll': 14.175803184509277}\n",
      "2019-03-08 15:06:14.415983 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2891920804977417, 'train_time': 10.790825325006153, 'train_nmll': 0.9992271661758423, 'test_nmll': 1.5663758516311646, 'train_nll': 90.07125854492188, 'test_nll': 15.663758277893066}\n",
      "2019-03-08 15:06:26.981030 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.37503930926322937, 'train_time': 12.561285497999052, 'train_nmll': 1.061711072921753, 'test_nmll': 1.0878263711929321, 'train_nll': 94.95811462402344, 'test_nll': 10.878263473510742}\n",
      "2019-03-08 15:06:32.861776 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.516363501548767, 'train_time': 5.877031695999904, 'train_nmll': 1.0267144441604614, 'test_nmll': 1.6736032962799072, 'train_nll': 92.06014251708984, 'test_nll': 16.736032485961914}\n",
      "2019-03-08 15:06:41.207588 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.46713417768478394, 'train_time': 8.340748134010937, 'train_nmll': 1.0050073862075806, 'test_nmll': 1.1193478107452393, 'train_nll': 91.04191589355469, 'test_nll': 11.19347858428955}\n",
      "2019-03-08 15:06:56.602950 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.4267014265060425, 'train_time': 15.361403490009252, 'train_nmll': 3.201448678970337, 'test_nmll': 3.3154430389404297, 'train_nll': 288.19879150390625, 'test_nll': 33.1544303894043}\n",
      "2019-03-08 15:07:06.160606 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1041259765625, 'train_time': 9.553459325019503, 'train_nmll': 0.9991356134414673, 'test_nmll': 1.4507403373718262, 'train_nll': 89.73567962646484, 'test_nll': 14.507403373718262}\n",
      "2019-03-08 15:07:12.806443 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2560138702392578, 'train_time': 6.642388061998645, 'train_nmll': 0.9712539911270142, 'test_nmll': 1.5591249465942383, 'train_nll': 87.07232666015625, 'test_nll': 15.591249465942383}\n",
      "2019-03-08 15:07:21.287151 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0348613262176514, 'train_time': 8.47683255499578, 'train_nmll': 0.9828216433525085, 'test_nmll': 1.4479448795318604, 'train_nll': 88.63973236083984, 'test_nll': 14.479448318481445}\n",
      "2019-03-08 15:07:33.877151 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4158266484737396, 'train_time': 12.586568707978586, 'train_nmll': 1.0933942794799805, 'test_nmll': 1.124515175819397, 'train_nll': 98.42231750488281, 'test_nll': 11.245152473449707}\n",
      "2019-03-08 15:07:44.382901 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9419499635696411, 'train_time': 10.50200015900191, 'train_nmll': 0.9658219814300537, 'test_nmll': 1.4204213619232178, 'train_nll': 87.21393585205078, 'test_nll': 14.204214096069336}\n",
      "2019-03-08 15:07:52.829344 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.258671760559082, 'train_time': 8.4427901559975, 'train_nmll': 0.9704421758651733, 'test_nmll': 1.5440387725830078, 'train_nll': 86.83373260498047, 'test_nll': 15.440387725830078}\n",
      "2019-03-08 15:08:00.127715 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.3478609025478363, 'train_time': 7.2941107240039855, 'train_nmll': 1.0440131425857544, 'test_nmll': 1.0646817684173584, 'train_nll': 93.72832489013672, 'test_nll': 10.646818161010742}\n",
      "2019-03-08 15:08:09.169166 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.6026157140731812, 'train_time': 9.032635479990859, 'train_nmll': 0.9664391279220581, 'test_nmll': 1.7534034252166748, 'train_nll': 87.5052261352539, 'test_nll': 17.534034729003906}\n",
      "2019-03-08 15:08:20.289255 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.42208951711654663, 'train_time': 11.116177693009377, 'train_nmll': 1.0533645153045654, 'test_nmll': 1.1185230016708374, 'train_nll': 94.61788940429688, 'test_nll': 11.185230255126953}\n",
      "2019-03-08 15:08:30.886381 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.184430480003357, 'train_time': 10.545249507995322, 'train_nmll': 0.9832555055618286, 'test_nmll': 1.5123820304870605, 'train_nll': 88.08782958984375, 'test_nll': 15.123820304870605}\n",
      "2019-03-08 15:08:39.555056 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1516433954238892, 'train_time': 8.664568838983541, 'train_nmll': 0.9545626044273376, 'test_nmll': 1.4930527210235596, 'train_nll': 86.10311126708984, 'test_nll': 14.930527687072754}\n",
      "2019-03-08 15:08:46.041954 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0556879043579102, 'train_time': 6.482958781009074, 'train_nmll': 0.9404953122138977, 'test_nmll': 1.486177921295166, 'train_nll': 84.74574279785156, 'test_nll': 14.86177921295166}\n",
      "2019-03-08 15:08:54.904641 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.8493515253067017, 'train_time': 8.859086409007432, 'train_nmll': 0.9791635870933533, 'test_nmll': 1.356567621231079, 'train_nll': 88.08595275878906, 'test_nll': 13.565675735473633}\n",
      "2019-03-08 15:09:05.862016 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.48490095138549805, 'train_time': 10.953371312993113, 'train_nmll': 0.9951769709587097, 'test_nmll': 1.166094422340393, 'train_nll': 88.88180541992188, 'test_nll': 11.660943984985352}\n",
      "2019-03-08 15:09:16.744965 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.104874849319458, 'train_time': 10.878823446983006, 'train_nmll': 0.9181880354881287, 'test_nmll': 1.454992413520813, 'train_nll': 82.85807800292969, 'test_nll': 14.54992389678955}\n",
      "2019-03-08 15:09:40.714903 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2664117813110352, 'train_time': 23.96576027100673, 'train_nmll': 0.9668229818344116, 'test_nmll': 1.5337783098220825, 'train_nll': 87.17180633544922, 'test_nll': 15.33778190612793}\n",
      "2019-03-08 15:09:54.346550 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4699450135231018, 'train_time': 13.626371013000607, 'train_nmll': 1.0153228044509888, 'test_nmll': 1.1281086206436157, 'train_nll': 91.12571716308594, 'test_nll': 11.281085014343262}\n",
      "2019-03-08 15:10:01.402586 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.5446791648864746, 'train_time': 7.052488549001282, 'train_nmll': 0.9388090372085571, 'test_nmll': 1.7114429473876953, 'train_nll': 84.39234924316406, 'test_nll': 17.114429473876953}\n",
      "2019-03-08 15:10:10.524973 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.38224419951438904, 'train_time': 9.11846009301371, 'train_nmll': 1.030700445175171, 'test_nmll': 1.0911729335784912, 'train_nll': 92.92215728759766, 'test_nll': 10.911728858947754}\n",
      "2019-03-08 15:10:18.713199 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1017029285430908, 'train_time': 8.14230477399542, 'train_nmll': 0.9826372861862183, 'test_nmll': 1.4719146490097046, 'train_nll': 88.1477279663086, 'test_nll': 14.719146728515625}\n",
      "2019-03-08 15:10:29.352893 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2436716556549072, 'train_time': 10.635857783985557, 'train_nmll': 0.9672158360481262, 'test_nmll': 1.5310734510421753, 'train_nll': 86.9371337890625, 'test_nll': 15.310734748840332}\n",
      "2019-03-08 15:10:36.738208 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1489589214324951, 'train_time': 7.381895032012835, 'train_nmll': 0.9663001298904419, 'test_nmll': 1.4980831146240234, 'train_nll': 87.083984375, 'test_nll': 14.980831146240234}\n",
      "2019-03-08 15:11:35.071079 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.8871657252311707, 'train_time': 58.32927733199904, 'train_nmll': 1.0266515016555786, 'test_nmll': 1.3559834957122803, 'train_nll': 92.32524108886719, 'test_nll': 13.559835433959961}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 15:11:48.605765 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4645121693611145, 'train_time': 13.53104011301184, 'train_nmll': 1.0044283866882324, 'test_nmll': 1.1418852806091309, 'train_nll': 90.75833892822266, 'test_nll': 11.418852806091309}\n",
      "2019-03-08 15:12:00.856924 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9186315536499023, 'train_time': 12.246538225008408, 'train_nmll': 0.9473071694374084, 'test_nmll': 1.3821531534194946, 'train_nll': 85.39738464355469, 'test_nll': 13.821531295776367}\n",
      "2019-03-08 15:12:11.604944 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2714616060256958, 'train_time': 10.744213111000136, 'train_nmll': 0.9681367874145508, 'test_nmll': 1.54685378074646, 'train_nll': 87.51719665527344, 'test_nll': 15.468536376953125}\n",
      "2019-03-08 15:12:22.114290 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4702087938785553, 'train_time': 10.505714958999306, 'train_nmll': 1.0089061260223389, 'test_nmll': 1.1279317140579224, 'train_nll': 90.51121520996094, 'test_nll': 11.279316902160645}\n",
      "2019-03-08 15:12:33.051872 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.4155882596969604, 'train_time': 10.933622295007808, 'train_nmll': 0.9382663369178772, 'test_nmll': 1.6363284587860107, 'train_nll': 84.4575424194336, 'test_nll': 16.363285064697266}\n",
      "2019-03-08 15:12:42.710460 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.41707873344421387, 'train_time': 9.654794551985105, 'train_nmll': 1.0376555919647217, 'test_nmll': 1.1212183237075806, 'train_nll': 93.42584991455078, 'test_nll': 11.212182998657227}\n",
      "2019-03-08 15:12:49.742463 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3043005466461182, 'train_time': 6.995928960997844, 'train_nmll': 0.9366692304611206, 'test_nmll': 1.5868113040924072, 'train_nll': 84.3223876953125, 'test_nll': 15.86811351776123}\n",
      "2019-03-08 15:12:56.178058 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1055748462677002, 'train_time': 6.432229575002566, 'train_nmll': 0.9376548528671265, 'test_nmll': 1.4537413120269775, 'train_nll': 84.11782836914062, 'test_nll': 14.537413597106934}\n",
      "2019-03-08 15:13:01.287062 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1198674440383911, 'train_time': 5.100372582994169, 'train_nmll': 0.935745894908905, 'test_nmll': 1.479406714439392, 'train_nll': 84.37083435058594, 'test_nll': 14.7940673828125}\n",
      "2019-03-08 15:13:10.352968 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0842090845108032, 'train_time': 9.06101180499536, 'train_nmll': 0.9669024348258972, 'test_nmll': 1.469557762145996, 'train_nll': 86.81239318847656, 'test_nll': 14.695577621459961}\n",
      "2019-03-08 15:13:18.526658 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.44448333978652954, 'train_time': 8.162334691005526, 'train_nmll': 0.9674184322357178, 'test_nmll': 1.147995948791504, 'train_nll': 86.76567077636719, 'test_nll': 11.479959487915039}\n",
      "2019-03-08 15:13:23.354915 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9860723614692688, 'train_time': 4.82379639800638, 'train_nmll': 0.9072861671447754, 'test_nmll': 1.4226707220077515, 'train_nll': 82.16071319580078, 'test_nll': 14.226707458496094}\n",
      "2019-03-08 15:13:29.031567 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2968487739562988, 'train_time': 5.672122498013778, 'train_nmll': 0.9102757573127747, 'test_nmll': 1.5705957412719727, 'train_nll': 82.14610290527344, 'test_nll': 15.705957412719727}\n",
      "2019-03-08 15:13:36.852749 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.5033441781997681, 'train_time': 7.817362237983616, 'train_nmll': 0.9913091063499451, 'test_nmll': 1.1637221574783325, 'train_nll': 89.40866088867188, 'test_nll': 11.637221336364746}\n",
      "2019-03-08 15:13:46.940454 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.4772553443908691, 'train_time': 10.083178017986938, 'train_nmll': 0.9466443657875061, 'test_nmll': 1.6667019128799438, 'train_nll': 85.33541870117188, 'test_nll': 16.66701889038086}\n",
      "2019-03-08 15:13:50.591994 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 14555.5439453125, 'train_time': 3.647270802001003, 'train_nmll': 0.4527634382247925, 'test_nmll': 0.4850500524044037, 'train_nll': 40.74871063232422, 'test_nll': 4.850500583648682}\n",
      "2019-03-08 15:13:58.366779 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3385648727416992, 'train_time': 7.739721984020434, 'train_nmll': 0.9012992978096008, 'test_nmll': 1.6089929342269897, 'train_nll': 81.18365478515625, 'test_nll': 16.089929580688477}\n",
      "2019-03-08 15:14:06.418222 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0753259658813477, 'train_time': 8.047699746995931, 'train_nmll': 0.9520498514175415, 'test_nmll': 1.4485552310943604, 'train_nll': 85.99250030517578, 'test_nll': 14.485551834106445}\n",
      "2019-03-08 15:14:14.807631 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0841333866119385, 'train_time': 8.385808560997248, 'train_nmll': 0.9480772018432617, 'test_nmll': 1.4573451280593872, 'train_nll': 85.00618743896484, 'test_nll': 14.573450088500977}\n",
      "2019-03-08 15:14:21.032865 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0052220821380615, 'train_time': 6.216639157006284, 'train_nmll': 0.9327973127365112, 'test_nmll': 1.4257138967514038, 'train_nll': 84.13594055175781, 'test_nll': 14.257139205932617}\n",
      "2019-03-08 15:14:26.918334 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.46403783559799194, 'train_time': 5.877387982996879, 'train_nmll': 0.9657745957374573, 'test_nmll': 1.1550536155700684, 'train_nll': 87.23202514648438, 'test_nll': 11.550536155700684}\n",
      "2019-03-08 15:14:33.385422 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9807852506637573, 'train_time': 6.463151389994891, 'train_nmll': 0.9175503849983215, 'test_nmll': 1.4154622554779053, 'train_nll': 82.22712707519531, 'test_nll': 14.154622077941895}\n",
      "2019-03-08 15:14:41.316019 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3318709135055542, 'train_time': 7.925982604996534, 'train_nmll': 0.9394071102142334, 'test_nmll': 1.585794448852539, 'train_nll': 84.61328887939453, 'test_nll': 15.85794448852539}\n",
      "2019-03-08 15:14:46.074233 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4467136263847351, 'train_time': 4.754520652000792, 'train_nmll': 1.0162067413330078, 'test_nmll': 1.1313002109527588, 'train_nll': 91.4363784790039, 'test_nll': 11.313002586364746}\n",
      "2019-03-08 15:14:53.352277 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.5116398334503174, 'train_time': 7.274240131984698, 'train_nmll': 0.9037580490112305, 'test_nmll': 1.69339120388031, 'train_nll': 81.46076965332031, 'test_nll': 16.933910369873047}\n",
      "2019-03-08 15:15:01.567132 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.3640139698982239, 'train_time': 8.210695558984298, 'train_nmll': 0.9726542234420776, 'test_nmll': 1.0889157056808472, 'train_nll': 87.70011138916016, 'test_nll': 10.88915729522705}\n",
      "2019-03-08 15:15:12.286372 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3278634548187256, 'train_time': 10.670382092008367, 'train_nmll': 0.9326550960540771, 'test_nmll': 1.5994547605514526, 'train_nll': 83.68769836425781, 'test_nll': 15.994547843933105}\n",
      "2019-03-08 15:15:19.946889 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.056922197341919, 'train_time': 7.656296700006351, 'train_nmll': 0.9724410772323608, 'test_nmll': 1.4299392700195312, 'train_nll': 87.6122055053711, 'test_nll': 14.299392700195312}\n",
      "2019-03-08 15:15:30.747721 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0736019611358643, 'train_time': 10.796871071011992, 'train_nmll': 0.9373361468315125, 'test_nmll': 1.4535298347473145, 'train_nll': 84.19023132324219, 'test_nll': 14.535297393798828}\n",
      "2019-03-08 15:15:38.251117 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.9825762510299683, 'train_time': 7.499322062998544, 'train_nmll': 0.9449828863143921, 'test_nmll': 1.411512851715088, 'train_nll': 85.79063415527344, 'test_nll': 14.115127563476562}\n",
      "2019-03-08 15:15:43.357522 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.49255019426345825, 'train_time': 5.102721420000307, 'train_nmll': 0.9324808120727539, 'test_nmll': 1.1705528497695923, 'train_nll': 83.37359619140625, 'test_nll': 11.705528259277344}\n",
      "2019-03-08 15:15:50.760158 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0688283443450928, 'train_time': 7.398843098024372, 'train_nmll': 0.953478991985321, 'test_nmll': 1.4451086521148682, 'train_nll': 86.40673828125, 'test_nll': 14.451085090637207}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 15:15:57.346449 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3110870122909546, 'train_time': 6.5820100720156915, 'train_nmll': 0.9364617466926575, 'test_nmll': 1.5677032470703125, 'train_nll': 84.42181396484375, 'test_nll': 15.677032470703125}\n",
      "2019-03-08 15:16:02.239260 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4492871165275574, 'train_time': 4.8884117559937295, 'train_nmll': 1.0273045301437378, 'test_nmll': 1.1396987438201904, 'train_nll': 92.41481018066406, 'test_nll': 11.396987915039062}\n",
      "2019-03-08 15:16:19.007742 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.534727931022644, 'train_time': 16.765126939018955, 'train_nmll': 0.8192818760871887, 'test_nmll': 1.6977558135986328, 'train_nll': 74.11882019042969, 'test_nll': 16.977558135986328}\n",
      "2019-03-08 15:16:34.659871 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.36475878953933716, 'train_time': 15.648522987001343, 'train_nmll': 0.982088565826416, 'test_nmll': 1.0931121110916138, 'train_nll': 88.79867553710938, 'test_nll': 10.931120872497559}\n",
      "2019-03-08 15:16:45.515436 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2926673889160156, 'train_time': 10.821938862005481, 'train_nmll': 0.9128040671348572, 'test_nmll': 1.585566520690918, 'train_nll': 82.63272857666016, 'test_nll': 15.85566520690918}\n",
      "2019-03-08 15:16:56.791704 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1040074825286865, 'train_time': 11.272890619991813, 'train_nmll': 1.0002782344818115, 'test_nmll': 1.4599664211273193, 'train_nll': 89.98294830322266, 'test_nll': 14.599664688110352}\n",
      "2019-03-08 15:17:06.408351 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.1043742895126343, 'train_time': 9.613037253002403, 'train_nmll': 0.9033283591270447, 'test_nmll': 1.4726111888885498, 'train_nll': 81.0819091796875, 'test_nll': 14.726112365722656}\n",
      "2019-03-08 15:17:13.023405 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.000396966934204, 'train_time': 6.611092228005873, 'train_nmll': 0.9476960301399231, 'test_nmll': 1.4186090230941772, 'train_nll': 85.55326080322266, 'test_nll': 14.186089515686035}\n",
      "2019-03-08 15:17:18.520363 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.48161569237709045, 'train_time': 5.492857509001624, 'train_nmll': 0.9706627726554871, 'test_nmll': 1.1663285493850708, 'train_nll': 87.37164306640625, 'test_nll': 11.663284301757812}\n",
      "2019-03-08 15:17:31.445952 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0626261234283447, 'train_time': 12.920789956027875, 'train_nmll': 0.9624137282371521, 'test_nmll': 1.4452621936798096, 'train_nll': 86.44400024414062, 'test_nll': 14.452621459960938}\n",
      "2019-03-08 15:17:39.413226 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.2979059219360352, 'train_time': 7.963040558999637, 'train_nmll': 0.9142628908157349, 'test_nmll': 1.5673754215240479, 'train_nll': 82.27994537353516, 'test_nll': 15.673752784729004}\n",
      "2019-03-08 15:17:48.263238 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.44766831398010254, 'train_time': 8.846261655999115, 'train_nmll': 0.9913498759269714, 'test_nmll': 1.1263883113861084, 'train_nll': 89.08509826660156, 'test_nll': 11.263883590698242}\n",
      "2019-03-08 15:18:02.592238 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.511136531829834, 'train_time': 14.324933700001566, 'train_nmll': 0.912875771522522, 'test_nmll': 1.6904661655426025, 'train_nll': 82.1906509399414, 'test_nll': 16.904661178588867}\n",
      "2019-03-08 15:18:08.679305 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.33319205045700073, 'train_time': 6.082576625980437, 'train_nmll': 0.9835720658302307, 'test_nmll': 1.07869553565979, 'train_nll': 88.79692077636719, 'test_nll': 10.786954879760742}\n",
      "2019-03-08 15:18:22.139409 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3386750221252441, 'train_time': 13.424058152013458, 'train_nmll': 0.9348945617675781, 'test_nmll': 1.606529951095581, 'train_nll': 84.1625747680664, 'test_nll': 16.06529998779297}\n",
      "2019-03-08 15:18:35.643122 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0858707427978516, 'train_time': 13.500065583997639, 'train_nmll': 0.951804518699646, 'test_nmll': 1.4491703510284424, 'train_nll': 85.4920883178711, 'test_nll': 14.491703033447266}\n",
      "2019-03-08 15:18:49.809229 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0673913955688477, 'train_time': 14.16265512802056, 'train_nmll': 0.9204403162002563, 'test_nmll': 1.456091046333313, 'train_nll': 82.89196014404297, 'test_nll': 14.56091022491455}\n",
      "2019-03-08 15:18:56.416061 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.011030912399292, 'train_time': 6.601286402990809, 'train_nmll': 0.9607057571411133, 'test_nmll': 1.4277359247207642, 'train_nll': 86.21381378173828, 'test_nll': 14.277359008789062}\n",
      "2019-03-08 15:19:05.023215 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4694078862667084, 'train_time': 8.603637141990475, 'train_nmll': 0.9641727805137634, 'test_nmll': 1.1598896980285645, 'train_nll': 86.8424301147461, 'test_nll': 11.598896980285645}\n",
      "2019-03-08 15:19:12.069974 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0582274198532104, 'train_time': 7.042974508018233, 'train_nmll': 0.9311338067054749, 'test_nmll': 1.443398118019104, 'train_nll': 83.75886535644531, 'test_nll': 14.433980941772461}\n",
      "2019-03-08 15:19:20.320947 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3178867101669312, 'train_time': 8.24680830698344, 'train_nmll': 0.891470193862915, 'test_nmll': 1.5789560079574585, 'train_nll': 80.2122802734375, 'test_nll': 15.789560317993164}\n",
      "2019-03-08 15:19:32.597997 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.43628501892089844, 'train_time': 12.273051165015204, 'train_nmll': 0.9669848680496216, 'test_nmll': 1.1265791654586792, 'train_nll': 87.27520751953125, 'test_nll': 11.265791893005371}\n",
      "2019-03-08 15:19:41.358560 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.5120651721954346, 'train_time': 8.756851393001853, 'train_nmll': 0.9647009968757629, 'test_nmll': 1.691293716430664, 'train_nll': 86.98825073242188, 'test_nll': 16.91293716430664}\n",
      "2019-03-08 15:19:50.093258 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.34249916672706604, 'train_time': 8.730969169992022, 'train_nmll': 0.9889064431190491, 'test_nmll': 1.083565354347229, 'train_nll': 88.67364501953125, 'test_nll': 10.835652351379395}\n",
      "2019-03-08 15:19:57.090135 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3374184370040894, 'train_time': 6.96095191698987, 'train_nmll': 0.9105943441390991, 'test_nmll': 1.6058841943740845, 'train_nll': 82.09205627441406, 'test_nll': 16.058841705322266}\n",
      "2019-03-08 15:20:04.391265 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0960546731948853, 'train_time': 7.293816680001328, 'train_nmll': 0.9723156690597534, 'test_nmll': 1.4561331272125244, 'train_nll': 87.40318298339844, 'test_nll': 14.561330795288086}\n",
      "2019-03-08 15:20:14.859713 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.082577109336853, 'train_time': 10.46483418601565, 'train_nmll': 0.943576455116272, 'test_nmll': 1.4644416570663452, 'train_nll': 84.564453125, 'test_nll': 14.644416809082031}\n",
      "2019-03-08 15:20:23.525985 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0157155990600586, 'train_time': 8.662534165981924, 'train_nmll': 0.9385365843772888, 'test_nmll': 1.431720495223999, 'train_nll': 84.47598266601562, 'test_nll': 14.317204475402832}\n",
      "2019-03-08 15:20:30.880888 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.47582483291625977, 'train_time': 7.351173478993587, 'train_nmll': 0.9758017063140869, 'test_nmll': 1.163578987121582, 'train_nll': 87.80096435546875, 'test_nll': 11.63578987121582}\n",
      "2019-03-08 15:20:51.572025 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0869505405426025, 'train_time': 20.687395229993854, 'train_nmll': 0.9585455060005188, 'test_nmll': 1.4568027257919312, 'train_nll': 86.07762908935547, 'test_nll': 14.56802749633789}\n",
      "2019-03-08 15:21:04.104159 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3208587169647217, 'train_time': 12.52798167601577, 'train_nmll': 0.91215980052948, 'test_nmll': 1.5801475048065186, 'train_nll': 81.94279479980469, 'test_nll': 15.801475524902344}\n",
      "2019-03-08 15:21:12.695232 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4632285535335541, 'train_time': 8.587373725982616, 'train_nmll': 0.9943718910217285, 'test_nmll': 1.1404337882995605, 'train_nll': 89.79940795898438, 'test_nll': 11.404337882995605}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 15:21:22.674184 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.5242290496826172, 'train_time': 9.975181877991417, 'train_nmll': 0.9176421165466309, 'test_nmll': 1.696925401687622, 'train_nll': 81.82240295410156, 'test_nll': 16.969253540039062}\n",
      "2019-03-08 15:21:35.424935 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.34042757749557495, 'train_time': 12.744680536998203, 'train_nmll': 0.9778456687927246, 'test_nmll': 1.0806939601898193, 'train_nll': 87.93225860595703, 'test_nll': 10.806940078735352}\n",
      "2019-03-08 15:21:47.655359 - {'fold': 0, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.324361801147461, 'train_time': 12.19336812698748, 'train_nmll': 0.8906404972076416, 'test_nmll': 1.6017539501190186, 'train_nll': 79.98597717285156, 'test_nll': 16.017539978027344}\n",
      "2019-03-08 15:21:56.214725 - {'fold': 1, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0952155590057373, 'train_time': 8.555743672011886, 'train_nmll': 0.9813507795333862, 'test_nmll': 1.4565393924713135, 'train_nll': 88.09872436523438, 'test_nll': 14.565393447875977}\n",
      "2019-03-08 15:22:19.942239 - {'fold': 2, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0612150430679321, 'train_time': 23.724173941998743, 'train_nmll': 0.9341944456100464, 'test_nmll': 1.4480934143066406, 'train_nll': 84.17921447753906, 'test_nll': 14.480934143066406}\n",
      "2019-03-08 15:22:29.880005 - {'fold': 3, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.018606424331665, 'train_time': 9.933286454004701, 'train_nmll': 0.997078001499176, 'test_nmll': 1.432046890258789, 'train_nll': 89.6268081665039, 'test_nll': 14.32046890258789}\n",
      "2019-03-08 15:22:47.184461 - {'fold': 4, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.4750075936317444, 'train_time': 17.299792508012615, 'train_nmll': 0.9638537764549255, 'test_nmll': 1.1617319583892822, 'train_nll': 86.47439575195312, 'test_nll': 11.617319107055664}\n",
      "2019-03-08 15:22:57.683080 - {'fold': 5, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.0708129405975342, 'train_time': 10.494881516002351, 'train_nmll': 0.9277992844581604, 'test_nmll': 1.4533731937408447, 'train_nll': 83.54325866699219, 'test_nll': 14.533732414245605}\n",
      "2019-03-08 15:23:11.374290 - {'fold': 6, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.3173595666885376, 'train_time': 13.686363710003207, 'train_nmll': 0.914210855960846, 'test_nmll': 1.5820566415786743, 'train_nll': 82.56660461425781, 'test_nll': 15.820565223693848}\n",
      "2019-03-08 15:23:23.272280 - {'fold': 7, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.47770166397094727, 'train_time': 11.893361641006777, 'train_nmll': 1.0005968809127808, 'test_nmll': 1.1483808755874634, 'train_nll': 90.15911865234375, 'test_nll': 11.483808517456055}\n",
      "2019-03-08 15:23:30.602997 - {'fold': 8, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 1.5195785760879517, 'train_time': 7.327050480002072, 'train_nmll': 0.9433557391166687, 'test_nmll': 1.6992931365966797, 'train_nll': 85.13048553466797, 'test_nll': 16.992931365966797}\n",
      "2019-03-08 15:23:43.262850 - {'fold': 9, 'repeat': 0, 'n': 100, 'd': 7, 'mse': 0.3437102735042572, 'train_time': 12.656239453004673, 'train_nmll': 0.9817245602607727, 'test_nmll': 1.0836557149887085, 'train_nll': 88.3487319946289, 'test_nll': 10.836557388305664}\n",
      "energy starting\n",
      "2019-03-08 15:25:53.453431 - {'fold': 0, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.26653924584388733, 'train_time': 130.1528503790032, 'train_nmll': 9513.7333984375, 'test_nmll': 74655.65625, 'train_nll': 6574202.5, 'test_nll': 5748590.5}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 1, 'n': 768, 'd': 6}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 15:28:05.444375 - {'fold': 1, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 1237984863059968.0, 'train_time': 124.27751278699725, 'train_nmll': 0.4527634382247925, 'test_nmll': 0.45276355743408203, 'train_nll': 312.8595275878906, 'test_nll': 34.86279296875}\n",
      "2019-03-08 15:30:21.091948 - {'fold': 2, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.2931789755821228, 'train_time': 135.64306496098288, 'train_nmll': 6170126.0, 'test_nmll': 1362.8203125, 'train_nll': 4263557632.0, 'test_nll': 104951.4375}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 3, 'n': 768, 'd': 6}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 15:33:02.381483 - {'fold': 3, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 0.004105998203158379, 'train_time': 155.78467581799487, 'train_nmll': -3.439941644668579, 'test_nmll': 16.673669815063477, 'train_nll': -3073.14599609375, 'test_nll': 1273.8568115234375}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\\n    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\\n    preconditioner=preconditioner,\\n  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\\n    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\\nRuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\\n', 'fold': 4, 'n': 768, 'd': 6}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 101, in forward\n",
      "    solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 108, in _solve\n",
      "    preconditioner=preconditioner,\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/linear_cg.py\", line 157, in linear_cg\n",
      "    raise RuntimeError(\"NaNs encounterd when trying to perform matrix-vector multiplication\")\n",
      "RuntimeError: NaNs encounterd when trying to perform matrix-vector multiplication\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-08 15:33:51.105438 - {'fold': 4, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 9702.6005859375, 'train_time': 7.179702072025975, 'train_nmll': 0.4527634382247925, 'test_nmll': 0.45276355743408203, 'train_nll': 312.8595275878906, 'test_nll': 34.86279296875}\n",
      "2019-03-08 15:34:26.829548 - {'fold': 5, 'repeat': 0, 'n': 768, 'd': 6, 'mse': 3.0478029251098633, 'train_time': 35.721320689015556, 'train_nmll': 395442.65625, 'test_nmll': 16.359071731567383, 'train_nll': 273250720.0, 'test_nll': 1286.8359375}\n",
      "{'error': 'Traceback (most recent call last):\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\\n    **training_options)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\\n    patience=patience, smooth=smooth)\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\\n    loss = optimizer_.step(closure).item()\\n  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\\n    loss = float(closure())\\n  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\\n    loss = -gp_mll(output, ys)\\n  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\\n    outputs = self.forward(*inputs, **kwargs)\\n  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\\n    res = output.log_prob(target)\\n  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\\n    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\\n  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\\n    )(*args)\\n  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\\n    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\\n  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\\n    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\\n  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\\n    chol = torch.cholesky(inner_mat)\\nRuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\\n', 'fold': 6, 'n': 768, 'd': 6}\n",
      "errors:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 166, in run_experiment\n",
      "    **training_options)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py\", line 268, in train_SE_gp\n",
      "    patience=patience, smooth=smooth)\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 99, in fit_gp_model\n",
      "    loss = optimizer_.step(closure).item()\n",
      "  File \"/home/ian/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\", line 213, in step\n",
      "    loss = float(closure())\n",
      "  File \"/home/ian/Documents/Research/Scalable_GPs/gp_helpers.py\", line 93, in closure\n",
      "    loss = -gp_mll(output, ys)\n",
      "  File \"/home/ian/gpytorch/gpytorch/module.py\", line 20, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/ian/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\", line 28, in forward\n",
      "    res = output.log_prob(target)\n",
      "  File \"/home/ian/gpytorch/gpytorch/distributions/multivariate_normal.py\", line 125, in log_prob\n",
      "    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/lazy_tensor.py\", line 762, in inv_quad_logdet\n",
      "    )(*args)\n",
      "  File \"/home/ian/gpytorch/gpytorch/functions/_inv_quad_log_det.py\", line 75, in forward\n",
      "    preconditioner, logdet_correction = lazy_tsr.detach()._preconditioner()\n",
      "  File \"/home/ian/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\", line 67, in _preconditioner\n",
      "    self._piv_chol_self, self._piv_chol_self, self._diag_tensor.diag(), logdet=True\n",
      "  File \"/home/ian/gpytorch/gpytorch/utils/woodbury.py\", line 68, in woodbury_factor\n",
      "    chol = torch.cholesky(inner_mat)\n",
      "RuntimeError: cholesky: For batch 0: U(1,1) is zero, singular U.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fc21ea0f77c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrp_experiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'autos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fertility'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'energy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yacht'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrp_experiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrp_compare_ablation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./test-run-smoothed-ard.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/rp_experiments.py\u001b[0m in \u001b[0;36mrp_compare_ablation\u001b[0;34m(filename, fit, dsets, optimizer, lr, patience, verbose, include_se)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 result = run_experiment(train_SE_gp, options, dataset=dataset,\n\u001b[0;32m--> 490\u001b[0;31m                                         split=0.1, cv=True, repeats=1)\n\u001b[0m\u001b[1;32m    491\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/rp_experiments.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(training_routine, training_options, dataset, split, cv, addl_metrics, repeats, error_repeats)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     model_metrics, ypred = training_routine(trainX, trainY, testX,\n\u001b[1;32m    165\u001b[0m                                                             \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                                             **training_options)\n\u001b[0m\u001b[1;32m    167\u001b[0m                     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/rp_experiments.py\u001b[0m in \u001b[0;36mtrain_SE_gp\u001b[0;34m(trainX, trainY, testX, testY, ard, optimizer, n_epochs, lr, verbose, patience, smooth)\u001b[0m\n\u001b[1;32m    266\u001b[0m     fit_gp_model(model, likelihood, trainX, trainY, optimizer=optimizer_,\n\u001b[1;32m    267\u001b[0m                  \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp_mll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                  patience=patience, smooth=smooth)\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mfit_gp_model\u001b[0;34m(gp_model, gp_likelihood, xs, ys, optimizer, lr, gp_mll, n_epochs, verbose, patience, conv_tol, check_conv, smooth)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0;31m# the reason we do this: in a stochastic setting,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;31m# no use to re-evaluate that function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                     \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0mabs_grad_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgp_mll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 print(\n",
      "\u001b[0;32m~/gpytorch/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_validate_module_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, output, target, *params)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Get the log prob of the marginal distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Add terms for SGPR / when inducing points are learned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Get log determininat and first part of quadratic form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0minv_quad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_quad_logdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minv_quad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36minv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mprobe_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobe_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mprobe_vector_norms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobe_vector_norms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         )(*args)\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minv_quad_term\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce_inv_quad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/functions/_inv_quad_log_det.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mt_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdet\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_logdet_forward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0msolves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreconditioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tridiag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_random_probes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, rhs, preconditioner, num_tridiag)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cg_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mmax_tridiag_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_lanczos_quadrature_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mpreconditioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreconditioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/utils/linear_cg.py\u001b[0m in \u001b[0;36mlinear_cg\u001b[0;34m(matmul_closure, rhs, n_tridiag, tolerance, eps, stop_updating_after, max_iter, max_tridiag_iter, initial_guess, preconditioner)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# Get next alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# alpha_{k} = (residual_{k-1}^T precon_residual{k-1}) / (p_vec_{k-1}^T mat p_vec_{k-1})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mmvms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprecond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmul_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         return torch.addcmul(\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diag_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mrhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/non_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_quad_form_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import rp_experiments\n",
    "dsets = ['autos', 'fertility', 'energy', 'yacht']\n",
    "rp_experiments.rp_compare_ablation(fit=True, filename='./test-run-smoothed-ard.csv', dsets=dsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset    RP     k   \n",
       "autos      False  0.0     1\n",
       "           True   1.0     6\n",
       "                  4.0     7\n",
       "                  10.0    4\n",
       "fertility  False  0.0     1\n",
       "           True   1.0     2\n",
       "                  4.0     1\n",
       "                  10.0    0\n",
       "Name: error, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./test-run-smoothed-ard.csv')\n",
    "df = df[df['dataset'] != 'challenger']\n",
    "df.loc[df['RP'] == False, ['k', 'J']] = 0\n",
    "display(df.groupby(['dataset', 'RP', 'k'])['error'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = torch.load('covar.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0054, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0054,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0054]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1853)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(covar - torch.diag(covar.diag())).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = gpytorch.lazify(covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_decomp = covar.root_decomposition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evl = root_decomp.root.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 0.0000e+00,  1.9063e-03,  2.6259e-10,  5.7376e-12,  1.6689e-09,\n",
       "         -1.9552e-08],\n",
       "        [ 0.0000e+00,  2.6259e-10,  4.5801e-03,  1.3733e-10,  7.0385e-10,\n",
       "         -4.8336e-09],\n",
       "        [ 0.0000e+00,  5.7376e-12,  1.3733e-10,  5.4525e-03,  1.2785e-10,\n",
       "         -8.3247e-10],\n",
       "        [ 0.0000e+00,  1.6689e-09,  7.0385e-10,  1.2785e-10,  4.7223e-02,\n",
       "          2.2299e-08],\n",
       "        [ 0.0000e+00, -1.9552e-08, -4.8336e-09, -8.3247e-10,  2.2299e-08,\n",
       "          1.3963e-01]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evl.t().matmul(evl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Adding jitter of 1e-06 to the diagonal did not make A p.d.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/gpytorch/gpytorch/utils/cholesky.py\u001b[0m in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# TODO: Remove once fixed in pytorch (#16780)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cholesky: U(140,140) is zero, singular U.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/gpytorch/gpytorch/utils/cholesky.py\u001b[0m in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAprime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;31m# TODO: Remove once fixed in pytorch (#16780)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cholesky: U(140,140) is zero, singular U.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-9f613a0232a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsd_safe_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/gpytorch/gpytorch/utils/cholesky.py\u001b[0m in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"singular\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Adding jitter of {} to the diagonal did not make A p.d.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A not p.d., added jitter of {} to the diagonal\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Adding jitter of 1e-06 to the diagonal did not make A p.d."
     ]
    }
   ],
   "source": [
    "gpytorch.utils.cholesky.psd_safe_cholesky(covar.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr = 3\n",
    "nc = 2\n",
    "maxSquare = [[-1]*nc]*nr\n",
    "maxSquare[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0111\n",
      "1101\n",
      "0111\n"
     ]
    }
   ],
   "source": [
    "for f in [\"0111\", \"1101\", \"0111\"]:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rp_experiments' from '/home/ian/Documents/Research/Scalable_GPs/rp_experiments.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rp_experiments\n",
    "import imp\n",
    "imp.reload(rp_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dict(verbose=True, ard=False, activation=None,\n",
    "                optimizer='adam', n_epochs=1000,\n",
    "                lr=0.1, patience=20, k=1, J=10,\n",
    "                smooth=True, noise_prior=True,\n",
    "                ski=False, grid_ratio=10.0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/1000 - Loss: 1.582, Noise: 0.6931\n",
      "Iter 2/1000 - Loss: 1.538, Noise: 0.6445\n",
      "Iter 3/1000 - Loss: 1.540, Noise: 0.6795\n",
      "Iter 4/1000 - Loss: 1.513, Noise: 0.6841\n",
      "Iter 5/1000 - Loss: 1.507, Noise: 0.6647\n",
      "Iter 6/1000 - Loss: 1.492, Noise: 0.6368\n",
      "Iter 7/1000 - Loss: 1.525, Noise: 0.6270\n",
      "Iter 8/1000 - Loss: 1.469, Noise: 0.6444\n",
      "Iter 9/1000 - Loss: 1.516, Noise: 0.6604\n",
      "Iter 10/1000 - Loss: 1.460, Noise: 0.6815\n",
      "Iter 11/1000 - Loss: 1.479, Noise: 0.6964\n",
      "Iter 12/1000 - Loss: 1.463, Noise: 0.7022\n",
      "Iter 13/1000 - Loss: 1.483, Noise: 0.6987\n",
      "Iter 14/1000 - Loss: 1.440, Noise: 0.6917\n",
      "Iter 15/1000 - Loss: 1.447, Noise: 0.6794\n",
      "Iter 16/1000 - Loss: 1.441, Noise: 0.6705\n",
      "Iter 17/1000 - Loss: 1.435, Noise: 0.6648\n",
      "Iter 18/1000 - Loss: 1.439, Noise: 0.6642\n",
      "Iter 19/1000 - Loss: 1.413, Noise: 0.6729\n",
      "Iter 20/1000 - Loss: 1.422, Noise: 0.6844\n",
      "Iter 21/1000 - Loss: 1.423, Noise: 0.7004\n",
      "Iter 22/1000 - Loss: 1.410, Noise: 0.7162\n",
      "Iter 23/1000 - Loss: 1.399, Noise: 0.7263\n",
      "Iter 24/1000 - Loss: 1.401, Noise: 0.7293\n",
      "Iter 25/1000 - Loss: 1.404, Noise: 0.7270\n",
      "Iter 26/1000 - Loss: 1.400, Noise: 0.7230\n",
      "Iter 27/1000 - Loss: 1.389, Noise: 0.7203\n",
      "Iter 28/1000 - Loss: 1.405, Noise: 0.7199\n",
      "Iter 29/1000 - Loss: 1.389, Noise: 0.7225\n",
      "Iter 30/1000 - Loss: 1.401, Noise: 0.7238\n",
      "Iter 31/1000 - Loss: 1.393, Noise: 0.7285\n",
      "Iter 32/1000 - Loss: 1.387, Noise: 0.7374\n",
      "Iter 33/1000 - Loss: 1.390, Noise: 0.7446\n",
      "Iter 34/1000 - Loss: 1.393, Noise: 0.7494\n",
      "Iter 35/1000 - Loss: 1.387, Noise: 0.7556\n",
      "Iter 36/1000 - Loss: 1.378, Noise: 0.7603\n",
      "Iter 37/1000 - Loss: 1.390, Noise: 0.7591\n",
      "Iter 38/1000 - Loss: 1.390, Noise: 0.7599\n",
      "Iter 39/1000 - Loss: 1.387, Noise: 0.7634\n",
      "Iter 40/1000 - Loss: 1.386, Noise: 0.7670\n",
      "Iter 41/1000 - Loss: 1.384, Noise: 0.7703\n",
      "Iter 42/1000 - Loss: 1.390, Noise: 0.7708\n",
      "Iter 43/1000 - Loss: 1.384, Noise: 0.7699\n",
      "Iter 44/1000 - Loss: 1.383, Noise: 0.7688\n",
      "Iter 45/1000 - Loss: 1.388, Noise: 0.7664\n",
      "Iter 46/1000 - Loss: 1.383, Noise: 0.7683\n",
      "Iter 47/1000 - Loss: 1.381, Noise: 0.7718\n",
      "Iter 48/1000 - Loss: 1.384, Noise: 0.7753\n",
      "Iter 49/1000 - Loss: 1.383, Noise: 0.7811\n",
      "Iter 50/1000 - Loss: 1.379, Noise: 0.7851\n",
      "Iter 51/1000 - Loss: 1.391, Noise: 0.7837\n",
      "Iter 52/1000 - Loss: 1.379, Noise: 0.7848\n",
      "Iter 53/1000 - Loss: 1.380, Noise: 0.7819\n",
      "Iter 54/1000 - Loss: 1.381, Noise: 0.7776\n",
      "Iter 55/1000 - Loss: 1.380, Noise: 0.7744\n",
      "Iter 56/1000 - Loss: 1.387, Noise: 0.7735\n",
      "Iter 57/1000 - Loss: 1.381, Noise: 0.7777\n",
      "Iter 58/1000 - Loss: 1.385, Noise: 0.7841\n",
      "Iter 59/1000 - Loss: 1.383, Noise: 0.7905\n",
      "Iter 60/1000 - Loss: 1.382, Noise: 0.7951\n",
      "Iter 61/1000 - Loss: 1.384, Noise: 0.7916\n",
      "Iter 62/1000 - Loss: 1.381, Noise: 0.7865\n",
      "Iter 63/1000 - Loss: 1.384, Noise: 0.7807\n",
      "Iter 64/1000 - Loss: 1.381, Noise: 0.7775\n",
      "Iter 65/1000 - Loss: 1.383, Noise: 0.7778\n",
      "Iter 66/1000 - Loss: 1.382, Noise: 0.7791\n",
      "Iter 67/1000 - Loss: 1.385, Noise: 0.7826\n",
      "Iter 68/1000 - Loss: 1.388, Noise: 0.7894\n",
      "Iter 69/1000 - Loss: 1.383, Noise: 0.7945\n",
      "Iter 70/1000 - Loss: 1.383, Noise: 0.7937\n",
      "Iter 71/1000 - Loss: 1.379, Noise: 0.7897\n",
      "Iter 72/1000 - Loss: 1.382, Noise: 0.7788\n",
      "Iter 73/1000 - Loss: 1.379, Noise: 0.7681\n",
      "Iter 74/1000 - Loss: 1.384, Noise: 0.7621\n",
      "Iter 75/1000 - Loss: 1.385, Noise: 0.7657\n",
      "Iter 76/1000 - Loss: 1.384, Noise: 0.7788\n",
      "Iter 77/1000 - Loss: 1.383, Noise: 0.7921\n",
      "Iter 78/1000 - Loss: 1.383, Noise: 0.7981\n",
      "Iter 79/1000 - Loss: 1.378, Noise: 0.7956\n",
      "Iter 80/1000 - Loss: 1.380, Noise: 0.7851\n",
      "Iter 81/1000 - Loss: 1.383, Noise: 0.7709\n",
      "Iter 82/1000 - Loss: 1.384, Noise: 0.7618\n",
      "Iter 83/1000 - Loss: 1.387, Noise: 0.7572\n",
      "Reached convergence at 1.3869709968566895, MA 1.3829266369342803 - 1.382843828201294 < 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/gpytorch/gpytorch/models/exact_gp.py:190: UserWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  \"The input matches the stored training data. Did you forget to call model.train()?\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-10 22:26:45.126955 - {'fold': 0, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.6444960236549377, 'train_time': 16.13184176100185, 'prior_train_nmll': 1.3833636045455933, 'train_nll': 219.05364990234375, 'test_nll': 24.55875015258789, 'train_mse': 0.6968062520027161}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d</th>\n",
       "      <th>fold</th>\n",
       "      <th>mse</th>\n",
       "      <th>n</th>\n",
       "      <th>prior_train_nmll</th>\n",
       "      <th>repeat</th>\n",
       "      <th>test_nll</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>train_nll</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.644496</td>\n",
       "      <td>194</td>\n",
       "      <td>1.383364</td>\n",
       "      <td>0</td>\n",
       "      <td>24.55875</td>\n",
       "      <td>0.696806</td>\n",
       "      <td>219.05365</td>\n",
       "      <td>16.131842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    d  fold       mse    n  prior_train_nmll  repeat  test_nll  train_mse  \\\n",
       "0  31     0  0.644496  194          1.383364       0  24.55875   0.696806   \n",
       "\n",
       "   train_nll  train_time  \n",
       "0  219.05365   16.131842  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp_experiments.run_experiment(\n",
    "    rp_experiments.train_additive_rp_gp,\n",
    "    options, 'breastcancer', .1, cv=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dict(verbose=True, ard=False, activation=None,\n",
    "                optimizer='adam', n_epochs=1000,\n",
    "                lr=0.1, patience=20, k=1, J=10,\n",
    "                smooth=True, noise_prior=True,\n",
    "                ski=True, grid_ratio=2.0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/1000 - Loss: 1.521, Noise: 0.6931\n",
      "Iter 2/1000 - Loss: 1.510, Noise: 0.6444\n",
      "Iter 3/1000 - Loss: 1.494, Noise: 0.6232\n",
      "Iter 4/1000 - Loss: 1.503, Noise: 0.6244\n",
      "Iter 5/1000 - Loss: 1.476, Noise: 0.6379\n",
      "Iter 6/1000 - Loss: 1.477, Noise: 0.6489\n",
      "Iter 7/1000 - Loss: 1.491, Noise: 0.6590\n",
      "Iter 8/1000 - Loss: 1.452, Noise: 0.6767\n",
      "Iter 9/1000 - Loss: 1.463, Noise: 0.6807\n",
      "Iter 10/1000 - Loss: 1.435, Noise: 0.6778\n",
      "Iter 11/1000 - Loss: 1.439, Noise: 0.6666\n",
      "Iter 12/1000 - Loss: 1.422, Noise: 0.6522\n",
      "Iter 13/1000 - Loss: 1.454, Noise: 0.6380\n",
      "Iter 14/1000 - Loss: 1.443, Noise: 0.6336\n",
      "Iter 15/1000 - Loss: 1.426, Noise: 0.6379\n",
      "Iter 16/1000 - Loss: 1.397, Noise: 0.6480\n",
      "Iter 17/1000 - Loss: 1.420, Noise: 0.6620\n",
      "Iter 18/1000 - Loss: 1.394, Noise: 0.6791\n",
      "Iter 19/1000 - Loss: 1.397, Noise: 0.6904\n",
      "Iter 20/1000 - Loss: 1.389, Noise: 0.6972\n",
      "Iter 21/1000 - Loss: 1.397, Noise: 0.6986\n",
      "Iter 22/1000 - Loss: 1.386, Noise: 0.6982\n",
      "Iter 23/1000 - Loss: 1.396, Noise: 0.6934\n",
      "Iter 24/1000 - Loss: 1.379, Noise: 0.6896\n",
      "Iter 25/1000 - Loss: 1.371, Noise: 0.6850\n",
      "Iter 26/1000 - Loss: 1.375, Noise: 0.6781\n",
      "Iter 27/1000 - Loss: 1.377, Noise: 0.6747\n",
      "Iter 28/1000 - Loss: 1.374, Noise: 0.6762\n",
      "Iter 29/1000 - Loss: 1.368, Noise: 0.6831\n",
      "Iter 30/1000 - Loss: 1.363, Noise: 0.6909\n",
      "Iter 31/1000 - Loss: 1.374, Noise: 0.6997\n",
      "Iter 32/1000 - Loss: 1.372, Noise: 0.7091\n",
      "Iter 33/1000 - Loss: 1.372, Noise: 0.7189\n",
      "Iter 34/1000 - Loss: 1.369, Noise: 0.7273\n",
      "Iter 35/1000 - Loss: 1.367, Noise: 0.7323\n",
      "Iter 36/1000 - Loss: 1.369, Noise: 0.7328\n",
      "Iter 37/1000 - Loss: 1.365, Noise: 0.7293\n",
      "Iter 38/1000 - Loss: 1.361, Noise: 0.7210\n",
      "Iter 39/1000 - Loss: 1.367, Noise: 0.7105\n",
      "Iter 40/1000 - Loss: 1.370, Noise: 0.7046\n",
      "Iter 41/1000 - Loss: 1.365, Noise: 0.7052\n",
      "Iter 42/1000 - Loss: 1.367, Noise: 0.7093\n",
      "Iter 43/1000 - Loss: 1.367, Noise: 0.7177\n",
      "Iter 44/1000 - Loss: 1.368, Noise: 0.7291\n",
      "Iter 45/1000 - Loss: 1.367, Noise: 0.7404\n",
      "Iter 46/1000 - Loss: 1.365, Noise: 0.7457\n",
      "Iter 47/1000 - Loss: 1.367, Noise: 0.7468\n",
      "Iter 48/1000 - Loss: 1.370, Noise: 0.7430\n",
      "Iter 49/1000 - Loss: 1.364, Noise: 0.7365\n",
      "Iter 50/1000 - Loss: 1.364, Noise: 0.7303\n",
      "Iter 51/1000 - Loss: 1.363, Noise: 0.7249\n",
      "Iter 52/1000 - Loss: 1.363, Noise: 0.7190\n",
      "Iter 53/1000 - Loss: 1.369, Noise: 0.7173\n",
      "Iter 54/1000 - Loss: 1.373, Noise: 0.7228\n",
      "Iter 55/1000 - Loss: 1.364, Noise: 0.7298\n",
      "Iter 56/1000 - Loss: 1.368, Noise: 0.7382\n",
      "Iter 57/1000 - Loss: 1.367, Noise: 0.7463\n",
      "Iter 58/1000 - Loss: 1.365, Noise: 0.7498\n",
      "Iter 59/1000 - Loss: 1.367, Noise: 0.7477\n",
      "Iter 60/1000 - Loss: 1.367, Noise: 0.7416\n",
      "Iter 61/1000 - Loss: 1.368, Noise: 0.7332\n",
      "Iter 62/1000 - Loss: 1.365, Noise: 0.7248\n",
      "Iter 63/1000 - Loss: 1.360, Noise: 0.7181\n",
      "Iter 64/1000 - Loss: 1.366, Noise: 0.7136\n",
      "Iter 65/1000 - Loss: 1.364, Noise: 0.7149\n",
      "Iter 66/1000 - Loss: 1.369, Noise: 0.7208\n",
      "Iter 67/1000 - Loss: 1.365, Noise: 0.7295\n",
      "Iter 68/1000 - Loss: 1.370, Noise: 0.7357\n",
      "Iter 69/1000 - Loss: 1.367, Noise: 0.7415\n",
      "Iter 70/1000 - Loss: 1.362, Noise: 0.7421\n",
      "Iter 71/1000 - Loss: 1.364, Noise: 0.7381\n",
      "Iter 72/1000 - Loss: 1.364, Noise: 0.7296\n",
      "Iter 73/1000 - Loss: 1.365, Noise: 0.7212\n",
      "Iter 74/1000 - Loss: 1.367, Noise: 0.7159\n",
      "Iter 75/1000 - Loss: 1.359, Noise: 0.7148\n",
      "Iter 76/1000 - Loss: 1.369, Noise: 0.7152\n",
      "Iter 77/1000 - Loss: 1.366, Noise: 0.7173\n",
      "Iter 78/1000 - Loss: 1.366, Noise: 0.7194\n",
      "Iter 79/1000 - Loss: 1.362, Noise: 0.7224\n",
      "Iter 80/1000 - Loss: 1.362, Noise: 0.7232\n",
      "Iter 81/1000 - Loss: 1.367, Noise: 0.7240\n",
      "Iter 82/1000 - Loss: 1.368, Noise: 0.7264\n",
      "Iter 83/1000 - Loss: 1.368, Noise: 0.7275\n",
      "Iter 84/1000 - Loss: 1.361, Noise: 0.7281\n",
      "Iter 85/1000 - Loss: 1.360, Noise: 0.7254\n",
      "Iter 86/1000 - Loss: 1.365, Noise: 0.7215\n",
      "Iter 87/1000 - Loss: 1.365, Noise: 0.7179\n",
      "Iter 88/1000 - Loss: 1.365, Noise: 0.7159\n",
      "Iter 89/1000 - Loss: 1.368, Noise: 0.7141\n",
      "Iter 90/1000 - Loss: 1.370, Noise: 0.7152\n",
      "Iter 91/1000 - Loss: 1.365, Noise: 0.7177\n",
      "Iter 92/1000 - Loss: 1.371, Noise: 0.7240\n",
      "Iter 93/1000 - Loss: 1.363, Noise: 0.7321\n",
      "Iter 94/1000 - Loss: 1.368, Noise: 0.7351\n",
      "Iter 95/1000 - Loss: 1.364, Noise: 0.7353\n",
      "Reached convergence at 1.3640918731689453, MA 1.3653859794139862 - 1.3656383395195006 < 0.0001\n",
      "2019-03-10 22:31:02.748281 - {'fold': 0, 'repeat': 0, 'n': 194, 'd': 31, 'mse': 0.7351540327072144, 'train_time': 226.66779689694522, 'prior_train_nmll': 1.3674287796020508, 'train_nll': 213.2962646484375, 'test_nll': 25.594863891601562, 'train_mse': 0.6364442706108093}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d</th>\n",
       "      <th>fold</th>\n",
       "      <th>mse</th>\n",
       "      <th>n</th>\n",
       "      <th>prior_train_nmll</th>\n",
       "      <th>repeat</th>\n",
       "      <th>test_nll</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>train_nll</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.735154</td>\n",
       "      <td>194</td>\n",
       "      <td>1.367429</td>\n",
       "      <td>0</td>\n",
       "      <td>25.594864</td>\n",
       "      <td>0.636444</td>\n",
       "      <td>213.296265</td>\n",
       "      <td>226.667797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    d  fold       mse    n  prior_train_nmll  repeat   test_nll  train_mse  \\\n",
       "0  31     0  0.735154  194          1.367429       0  25.594864   0.636444   \n",
       "\n",
       "    train_nll  train_time  \n",
       "0  213.296265  226.667797  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp_experiments.run_experiment(\n",
    "    rp_experiments.train_additive_rp_gp,\n",
    "    options, 'breastcancer', .1, cv=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dict(verbose=True, ard=False, activation=None,\n",
    "                optimizer='adam', n_epochs=1000,\n",
    "                lr=0.1, patience=20, k=1, J=1,\n",
    "                smooth=True, noise_prior=True,\n",
    "                ski=True, grid_ratio=1.0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/1000 - Loss: 1.459, Noise: 0.6931\n",
      "Iter 2/1000 - Loss: 1.445, Noise: 0.7444\n",
      "Iter 3/1000 - Loss: 1.434, Noise: 0.7972\n",
      "Iter 4/1000 - Loss: 1.427, Noise: 0.8507\n",
      "Iter 5/1000 - Loss: 1.423, Noise: 0.9035\n",
      "Iter 6/1000 - Loss: 1.421, Noise: 0.9542\n",
      "Iter 7/1000 - Loss: 1.421, Noise: 1.0013\n",
      "Iter 8/1000 - Loss: 1.421, Noise: 1.0433\n",
      "Iter 9/1000 - Loss: 1.422, Noise: 1.0792\n",
      "Iter 10/1000 - Loss: 1.423, Noise: 1.1082\n",
      "Iter 11/1000 - Loss: 1.424, Noise: 1.1299\n",
      "Iter 12/1000 - Loss: 1.425, Noise: 1.1445\n",
      "Iter 13/1000 - Loss: 1.425, Noise: 1.1521\n",
      "Iter 14/1000 - Loss: 1.425, Noise: 1.1534\n",
      "Iter 15/1000 - Loss: 1.425, Noise: 1.1491\n",
      "Iter 16/1000 - Loss: 1.424, Noise: 1.1401\n",
      "Iter 17/1000 - Loss: 1.424, Noise: 1.1270\n",
      "Iter 18/1000 - Loss: 1.423, Noise: 1.1108\n",
      "Iter 19/1000 - Loss: 1.422, Noise: 1.0923\n",
      "Iter 20/1000 - Loss: 1.421, Noise: 1.0723\n",
      "Iter 21/1000 - Loss: 1.421, Noise: 1.0516\n",
      "Iter 22/1000 - Loss: 1.420, Noise: 1.0310\n",
      "Iter 23/1000 - Loss: 1.420, Noise: 1.0111\n",
      "Iter 24/1000 - Loss: 1.420, Noise: 0.9927\n",
      "Iter 25/1000 - Loss: 1.420, Noise: 0.9763\n",
      "Iter 26/1000 - Loss: 1.420, Noise: 0.9625\n",
      "Iter 27/1000 - Loss: 1.420, Noise: 0.9516\n",
      "Iter 28/1000 - Loss: 1.420, Noise: 0.9438\n",
      "Iter 29/1000 - Loss: 1.420, Noise: 0.9392\n",
      "Iter 30/1000 - Loss: 1.420, Noise: 0.9377\n",
      "Iter 31/1000 - Loss: 1.420, Noise: 0.9392\n",
      "Iter 32/1000 - Loss: 1.420, Noise: 0.9434\n",
      "Iter 33/1000 - Loss: 1.420, Noise: 0.9497\n",
      "Iter 34/1000 - Loss: 1.420, Noise: 0.9578\n",
      "Iter 35/1000 - Loss: 1.420, Noise: 0.9671\n",
      "Iter 36/1000 - Loss: 1.419, Noise: 0.9772\n",
      "Iter 37/1000 - Loss: 1.419, Noise: 0.9874\n",
      "Iter 38/1000 - Loss: 1.419, Noise: 0.9972\n",
      "Iter 39/1000 - Loss: 1.419, Noise: 1.0063\n",
      "Iter 40/1000 - Loss: 1.419, Noise: 1.0142\n",
      "Iter 41/1000 - Loss: 1.419, Noise: 1.0206\n",
      "Iter 42/1000 - Loss: 1.419, Noise: 1.0254\n",
      "Iter 43/1000 - Loss: 1.419, Noise: 1.0284\n",
      "Iter 44/1000 - Loss: 1.419, Noise: 1.0296\n",
      "Iter 45/1000 - Loss: 1.419, Noise: 1.0291\n",
      "Iter 46/1000 - Loss: 1.419, Noise: 1.0272\n",
      "Iter 47/1000 - Loss: 1.419, Noise: 1.0239\n",
      "Iter 48/1000 - Loss: 1.419, Noise: 1.0196\n",
      "Iter 49/1000 - Loss: 1.419, Noise: 1.0146\n",
      "Iter 50/1000 - Loss: 1.419, Noise: 1.0092\n",
      "Iter 51/1000 - Loss: 1.419, Noise: 1.0037\n",
      "Iter 52/1000 - Loss: 1.419, Noise: 0.9984\n",
      "Iter 53/1000 - Loss: 1.419, Noise: 0.9936\n",
      "Iter 54/1000 - Loss: 1.419, Noise: 0.9894\n",
      "Iter 55/1000 - Loss: 1.419, Noise: 0.9862\n",
      "Iter 56/1000 - Loss: 1.419, Noise: 0.9839\n",
      "Iter 57/1000 - Loss: 1.419, Noise: 0.9827\n",
      "Iter 58/1000 - Loss: 1.419, Noise: 0.9825\n",
      "Iter 59/1000 - Loss: 1.419, Noise: 0.9832\n",
      "Iter 60/1000 - Loss: 1.419, Noise: 0.9847\n",
      "Iter 61/1000 - Loss: 1.419, Noise: 0.9869\n",
      "Iter 62/1000 - Loss: 1.419, Noise: 0.9896\n",
      "Iter 63/1000 - Loss: 1.419, Noise: 0.9925\n",
      "Iter 64/1000 - Loss: 1.419, Noise: 0.9955\n",
      "Iter 65/1000 - Loss: 1.419, Noise: 0.9984\n",
      "Iter 66/1000 - Loss: 1.419, Noise: 1.0010\n",
      "Iter 67/1000 - Loss: 1.418, Noise: 1.0032\n",
      "Iter 68/1000 - Loss: 1.418, Noise: 1.0048\n",
      "Iter 69/1000 - Loss: 1.418, Noise: 1.0058\n",
      "Iter 70/1000 - Loss: 1.418, Noise: 1.0063\n",
      "Iter 71/1000 - Loss: 1.418, Noise: 1.0062\n",
      "Iter 72/1000 - Loss: 1.418, Noise: 1.0055\n",
      "Iter 73/1000 - Loss: 1.418, Noise: 1.0044\n",
      "Iter 74/1000 - Loss: 1.418, Noise: 1.0030\n",
      "Iter 75/1000 - Loss: 1.418, Noise: 1.0014\n",
      "Iter 76/1000 - Loss: 1.418, Noise: 0.9997\n",
      "Iter 77/1000 - Loss: 1.418, Noise: 0.9981\n",
      "Iter 78/1000 - Loss: 1.418, Noise: 0.9966\n",
      "Iter 79/1000 - Loss: 1.418, Noise: 0.9953\n",
      "Iter 80/1000 - Loss: 1.418, Noise: 0.9943\n",
      "Iter 81/1000 - Loss: 1.418, Noise: 0.9937\n",
      "Iter 82/1000 - Loss: 1.418, Noise: 0.9934\n",
      "Iter 83/1000 - Loss: 1.418, Noise: 0.9934\n",
      "Iter 84/1000 - Loss: 1.418, Noise: 0.9938\n",
      "Iter 85/1000 - Loss: 1.418, Noise: 0.9944\n",
      "Iter 86/1000 - Loss: 1.418, Noise: 0.9952\n",
      "Iter 87/1000 - Loss: 1.418, Noise: 0.9961\n",
      "Iter 88/1000 - Loss: 1.418, Noise: 0.9971\n",
      "Iter 89/1000 - Loss: 1.418, Noise: 0.9980\n",
      "Iter 90/1000 - Loss: 1.418, Noise: 0.9988\n",
      "Iter 91/1000 - Loss: 1.418, Noise: 0.9995\n",
      "Iter 92/1000 - Loss: 1.418, Noise: 1.0000\n",
      "Iter 93/1000 - Loss: 1.418, Noise: 1.0003\n",
      "Iter 94/1000 - Loss: 1.418, Noise: 1.0004\n",
      "Iter 95/1000 - Loss: 1.418, Noise: 1.0003\n",
      "Iter 96/1000 - Loss: 1.418, Noise: 1.0000\n",
      "Iter 97/1000 - Loss: 1.418, Noise: 0.9996\n",
      "Iter 98/1000 - Loss: 1.418, Noise: 0.9991\n",
      "Iter 99/1000 - Loss: 1.418, Noise: 0.9985\n",
      "Iter 100/1000 - Loss: 1.418, Noise: 0.9980\n",
      "Iter 101/1000 - Loss: 1.418, Noise: 0.9974\n",
      "Iter 102/1000 - Loss: 1.418, Noise: 0.9970\n",
      "Iter 103/1000 - Loss: 1.418, Noise: 0.9967\n",
      "Iter 104/1000 - Loss: 1.418, Noise: 0.9965\n",
      "Iter 105/1000 - Loss: 1.418, Noise: 0.9964\n",
      "Iter 106/1000 - Loss: 1.418, Noise: 0.9965\n",
      "Iter 107/1000 - Loss: 1.418, Noise: 0.9966\n",
      "Iter 108/1000 - Loss: 1.418, Noise: 0.9968\n",
      "Iter 109/1000 - Loss: 1.418, Noise: 0.9971\n",
      "Iter 110/1000 - Loss: 1.418, Noise: 0.9974\n",
      "Iter 111/1000 - Loss: 1.418, Noise: 0.9978\n",
      "Iter 112/1000 - Loss: 1.418, Noise: 0.9981\n",
      "Iter 113/1000 - Loss: 1.418, Noise: 0.9983\n",
      "Reached convergence at 1.4182064533233643, MA 1.4183347404003144 - 1.4182366371154784 < 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f459b6ee0e4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     rp_experiments.run_experiment(\n\u001b[1;32m      3\u001b[0m         \u001b[0mrp_experiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_additive_rp_gp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pumadyn32nm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     )\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/rp_experiments.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(training_routine, training_options, dataset, split, cv, addl_metrics, repeats, error_repeats)\u001b[0m\n\u001b[1;32m    157\u001b[0m                     model_metrics, ypred = training_routine(trainX, trainY, testX,\n\u001b[1;32m    158\u001b[0m                                                             \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                                                             **training_options)\n\u001b[0m\u001b[1;32m    160\u001b[0m                     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/rp_experiments.py\u001b[0m in \u001b[0;36mtrain_additive_rp_gp\u001b[0;34m(trainX, trainY, testX, testY, k, J, ard, activation, optimizer, n_epochs, lr, verbose, patience, smooth, noise_prior, ski, grid_ratio)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mtrain_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mtest_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mmodel_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_nll'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mmodel_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_nll'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mmodel_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_mse'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Get log determininat and first part of quadratic form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0minv_quad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_quad_logdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minv_quad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36minv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mprobe_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobe_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mprobe_vector_norms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobe_vector_norms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         )(*args)\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minv_quad_term\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce_inv_quad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/functions/_inv_quad_log_det.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mt_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdet\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_logdet_forward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0msolves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreconditioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tridiag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_random_probes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, rhs, preconditioner, num_tridiag)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cg_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mmax_tridiag_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_lanczos_quadrature_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mpreconditioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreconditioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/utils/linear_cg.py\u001b[0m in \u001b[0;36mlinear_cg\u001b[0;34m(matmul_closure, rhs, n_tridiag, tolerance, eps, stop_updating_after, max_iter, max_tridiag_iter, initial_guess, preconditioner)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# Get next alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# alpha_{k} = (residual_{k-1}^T precon_residual{k-1}) / (p_vec_{k-1}^T mat p_vec_{k-1})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mmvms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprecond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmul_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         return torch.addcmul(\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diag_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mrhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_quad_form_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_quad_form_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/constant_mul_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanded_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_quad_form_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_quad_form_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/interpolated_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# left_interp * base_lazy_tensor * right_interp^T * rhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mleft_interp_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_interp_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbdsmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_interp_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Squeeze if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/utils/sparse.py\u001b[0m in \u001b[0;36mbdsmm\u001b[0;34m(sparse, dense)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdsmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with gpytorch.settings.fast_pred_samples(True):\n",
    "    rp_experiments.run_experiment(\n",
    "        rp_experiments.train_additive_rp_gp,\n",
    "        options, 'pumadyn32nm', .1, cv=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dict(verbose=True, ard=False, activation=None,\n",
    "                optimizer='adam', n_epochs=1000,\n",
    "                lr=0.1, patience=20, k=1, J=1,\n",
    "                smooth=True, noise_prior=True,\n",
    "                ski=False, grid_ratio=1.0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with gpytorch.settings.fast_pred_samples(True):\n",
    "    rp_experiments.run_experiment(\n",
    "        rp_experiments.train_additive_rp_gp,\n",
    "        options, 'pumadyn32nm', .1, cv=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel, GridInterpolationKernel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rp_experiments\n",
    "import rp\n",
    "import gp_helpers\n",
    "import rp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = rp_experiments.load_dataset('autos')\n",
    "\n",
    "n_per_fold = int(np.ceil(len(auto)*0.1))\n",
    "fold = 4\n",
    "\n",
    "train = auto.iloc[:n_per_fold*fold]  # if fold=0, none before fold\n",
    "test = auto.iloc[n_per_fold*fold:n_per_fold*(fold+1)]\n",
    "train = pd.concat([train,\n",
    "                   auto.iloc[n_per_fold*(fold+1):]])\n",
    "trainX = torch.Tensor(train.values[:, 1:-1])\n",
    "trainY = torch.Tensor(train.values[:, -1])\n",
    "testX = torch.Tensor(test.values[:, 1:-1])\n",
    "testY = torch.Tensor(test.values[:, -1])\n",
    "\n",
    "# X = torch.Tensor(auto.values[:, 1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "ns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/gpytorch/gpytorch/models/exact_gp.py:190: UserWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  \"The input matches the stored training data. Did you forget to call model.train()?\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d4d9a01786cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynthX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/models/exact_gp.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mpredictive_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexact_predictive_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_train_covar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mpredictive_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexact_predictive_covar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_test_covar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_train_covar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_tasks\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/models/exact_prediction_strategies.py\u001b[0m in \u001b[0;36mexact_predictive_covar\u001b[0;34m(self, test_test_covar, test_train_covar)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mtest_train_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_train_covar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mtrain_test_covar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_train_covar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mcovar_correction_rhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_train_covar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_test_covar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtest_test_covar\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMatmulLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_train_covar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovar_correction_rhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36minv_matmul\u001b[0;34m(self, right_tensor, left_tensor)\u001b[0m\n\u001b[1;32m    685\u001b[0m         )\n\u001b[1;32m    686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/functions/_inv_matmul.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_tensor\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0msolves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreconditioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, rhs, preconditioner, num_tridiag)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_cg_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mmax_tridiag_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_lanczos_quadrature_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mpreconditioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreconditioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/utils/linear_cg.py\u001b[0m in \u001b[0;36mlinear_cg\u001b[0;34m(matmul_closure, rhs, n_tridiag, tolerance, eps, stop_updating_after, max_iter, max_tridiag_iter, initial_guess, preconditioner)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# Get next alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# alpha_{k} = (residual_{k-1}^T precon_residual{k-1}) / (p_vec_{k-1}^T mat p_vec_{k-1})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mmvms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprecond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_conjugate_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmul_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         return torch.addcmul(\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diag_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mrhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/non_lazy_tensor.py\u001b[0m in \u001b[0;36m_matmul\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_quad_form_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_vecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n in np.arange(1000, 10000, 1000):\n",
    "    n = int(n)\n",
    "    print(n)\n",
    "    d = 1\n",
    "    k = 1\n",
    "    J = 1\n",
    "    synthX = torch.randn(n, 1)\n",
    "    synthY = torch.sin(synthX).squeeze()\n",
    "    Ws = [rp.gen_rp(d, k) for i in range(J)]\n",
    "    bs = [torch.zeros(1, k) for i in range(J)]\n",
    "#     base_kernels = [GridInterpolationKernel(RBFKernel(), n, num_dims=k) for i in range(J)]\n",
    "#     kernel = gp_helpers.RPKernel(J, k, d, base_kernels, Ws, bs, activation=None)\n",
    "#     kernel = base_kernels[0]\n",
    "    kernel = RBFKernel()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    with gpytorch.settings.fast_pred_var(False):\n",
    "        model = gp_helpers.ExactGPModel(synthX, synthY, likelihood, kernel)\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    model.eval()\n",
    "    p = model(synthX)\n",
    "    t2 = time.perf_counter()\n",
    "    times.append(t2-t1)\n",
    "    ns.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6687fffa20>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ns, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2759913080>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ns, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f27599d9320>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ns, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski_ns = ns\n",
    "ski_times = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f51af5e7048>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ns, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6701,  1.3693, -0.1794,  0.0087,  1.3041,  1.5798, -0.7688,  1.3515,\n",
       "        -0.2587,  0.3251, -0.5411, -1.1535, -0.0922,  0.2849,  1.5458,  0.7118,\n",
       "        -0.6772, -0.6183, -0.7674,  0.1365, -1.3754,  1.6055,  1.3082, -0.5172,\n",
       "        -0.3615,  0.3904,  0.8537, -0.2502, -0.9713,  0.1645,  2.7172, -0.4749,\n",
       "         0.9010,  0.1152, -1.0904,  0.0699, -0.2126,  1.8005,  0.9124, -0.4749,\n",
       "        -0.8363,  0.0942, -0.8034, -1.0197,  0.6391, -0.2099, -0.8135, -0.5178,\n",
       "         0.2022, -0.5388,  1.2630,  0.7822,  1.0631, -1.0426, -1.0070, -0.7864,\n",
       "         0.5804, -0.7024,  0.1316, -0.9195,  0.8401,  1.3991, -0.9229,  0.4088,\n",
       "        -0.8253, -0.2812,  1.5433, -1.0515, -0.6883, -0.8532,  1.2665, -0.2370,\n",
       "        -0.7681, -0.5375, -0.7393, -0.7840,  1.3517, -0.7232,  1.2349, -0.7053,\n",
       "        -0.7588, -0.5375,  0.1457,  0.2167, -1.1534, -0.2165,  0.3020,  1.3911,\n",
       "         0.2302,  0.3567, -0.7529, -0.7793, -1.2651, -0.4846, -0.9547, -0.7151,\n",
       "        -0.6399,  1.3853, -0.5458,  0.1642,  1.1504,  1.3734, -0.2993,  1.0934,\n",
       "         0.7531, -0.7902, -0.8251,  0.9076, -0.7681,  0.4472,  0.5892,  1.0348,\n",
       "        -0.6751,  0.0676,  1.0578, -0.8554,  1.9753,  2.5460,  1.4650, -1.3706,\n",
       "        -1.0423, -0.7547, -1.2447, -0.8006, -0.9774, -0.0267,  0.1570,  0.1412,\n",
       "         0.7996,  0.5541, -0.6537, -0.4145, -0.8500,  0.4792,  0.0699, -0.6758,\n",
       "        -0.5575,  1.9450, -0.7326,  0.6391,  0.9778, -0.3015, -0.6611],\n",
       "       grad_fn=<Matmul>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.matmul((K+torch.eye(n)*1/100).inv_matmul(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "d = 1\n",
    "k = 1\n",
    "J = 1\n",
    "Ws = [rp.gen_rp(d, k) for i in range(J)]\n",
    "bs = [torch.zeros(1, k) for i in range(J)]\n",
    "base_kernels = [GridInterpolationKernel(RBFKernel(), n, num_dims=k) for i in range(J)]\n",
    "kernel = gp_helpers.RPKernel(J, k, d, base_kernels, Ws, bs, activation=None)\n",
    "synthX = torch.randn(100, 1)\n",
    "synthY = torch.sin(synthX).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 0.865, Noise: 0.6931\n",
      "Iter 2/100 - Loss: 0.832, Noise: 0.6444\n",
      "Iter 3/100 - Loss: 0.798, Noise: 0.5981\n",
      "Iter 4/100 - Loss: 0.764, Noise: 0.5542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3081147e69e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mgp_mll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mfit_gp_model\u001b[0;34m(gp_model, gp_likelihood, xs, ys, optimizer, lr, gp_mll, n_epochs, verbose, patience, conv_tol, check_conv, smooth)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/GPyTorchEnv2/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/Scalable_GPs/gp_helpers.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgp_mll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 print(\n",
      "\u001b[0;32m~/gpytorch/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_validate_module_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, output, target, *params)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Get the log prob of the marginal distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Add terms for SGPR / when inducing points are learned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Get log determininat and first part of quadratic form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0minv_quad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_quad_logdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_quad_rhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minv_quad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36minv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mprobe_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobe_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mprobe_vector_norms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobe_vector_norms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         )(*args)\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minv_quad_term\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce_inv_quad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/functions/_inv_quad_log_det.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mlazy_tsr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmatrix_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mpreconditioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet_correction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preconditioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Collect terms for LinearCG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py\u001b[0m in \u001b[0;36m_preconditioner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_woodbury_cache\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_preconditioner_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_piv_chol_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpivoted_cholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivoted_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_piv_chol_self\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 warnings.warn(\n",
      "\u001b[0;32m~/gpytorch/gpytorch/utils/pivoted_cholesky.py\u001b[0m in \u001b[0;36mpivoted_cholesky\u001b[0;34m(matrix, max_iter, error_tol)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0merror_tol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mpermuted_diags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_diag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmax_diag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_diag_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermuted_diags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmax_diag_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_diag_indices\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = gp_helpers.ExactGPModel(synthX, synthY, likelihood, kernel)\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "gp_helpers.fit_gp_model(\n",
    "    model,\n",
    "    likelihood,\n",
    "    synthX, synthY,\n",
    "    torch.optim.Adam,\n",
    "    lr=0.1, \n",
    "    n_epochs=100,\n",
    "    gp_mll=mll,\n",
    "    verbose=True,\n",
    "    patience=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/gpytorch/gpytorch/models/exact_gp.py:190: UserWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  \"The input matches the stored training data. Did you forget to call model.train()?\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(synthX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "train_x = torch.linspace(0, 1, 1000)\n",
    "train_y = torch.sin(train_x * (4 * math.pi) + torch.randn(train_x.size()) * 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
